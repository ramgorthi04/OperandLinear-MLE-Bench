{
  "cells": [
    {
      "id": "818876bc-0742-42d1-9c9c-6483703efe9b",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: LAL Automated Essay Scoring 2.0\n",
        "\n",
        "Objectives:\n",
        "- Establish strong, reliable CV with QWK and lock splits.\n",
        "- Build fast baseline (TF-IDF + linear model) to get quick OOF and LB.\n",
        "- Iterate with feature engineering and modern text models; aim for medal.\n",
        "\n",
        "Validation:\n",
        "- Use StratifiedKFold on binned score distribution (stratify target).\n",
        "- 5 folds, multiple seeds (cache folds).\n",
        "- Optimize rounding (or isotonic/ordinal mapping) to maximize QWK on OOF.\n",
        "- Fit transforms inside folds only; cache vectorizers to disk.\n",
        "\n",
        "Baseline v1 (fast):\n",
        "- Text only: char/word TF-IDF + Ridge/LinearSVR.\n",
        "- Add NB-SVM style log-count ratio features.\n",
        "- Predict float scores; apply optimized rounding to integer labels.\n",
        "- Evaluate OOF QWK; produce submission.\n",
        "\n",
        "Feature Engineering v2:\n",
        "- NLP stats: length, unique ratio, punctuation, sentence count, syllables, readability (FKGL), spelling error counts.\n",
        "- Lexical richness: TTR, MTLD (approx), POS tag counts.\n",
        "- Misspell correction? Keep raw; only count features to avoid leakage.\n",
        "- Combine TF-IDF with numeric features via stacking or concatenation.\n",
        "\n",
        "Modeling v2:\n",
        "- CatBoost (GPU) on dense features + TF-IDF SVD projections.\n",
        "- XGBoost (GPU) with monotone constraints not needed; tune depth/eta early stop.\n",
        "\n",
        "Transformer track (parallel, GPU):\n",
        "- Start with DeBERTa-v3-base/large or RoBERTa-large (cu121 stack).\n",
        "- Truncate to max tokens (e.g., 1024 via Longformer/DeBERTa-v3-long if feasible).\n",
        "- Regression head; train with MSE + QWK-aware post-processing.\n",
        "- Use gradient accumulation, mixed precision, early stopping.\n",
        "- Cache OOF/test preds; blend with classical models.\n",
        "\n",
        "Blending:\n",
        "- Weighted average using OOF QWK for weights; optionally logistic regression meta on OOF.\n",
        "- Calibrate via optimized rounding per prompt if prompt available (check cols).\n",
        "\n",
        "Risk checks:\n",
        "- No leakage from test during vectorizer fitting.\n",
        "- Deterministic seeds; save folds to folds.csv.\n",
        "- Log per-fold timings and scores.\n",
        "\n",
        "Milestones (request expert review at each):\n",
        "1) Plan + environment check\n",
        "2) Data load + EDA + CV design\n",
        "3) Baseline TF-IDF model + OOF\n",
        "4) FE v2 + GBDT model\n",
        "5) Transformer baseline + OOF\n",
        "6) Blend + finalize submission\n",
        "\n",
        "Questions for experts:\n",
        "- Best CV protocol for AES2 (any prompt-based stratification needed)?\n",
        "- Top text features beyond TF-IDF shown to help in AES2?\n",
        "- Recommended long-context model choice and tokenization strategy under 24h?\n",
        "- Common pitfalls that tank LB vs CV in this comp?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "54124490-2e87-4a8f-9716-cc5fc63d2d7a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment + quick EDA\n",
        "import os, sys, subprocess, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def run(cmd):\n",
        "    return subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True).stdout\n",
        "\n",
        "print('=== NVIDIA-SMI ===', flush=True)\n",
        "print(run(['bash','-lc','nvidia-smi || true']))\n",
        "\n",
        "t0=time.time()\n",
        "train_path='train.csv'; test_path='test.csv'\n",
        "print('Loading data...', flush=True)\n",
        "train=pd.read_csv(train_path)\n",
        "test=pd.read_csv(test_path)\n",
        "print(f'train shape: {train.shape}, test shape: {test.shape}', flush=True)\n",
        "print('train columns:', list(train.columns))\n",
        "print('test columns:', list(test.columns))\n",
        "\n",
        "# Identify id, text, target, prompt columns heuristically\n",
        "id_col = 'essay_id' if 'essay_id' in train.columns else (train.columns[0])\n",
        "text_col_candidates = [c for c in train.columns if 'text' in c.lower() or 'essay' in c.lower() or 'content' in c.lower()]\n",
        "text_col = text_col_candidates[0] if text_col_candidates else None\n",
        "target_col = 'score' if 'score' in train.columns else None\n",
        "prompt_col = None\n",
        "for c in train.columns:\n",
        "    if 'prompt' in c.lower() or 'topic' in c.lower():\n",
        "        prompt_col = c; break\n",
        "\n",
        "# Fix: enforce correct text column if available\n",
        "if 'full_text' in train.columns:\n",
        "    text_col = 'full_text'\n",
        "\n",
        "print(f'Heuristic cols -> id: {id_col}, text: {text_col}, target: {target_col}, prompt: {prompt_col}')\n",
        "\n",
        "if target_col is not None:\n",
        "    y = train[target_col].dropna()\n",
        "    print('Target stats:', y.describe())\n",
        "    print('Unique scores:', np.sort(y.unique())[:20], '... total', y.nunique())\n",
        "\n",
        "if text_col is not None:\n",
        "    lens = train[text_col].astype(str).str.len()\n",
        "    print('Text length (chars) percentiles:', np.percentile(lens.to_numpy(), [1,5,25,50,75,90,95,99]))\n",
        "\n",
        "if prompt_col is not None:\n",
        "    print('Unique prompts in train:', train[prompt_col].nunique())\n",
        "    if prompt_col in test.columns:\n",
        "        print('Unique prompts in test:', test[prompt_col].nunique())\n",
        "        seen = set(train[prompt_col].unique()); unseen = [p for p in test[prompt_col].unique() if p not in seen]\n",
        "        print('Unseen prompts in test:', unseen[:10], 'count:', len(unseen))\n",
        "\n",
        "print('Head (selected):')\n",
        "cols_show = [c for c in [id_col, prompt_col, target_col, text_col] if c is not None]\n",
        "print(train[cols_show].head(3).to_dict(orient='records'))\n",
        "print(f'Done in {time.time()-t0:.2f}s', flush=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== NVIDIA-SMI ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep 28 22:33:13 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     182MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\nLoading data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape: (15576, 3), test shape: (1731, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train columns: ['essay_id', 'full_text', 'score']\ntest columns: ['essay_id', 'full_text']\nHeuristic cols -> id: essay_id, text: essay_id, target: score, prompt: None\nTarget stats: count    15576.000000\nmean         2.950116\nstd          1.044384\nmin          1.000000\n25%          2.000000\n50%          3.000000\n75%          4.000000\nmax          6.000000\nName: score, dtype: float64\nUnique scores: [1 2 3 4 5 6] ... total 6\nText length (chars) percentiles: [7. 7. 7. 7. 7. 7. 7. 7.]\nHead (selected):\n[{'essay_id': '663d2cf', 'score': 3}, {'essay_id': '3a20bfb', 'score': 2}, {'essay_id': '6adae64', 'score': 3}]\nDone in 0.25s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_183/456210942.py:51: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n  print(train[cols_show].head(3).to_dict(orient='records'))\n"
          ]
        }
      ]
    },
    {
      "id": "fd6e4ab7-e502-41fb-9425-b3215b65d2f9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build and save folds (5-fold StratifiedKFold on score x length bins)\n",
        "import pandas as pd, numpy as np, time\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "t0=time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "id_col, text_col, target_col = 'essay_id', 'full_text', 'score'\n",
        "\n",
        "# Basic sanity\n",
        "assert {id_col, text_col, target_col}.issubset(train.columns), f\"Missing required columns in train: {train.columns}\"\n",
        "train = train.copy()\n",
        "\n",
        "# Create stratification label: combine score and length bin\n",
        "y = train[target_col].astype(int).values\n",
        "lens = train[text_col].astype(str).str.len().values\n",
        "len_series = pd.Series(lens)\n",
        "nq = int(np.clip(len_series.nunique(), 4, 10))\n",
        "len_bins = pd.qcut(len_series, q=nq, duplicates='drop', labels=False)\n",
        "len_bins = len_bins.astype('float64').fillna(len_bins.median()).astype(int).values\n",
        "strat = y * 100 + len_bins  # joint bins\n",
        "\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "folds = np.full(len(train), -1, dtype=int)\n",
        "for fold, (_, val_idx) in enumerate(skf.split(train, strat)):\n",
        "    folds[val_idx] = fold\n",
        "\n",
        "assert (folds>=0).all(), 'Unassigned folds found'\n",
        "df_folds = train[[id_col, target_col]].copy()\n",
        "df_folds['fold'] = folds\n",
        "df_folds.to_csv('folds.csv', index=False)\n",
        "\n",
        "# Print per-fold stats\n",
        "print('Folds saved to folds.csv')\n",
        "for f in range(n_splits):\n",
        "    idx = folds==f\n",
        "    print(f'Fold {f}: n={idx.sum()}, score dist=', dict(pd.Series(y[idx]).value_counts().sort_index()))\n",
        "\n",
        "print(f'Done in {time.time()-t0:.2f}s', flush=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folds saved to folds.csv\nFold 0: n=3116, score dist= {1: 225, 2: 852, 3: 1125, 4: 713, 5: 174, 6: 27}\nFold 1: n=3115, score dist= {1: 223, 2: 851, 3: 1126, 4: 713, 5: 175, 6: 27}\nFold 2: n=3115, score dist= {1: 226, 2: 851, 3: 1124, 4: 712, 5: 175, 6: 27}\nFold 3: n=3115, score dist= {1: 225, 2: 847, 3: 1127, 4: 713, 5: 176, 6: 27}\nFold 4: n=3115, score dist= {1: 225, 2: 848, 3: 1127, 4: 712, 5: 176, 6: 27}\nDone in 0.22s\n"
          ]
        }
      ]
    },
    {
      "id": "545b2543-8e02-43b1-b987-bf82a8314a79",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Baseline v1: TF-IDF (word+char_wb) + Ridge with 5-fold CV, OOF QWK, global thresholds, and submission\n",
        "import time, numpy as np, pandas as pd, sys\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "def qwk(y_true_int, y_pred_int):\n",
        "    return cohen_kappa_score(y_true_int, y_pred_int, weights='quadratic')\n",
        "\n",
        "def apply_thresholds(pred, th):\n",
        "    # thresholds between classes 1..6; th length 5\n",
        "    bins = [-np.inf] + list(th) + [np.inf]\n",
        "    return np.digitize(pred, bins)  # returns 1..6\n",
        "\n",
        "def optimize_thresholds(y_true, preds, iters=3, step=0.05):\n",
        "    th = np.array([1.5, 2.5, 3.5, 4.5, 5.5], dtype=float)\n",
        "    best = qwk(y_true, apply_thresholds(preds, th))\n",
        "    for _ in range(iters):\n",
        "        for i in range(len(th)):\n",
        "            lo = th[i] - 0.5\n",
        "            hi = th[i] + 0.5\n",
        "            # ensure monotonicity with neighbors\n",
        "            if i>0: lo = max(lo, th[i-1] + 0.01)\n",
        "            if i<len(th)-1: hi = min(hi, th[i+1] - 0.01)\n",
        "            grid = np.arange(lo, hi + 1e-9, step)\n",
        "            local_best = best; local_val = th[i]\n",
        "            for g in grid:\n",
        "                th_try = th.copy(); th_try[i] = g\n",
        "                score = qwk(y_true, apply_thresholds(preds, th_try))\n",
        "                if score > local_best:\n",
        "                    local_best = score; local_val = g\n",
        "            th[i] = local_val; best = local_best\n",
        "    return th, best\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds.csv')\n",
        "\n",
        "id_col, text_col, target_col = 'essay_id', 'full_text', 'score'  \n",
        "assert {id_col, text_col, target_col}.issubset(train.columns)\n",
        "\n",
        "y = train[target_col].astype(int).values\n",
        "X_text = train[text_col].astype(str).values\n",
        "X_test_text = test[text_col].astype(str).values\n",
        "\n",
        "n_splits = int(folds['fold'].max()) + 1\n",
        "oof = np.zeros(len(train), dtype=np.float32)\n",
        "test_pred_folds = np.zeros((len(test), n_splits), dtype=np.float32)\n",
        "\n",
        "# Vectorizer configs\n",
        "word_vec_kwargs = dict(lowercase=True, analyzer='word', ngram_range=(1,2), min_df=2, max_features=150_000, sublinear_tf=True, dtype=np.float32)\n",
        "char_vec_kwargs = dict(lowercase=True, analyzer='char_wb', ngram_range=(3,5), min_df=2, max_features=200_000, sublinear_tf=True, dtype=np.float32)\n",
        "\n",
        "for f in range(n_splits):\n",
        "    f_t0 = time.time()\n",
        "    tr_idx = folds.index[folds['fold']!=f].to_numpy()\n",
        "    va_idx = folds.index[folds['fold']==f].to_numpy()\n",
        "    print(f'Fold {f} start: tr={len(tr_idx)} va={len(va_idx)}', flush=True)\n",
        "\n",
        "    Xtr = X_text[tr_idx]; Xva = X_text[va_idx]\n",
        "    ytr = y[tr_idx]\n",
        "\n",
        "    # Fit vectorizers on training fold only\n",
        "    wv = TfidfVectorizer(**word_vec_kwargs)\n",
        "    cv = TfidfVectorizer(**char_vec_kwargs)\n",
        "    Xtr_w = wv.fit_transform(Xtr)\n",
        "    Xtr_c = cv.fit_transform(Xtr)\n",
        "    Xtr_all = hstack([Xtr_w, Xtr_c], format='csr')\n",
        "    del Xtr_w, Xtr_c\n",
        "\n",
        "    Xva_w = wv.transform(Xva)\n",
        "    Xva_c = cv.transform(Xva)\n",
        "    Xva_all = hstack([Xva_w, Xva_c], format='csr')\n",
        "    del Xva_w, Xva_c\n",
        "\n",
        "    Xte_w = wv.transform(X_test_text)\n",
        "    Xte_c = cv.transform(X_test_text)\n",
        "    Xte_all = hstack([Xte_w, Xte_c], format='csr')\n",
        "    del Xte_w, Xte_c\n",
        "\n",
        "    # Model\n",
        "    model = Ridge(alpha=4.0, random_state=SEED)\n",
        "    model.fit(Xtr_all, ytr)\n",
        "    oof_pred = model.predict(Xva_all).astype(np.float32)\n",
        "    test_pred = model.predict(Xte_all).astype(np.float32)\n",
        "    oof[va_idx] = oof_pred\n",
        "    test_pred_folds[:, f] = test_pred\n",
        "\n",
        "    # Cleanup to free memory\n",
        "    del Xtr_all, Xva_all, Xte_all, model\n",
        "    print(f'Fold {f} done in {time.time()-f_t0:.1f}s', flush=True)\n",
        "\n",
        "# Evaluate OOF and optimize thresholds\n",
        "base_th = np.array([1.5,2.5,3.5,4.5,5.5])\n",
        "oof_int_base = apply_thresholds(oof, base_th)\n",
        "oof_qwk_base = qwk(y, oof_int_base)\n",
        "opt_th, oof_qwk_opt = optimize_thresholds(y, oof, iters=3, step=0.05)\n",
        "print(f'OOF QWK base={oof_qwk_base:.5f} opt={oof_qwk_opt:.5f} thresholds={opt_th}')\n",
        "\n",
        "# Finalize test predictions\n",
        "test_pred = test_pred_folds.mean(axis=1)\n",
        "test_pred_int = apply_thresholds(test_pred, opt_th)\n",
        "test_pred_int = np.clip(test_pred_int, 1, 6).astype(int)\n",
        "\n",
        "# Save artifacts\n",
        "pd.DataFrame({'essay_id': train[id_col], 'oof_pred': oof, 'oof_int': apply_thresholds(oof, opt_th), 'y': y}).to_csv('oof_baseline.csv', index=False)\n",
        "pd.DataFrame({'essay_id': test[id_col], 'score': test_pred_int}).to_csv('submission_ridge.csv', index=False)\n",
        "np.save('test_ridge.npy', test_pred.astype(np.float32))\n",
        "print('Saved oof_baseline.csv, submission_ridge.csv, and test_ridge.npy')\n",
        "print(f'Total time: {time.time()-t0:.1f}s', flush=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 start: tr=12460 va=3116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 done in 26.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 start: tr=12461 va=3115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 done in 27.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 start: tr=12461 va=3115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 done in 27.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 start: tr=12461 va=3115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 done in 27.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 start: tr=12461 va=3115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 done in 27.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF QWK base=0.74059 opt=0.78642 thresholds=[1.9  2.66 3.35 4.1  4.7 ]\nSaved oof_baseline.csv, submission_ridge.csv, and test_ridge.npy\nTotal time: 137.3s\n"
          ]
        }
      ]
    },
    {
      "id": "a5edc346-3951-4435-ac54-c6ee284fdcc9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Classical v2: Numeric FE + TF-IDF SVD(384) + CatBoost (GPU) per-fold; cache OOF/test preds\n",
        "import time, os, sys, numpy as np, pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from scipy.sparse import hstack\n",
        "import re\n",
        "import subprocess\n",
        "\n",
        "def ensure_pkg(pkg):\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "    except ImportError:\n",
        "        print(f'Installing {pkg}...', flush=True)\n",
        "        subprocess.run([sys.executable, '-m', 'pip', 'install', pkg], check=True)\n",
        "\n",
        "ensure_pkg('catboost')\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "\n",
        "SEED=42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "def qwk_int(y_true, y_pred_int):\n",
        "    return cohen_kappa_score(y_true, y_pred_int, weights='quadratic')\n",
        "\n",
        "def text_stats(s: str):\n",
        "    s = '' if pd.isna(s) else str(s)\n",
        "    n_chars = len(s)\n",
        "    n_newlines = s.count('\\n')\n",
        "    # simple sentence split on .!?\n",
        "    sents = re.split(r'[.!?]+', s)\n",
        "    sents = [t for t in sents if t.strip()]\n",
        "    n_sents = max(1, len(sents))\n",
        "    words = re.findall(r\"\\b\\w+\\b\", s)\n",
        "    n_words = len(words)\n",
        "    avg_word_len = (sum(len(w) for w in words) / n_words) if n_words>0 else 0.0\n",
        "    sent_lens = [len(re.findall(r\"\\b\\w+\\b\", t)) for t in sents]\n",
        "    avg_sent_len_w = (sum(sent_lens) / n_sents) if n_sents>0 else 0.0\n",
        "    std_sent_len_w = (np.std(sent_lens) if n_sents>1 else 0.0)\n",
        "    uniq = len(set(w.lower() for w in words)) if n_words>0 else 0\n",
        "    ttr = (uniq / n_words) if n_words>0 else 0.0\n",
        "    hapax = sum(1 for w in set(words) if words.count(w)==1)\n",
        "    hapax_ratio = (hapax / n_words) if n_words>0 else 0.0\n",
        "    long_words = sum(1 for w in words if len(w)>=7)\n",
        "    pct_long = (100.0 * long_words / n_words) if n_words>0 else 0.0\n",
        "    punct = re.findall(r\"[\\p{Punct}]\", s) if hasattr(re, 'P') else re.findall(r\"[\\.,;:!\\?\\-\\(\\)\\'\\\"\\[\\]]\", s)\n",
        "    punct_cnt = len(punct)\n",
        "    punct_pct = (100.0 * punct_cnt / max(1, n_chars))\n",
        "    commas = s.count(','); periods = s.count('.')\n",
        "    commas_per_100w = (100.0 * commas / max(1, n_words))\n",
        "    periods_per_100w = (100.0 * periods / max(1, n_words))\n",
        "    uppercase_pct = (100.0 * sum(1 for ch in s if ch.isupper()) / max(1, n_chars))\n",
        "    digits_per_100w = (100.0 * sum(1 for ch in s if ch.isdigit()) / max(1, n_words))\n",
        "    # FKGL approximation\n",
        "    syllables = 0\n",
        "    for w in words:\n",
        "        syl = max(1, len(re.findall(r'[aeiouyAEIOUY]+', w)))\n",
        "        syllables += syl\n",
        "    fkgl = 0.39 * (n_words / max(1, n_sents)) + 11.8 * (syllables / max(1, n_words)) - 15.59 if n_words>0 else 0.0\n",
        "    return [n_chars, n_words, n_sents, n_newlines, avg_word_len, avg_sent_len_w, std_sent_len_w,\n",
        "            ttr, hapax_ratio, pct_long, punct_pct, commas_per_100w, periods_per_100w,\n",
        "            uppercase_pct, digits_per_100w, fkgl]\n",
        "\n",
        "num_cols = [\n",
        "    'n_chars','n_words','n_sents','n_newlines','avg_word_len','avg_sent_len_w','std_sent_len_w',\n",
        "    'ttr','hapax_ratio','pct_long','punct_pct','commas_per_100w','periods_per_100w',\n",
        "    'uppercase_pct','digits_per_100w','fkgl'\n",
        "]\n",
        "\n",
        "def build_numeric(df, text_col):\n",
        "    feats = np.vstack([text_stats(t) for t in df[text_col].astype(str).values])\n",
        "    return pd.DataFrame(feats, columns=num_cols, index=df.index)\n",
        "\n",
        "t0=time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds.csv')\n",
        "id_col, text_col, target_col = 'essay_id','full_text','score'\n",
        "y = train[target_col].astype(int).values\n",
        "\n",
        "print('Computing numeric features...', flush=True)\n",
        "num_tr = build_numeric(train, text_col)\n",
        "num_te = build_numeric(test, text_col)\n",
        "\n",
        "# TF-IDF for SVD\n",
        "word_vec_kwargs = dict(lowercase=True, analyzer='word', ngram_range=(1,2), min_df=2, max_features=150_000, sublinear_tf=True, dtype=np.float32)\n",
        "char_vec_kwargs = dict(lowercase=True, analyzer='char_wb', ngram_range=(3,5), min_df=2, max_features=200_000, sublinear_tf=True, dtype=np.float32)\n",
        "\n",
        "n_splits = int(folds['fold'].max()) + 1\n",
        "oof = np.zeros(len(train), dtype=np.float32)\n",
        "test_pred_f = np.zeros((len(test), n_splits), dtype=np.float32)\n",
        "\n",
        "for f in range(n_splits):\n",
        "    f_t=time.time()\n",
        "    tr_idx = folds.index[folds['fold']!=f].to_numpy()\n",
        "    va_idx = folds.index[folds['fold']==f].to_numpy()\n",
        "    print(f'[CatBoost] Fold {f} start: tr={len(tr_idx)} va={len(va_idx)}', flush=True)\n",
        "\n",
        "    # Text vectorizers fit on train fold\n",
        "    wv = TfidfVectorizer(**word_vec_kwargs)\n",
        "    cv = TfidfVectorizer(**char_vec_kwargs)\n",
        "    Xtr_w = wv.fit_transform(train.loc[tr_idx, text_col].astype(str).values)\n",
        "    Xtr_c = cv.fit_transform(train.loc[tr_idx, text_col].astype(str).values)\n",
        "    Xtr_tfidf = hstack([Xtr_w, Xtr_c], format='csr')\n",
        "    Xva_tfidf = hstack([wv.transform(train.loc[va_idx, text_col].astype(str).values),\n",
        "                        cv.transform(train.loc[va_idx, text_col].astype(str).values)], format='csr')\n",
        "    Xte_tfidf = hstack([wv.transform(test[text_col].astype(str).values),\n",
        "                        cv.transform(test[text_col].astype(str).values)], format='csr')\n",
        "    del Xtr_w, Xtr_c\n",
        "\n",
        "    # SVD fit on train fold only\n",
        "    svd = TruncatedSVD(n_components=384, random_state=SEED)\n",
        "    Xtr_svd = svd.fit_transform(Xtr_tfidf)\n",
        "    Xva_svd = svd.transform(Xva_tfidf)\n",
        "    Xte_svd = svd.transform(Xte_tfidf)\n",
        "\n",
        "    # Scale SVD and numeric\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    Xtr_dense = np.hstack([scaler.fit_transform(Xtr_svd), scaler.fit_transform(num_tr.loc[tr_idx, :].values)])\n",
        "    # Note: use the same scaler separately for numeric to avoid leakage? Simpler: fit one scaler on concatenated feats.\n",
        "    # Recompute scaler on concatenated to ensure consistency\n",
        "    scaler2 = StandardScaler(with_mean=True, with_std=True)\n",
        "    Xtr_concat = np.hstack([Xtr_svd, num_tr.loc[tr_idx, :].values])\n",
        "    Xtr_dense = scaler2.fit_transform(Xtr_concat)\n",
        "    Xva_dense = scaler2.transform(np.hstack([Xva_svd, num_tr.loc[va_idx, :].values]))\n",
        "    Xte_dense = scaler2.transform(np.hstack([Xte_svd, num_te.values]))\n",
        "\n",
        "    # CatBoost (GPU) with early stopping\n",
        "    params = dict(loss_function='RMSE', depth=6, learning_rate=0.05, l2_leaf_reg=4.0,\n",
        "                  random_seed=SEED, task_type='GPU', devices='0',\n",
        "                  iterations=2000, od_type='Iter', od_wait=100, verbose=False)\n",
        "    model = CatBoostRegressor(**params)\n",
        "    model.fit(Xtr_dense, y[tr_idx], eval_set=(Xva_dense, y[va_idx]))\n",
        "    oof[va_idx] = model.predict(Xva_dense).astype(np.float32)\n",
        "    test_pred_f[:, f] = model.predict(Xte_dense).astype(np.float32)\n",
        "\n",
        "    # cleanup\n",
        "    del Xtr_tfidf, Xva_tfidf, Xte_tfidf, Xtr_svd, Xva_svd, Xte_svd, Xtr_dense, Xva_dense, Xte_dense, model, svd, scaler2\n",
        "    print(f'[CatBoost] Fold {f} done in {time.time()-f_t:.1f}s', flush=True)\n",
        "\n",
        "# Save OOF and test preds\n",
        "pd.DataFrame({'essay_id': train[id_col], 'oof_cat': oof, 'y': y}).to_csv('oof_cat.csv', index=False)\n",
        "np.save('test_cat.npy', test_pred_f.mean(axis=1))\n",
        "print('Saved oof_cat.csv and test_cat.npy; re-opt thresholds and blend later.', flush=True)\n",
        "print(f'Total time: {time.time()-t0:.1f}s', flush=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing numeric features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost] Fold 0 start: tr=12460 va=3116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost] Fold 0 done in 114.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost] Fold 1 start: tr=12461 va=3115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost] Fold 1 done in 104.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost] Fold 2 start: tr=12461 va=3115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost] Fold 2 done in 114.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost] Fold 3 start: tr=12461 va=3115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost] Fold 3 done in 113.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost] Fold 4 start: tr=12461 va=3115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost] Fold 4 done in 116.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_cat.csv and test_cat.npy; re-opt thresholds and blend later.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time: 581.5s\n"
          ]
        }
      ]
    },
    {
      "id": "ba436b86-5178-4cb4-b771-0588e9032c59",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Postprocessing: Optimize thresholds on CatBoost OOF and create classical submission\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def qwk(y_true_int, y_pred_int):\n",
        "    return cohen_kappa_score(y_true_int, y_pred_int, weights='quadratic')\n",
        "\n",
        "def apply_thresholds(pred, th):\n",
        "    bins = [-np.inf] + list(th) + [np.inf]\n",
        "    return np.digitize(pred, bins)\n",
        "\n",
        "def optimize_thresholds(y_true, preds, iters=3, step=0.05):\n",
        "    th = np.array([1.5, 2.5, 3.5, 4.5, 5.5], dtype=float)\n",
        "    best = qwk(y_true, apply_thresholds(preds, th))\n",
        "    for _ in range(iters):\n",
        "        for i in range(len(th)):\n",
        "            lo = th[i] - 0.5; hi = th[i] + 0.5\n",
        "            if i>0: lo = max(lo, th[i-1] + 0.01)\n",
        "            if i<len(th)-1: hi = min(hi, th[i+1] - 0.01)\n",
        "            grid = np.arange(lo, hi + 1e-9, step)\n",
        "            local_best, local_val = best, th[i]\n",
        "            for g in grid:\n",
        "                th_try = th.copy(); th_try[i] = g\n",
        "                score = qwk(y_true, apply_thresholds(preds, th_try))\n",
        "                if score > local_best:\n",
        "                    local_best, local_val = score, g\n",
        "            th[i] = local_val; best = local_best\n",
        "    return th, best\n",
        "\n",
        "t0=time.time()\n",
        "assert os.path.exists('oof_cat.csv') and os.path.exists('test_cat.npy'), 'CatBoost OOF/test not found yet.'\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "oof_cat = pd.read_csv('oof_cat.csv')\n",
        "test_cat = np.load('test_cat.npy')\n",
        "\n",
        "y = oof_cat['y'].astype(int).values\n",
        "pred_oof = oof_cat['oof_cat'].values.astype(np.float32)\n",
        "base_th = np.array([1.5,2.5,3.5,4.5,5.5])\n",
        "oof_qwk_base = qwk(y, apply_thresholds(pred_oof, base_th))\n",
        "opt_th, oof_qwk_opt = optimize_thresholds(y, pred_oof, iters=3, step=0.05)\n",
        "print(f'[CatBoost] OOF QWK base={oof_qwk_base:.5f} opt={oof_qwk_opt:.5f} thresholds={opt_th}', flush=True)\n",
        "\n",
        "test_int = apply_thresholds(test_cat, opt_th)\n",
        "test_int = np.clip(test_int, 1, 6).astype(int)\n",
        "sub_cat = pd.DataFrame({'essay_id': test['essay_id'], 'score': test_int})\n",
        "sub_cat.to_csv('submission_classical.csv', index=False)\n",
        "print('Saved submission_classical.csv', flush=True)\n",
        "print(f'Done in {time.time()-t0:.1f}s', flush=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost] OOF QWK base=0.79489 opt=0.81219 thresholds=[1.75 2.6  3.45 4.3  4.95]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_classical.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done in 1.9s\n"
          ]
        }
      ]
    },
    {
      "id": "fbccfa0f-241a-4d8f-90ee-c3e24582077c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blend classical (CatBoost) and DeBERTa (prefer SW64 if available); optimize thresholds; optional isotonic calibration; write final submission\n",
        "import os, numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "def qwk(y_true_int, y_pred_int):\n",
        "    return cohen_kappa_score(y_true_int, y_pred_int, weights='quadratic')\n",
        "\n",
        "def apply_thresholds(pred, th):\n",
        "    bins = [-np.inf] + list(th) + [np.inf]\n",
        "    return np.digitize(pred, bins)\n",
        "\n",
        "def optimize_thresholds(y_true, preds, iters=3, step=0.05):\n",
        "    th = np.array([1.5, 2.5, 3.5, 4.5, 5.5], dtype=float)\n",
        "    best = qwk(y_true, apply_thresholds(preds, th))\n",
        "    for _ in range(iters):\n",
        "        for i in range(len(th)):\n",
        "            lo = th[i] - 0.5; hi = th[i] + 0.5\n",
        "            if i>0: lo = max(lo, th[i-1] + 0.01)\n",
        "            if i<len(th)-1: hi = min(hi, th[i+1] - 0.01)\n",
        "            grid = np.arange(lo, hi + 1e-9, step)\n",
        "            local_best, local_val = best, th[i]\n",
        "            for g in grid:\n",
        "                th_try = th.copy(); th_try[i] = g\n",
        "                score = qwk(y_true, apply_thresholds(preds, th_try))\n",
        "                if score > local_best:\n",
        "                    local_best, local_val = score, g\n",
        "            th[i] = local_val; best = local_best\n",
        "    return th, best\n",
        "\n",
        "t0=time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Load CatBoost\n",
        "oof_cat = pd.read_csv('oof_cat.csv')\n",
        "test_cat = np.load('test_cat.npy').astype(np.float32)\n",
        "y = oof_cat['y'].astype(int).values\n",
        "pred_cat = oof_cat['oof_cat'].values.astype(np.float32)\n",
        "\n",
        "# Load DeBERTa: prefer SW64 artifacts, else base\n",
        "deb_oof_path, deb_test_path = None, None\n",
        "if os.path.exists('oof_deberta_base_sw64.csv') and os.path.exists('test_deberta_base_sw64.npy'):\n",
        "    deb_oof_path, deb_test_path = 'oof_deberta_base_sw64.csv', 'test_deberta_base_sw64.npy'\n",
        "elif os.path.exists('oof_deberta_base.csv') and os.path.exists('test_deberta_base.npy'):\n",
        "    deb_oof_path, deb_test_path = 'oof_deberta_base.csv', 'test_deberta_base.npy'\n",
        "\n",
        "has_deb = deb_oof_path is not None\n",
        "if not has_deb:\n",
        "    print('DeBERTa artifacts not found; using CatBoost only.', flush=True)\n",
        "    opt_th, oof_qwk_opt = optimize_thresholds(y, pred_cat, iters=3, step=0.05)\n",
        "    test_int = apply_thresholds(np.clip(test_cat,1,6), opt_th)\n",
        "    test_int = np.clip(test_int, 1, 6).astype(int)\n",
        "    pd.DataFrame({'essay_id': test['essay_id'], 'score': test_int}).to_csv('submission_blend.csv', index=False)\n",
        "    print(f'Classical-only OOF QWK={oof_qwk_opt:.5f}; wrote submission_blend.csv in {time.time()-t0:.1f}s')\n",
        "else:\n",
        "    oof_deb = pd.read_csv(deb_oof_path)\n",
        "    pred_deb = oof_deb['oof_deberta'].values.astype(np.float32)\n",
        "    test_deb = np.load(deb_test_path).astype(np.float32)\n",
        "    # Grid-search blend weight on OOF\n",
        "    best = (-1.0, None, None, None, None)  # (qwk, w, th, use_iso(bool), info_str)\n",
        "    for w in np.linspace(0.4, 0.7, 13):\n",
        "        blend_oof = np.clip(w*pred_deb + (1.0-w)*pred_cat, 1, 6)\n",
        "        # Uncalibrated thresholds\n",
        "        th_u, q_u = optimize_thresholds(y, blend_oof, iters=3, step=0.05)\n",
        "        info_u = f'w={w:.2f} unc'  # tag\n",
        "        if q_u > best[0]:\n",
        "            best = (q_u, w, th_u, False, info_u)\n",
        "        # Isotonic calibration then thresholds\n",
        "        iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "        iso.fit(blend_oof, y)\n",
        "        oof_cal = np.clip(iso.predict(blend_oof), 1, 6).astype(np.float32)\n",
        "        th_i, q_i = optimize_thresholds(y, oof_cal, iters=3, step=0.05)\n",
        "        info_i = f'w={w:.2f} iso'\n",
        "        if q_i > best[0]:\n",
        "            best = (q_i, w, th_i, True, info_i)\n",
        "    best_qwk, best_w, best_th, use_iso, tag = best\n",
        "    print(f'Blend search: best OOF QWK={best_qwk:.5f} at {tag}, th={np.round(best_th,3)}', flush=True)\n",
        "    # Apply to test\n",
        "    blend_test = np.clip(best_w*test_deb + (1.0-best_w)*test_cat, 1, 6)\n",
        "    if use_iso:\n",
        "        iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "        iso.fit(np.clip(best_w*pred_deb + (1.0-best_w)*pred_cat,1,6), y)\n",
        "        blend_test = np.clip(iso.predict(blend_test), 1, 6).astype(np.float32)\n",
        "    test_int = apply_thresholds(blend_test, best_th)\n",
        "    test_int = np.clip(test_int, 1, 6).astype(int)\n",
        "    pd.DataFrame({'essay_id': test['essay_id'], 'score': test_int}).to_csv('submission_blend.csv', index=False)\n",
        "    print(f'Wrote submission_blend.csv in {time.time()-t0:.1f}s', flush=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blend search: best OOF QWK=0.82764 at w=0.43 iso, th=[1.75 2.55 3.4  4.25 5.05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_blend.csv in 43.4s\n"
          ]
        }
      ]
    },
    {
      "id": "0275e1fe-4ad3-4398-a74e-ae69e584636e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Seed-bagging DeBERTa + CatBoost blend with isotonic and global thresholds; robust to partial availability\n",
        "import os, numpy as np, pandas as pd, time\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "def qwk(y_true_int, y_pred_int):\n",
        "    return cohen_kappa_score(y_true_int, y_pred_int, weights='quadratic')\n",
        "\n",
        "def apply_thresholds(pred, th):\n",
        "    bins = [-np.inf] + list(th) + [np.inf]\n",
        "    return np.digitize(pred, bins)\n",
        "\n",
        "def optimize_thresholds(y_true, preds, iters=3, step=0.05):\n",
        "    th = np.array([1.5, 2.5, 3.5, 4.5, 5.5], dtype=float)\n",
        "    best = qwk(y_true, apply_thresholds(preds, th))\n",
        "    for _ in range(iters):\n",
        "        for i in range(len(th)):\n",
        "            lo = th[i] - 0.5; hi = th[i] + 0.5\n",
        "            if i>0: lo = max(lo, th[i-1] + 0.01)\n",
        "            if i<len(th)-1: hi = min(hi, th[i+1] - 0.01)\n",
        "            grid = np.arange(lo, hi + 1e-9, step)\n",
        "            local_best, local_val = best, th[i]\n",
        "            for g in grid:\n",
        "                th_try = th.copy(); th_try[i] = g\n",
        "                score = qwk(y_true, apply_thresholds(preds, th_try))\n",
        "                if score > local_best:\n",
        "                    local_best, local_val = score, g\n",
        "            th[i] = local_val; best = local_best\n",
        "    return th, best\n",
        "\n",
        "def load_seed_views(seed_tag, base_prefix):\n",
        "    # Returns dict with keys: 'oof', 'test' for best-available DeB view for a seed\n",
        "    # Preference: combined TTA -> manual combine (sw64/sw128/ht) -> sw64 only -> base sw64 (seed 42 legacy)\n",
        "    oof, test = None, None\n",
        "    # 1) Combined TTA artifacts\n",
        "    oof_tta = f'oof_deberta_base_{base_prefix}.csv'\n",
        "    test_tta = f'test_deberta_base_{base_prefix}.npy'\n",
        "    if os.path.exists(oof_tta) and os.path.exists(test_tta):\n",
        "        df = pd.read_csv(oof_tta); oof = df['oof_deberta'].values.astype(np.float32); test = np.load(test_tta).astype(np.float32)\n",
        "        return oof, test, f'{seed_tag}:combined'\n",
        "    # 2) Manual combine of views if available\n",
        "    parts = {}\n",
        "    for view in ['sw64','sw128','ht']:\n",
        "        oof_p = f'oof_deberta_base_{base_prefix}_{view}.csv'\n",
        "        test_p = f'test_deberta_base_{base_prefix}_{view}.npy'\n",
        "        if os.path.exists(oof_p) and os.path.exists(test_p):\n",
        "            dfv = pd.read_csv(oof_p); parts[view] = (dfv['oof_deberta'].values.astype(np.float32), np.load(test_p).astype(np.float32))\n",
        "    if parts:\n",
        "        oof = np.zeros_like(next(iter(parts.values()))[0], dtype=np.float32)\n",
        "        test = np.zeros_like(next(iter(parts.values()))[1], dtype=np.float32)\n",
        "        wsum = 0.0\n",
        "        if 'sw64' in parts:\n",
        "            oof += 0.4*parts['sw64'][0]; test += 0.4*parts['sw64'][1]; wsum += 0.4\n",
        "        if 'sw128' in parts:\n",
        "            oof += 0.4*parts['sw128'][0]; test += 0.4*parts['sw128'][1]; wsum += 0.4\n",
        "        if 'ht' in parts:\n",
        "            oof += 0.2*parts['ht'][0]; test += 0.2*parts['ht'][1]; wsum += 0.2\n",
        "        if wsum > 0: oof /= wsum; test /= wsum; return oof, test, f'{seed_tag}:views'\n",
        "    # 3) Single-view sw64\n",
        "    oof_p = f'oof_deberta_base_{base_prefix}_sw64.csv'; test_p = f'test_deberta_base_{base_prefix}_sw64.npy'\n",
        "    if os.path.exists(oof_p) and os.path.exists(test_p):\n",
        "        dfv = pd.read_csv(oof_p); return dfv['oof_deberta'].values.astype(np.float32), np.load(test_p).astype(np.float32), f'{seed_tag}:sw64'\n",
        "    # 4) Legacy base sw64 (seed 42)\n",
        "    if base_prefix == 'sw64':\n",
        "        if os.path.exists('oof_deberta_base_sw64.csv') and os.path.exists('test_deberta_base_sw64.npy'):\n",
        "            dfv = pd.read_csv('oof_deberta_base_sw64.csv'); return dfv['oof_deberta'].values.astype(np.float32), np.load('test_deberta_base_sw64.npy').astype(np.float32), f'{seed_tag}:legacy_sw64'\n",
        "    return None, None, None\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Load CatBoost\n",
        "oof_cat = pd.read_csv('oof_cat.csv')\n",
        "pred_cat_oof = oof_cat['oof_cat'].values.astype(np.float32)\n",
        "y = oof_cat['y'].astype(int).values\n",
        "test_cat = np.load('test_cat.npy').astype(np.float32)\n",
        "\n",
        "# Load DeBERTa seeds\n",
        "deb_oofs = []\n",
        "deb_tests = []\n",
        "tags = []\n",
        "\n",
        "# Seed 42 legacy (base sw64 artifacts)\n",
        "o42_oof, o42_test, tag42 = load_seed_views('s042', 'sw64')\n",
        "if o42_oof is not None:\n",
        "    deb_oofs.append(o42_oof); deb_tests.append(o42_test); tags.append(tag42)\n",
        "\n",
        "# Seed 777 (when available, prefer combined)\n",
        "o777_oof, o777_test, tag777 = load_seed_views('s777', 's777')\n",
        "if o777_oof is not None:\n",
        "    deb_oofs.append(o777_oof); deb_tests.append(o777_test); tags.append(tag777)\n",
        "\n",
        "# Seed 2025 (optional, in future)\n",
        "o2025_oof, o2025_test, tag2025 = load_seed_views('s2025', 's2025')\n",
        "if o2025_oof is not None:\n",
        "    deb_oofs.append(o2025_oof); deb_tests.append(o2025_test); tags.append(tag2025)\n",
        "\n",
        "assert len(deb_oofs) > 0, 'No DeBERTa seed artifacts found yet.'\n",
        "print('Loaded DeB seeds:', tags, flush=True)\n",
        "\n",
        "# Row-wise average across available seeds\n",
        "deb_oof_bag = np.mean(np.stack(deb_oofs, axis=1), axis=1).astype(np.float32)\n",
        "deb_test_bag = np.mean(np.stack(deb_tests, axis=1), axis=1).astype(np.float32)\n",
        "\n",
        "# Blend DeB bag with CatBoost; isotonic after blend; optimize thresholds on isotonic outputs\n",
        "best = (-1.0, None, None, None)  # (qwk, w_deb, best_th, use_iso_flag)\n",
        "for w in np.arange(0.55, 0.801, 0.02):\n",
        "    blend_oof = np.clip(w*deb_oof_bag + (1.0-w)*pred_cat_oof, 1, 6).astype(np.float32)\n",
        "    # isotonic\n",
        "    iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "    iso.fit(blend_oof, y)\n",
        "    oof_cal = np.clip(iso.predict(blend_oof), 1, 6).astype(np.float32)\n",
        "    th_i, q_i = optimize_thresholds(y, oof_cal, iters=3, step=0.05)\n",
        "    if q_i > best[0]:\n",
        "        best = (q_i, w, th_i, True)\n",
        "    # also check uncalibrated as fallback\n",
        "    th_u, q_u = optimize_thresholds(y, blend_oof, iters=3, step=0.05)\n",
        "    if q_u > best[0]:\n",
        "        best = (q_u, w, th_u, False)\n",
        "\n",
        "best_q, best_w, best_th, use_iso = best\n",
        "print(f'[SeedBag] Best OOF QWK={best_q:.5f} with w_deb={best_w:.2f}, iso={use_iso}, th={np.round(best_th,3)}', flush=True)\n",
        "\n",
        "# Apply to test\n",
        "blend_test = np.clip(best_w*deb_test_bag + (1.0-best_w)*test_cat, 1, 6).astype(np.float32)\n",
        "if use_iso:\n",
        "    # Fit iso on OOF blend for consistency\n",
        "    blend_oof_final = np.clip(best_w*deb_oof_bag + (1.0-best_w)*pred_cat_oof, 1, 6).astype(np.float32)\n",
        "    iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "    iso.fit(blend_oof_final, y)\n",
        "    blend_test = np.clip(iso.predict(blend_test), 1, 6).astype(np.float32)\n",
        "test_int = apply_thresholds(blend_test, best_th)\n",
        "test_int = np.clip(test_int, 1, 6).astype(int)\n",
        "pd.DataFrame({'essay_id': test['essay_id'], 'score': test_int}).to_csv('submission_bag.csv', index=False)\n",
        "print(f'[SeedBag] Wrote submission_bag.csv in {time.time()-t0:.1f}s', flush=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded DeB seeds: ['s042:combined', 's777:combined']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SeedBag] Best OOF QWK=0.82975 with w_deb=0.57, iso=True, th=[1.75 2.55 3.35 4.25 5.05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SeedBag] Wrote submission_bag.csv in 43.7s\n"
          ]
        }
      ]
    },
    {
      "id": "b66baf1e-dafa-4f0a-9e02-271bfcf230be",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CPU-only: Fixed partial-seed handling, TTA reweight with valid-fold mask, coverage-weighted bagging, per-seed iso, post-blend iso; write submission_bag_rew.csv\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "def qwk(y_true_int, y_pred_int):\n",
        "    return cohen_kappa_score(y_true_int, y_pred_int, weights='quadratic')\n",
        "\n",
        "def apply_thresholds(pred, th):\n",
        "    bins = [-np.inf] + list(th) + [np.inf]\n",
        "    return np.digitize(pred, bins)\n",
        "\n",
        "def optimize_thresholds_constrained(y_true, preds, iters=3, coarse_step=0.025, fine_step=0.005, base=None, max_delta=0.25):\n",
        "    base_th = np.array([1.5, 2.5, 3.5, 4.5, 5.5], dtype=float) if base is None else np.array(base, dtype=float)\n",
        "    th = base_th.copy()\n",
        "    best = qwk(y_true, apply_thresholds(preds, th))\n",
        "    # coarse passes\n",
        "    for _ in range(iters):\n",
        "        for i in range(5):\n",
        "            lo = base_th[i] - max_delta; hi = base_th[i] + max_delta\n",
        "            if i>0: lo = max(lo, th[i-1] + 0.01)\n",
        "            if i<4: hi = min(hi, th[i+1] - 0.01)\n",
        "            for g in np.arange(lo, hi + 1e-9, coarse_step):\n",
        "                th_try = th.copy(); th_try[i] = g\n",
        "                score = qwk(y_true, apply_thresholds(preds, th_try))\n",
        "                if score > best:\n",
        "                    best, th[i] = score, g\n",
        "    # fine pass around current th\n",
        "    for i in range(5):\n",
        "        lo = max(base_th[i] - max_delta, th[i] - coarse_step)\n",
        "        hi = min(base_th[i] + max_delta, th[i] + coarse_step)\n",
        "        if i>0: lo = max(lo, th[i-1] + 0.01)\n",
        "        if i<4: hi = min(hi, th[i+1] - 0.01)\n",
        "        for g in np.arange(lo, hi + 1e-9, fine_step):\n",
        "            th_try = th.copy(); th_try[i] = g\n",
        "            score = qwk(y_true, apply_thresholds(preds, th_try))\n",
        "            if score > best:\n",
        "                best, th[i] = score, g\n",
        "    return th, best\n",
        "\n",
        "def load_view(prefix):\n",
        "    oof = pd.read_csv(f'oof_deberta_base_{prefix}.csv')['oof_deberta'].values.astype(np.float32)\n",
        "    test = np.load(f'test_deberta_base_{prefix}.npy').astype(np.float32)\n",
        "    return oof, test\n",
        "\n",
        "def _find_oof_path(seed_prefix, view='sw64'):\n",
        "    cands = [\n",
        "        f'oof_deberta_base_{seed_prefix}_{view}.csv',\n",
        "        f'oof_deberta_base_{seed_prefix}.csv',\n",
        "    ]\n",
        "    if seed_prefix in ('sw64','legacy','base'):\n",
        "        cands.append('oof_deberta_base_sw64.csv')\n",
        "    for p in cands:\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def infer_trained_folds(seed_prefix, folds_df, n_splits, view='sw64'):\n",
        "    p = _find_oof_path(seed_prefix, view=view)\n",
        "    if p is None:\n",
        "        return [False]*n_splits\n",
        "    oof = pd.read_csv(p)['oof_deberta'].values.astype(np.float32)\n",
        "    trained = []\n",
        "    for f in range(n_splits):\n",
        "        va = folds_df.index[folds_df['fold']==f].to_numpy()\n",
        "        vals = oof[va]\n",
        "        avail = np.isfinite(vals) & (vals != 0.0)\n",
        "        trained.append(avail.mean() > 0.90)\n",
        "    return trained\n",
        "\n",
        "def add_seed(oof_arr, test_arr, seed_label, n_splits, folds_df, detect_label=None):\n",
        "    trained = infer_trained_folds(detect_label or seed_label, folds_df, n_splits, view='sw64')\n",
        "    folds_run = int(sum(trained))\n",
        "    oof = oof_arr.astype(np.float32).copy()\n",
        "    for f, ok in enumerate(trained):\n",
        "        if not ok:\n",
        "            va = folds_df.index[folds_df['fold']==f].to_numpy()\n",
        "            oof[va] = np.nan\n",
        "    w_cov = folds_run / float(n_splits)\n",
        "    print(f'[add_seed] {seed_label} | folds_run={folds_run}/{n_splits} | w_cov={w_cov:.3f}', flush=True)\n",
        "    return oof, test_arr.astype(np.float32), seed_label, folds_run, w_cov\n",
        "\n",
        "def best_tta(o64, o128, oht, t64, t128, tht, y, valid_mask=None, allow_no_ht=False, prefer_large_grid=False):\n",
        "    # Expert grid: bias to SW64, allow HT=0\n",
        "    grid = [\n",
        "        (0.70,0.30,0.00), (0.66,0.34,0.00), (0.60,0.40,0.00), (0.55,0.45,0.00),\n",
        "        (0.60,0.30,0.10), (0.55,0.35,0.10), (0.55,0.30,0.15), (0.50,0.40,0.10),\n",
        "    ]\n",
        "    if not prefer_large_grid:\n",
        "        # keep a couple of legacy safe points too\n",
        "        grid += [(0.55,0.30,0.15), (0.50,0.35,0.15)]\n",
        "    if allow_no_ht:\n",
        "        pass  # already included no-HT\n",
        "    best = (-1.0, None, None, None)\n",
        "    base_bins = [-np.inf,1.5,2.5,3.5,4.5,5.5,np.inf]\n",
        "    for w64,w128,wht in grid:\n",
        "        oof_c = (w64*o64 + w128*o128 + wht*oht).astype(np.float32)\n",
        "        vm = valid_mask if valid_mask is not None else np.isfinite(oof_c)\n",
        "        q = qwk(y[vm], np.digitize(oof_c[vm], base_bins))\n",
        "        if q > best[0]:\n",
        "            test_c = (w64*t64 + w128*t128 + wht*tht).astype(np.float32)\n",
        "            best = (q, (w64,w128,wht), oof_c, test_c)\n",
        "    return best\n",
        "\n",
        "def per_seed_iso(oof_seed, test_seed, y, valid_mask=None):\n",
        "    vm = valid_mask if valid_mask is not None else np.isfinite(oof_seed)\n",
        "    if vm.sum() < 10:\n",
        "        return oof_seed, test_seed\n",
        "    iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "    iso.fit(np.clip(oof_seed[vm],1,6), y[vm])\n",
        "    oof_cal = np.clip(iso.predict(np.clip(oof_seed,1,6)), 1, 6).astype(np.float32)\n",
        "    test_cal = np.clip(iso.predict(np.clip(test_seed,1,6)), 1, 6).astype(np.float32)\n",
        "    return oof_cal, test_cal\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds.csv')\n",
        "n_splits = int(folds['fold'].max()) + 1\n",
        "\n",
        "# Load CatBoost\n",
        "oof_cat = pd.read_csv('oof_cat.csv')\n",
        "y = oof_cat['y'].astype(int).values\n",
        "pred_cat_oof = oof_cat['oof_cat'].values.astype(np.float32)\n",
        "test_cat = np.load('test_cat.npy').astype(np.float32)\n",
        "\n",
        "deb_oofs_seeds = []\n",
        "deb_tests_seeds = []\n",
        "seed_cov_weights = []\n",
        "seed_names = []\n",
        "idx_sL = None  # track sL position for optional alpha search\n",
        "\n",
        "# Seed 42 (SW64 full) + per-seed iso\n",
        "if os.path.exists('oof_deberta_base_sw64.csv') and os.path.exists('test_deberta_base_sw64.npy'):\n",
        "    o42 = pd.read_csv('oof_deberta_base_sw64.csv')['oof_deberta'].values.astype(np.float32)\n",
        "    t42 = np.load('test_deberta_base_sw64.npy').astype(np.float32)\n",
        "    o42, t42 = per_seed_iso(o42, t42, y)\n",
        "    o42, t42, name42, fr42, wcov42 = add_seed(o42, t42, 's042_sw64', n_splits, folds, detect_label='sw64')\n",
        "    deb_oofs_seeds.append(o42); deb_tests_seeds.append(t42); seed_cov_weights.append(wcov42); seed_names.append(name42)\n",
        "\n",
        "# Seed 777 (views with TTA search) + per-seed iso\n",
        "if all(os.path.exists(p) for p in [\n",
        "    'oof_deberta_base_s777_sw64.csv','oof_deberta_base_s777_sw128.csv','oof_deberta_base_s777_ht.csv',\n",
        "    'test_deberta_base_s777_sw64.npy','test_deberta_base_s777_sw128.npy','test_deberta_base_s777_ht.npy']):\n",
        "    o64,t64 = load_view('s777_sw64')\n",
        "    o128,t128 = load_view('s777_sw128')\n",
        "    oht,tht = load_view('s777_ht')\n",
        "    q,w777,oof777,tst777 = best_tta(o64,o128,oht,t64,t128,tht,y, valid_mask=None, allow_no_ht=False)\n",
        "    oof777, tst777 = per_seed_iso(oof777, tst777, y)\n",
        "    o777, t777, name777, fr777, wcov777 = add_seed(oof777, tst777, f's777_{w777}', n_splits, folds, detect_label='s777')\n",
        "    deb_oofs_seeds.append(o777); deb_tests_seeds.append(t777); seed_cov_weights.append(wcov777); seed_names.append(name777)\n",
        "elif os.path.exists('oof_deberta_base_s777.csv') and os.path.exists('test_deberta_base_s777.npy'):\n",
        "    o777 = pd.read_csv('oof_deberta_base_s777.csv')['oof_deberta'].values.astype(np.float32)\n",
        "    t777 = np.load('test_deberta_base_s777.npy').astype(np.float32)\n",
        "    o777, t777 = per_seed_iso(o777, t777, y)\n",
        "    o777, t777, name777, fr777, wcov777 = add_seed(o777, t777, 's777_combined', n_splits, folds, detect_label='s777')\n",
        "    deb_oofs_seeds.append(o777); deb_tests_seeds.append(t777); seed_cov_weights.append(wcov777); seed_names.append(name777)\n",
        "\n",
        "# Seed 2025 (views with valid-mask TTA; allow no-HT) + per-seed iso under trained mask\n",
        "if all(os.path.exists(p) for p in [\n",
        "    'oof_deberta_base_s2025_sw64.csv','oof_deberta_base_s2025_sw128.csv','oof_deberta_base_s2025_ht.csv',\n",
        "    'test_deberta_base_s2025_sw64.npy','test_deberta_base_s2025_sw128.npy','test_deberta_base_s2025_ht.npy']):\n",
        "    o64,t64 = load_view('s2025_sw64')\n",
        "    o128,t128 = load_view('s2025_sw128')\n",
        "    oht,tht = load_view('s2025_ht')\n",
        "    trained_mask = (o64 != 0.0) & np.isfinite(o64)\n",
        "    q,w2025,oof2025,tst2025 = best_tta(o64,o128,oht,t64,t128,tht,y, valid_mask=trained_mask, allow_no_ht=True)\n",
        "    oof2025, tst2025 = per_seed_iso(oof2025, tst2025, y, valid_mask=trained_mask)\n",
        "    o2025, t2025, name2025, fr2025, wcov2025 = add_seed(oof2025, tst2025, f's2025_{w2025}', n_splits, folds, detect_label='s2025')\n",
        "    deb_oofs_seeds.append(o2025); deb_tests_seeds.append(t2025); seed_cov_weights.append(wcov2025); seed_names.append(name2025)\n",
        "elif os.path.exists('oof_deberta_base_s2025.csv') and os.path.exists('test_deberta_base_s2025.npy'):\n",
        "    o2025 = pd.read_csv('oof_deberta_base_s2025.csv')['oof_deberta'].values.astype(np.float32)\n",
        "    t2025 = np.load('test_deberta_base_s2025.npy').astype(np.float32)\n",
        "    mask = None\n",
        "    if os.path.exists('oof_deberta_base_s2025_sw64.csv'):\n",
        "        m64 = pd.read_csv('oof_deberta_base_s2025_sw64.csv')['oof_deberta'].values.astype(np.float32)\n",
        "        mask = (m64 != 0.0) & np.isfinite(m64)\n",
        "    o2025, t2025 = per_seed_iso(o2025, t2025, y, valid_mask=mask)\n",
        "    o2025, t2025, name2025, fr2025, wcov2025 = add_seed(o2025, t2025, 's2025_combined', n_splits, folds, detect_label='s2025')\n",
        "    deb_oofs_seeds.append(o2025); deb_tests_seeds.append(t2025); seed_cov_weights.append(wcov2025); seed_names.append(name2025)\n",
        "\n",
        "# DeBERTa-v3-Large partial seed 'sL' (views with valid-mask TTA; allow no-HT) + per-seed iso\n",
        "if all(os.path.exists(p) for p in [\n",
        "    'oof_deberta_base_sL_sw64.csv','oof_deberta_base_sL_sw128.csv','oof_deberta_base_sL_ht.csv',\n",
        "    'test_deberta_base_sL_sw64.npy','test_deberta_base_sL_sw128.npy','test_deberta_base_sL_ht.npy']):\n",
        "    o64L,t64L = load_view('sL_sw64')\n",
        "    o128L,t128L = load_view('sL_sw128')\n",
        "    ohtL,thtL = load_view('sL_ht')\n",
        "    trained_mask_L = (o64L != 0.0) & np.isfinite(o64L)\n",
        "    qL,wL,oofL,tstL = best_tta(o64L,o128L,ohtL,t64L,t128L,thtL,y, valid_mask=trained_mask_L, allow_no_ht=True, prefer_large_grid=True)\n",
        "    oofL, tstL = per_seed_iso(oofL, tstL, y, valid_mask=trained_mask_L)\n",
        "    oL, tL, nameL, frL, wcovL = add_seed(oofL, tstL, f'sL_{wL}', n_splits, folds, detect_label='sL')\n",
        "    idx_sL = len(deb_oofs_seeds)\n",
        "    deb_oofs_seeds.append(oL); deb_tests_seeds.append(tL); seed_cov_weights.append(wcovL); seed_names.append(nameL)\n",
        "\n",
        "assert len(deb_oofs_seeds) > 0, 'No DeBERTa seeds available for reweighting/blend.'\n",
        "print('Seeds in bag:', seed_names, flush=True)\n",
        "\n",
        "# Coverage-weighted averaging (consistent for OOF/test); OOF renormalizes row-wise over available seeds\n",
        "O = np.stack(deb_oofs_seeds, axis=1)  # (N, S) with NaNs in untrained folds\n",
        "T = np.stack(deb_tests_seeds, axis=1) # (Nt, S) no NaNs\n",
        "W = np.array(seed_cov_weights, dtype=np.float32)\n",
        "W = W / W.sum() if W.sum() > 0 else np.ones_like(W, dtype=np.float32)/len(W)\n",
        "A = np.isfinite(O).astype(np.float32)\n",
        "num = np.nansum(O * W[None, :], axis=1)\n",
        "den = (A * W[None, :]).sum(axis=1)\n",
        "deb_oof_bag_cov = (num / np.clip(den, 1e-6, None)).astype(np.float32)\n",
        "deb_test_bag_cov = (T * W[None, :]).sum(axis=1).astype(np.float32)\n",
        "\n",
        "# Optional alpha search to slightly upweight sL vs base bag\n",
        "deb_oof_base = deb_oof_bag_cov.copy()\n",
        "deb_test_base = deb_test_bag_cov.copy()\n",
        "if idx_sL is not None:\n",
        "    # Build base excluding sL\n",
        "    mask_cols = [i for i in range(O.shape[1]) if i != idx_sL]\n",
        "    if len(mask_cols) > 0:\n",
        "        Wb = W[mask_cols]; Wb = Wb / Wb.sum() if Wb.sum()>0 else Wb\n",
        "        Ob = O[:, mask_cols]; Tb = T[:, mask_cols]\n",
        "        Ab = np.isfinite(Ob).astype(np.float32)\n",
        "        deb_oof_base = (np.nansum(Ob * Wb[None,:], axis=1) / np.clip((Ab * Wb[None,:]).sum(axis=1), 1e-6, None)).astype(np.float32)\n",
        "        deb_test_base = (Tb * Wb[None,:]).sum(axis=1).astype(np.float32)\n",
        "\n",
        "# Blend DeB bag with CatBoost; try no-iso vs post-blend global iso with constrained thresholds\n",
        "def eval_blend(deb_oof, deb_test):\n",
        "    best = (-1.0, None, None, None)  # (q, w_deb, th, iso_or_None)\n",
        "    # coarse search\n",
        "    for w in np.arange(0.50, 0.81, 0.02):\n",
        "        blend_oof = w*deb_oof + (1.0-w)*pred_cat_oof\n",
        "        th_u, q_u = optimize_thresholds_constrained(y, blend_oof, iters=2, coarse_step=0.025, fine_step=0.005, base=[1.5,2.5,3.5,4.5,5.5], max_delta=0.25)\n",
        "        if q_u > best[0]: best = (q_u, w, th_u, None)\n",
        "        iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "        iso.fit(blend_oof, y)\n",
        "        oof_cal = iso.predict(blend_oof).astype(np.float32)\n",
        "        th_i, q_i = optimize_thresholds_constrained(y, oof_cal, iters=2, coarse_step=0.025, fine_step=0.005, base=[1.5,2.5,3.5,4.5,5.5], max_delta=0.25)\n",
        "        if q_i > best[0]: best = (q_i, w, th_i, iso)\n",
        "    # fine around best w\n",
        "    q0, w0, th0, iso0 = best\n",
        "    w_min = max(0.50, w0-0.03); w_max = min(0.80, w0+0.03)\n",
        "    for w in np.arange(w_min, w_max + 1e-9, 0.01):\n",
        "        blend_oof = w*deb_oof + (1.0-w)*pred_cat_oof\n",
        "        th_u, q_u = optimize_thresholds_constrained(y, blend_oof, iters=1, coarse_step=0.02, fine_step=0.005, base=[1.5,2.5,3.5,4.5,5.5], max_delta=0.25)\n",
        "        if q_u > best[0]: best = (q_u, w, th_u, None)\n",
        "        iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "        iso.fit(blend_oof, y)\n",
        "        oof_cal = iso.predict(blend_oof).astype(np.float32)\n",
        "        th_i, q_i = optimize_thresholds_constrained(y, oof_cal, iters=1, coarse_step=0.02, fine_step=0.005, base=[1.5,2.5,3.5,4.5,5.5], max_delta=0.25)\n",
        "        if q_i > best[0]: best = (q_i, w, th_i, iso)\n",
        "    return best\n",
        "\n",
        "# Evaluate coverage-weighted default bag\n",
        "best_cov = eval_blend(deb_oof_bag_cov, deb_test_bag_cov)\n",
        "best = best_cov\n",
        "\n",
        "# Optional alpha search if sL present: mix sL vs base\n",
        "if idx_sL is not None:\n",
        "    sL_oof = O[:, idx_sL].astype(np.float32)\n",
        "    sL_test = T[:, idx_sL].astype(np.float32)\n",
        "    vm_sL = np.isfinite(sL_oof)\n",
        "    # alpha range around coverage weight of sL\n",
        "    covL = seed_cov_weights[idx_sL]\n",
        "    alpha_lo = max(0.0, covL - 0.05); alpha_hi = min(1.0, covL + 0.10)\n",
        "    for a in np.arange(alpha_lo, alpha_hi + 1e-9, 0.02):\n",
        "        deb_oof_mix = (a * sL_oof + (1.0 - a) * deb_oof_base).astype(np.float32)\n",
        "        # keep NaNs where sL not trained; base already has values\n",
        "        # row-wise fill where sL is NaN\n",
        "        m = ~np.isfinite(sL_oof)\n",
        "        deb_oof_mix[m] = deb_oof_base[m]\n",
        "        deb_test_mix = (a * sL_test + (1.0 - a) * deb_test_base).astype(np.float32)\n",
        "        cand = eval_blend(deb_oof_mix, deb_test_mix)\n",
        "        if cand[0] > best[0]:\n",
        "            best = cand\n",
        "\n",
        "best_q, best_w, best_th, best_iso = best\n",
        "print(f'[Reweight+Blend-FIXED] OOF QWK={best_q:.5f} w_deb={best_w:.3f} th={np.round(best_th,3)}', flush=True)\n",
        "\n",
        "# Apply to test: clip only at the end (post-iso) before thresholds\n",
        "blend_test = (best_w*deb_test_bag_cov + (1.0-best_w)*test_cat).astype(np.float32)\n",
        "if best_iso is not None:\n",
        "    # Fit iso on OOF blend for consistency\n",
        "    blend_oof_final = (best_w*deb_oof_bag_cov + (1.0-best_w)*pred_cat_oof).astype(np.float32)\n",
        "    iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "    iso.fit(blend_oof_final, y)\n",
        "    blend_test = iso.predict(blend_test).astype(np.float32)\n",
        "blend_test = np.clip(blend_test, 1, 6).astype(np.float32)\n",
        "test_int = np.clip(apply_thresholds(blend_test, best_th), 1, 6).astype(int)\n",
        "pd.DataFrame({'essay_id': test['essay_id'], 'score': test_int}).to_csv('submission_bag_rew.csv', index=False)\n",
        "print(f'[Reweight+Blend-FIXED] Wrote submission_bag_rew.csv in {time.time()-t0:.1f}s', flush=True)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[add_seed] s042_sw64 | folds_run=5/5 | w_cov=1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[add_seed] s777_(0.55, 0.3, 0.15) | folds_run=5/5 | w_cov=1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[add_seed] s2025_(0.55, 0.3, 0.15) | folds_run=5/5 | w_cov=1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[add_seed] sL_(0.55, 0.3, 0.15) | folds_run=3/5 | w_cov=0.600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seeds in bag: ['s042_sw64', 's777_(0.55, 0.3, 0.15)', 's2025_(0.55, 0.3, 0.15)', 'sL_(0.55, 0.3, 0.15)']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Reweight+Blend-FIXED] OOF QWK=0.83235 w_deb=0.510 th=[1.75 2.55 3.39 4.25 5.25]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Reweight+Blend-FIXED] Wrote submission_bag_rew.csv in 494.9s\n"
          ]
        }
      ]
    },
    {
      "id": "75ffe3d5-f054-4303-b250-accd6d9834f8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CPU-only: Level-2 Ridge stacker on OOF predictions (DeB bag + CatBoost) with global isotonic + constrained thresholds\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "t0=time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds.csv')\n",
        "id_col, target_col = 'essay_id','score'\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = int(folds['fold'].max()) + 1\n",
        "\n",
        "def qwk(y_true_int, y_pred_int):\n",
        "    return cohen_kappa_score(y_true_int, y_pred_int, weights='quadratic')\n",
        "\n",
        "def apply_thresholds(pred, th):\n",
        "    bins = [-np.inf] + list(th) + [np.inf]\n",
        "    return np.digitize(pred, bins)\n",
        "\n",
        "def optimize_thresholds_constrained(y_true, preds, iters=3, step=0.05, base=None, max_delta=0.3):\n",
        "    th = np.array([1.5, 2.5, 3.5, 4.5, 5.5], dtype=float) if base is None else np.array(base, dtype=float)\n",
        "    base_th = th.copy()\n",
        "    best = qwk(y_true, apply_thresholds(preds, th))\n",
        "    for _ in range(iters):\n",
        "        for i in range(len(th)):\n",
        "            lo = max(base_th[i] - max_delta, th[i] - 0.5)\n",
        "            hi = min(base_th[i] + max_delta, th[i] + 0.5)\n",
        "            if i>0: lo = max(lo, th[i-1] + 0.01)\n",
        "            if i<len(th)-1: hi = min(hi, th[i+1] - 0.01)\n",
        "            grid = np.arange(lo, hi + 1e-9, step)\n",
        "            local_best, local_val = best, th[i]\n",
        "            for g in grid:\n",
        "                th_try = th.copy(); th_try[i] = g\n",
        "                score = qwk(y_true, apply_thresholds(preds, th_try))\n",
        "                if score > local_best:\n",
        "                    local_best, local_val = score, g\n",
        "            th[i] = local_val; best = local_best\n",
        "    return th, best\n",
        "\n",
        "# Load CatBoost OOF/test\n",
        "oof_cat = pd.read_csv('oof_cat.csv')\n",
        "pred_cat_oof = oof_cat['oof_cat'].values.astype(np.float32)\n",
        "test_cat = np.load('test_cat.npy').astype(np.float32)\n",
        "\n",
        "# Load DeB seeds and bag them (reuse chosen TTA for s777 if available)\n",
        "deb_oofs = []; deb_tests = []\n",
        "# Seed 42 (sw64)\n",
        "if os.path.exists('oof_deberta_base_sw64.csv') and os.path.exists('test_deberta_base_sw64.npy'):\n",
        "    df42 = pd.read_csv('oof_deberta_base_sw64.csv')\n",
        "    deb_oofs.append(df42['oof_deberta'].values.astype(np.float32))\n",
        "    deb_tests.append(np.load('test_deberta_base_sw64.npy').astype(np.float32))\n",
        "# Seed 777 per-views with chosen weights file or fallback to combined\n",
        "def load_view(prefix):\n",
        "    oof = pd.read_csv(f'oof_deberta_base_{prefix}.csv')['oof_deberta'].values.astype(np.float32)\n",
        "    testv = np.load(f'test_deberta_base_{prefix}.npy').astype(np.float32)\n",
        "    return oof, testv\n",
        "if all(os.path.exists(p) for p in [\n",
        "    'oof_deberta_base_s777_sw64.csv','oof_deberta_base_s777_sw128.csv','oof_deberta_base_s777_ht.csv',\n",
        "    'test_deberta_base_s777_sw64.npy','test_deberta_base_s777_sw128.npy','test_deberta_base_s777_ht.npy']):\n",
        "    o64,t64 = load_view('s777_sw64')\n",
        "    o128,t128 = load_view('s777_sw128')\n",
        "    oht,tht = load_view('s777_ht')\n",
        "    w = (0.4,0.4,0.2)\n",
        "    if os.path.exists('tta_weights_s777.txt'):\n",
        "        try:\n",
        "            txt = open('tta_weights_s777.txt').read().strip()\n",
        "            w = eval(txt)\n",
        "        except Exception:\n",
        "            pass\n",
        "    oof_777 = np.clip(w[0]*o64 + w[1]*o128 + w[2]*oht, 1, 6).astype(np.float32)\n",
        "    tst_777 = np.clip(w[0]*t64 + w[1]*t128 + w[2]*tht, 1, 6).astype(np.float32)\n",
        "    deb_oofs.append(oof_777); deb_tests.append(tst_777)\n",
        "elif os.path.exists('oof_deberta_base_s777.csv') and os.path.exists('test_deberta_base_s777.npy'):\n",
        "    df = pd.read_csv('oof_deberta_base_s777.csv')\n",
        "    deb_oofs.append(df['oof_deberta'].values.astype(np.float32))\n",
        "    deb_tests.append(np.load('test_deberta_base_s777.npy').astype(np.float32))\n",
        "\n",
        "assert len(deb_oofs)>0, 'No DeB OOF available for stacking.'\n",
        "deb_oof_bag = np.mean(np.stack(deb_oofs, axis=1), axis=1).astype(np.float32)\n",
        "deb_test_bag = np.mean(np.stack(deb_tests, axis=1), axis=1).astype(np.float32)\n",
        "\n",
        "# Build meta features\n",
        "X_oof = np.stack([deb_oof_bag, pred_cat_oof, deb_oof_bag - pred_cat_oof, np.abs(deb_oof_bag - pred_cat_oof)], axis=1).astype(np.float32)\n",
        "X_test = np.stack([deb_test_bag, test_cat, deb_test_bag - test_cat, np.abs(deb_test_bag - test_cat)], axis=1).astype(np.float32)\n",
        "\n",
        "# CV Ridge stacker\n",
        "oof_stack = np.zeros(len(train), dtype=np.float32)\n",
        "test_stack_f = np.zeros((len(test), n_splits), dtype=np.float32)\n",
        "for f in range(n_splits):\n",
        "    tr_idx = folds.index[folds['fold']!=f].to_numpy()\n",
        "    va_idx = folds.index[folds['fold']==f].to_numpy()\n",
        "    model = Ridge(alpha=1.0, random_state=42)\n",
        "    model.fit(X_oof[tr_idx], y[tr_idx].astype(float))\n",
        "    oof_stack[va_idx] = model.predict(X_oof[va_idx]).astype(np.float32)\n",
        "    test_stack_f[:, f] = model.predict(X_test).astype(np.float32)\n",
        "\n",
        "test_stack = test_stack_f.mean(axis=1).astype(np.float32)\n",
        "\n",
        "# Global isotonic on stacker outputs + constrained thresholds\n",
        "iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "iso.fit(np.clip(oof_stack,1,6), y)\n",
        "oof_cal = np.clip(iso.predict(np.clip(oof_stack,1,6)), 1, 6).astype(np.float32)\n",
        "th_opt, oof_q = optimize_thresholds_constrained(y, oof_cal, iters=3, step=0.05, base=[1.5,2.5,3.5,4.5,5.5], max_delta=0.3)\n",
        "print(f'[Stack] OOF QWK={oof_q:.5f} th={np.round(th_opt,3)}', flush=True)\n",
        "\n",
        "# Apply to test\n",
        "test_cal = np.clip(iso.predict(np.clip(test_stack,1,6)), 1, 6).astype(np.float32)\n",
        "test_int = np.clip(apply_thresholds(test_cal, th_opt), 1, 6).astype(int)\n",
        "pd.DataFrame({'essay_id': test[id_col], 'score': test_int}).to_csv('submission_stack.csv', index=False)\n",
        "print(f'[Stack] Wrote submission_stack.csv in {time.time()-t0:.1f}s', flush=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stack] OOF QWK=0.82930 th=[1.75 2.6  3.4  4.25 5.2 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stack] Wrote submission_stack.csv in 1.3s\n"
          ]
        }
      ]
    },
    {
      "id": "ec414fc1-111a-45de-8f2f-814cfb9b50cc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CPU-only: Fold-wise isotonic calibration on DeB bag + CatBoost with widened blend grid; write submission_bag_foldiso.csv\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def qwk(y_true_int, y_pred_int):\n",
        "    return cohen_kappa_score(y_true_int, y_pred_int, weights='quadratic')\n",
        "\n",
        "def apply_thresholds(pred, th):\n",
        "    bins = [-np.inf] + list(th) + [np.inf]\n",
        "    return np.digitize(pred, bins)\n",
        "\n",
        "def optimize_thresholds_constrained(y_true, preds, iters=3, step=0.05, base=None, max_delta=0.3):\n",
        "    th = np.array([1.5, 2.5, 3.5, 4.5, 5.5], dtype=float) if base is None else np.array(base, dtype=float)\n",
        "    base_th = th.copy()\n",
        "    best = qwk(y_true, apply_thresholds(preds, th))\n",
        "    for _ in range(iters):\n",
        "        for i in range(len(th)):\n",
        "            lo = max(base_th[i] - max_delta, th[i] - 0.5)\n",
        "            hi = min(base_th[i] + max_delta, th[i] + 0.5)\n",
        "            if i>0: lo = max(lo, th[i-1] + 0.01)\n",
        "            if i<len(th)-1: hi = min(hi, th[i+1] - 0.01)\n",
        "            grid = np.arange(lo, hi + 1e-9, step)\n",
        "            local_best, local_val = best, th[i]\n",
        "            for g in grid:\n",
        "                th_try = th.copy(); th_try[i] = g\n",
        "                score = qwk(y_true, apply_thresholds(preds, th_try))\n",
        "                if score > local_best:\n",
        "                    local_best, local_val = score, g\n",
        "            th[i] = local_val; best = local_best\n",
        "    return th, best\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds.csv')\n",
        "id_col, target_col = 'essay_id','score'\n",
        "y = train[target_col].astype(int).values\n",
        "n_splits = int(folds['fold'].max()) + 1\n",
        "\n",
        "# Load CatBoost\n",
        "oof_cat = pd.read_csv('oof_cat.csv')\n",
        "pred_cat_oof = oof_cat['oof_cat'].values.astype(np.float32)\n",
        "test_cat = np.load('test_cat.npy').astype(np.float32)\n",
        "\n",
        "# Load DeB bag (seed 42 sw64 + seed 777 combined or per-view w/ chosen weights) as in cell 9\n",
        "deb_oofs = []; deb_tests = []\n",
        "if os.path.exists('oof_deberta_base_sw64.csv') and os.path.exists('test_deberta_base_sw64.npy'):\n",
        "    df42 = pd.read_csv('oof_deberta_base_sw64.csv')\n",
        "    deb_oofs.append(df42['oof_deberta'].values.astype(np.float32))\n",
        "    deb_tests.append(np.load('test_deberta_base_sw64.npy').astype(np.float32))\n",
        "def load_view(prefix):\n",
        "    oof = pd.read_csv(f'oof_deberta_base_{prefix}.csv')['oof_deberta'].values.astype(np.float32)\n",
        "    testv = np.load(f'test_deberta_base_{prefix}.npy').astype(np.float32)\n",
        "    return oof, testv\n",
        "if all(os.path.exists(p) for p in [\n",
        "    'oof_deberta_base_s777_sw64.csv','oof_deberta_base_s777_sw128.csv','oof_deberta_base_s777_ht.csv',\n",
        "    'test_deberta_base_s777_sw64.npy','test_deberta_base_s777_sw128.npy','test_deberta_base_s777_ht.npy']):\n",
        "    o64,t64 = load_view('s777_sw64')\n",
        "    o128,t128 = load_view('s777_sw128')\n",
        "    oht,tht = load_view('s777_ht')\n",
        "    w = (0.4,0.4,0.2)\n",
        "    if os.path.exists('tta_weights_s777.txt'):\n",
        "        try: w = eval(open('tta_weights_s777.txt').read().strip())\n",
        "        except Exception: pass\n",
        "    deb_oofs.append(np.clip(w[0]*o64 + w[1]*o128 + w[2]*oht, 1, 6).astype(np.float32))\n",
        "    deb_tests.append(np.clip(w[0]*t64 + w[1]*t128 + w[2]*tht, 1, 6).astype(np.float32))\n",
        "elif os.path.exists('oof_deberta_base_s777.csv') and os.path.exists('test_deberta_base_s777.npy'):\n",
        "    df = pd.read_csv('oof_deberta_base_s777.csv')\n",
        "    deb_oofs.append(df['oof_deberta'].values.astype(np.float32))\n",
        "    deb_tests.append(np.load('test_deberta_base_s777.npy').astype(np.float32))\n",
        "\n",
        "assert len(deb_oofs)>0, 'No DeB seeds available.'\n",
        "deb_oof_bag = np.mean(np.stack(deb_oofs, axis=1), axis=1).astype(np.float32)\n",
        "deb_test_bag = np.mean(np.stack(deb_tests, axis=1), axis=1).astype(np.float32)\n",
        "\n",
        "# Fold-wise isotonic calibration: for each fold, fit iso on other folds and apply to held-out\n",
        "def foldwise_iso_oof(pred_float):\n",
        "    oof_cal = np.zeros_like(pred_float, dtype=np.float32)\n",
        "    for f in range(n_splits):\n",
        "        va_idx = folds.index[folds['fold']==f].to_numpy()\n",
        "        tr_idx = folds.index[folds['fold']!=f].to_numpy()\n",
        "        iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "        iso.fit(np.clip(pred_float[tr_idx],1,6), y[tr_idx])\n",
        "        oof_cal[va_idx] = np.clip(iso.predict(np.clip(pred_float[va_idx],1,6)), 1, 6).astype(np.float32)\n",
        "    return oof_cal\n",
        "\n",
        "best = (-1.0, None, None, None)  # q, w_deb, th, iso_models (None since fold-wise applied only on OOF)\n",
        "for w in np.arange(0.45, 0.851, 0.02):\n",
        "    blend_oof = np.clip(w*deb_oof_bag + (1.0-w)*pred_cat_oof, 1, 6).astype(np.float32)\n",
        "    oof_cal = foldwise_iso_oof(blend_oof)\n",
        "    th_i, q_i = optimize_thresholds_constrained(y, oof_cal, iters=3, step=0.05, base=[1.5,2.5,3.5,4.5,5.5], max_delta=0.3)\n",
        "    if q_i > best[0]: best = (q_i, w, th_i, None)\n",
        "\n",
        "best_q, best_w, best_th, _ = best\n",
        "print(f'[FoldISO] OOF QWK={best_q:.5f} w_deb={best_w:.3f} th={np.round(best_th,3)}', flush=True)\n",
        "\n",
        "# Train single global isotonic on full OOF blend with best weight for test application (safer for LB)\n",
        "blend_oof_best = np.clip(best_w*deb_oof_bag + (1.0-best_w)*pred_cat_oof, 1, 6).astype(np.float32)\n",
        "iso_global = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "iso_global.fit(blend_oof_best, y)\n",
        "\n",
        "# Apply to test consistently\n",
        "blend_test = np.clip(best_w*deb_test_bag + (1.0-best_w)*test_cat, 1, 6).astype(np.float32)\n",
        "test_cal = np.clip(iso_global.predict(blend_test), 1, 6).astype(np.float32)\n",
        "test_int = np.clip(apply_thresholds(test_cal, best_th), 1, 6).astype(int)\n",
        "pd.DataFrame({'essay_id': test[id_col], 'score': test_int}).to_csv('submission_bag_foldiso.csv', index=False)\n",
        "print(f'[FoldISO] Wrote submission_bag_foldiso.csv in {time.time()-t0:.1f}s', flush=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FoldISO] OOF QWK=0.82849 w_deb=0.570 th=[1.7  2.55 3.35 4.2  5.2 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FoldISO] Wrote submission_bag_foldiso.csv in 21.4s\n"
          ]
        }
      ]
    },
    {
      "id": "4e558faa-d203-4881-878f-9dcc99686d4b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Finalize submission: copy best current blend to submission.csv with sanity checks\n",
        "import pandas as pd, os\n",
        "src = 'submission_bag_rew.csv'\n",
        "dst = 'submission.csv'\n",
        "assert os.path.exists(src), f\"Missing {src}\"\n",
        "sub = pd.read_csv(src)\n",
        "assert set(sub.columns)=={'essay_id','score'}, f\"Bad columns: {sub.columns}\"\n",
        "assert sub['score'].between(1,6).all(), 'Scores out of bounds 1..6'\n",
        "sub.to_csv(dst, index=False)\n",
        "print('Wrote submission.csv from', src, 'n=', len(sub), 'unique scores:', sorted(sub['score'].unique()))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv from submission_bag_rew.csv n= 1731 unique scores: [1, 2, 3, 4, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "id": "b750a8c0-9729-4473-b554-68b382964d41",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fast rebag variant: broader alpha for sL (0..1), wider w_deb grid (0.45..0.90), optional skip per-seed isotonic; write submission.csv\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "def qwk(y_true_int, y_pred_int):\n",
        "    return cohen_kappa_score(y_true_int, y_pred_int, weights='quadratic')\n",
        "\n",
        "def apply_thresholds(pred, th):\n",
        "    bins = [-np.inf] + list(th) + [np.inf]\n",
        "    return np.digitize(pred, bins)\n",
        "\n",
        "def optimize_thresholds_constrained(y_true, preds, iters=2, coarse_step=0.025, fine_step=0.005, base=None, max_delta=0.30):\n",
        "    base_th = np.array([1.5, 2.5, 3.5, 4.5, 5.5], dtype=float) if base is None else np.array(base, dtype=float)\n",
        "    th = base_th.copy()\n",
        "    best = qwk(y_true, apply_thresholds(preds, th))\n",
        "    for _ in range(iters):\n",
        "        for i in range(5):\n",
        "            lo = base_th[i] - max_delta; hi = base_th[i] + max_delta\n",
        "            if i>0: lo = max(lo, th[i-1] + 0.01)\n",
        "            if i<4: hi = min(hi, th[i+1] - 0.01)\n",
        "            for g in np.arange(lo, hi + 1e-9, coarse_step):\n",
        "                th_try = th.copy(); th_try[i] = g\n",
        "                score = qwk(y_true, apply_thresholds(preds, th_try))\n",
        "                if score > best:\n",
        "                    best, th[i] = score, g\n",
        "    for i in range(5):\n",
        "        lo = max(base_th[i] - max_delta, th[i] - coarse_step)\n",
        "        hi = min(base_th[i] + max_delta, th[i] + coarse_step)\n",
        "        if i>0: lo = max(lo, th[i-1] + 0.01)\n",
        "        if i<4: hi = min(hi, th[i+1] - 0.01)\n",
        "        for g in np.arange(lo, hi + 1e-9, fine_step):\n",
        "            th_try = th.copy(); th_try[i] = g\n",
        "            score = qwk(y_true, apply_thresholds(preds, th_try))\n",
        "            if score > best:\n",
        "                best, th[i] = score, g\n",
        "    return th, best\n",
        "\n",
        "def load_view(prefix):\n",
        "    oof = pd.read_csv(f'oof_deberta_base_{prefix}.csv')['oof_deberta'].values.astype(np.float32)\n",
        "    test = np.load(f'test_deberta_base_{prefix}.npy').astype(np.float32)\n",
        "    return oof, test\n",
        "\n",
        "def best_tta(o64, o128, oht, t64, t128, tht, y, valid_mask=None, prefer_large_grid=False):\n",
        "    grid = [\n",
        "        (0.70,0.30,0.00),(0.66,0.34,0.00),(0.60,0.40,0.00),(0.55,0.45,0.00),\n",
        "        (0.60,0.30,0.10),(0.55,0.35,0.10),(0.55,0.30,0.15),(0.50,0.40,0.10)\n",
        "    ]\n",
        "    if prefer_large_grid:\n",
        "        grid += [(0.75,0.25,0.00),(0.72,0.28,0.00),(0.65,0.25,0.10),(0.62,0.28,0.10)]\n",
        "    best = (-1.0, None, None, None)\n",
        "    base_bins = [-np.inf,1.5,2.5,3.5,4.5,5.5,np.inf]\n",
        "    for w64,w128,wht in grid:\n",
        "        oof_c = (w64*o64 + w128*o128 + wht*oht).astype(np.float32)\n",
        "        vm = valid_mask if valid_mask is not None else np.isfinite(oof_c)\n",
        "        q = qwk(y[vm], np.digitize(oof_c[vm], base_bins))\n",
        "        if q > best[0]:\n",
        "            test_c = (w64*t64 + w128*t128 + wht*tht).astype(np.float32)\n",
        "            best = (q, (w64,w128,wht), oof_c, test_c)\n",
        "    return best\n",
        "\n",
        "def per_seed_iso(oof_seed, test_seed, y, valid_mask=None):\n",
        "    vm = valid_mask if valid_mask is not None else np.isfinite(oof_seed)\n",
        "    if vm.sum() < 10:\n",
        "        return oof_seed, test_seed\n",
        "    iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "    iso.fit(np.clip(oof_seed[vm],1,6), y[vm])\n",
        "    return np.clip(iso.predict(np.clip(oof_seed,1,6)),1,6).astype(np.float32), np.clip(iso.predict(np.clip(test_seed,1,6)),1,6).astype(np.float32)\n",
        "\n",
        "t0=time.time()\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv'); folds = pd.read_csv('folds.csv')\n",
        "n_splits = int(folds['fold'].max()) + 1\n",
        "oof_cat = pd.read_csv('oof_cat.csv'); y = oof_cat['y'].astype(int).values\n",
        "pred_cat_oof = oof_cat['oof_cat'].values.astype(np.float32); test_cat = np.load('test_cat.npy').astype(np.float32)\n",
        "\n",
        "# Build seeds\n",
        "deb_oofs = []; deb_tests = []; cov_w = []; names = []; idx_sL = None\n",
        "def add_seed(o, t, name, folds_run):\n",
        "    deb_oofs.append(o.astype(np.float32)); deb_tests.append(t.astype(np.float32)); cov_w.append(folds_run/float(n_splits)); names.append(name)\n",
        "\n",
        "# s042 sw64 (5/5)\n",
        "if os.path.exists('oof_deberta_base_sw64.csv'):\n",
        "    o42 = pd.read_csv('oof_deberta_base_sw64.csv')['oof_deberta'].values.astype(np.float32)\n",
        "    t42 = np.load('test_deberta_base_sw64.npy').astype(np.float32)\n",
        "    add_seed(o42, t42, 's042_sw64', n_splits)\n",
        "\n",
        "# s777: prefer per-view and TTA search\n",
        "if all(os.path.exists(p) for p in ['oof_deberta_base_s777_sw64.csv','oof_deberta_base_s777_sw128.csv','oof_deberta_base_s777_ht.csv',\n",
        "                                   'test_deberta_base_s777_sw64.npy','test_deberta_base_s777_sw128.npy','test_deberta_base_s777_ht.npy']):\n",
        "    o64,t64 = load_view('s777_sw64'); o128,t128 = load_view('s777_sw128'); oht,tht = load_view('s777_ht')\n",
        "    q,w,oofc,tstc = best_tta(o64,o128,oht,t64,t128,tht,y, valid_mask=None, prefer_large_grid=False)\n",
        "    add_seed(oofc, tstc, f's777_{w}', n_splits)\n",
        "elif os.path.exists('oof_deberta_base_s777.csv'):\n",
        "    o777 = pd.read_csv('oof_deberta_base_s777.csv')['oof_deberta'].values.astype(np.float32)\n",
        "    t777 = np.load('test_deberta_base_s777.npy').astype(np.float32)\n",
        "    add_seed(o777, t777, 's777_comb', n_splits)\n",
        "\n",
        "# s2025: use combined (5/5)\n",
        "if os.path.exists('oof_deberta_base_s2025.csv'):\n",
        "    o2025 = pd.read_csv('oof_deberta_base_s2025.csv')['oof_deberta'].values.astype(np.float32)\n",
        "    t2025 = np.load('test_deberta_base_s2025.npy').astype(np.float32)\n",
        "    add_seed(o2025, t2025, 's2025_comb', n_splits)\n",
        "\n",
        "# sL: per-view with masked TTA; folds_run inferred by non-zero OOF\n",
        "if all(os.path.exists(p) for p in ['oof_deberta_base_sL_sw64.csv','oof_deberta_base_sL_sw128.csv','oof_deberta_base_sL_ht.csv',\n",
        "                                   'test_deberta_base_sL_sw64.npy','test_deberta_base_sL_sw128.npy','test_deberta_base_sL_ht.npy']):\n",
        "    o64L,t64L = load_view('sL_sw64'); o128L,t128L = load_view('sL_sw128'); ohtL,thtL = load_view('sL_ht')\n",
        "    maskL = (o64L != 0.0) & np.isfinite(o64L)\n",
        "    qL,wL,oofL,tstL = best_tta(o64L,o128L,ohtL,t64L,t128L,thtL,y, valid_mask=maskL, prefer_large_grid=True)\n",
        "    folds_run_L = 0\n",
        "    for f in range(n_splits):\n",
        "        va = folds.index[folds['fold']==f].to_numpy();\n",
        "        if np.isfinite(o64L[va]).mean() > 0.8 and (o64L[va]!=0).mean() > 0.8:\n",
        "            folds_run_L += 1\n",
        "    add_seed(oofL, tstL, f'sL_{wL}', folds_run_L); idx_sL = len(deb_oofs)-1\n",
        "\n",
        "assert len(deb_oofs)>0, 'No DeB seeds found'\n",
        "W = np.array(cov_w, dtype=np.float32); W = W/W.sum() if W.sum()>0 else np.ones(len(cov_w),dtype=np.float32)/len(cov_w)\n",
        "O = np.stack(deb_oofs, axis=1); T = np.stack(deb_tests, axis=1)\n",
        "A = np.isfinite(O).astype(np.float32)\n",
        "deb_oof_bag = (np.nansum(O * W[None,:], axis=1) / np.clip((A * W[None,:]).sum(axis=1), 1e-6, None)).astype(np.float32)\n",
        "deb_test_bag = (T * W[None,:]).sum(axis=1).astype(np.float32)\n",
        "\n",
        "# Optionally skip per-seed isotonic entirely (try both modes and pick best)\n",
        "def try_mode(apply_seed_iso: bool):\n",
        "    if apply_seed_iso:\n",
        "        O_iso = []; T_iso = []\n",
        "        for j in range(O.shape[1]):\n",
        "            vm = np.isfinite(O[:,j])\n",
        "            oj,tj = per_seed_iso(O[:,j].copy(), T[:,j].copy(), y, valid_mask=vm)\n",
        "            O_iso.append(oj); T_iso.append(tj)\n",
        "        Oa = np.stack(O_iso,axis=1); Ta = np.stack(T_iso,axis=1)\n",
        "        deb_o = (np.nansum(Oa * W[None,:], axis=1) / np.clip((np.isfinite(Oa).astype(np.float32) * W[None,:]).sum(axis=1), 1e-6, None)).astype(np.float32)\n",
        "        deb_t = (Ta * W[None,:]).sum(axis=1).astype(np.float32)\n",
        "    else:\n",
        "        deb_o, deb_t = deb_oof_bag, deb_test_bag\n",
        "\n",
        "    # sL alpha mix (if present) across 0..1\n",
        "    if idx_sL is not None:\n",
        "        base_cols = [i for i in range(O.shape[1]) if i != idx_sL]\n",
        "        Wb = W[base_cols]; Wb = Wb/Wb.sum() if Wb.sum()>0 else Wb\n",
        "        Ob = O[:, base_cols]; Tb = T[:, base_cols]\n",
        "        deb_o_base = (np.nansum(Ob * Wb[None,:], axis=1) / np.clip((np.isfinite(Ob).astype(np.float32)*Wb[None,:]).sum(axis=1),1e-6,None)).astype(np.float32)\n",
        "        deb_t_base = (Tb * Wb[None,:]).sum(axis=1).astype(np.float32)\n",
        "        sLo = O[:, idx_sL]; sLt = T[:, idx_sL]\n",
        "        best_local = (-1.0, None, None, None, None)  # q, a, w, th, iso_model\n",
        "        for a in np.arange(0.0, 1.0+1e-9, 0.02):\n",
        "            mix_o = a*sLo + (1.0-a)*deb_o_base\n",
        "            m = ~np.isfinite(sLo)\n",
        "            mix_o[m] = deb_o_base[m]\n",
        "            mix_t = a*sLt + (1.0-a)*deb_t_base\n",
        "            # blend with CatBoost over wide range\n",
        "            best = (-1.0, None, None, None)\n",
        "            for w in np.arange(0.45, 0.90+1e-9, 0.01):\n",
        "                blend_o = w*mix_o + (1.0-w)*pred_cat_oof\n",
        "                th_u, q_u = optimize_thresholds_constrained(y, blend_o, iters=2)\n",
        "                if q_u > best[0]: best = (q_u, w, th_u, None)\n",
        "                iso = IsotonicRegression(increasing=True, out_of_bounds='clip'); iso.fit(blend_o, y)\n",
        "                o_cal = iso.predict(blend_o).astype(np.float32)\n",
        "                th_i, q_i = optimize_thresholds_constrained(y, o_cal, iters=2)\n",
        "                if q_i > best[0]: best = (q_i, w, th_i, iso)\n",
        "            if best[0] > best_local[0]:\n",
        "                best_local = (best[0], a, best[1], best[2], best[3])\n",
        "        return best_local  # q, a, w, th, iso\n",
        "    else:\n",
        "        best = (-1.0, None, None, None)\n",
        "        for w in np.arange(0.45, 0.90+1e-9, 0.01):\n",
        "            blend_o = w*deb_o + (1.0-w)*pred_cat_oof\n",
        "            th_u, q_u = optimize_thresholds_constrained(y, blend_o, iters=2)\n",
        "            if q_u > best[0]: best = (q_u, w, th_u, None)\n",
        "            iso = IsotonicRegression(increasing=True, out_of_bounds='clip'); iso.fit(blend_o, y)\n",
        "            o_cal = iso.predict(blend_o).astype(np.float32)\n",
        "            th_i, q_i = optimize_thresholds_constrained(y, o_cal, iters=2)\n",
        "            if q_i > best[0]: best = (q_i, w, th_i, iso)\n",
        "        return (best[0], None, best[1], best[2], best[3])\n",
        "\n",
        "# Try with and without per-seed iso; pick best\n",
        "cand1 = try_mode(apply_seed_iso=True)\n",
        "cand2 = try_mode(apply_seed_iso=False)\n",
        "best = cand1 if cand1[0] >= cand2[0] else cand2\n",
        "best_q, best_a, best_w, best_th, best_iso = best\n",
        "print(f'[FAST-BAG] Best OOF={best_q:.5f} a_sL={best_a} w_deb={best_w:.3f} th={np.round(best_th,3)} iso={best_iso is not None}', flush=True)\n",
        "\n",
        "# Build final deb mix for test based on best setting\n",
        "if idx_sL is not None and best_a is not None:\n",
        "    base_cols = [i for i in range(O.shape[1]) if i != idx_sL]\n",
        "    Wb = W[base_cols]; Wb = Wb/Wb.sum() if Wb.sum()>0 else Wb\n",
        "    Ob = O[:, base_cols]; Tb = T[:, base_cols]\n",
        "    deb_o_base = (np.nansum(Ob * Wb[None,:], axis=1) / np.clip((np.isfinite(Ob).astype(np.float32)*Wb[None,:]).sum(axis=1),1e-6,None)).astype(np.float32)\n",
        "    deb_t_base = (Tb * Wb[None,:]).sum(axis=1).astype(np.float32)\n",
        "    sLo = O[:, idx_sL]; sLt = T[:, idx_sL]\n",
        "    deb_o_final = best_a*sLo + (1.0-best_a)*deb_o_base\n",
        "    m = ~np.isfinite(sLo); deb_o_final[m] = deb_o_base[m]\n",
        "    deb_t_final = best_a*sLt + (1.0-best_a)*deb_t_base\n",
        "else:\n",
        "    deb_o_final = deb_oof_bag; deb_t_final = deb_test_bag\n",
        "\n",
        "blend_test = (best_w*deb_t_final + (1.0-best_w)*test_cat).astype(np.float32)\n",
        "if best_iso is not None:\n",
        "    blend_oof_for_iso = (best_w*deb_o_final + (1.0-best_w)*pred_cat_oof).astype(np.float32)\n",
        "    iso_final = IsotonicRegression(increasing=True, out_of_bounds='clip'); iso_final.fit(blend_oof_for_iso, y)\n",
        "    blend_test = iso_final.predict(blend_test).astype(np.float32)\n",
        "blend_test = np.clip(blend_test, 1, 6).astype(np.float32)\n",
        "test_int = np.clip(apply_thresholds(blend_test, best_th), 1, 6).astype(int)\n",
        "pd.DataFrame({'essay_id': test['essay_id'], 'score': test_int}).to_csv('submission.csv', index=False)\n",
        "print('[FAST-BAG] Wrote submission.csv in %.1fs' % (time.time()-t0), flush=True)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 178\u001b[39m\n\u001b[32m    175\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (best[\u001b[32m0\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m, best[\u001b[32m1\u001b[39m], best[\u001b[32m2\u001b[39m], best[\u001b[32m3\u001b[39m])\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# Try with and without per-seed iso; pick best\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m cand1 = \u001b[43mtry_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapply_seed_iso\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m cand2 = try_mode(apply_seed_iso=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    180\u001b[39m best = cand1 \u001b[38;5;28;01mif\u001b[39;00m cand1[\u001b[32m0\u001b[39m] >= cand2[\u001b[32m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m cand2\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 160\u001b[39m, in \u001b[36mtry_mode\u001b[39m\u001b[34m(apply_seed_iso)\u001b[39m\n\u001b[32m    158\u001b[39m     iso = IsotonicRegression(increasing=\u001b[38;5;28;01mTrue\u001b[39;00m, out_of_bounds=\u001b[33m'\u001b[39m\u001b[33mclip\u001b[39m\u001b[33m'\u001b[39m); iso.fit(blend_o, y)\n\u001b[32m    159\u001b[39m     o_cal = iso.predict(blend_o).astype(np.float32)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     th_i, q_i = \u001b[43moptimize_thresholds_constrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo_cal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m q_i > best[\u001b[32m0\u001b[39m]: best = (q_i, w, th_i, iso)\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m best[\u001b[32m0\u001b[39m] > best_local[\u001b[32m0\u001b[39m]:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36moptimize_thresholds_constrained\u001b[39m\u001b[34m(y_true, preds, iters, coarse_step, fine_step, base, max_delta)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m np.arange(lo, hi + \u001b[32m1e-9\u001b[39m, coarse_step):\n\u001b[32m     23\u001b[39m     th_try = th.copy(); th_try[i] = g\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     score = \u001b[43mqwk\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_thresholds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mth_try\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m score > best:\n\u001b[32m     26\u001b[39m         best, th[i] = score, g\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mqwk\u001b[39m\u001b[34m(y_true_int, y_pred_int)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mqwk\u001b[39m(y_true_int, y_pred_int):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcohen_kappa_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquadratic\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py:213\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    209\u001b[39m         skip_parameter_validation=(\n\u001b[32m    210\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    211\u001b[39m         )\n\u001b[32m    212\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    215\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    216\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    217\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    219\u001b[39m     msg = re.sub(\n\u001b[32m    220\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    221\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    222\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    223\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:713\u001b[39m, in \u001b[36mcohen_kappa_score\u001b[39m\u001b[34m(y1, y2, labels, weights, sample_weight)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m    640\u001b[39m     {\n\u001b[32m    641\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my1\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    648\u001b[39m )\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcohen_kappa_score\u001b[39m(y1, y2, *, labels=\u001b[38;5;28;01mNone\u001b[39;00m, weights=\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    650\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Compute Cohen's kappa: a statistic that measures inter-annotator agreement.\u001b[39;00m\n\u001b[32m    651\u001b[39m \n\u001b[32m    652\u001b[39m \u001b[33;03m    This function computes Cohen's kappa [1]_, a score that expresses the level\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    711\u001b[39m \u001b[33;03m    np.float64(0.6875)\u001b[39;00m\n\u001b[32m    712\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     confusion = \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    714\u001b[39m     n_classes = confusion.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    715\u001b[39m     sum0 = np.sum(confusion, axis=\u001b[32m0\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py:186\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    184\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m func_sig = signature(func)\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:377\u001b[39m, in \u001b[36mconfusion_matrix\u001b[39m\u001b[34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[39m\n\u001b[32m    375\u001b[39m     label_to_ind = {y: x \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels)}\n\u001b[32m    376\u001b[39m     y_pred = np.array([label_to_ind.get(x, n_labels + \u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m y_pred])\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     y_true = np.array(\u001b[43m[\u001b[49m\u001b[43mlabel_to_ind\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_labels\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    379\u001b[39m \u001b[38;5;66;03m# intersect y_pred, y_true with labels, eliminate items not in labels\u001b[39;00m\n\u001b[32m    380\u001b[39m ind = np.logical_and(y_pred < n_labels, y_true < n_labels)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:377\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    375\u001b[39m     label_to_ind = {y: x \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels)}\n\u001b[32m    376\u001b[39m     y_pred = np.array([label_to_ind.get(x, n_labels + \u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m y_pred])\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     y_true = np.array([label_to_ind.get(x, n_labels + \u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m y_true])\n\u001b[32m    379\u001b[39m \u001b[38;5;66;03m# intersect y_pred, y_true with labels, eliminate items not in labels\u001b[39;00m\n\u001b[32m    380\u001b[39m ind = np.logical_and(y_pred < n_labels, y_true < n_labels)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}