{
  "cells": [
    {
      "id": "977c73f5-7eca-44c1-baa5-6d8ea3921eec",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: LMSYS Chatbot Arena Preference Prediction\n",
        "\n",
        "Goals:\n",
        "- Establish environment (GPU check) and robust baseline quickly\n",
        "- Build deterministic CV mirroring test\n",
        "- Fast baseline: text-only TF-IDF + linear/logistic models; add model/meta features\n",
        "- Iterate with calibrated models and blends (e.g., LR, Linear SVM, NB-SVM, XGBoost on sparse)\n",
        "- Cache features and OOF/test logits; error analysis loop\n",
        "\n",
        "Initial Milestones:\n",
        "1) Env check + data loading sanity\n",
        "2) EDA: target distribution, columns, text lengths, missingness\n",
        "3) CV protocol: StratifiedKFold on target with fixed seed; save folds\n",
        "4) Baseline v1: TF-IDF on prompts + responses; simple linear model, class_weight balanced\n",
        "5) Baseline v2: add engineered features (lengths, punctuation, toxicity/sentiment proxies if quick), per-position features to counter position bias\n",
        "6) Calibrate (Platt/isotonic) and blend diverse seeds/models\n",
        "7) Generate submission; iterate via OOF diagnostics\n",
        "\n",
        "Discipline:\n",
        "- Log timings per fold; cache sparse matrices\n",
        "- Fit transforms inside folds only; avoid leakage\n",
        "- Request expert review after baseline and major changes\n",
        "\n",
        "Next action: run environment check and peek at data heads."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "3943e981-1b7a-47d3-8c58-ee2a2ba92bc0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat(timespec='seconds')}Z] {msg}\", flush=True)\n",
        "\n",
        "# 1) GPU environment check\n",
        "log(\"Running nvidia-smi (GPU check)...\")\n",
        "try:\n",
        "    res = subprocess.run(['bash','-lc','nvidia-smi || true'], capture_output=True, text=True, timeout=30)\n",
        "    print(res.stdout)\n",
        "except Exception as e:\n",
        "    log(f\"nvidia-smi failed: {e}\")\n",
        "\n",
        "# 2) Load data heads and shapes\n",
        "log(\"Loading train.csv and test.csv heads...\")\n",
        "train_path = 'train.csv'\n",
        "test_path = 'test.csv'\\\n",
        "\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "log(f\"train.shape={train.shape} test.shape={test.shape}\")\n",
        "log(\"train columns:\")\n",
        "print(train.columns.tolist())\n",
        "log(\"test columns:\")\n",
        "print(test.columns.tolist())\n",
        "\n",
        "# 3) Inspect target distribution\n",
        "target_col = 'winner_model_b'\n",
        "if target_col in train.columns:\n",
        "    vc = train[target_col].value_counts(dropna=False)\n",
        "    vcn = train[target_col].value_counts(normalize=True, dropna=False)\n",
        "    log(\"Target counts:\")\n",
        "    print(vc)\n",
        "    log(\"Target fractions:\")\n",
        "    print(vcn)\n",
        "else:\n",
        "    log(f\"Target column {target_col} not found in train.csv\")\n",
        "\n",
        "# 4) Quick peek at text fields and lengths if present\n",
        "text_cols = [c for c in train.columns if train[c].dtype == 'object']\n",
        "log(f\"Detected object (likely text) columns: {text_cols[:10]}{'...' if len(text_cols)>10 else ''}\")\n",
        "for c in text_cols[:5]:\n",
        "    lens = train[c].fillna('').str.len()\n",
        "    log(f\"len({c}): mean={lens.mean():.1f} std={lens.std():.1f} min={lens.min()} p50={lens.median():.1f} p95={lens.quantile(0.95):.1f} max={lens.max()}\")\n",
        "\n",
        "log(\"Head(train):\")\n",
        "print(train.head(3))\n",
        "log(\"Head(test):\")\n",
        "print(test.head(3))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T20:34:51Z] Running nvidia-smi (GPU check)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep 24 20:34:51 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     128MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\n[2025-09-24T20:34:51Z] Loading train.csv and test.csv heads...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T20:34:52Z] train.shape=(51729, 9) test.shape=(5748, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T20:34:52Z] train columns:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie']\n[2025-09-24T20:34:52Z] test columns:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['id', 'prompt', 'response_a', 'response_b']\n[2025-09-24T20:34:52Z] Target counts:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "winner_model_b\n0    34094\n1    17635\nName: count, dtype: int64\n[2025-09-24T20:34:52Z] Target fractions:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "winner_model_b\n0    0.659089\n1    0.340911\nName: proportion, dtype: float64\n[2025-09-24T20:34:52Z] Detected object (likely text) columns: ['model_a', 'model_b', 'prompt', 'response_a', 'response_b']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T20:34:52Z] len(model_a): mean=14.4 std=4.7 min=6 p50=14.0 p95=24.0 max=30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T20:34:52Z] len(model_b): mean=14.4 std=4.7 min=6 p50=14.0 p95=25.0 max=30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T20:34:52Z] len(prompt): mean=367.0 std=1056.1 min=7 p50=96.0 p95=1472.0 max=33056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T20:34:52Z] len(response_a): mean=1377.2 std=1518.6 min=4 p50=1076.0 p95=3708.0 max=54058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T20:34:52Z] len(response_b): mean=1385.9 std=1546.9 min=4 p50=1083.0 p95=3696.0 max=53830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T20:34:52Z] Head(train):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           id             model_a             model_b  \\\n0  2444074745      zephyr-7b-beta     llama-2-7b-chat   \n1  1805535695  gpt-3.5-turbo-0613    llama-2-13b-chat   \n2  2454781969    claude-instant-1  gpt-4-0125-preview   \n\n                                              prompt  \\\n0  [\"Can the Orca Cloud Security Platform detect ...   \n1  [\"Write 3 sensational twists for a thriller \",...   \n2  [\"Create some creatively mocking sentences abo...   \n\n                                          response_a  \\\n0  [\"Yes, the Orca Cloud Security Platform can de...   \n1  [\"1. The Protagonist's Best Friend is the Mast...   \n2  [\"I apologize, upon further reflection I don't...   \n\n                                          response_b  winner_model_a  \\\n0  [\"Yes, the Orca Cloud Security Platform can de...               0   \n1  [\"Sure, here are three sensational twists for ...               1   \n2  [\"Sure! Just remember, this is all in good fun...               0   \n\n   winner_model_b  winner_tie  \n0               1           0  \n1               0           0  \n2               1           0  \n[2025-09-24T20:34:52Z] Head(test):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           id                                             prompt  \\\n0  3297560222  [\"What can you tell me about Maarten van Vulpe...   \n1  2556155375  [\"is cebu island a good place to travel to in ...   \n2  1793939629  [\"Hi, we've been trying to reach you about you...   \n\n                                          response_a  \\\n0  [\"Maarten van Vulpen (also spelled Marten or M...   \n1  [\"Yes, Cebu Island is a great place to visit i...   \n2  [\"I'm glad you reached out, but I must let you...   \n\n                                          response_b  \n0  [\"Maarten van Vulpen is not a widely known pub...  \n1  [\"Cebu Island can be a good place to travel to...  \n2  [\"\\\"I understand that you're calling about my ...  \n"
          ]
        }
      ]
    },
    {
      "id": "2b4a0499-f904-4b0f-9002-b1de059af328",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Confirm submission format and prepare 3-class target + groups\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from hashlib import blake2b\n",
        "\n",
        "def norm_prompt(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    # lightweight normalization: strip brackets/quotes often present in dataset, lowercase, collapse spaces\n",
        "    t = s.strip()\n",
        "    if t.startswith('[\"') and t.endswith('\"]'):\n",
        "        t = t[2:-2]\n",
        "    t = t.replace('\\n', ' ').replace('\\r', ' ')\n",
        "    t = ' '.join(t.split())\n",
        "    return t.lower()\n",
        "\n",
        "def hhash(*parts: str, nbytes: int = 8) -> int:\n",
        "    h = blake2b(digest_size=nbytes)\n",
        "    for p in parts:\n",
        "        if p is None:\n",
        "            p = ''\n",
        "        if not isinstance(p, str):\n",
        "            p = str(p)\n",
        "        h.update(p.encode('utf-8', errors='ignore'))\n",
        "        h.update(b'|')\n",
        "    return int.from_bytes(h.digest(), 'little', signed=False)\n",
        "\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "print('sample_submission.columns:', sample_sub.columns.tolist())\n",
        "assert ['id','winner_model_a','winner_model_b','winner_tie'] == sample_sub.columns.tolist(), 'Unexpected submission columns order'\n",
        "\n",
        "# Build 3-class labels: 0=A wins, 1=B wins, 2=Tie\n",
        "train = pd.read_csv('train.csv')\n",
        "y_cols = ['winner_model_a','winner_model_b','winner_tie']\n",
        "y_mat = train[y_cols].values.astype(int)\n",
        "y = y_mat.argmax(axis=1)\n",
        "cls_counts = pd.Series(y).value_counts().sort_index()\n",
        "print('3-class counts (A,B,Tie):', cls_counts.to_dict())\n",
        "print('3-class fractions:', (cls_counts/len(y)).round(4).to_dict())\n",
        "\n",
        "# Grouping by prompt (normalized). For swap-aug later, we'll group by unordered pair; for now, prompt groups:\n",
        "prompt_norm = train['prompt'].map(norm_prompt)\n",
        "groups_prompt = prompt_norm.map(lambda s: hhash(s))\n",
        "print('Unique prompt groups:', groups_prompt.nunique(), 'rows:', len(groups_prompt))\n",
        "\n",
        "# Basic sanity: no model name features in test; confirm absence\n",
        "test = pd.read_csv('test.csv')\n",
        "print('Test has model_a/model_b?', {'model_a' in test.columns, 'model_b' in test.columns})\n",
        "\n",
        "# Save quick artifacts for next steps (in-memory here, will rebuild in train pipeline)\n",
        "del sample_sub"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_submission.columns: ['id', 'winner_model_a', 'winner_model_b', 'winner_tie']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3-class counts (A,B,Tie): {0: 18074, 1: 17635, 2: 16020}\n3-class fractions: {0: 0.3494, 1: 0.3409, 2: 0.3097}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique prompt groups: 46580 rows: 51729\nTest has model_a/model_b? {False}\n"
          ]
        }
      ]
    },
    {
      "id": "8f5068e4-27f1-4279-a633-2445ad7b3731",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Baseline v2: faster TF-IDF diffs + extra scalars + multinomial LR with StratifiedGroupKFold\n",
        "import re, time\n",
        "from time import perf_counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from hashlib import blake2b\n",
        "from scipy import sparse as sp\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "def log(msg):\n",
        "    from datetime import datetime\n",
        "    print(f\"[{datetime.utcnow().isoformat(timespec='seconds')}Z] {msg}\", flush=True)\n",
        "\n",
        "def norm_prompt(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    t = s.strip()\n",
        "    if t.startswith('[\"') and t.endswith('\"]'):\n",
        "        t = t[2:-2]\n",
        "    t = t.replace('\\n', ' ').replace('\\r', ' ')\n",
        "    t = ' '.join(t.split())\n",
        "    return t.lower()\n",
        "\n",
        "def hhash(*parts: str, nbytes: int = 8) -> int:\n",
        "    h = blake2b(digest_size=nbytes)\n",
        "    for p in parts:\n",
        "        if p is None:\n",
        "            p = ''\n",
        "        if not isinstance(p, str):\n",
        "            p = str(p)\n",
        "        h.update(p.encode('utf-8', errors='ignore'))\n",
        "        h.update(b'|')\n",
        "    return int.from_bytes(h.digest(), 'little', signed=False)\n",
        "\n",
        "def truncate_head_tail(s: str, head: int = 4000, tail: int = 1000) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    if len(s) <= head + tail:\n",
        "        return s\n",
        "    return s[:head] + s[-tail:]\n",
        "\n",
        "# Regexes for numeric/stylometry counts\n",
        "re_url = re.compile(r'https?://|www\\.')\n",
        "re_listline = re.compile(r'(?m)^(?:\\s*[-*\\u2022])')\n",
        "re_digit = re.compile(r'\\d')\n",
        "re_codefence = re.compile(r'```')\n",
        "re_quote = re.compile(r'\"|\\u201c|\\u201d|\\'')\n",
        "re_refusal = re.compile(r\"\\b(i\\s+cannot|i\\s+can\\'t|i\\s+cant|sorry|apologize|unable|policy|safety|as an ai)\\b\", re.I)\n",
        "\n",
        "FEATS = ['loglen_char','loglen_word','url','newline','qmark','exclam','listmark','digit','code','quote','refusal']\n",
        "\n",
        "def basic_counts(s: str):\n",
        "    if not isinstance(s, str):\n",
        "        s = ''\n",
        "    return {\n",
        "        'loglen_char': np.log1p(len(s)),\n",
        "        'loglen_word': np.log1p(len(s.split())),\n",
        "        'url': len(re_url.findall(s)),\n",
        "        'newline': s.count('\\n'),\n",
        "        'qmark': s.count('?'),\n",
        "        'exclam': s.count('!'),\n",
        "        'listmark': len(re_listline.findall(s)),\n",
        "        'digit': len(re_digit.findall(s)),\n",
        "        'code': len(re_codefence.findall(s)),\n",
        "        'quote': len(re_quote.findall(s)),\n",
        "        'refusal': len(re_refusal.findall(s)),\n",
        "    }\n",
        "\n",
        "def counts_array(texts):\n",
        "    n = len(texts)\n",
        "    M = np.zeros((n, len(FEATS)), dtype=np.float32)\n",
        "    for i, s in enumerate(texts):\n",
        "        c = basic_counts(s)\n",
        "        for j, f in enumerate(FEATS):\n",
        "            M[i, j] = c[f]\n",
        "    return M\n",
        "\n",
        "def cosine_rows(X, Y):\n",
        "    # Inputs are L2-normalized TF-IDF; cosine = dot product\n",
        "    return np.asarray(X.multiply(Y).sum(axis=1)).ravel()\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# 3-class labels\n",
        "y_cols = ['winner_model_a','winner_model_b','winner_tie']\n",
        "y = train[y_cols].values.argmax(axis=1)\n",
        "\n",
        "# Preprocess texts (truncate head+tail)\n",
        "prompt_tr = train['prompt'].astype(str).map(truncate_head_tail)\n",
        "pa_tr = train['response_a'].astype(str).map(truncate_head_tail)\n",
        "pb_tr = train['response_b'].astype(str).map(truncate_head_tail)\n",
        "prompt_tr_te = test['prompt'].astype(str).map(truncate_head_tail)\n",
        "ra_tr_te = test['response_a'].astype(str).map(truncate_head_tail)\n",
        "rb_tr_te = test['response_b'].astype(str).map(truncate_head_tail)\n",
        "\n",
        "# Precompute numeric/stylometry counts once (A and B) for train and test\n",
        "t_counts0 = perf_counter()\n",
        "A_counts = counts_array(pa_tr.tolist())  # (n_train, k)\n",
        "B_counts = counts_array(pb_tr.tolist())\n",
        "A_counts_te = counts_array(ra_tr_te.tolist())\n",
        "B_counts_te = counts_array(rb_tr_te.tolist())\n",
        "log(f\"Precomputed counts in {perf_counter()-t_counts0:.1f}s\")\n",
        "\n",
        "# Groups by normalized prompt\n",
        "groups = train['prompt'].map(norm_prompt).map(lambda s: hhash(s))\n",
        "\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof = np.zeros((len(train), 3), dtype=np.float32)\n",
        "test_pred = np.zeros((len(test), 3), dtype=np.float32)\n",
        "\n",
        "start_all = perf_counter()\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv.split(train, y, groups=groups)):\n",
        "    t0 = perf_counter()\n",
        "    log(f\"Fold {fold} start: tr={len(tr_idx)} va={len(va_idx)}\")\n",
        "    resp_tr_corpus = pd.concat([pa_tr.iloc[tr_idx], pb_tr.iloc[tr_idx]], axis=0).tolist()\n",
        "\n",
        "    # TF-IDF vectorizers (reduced caps for speed) fit on train-fold responses\n",
        "    t_vec = perf_counter()\n",
        "    tfidf_char = TfidfVectorizer(analyzer='char', ngram_range=(3,6), min_df=5, max_features=200000,\n",
        "                                 sublinear_tf=True, dtype=np.float32, norm='l2')\n",
        "    _ = tfidf_char.fit_transform(resp_tr_corpus)\n",
        "    tfidf_word = TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=5, max_features=120000,\n",
        "                                 sublinear_tf=True, dtype=np.float32, lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\", norm='l2')\n",
        "    _ = tfidf_word.fit_transform(resp_tr_corpus)\n",
        "    log(f\"Fold {fold} vectorizers fit in {perf_counter()-t_vec:.1f}s\")\n",
        "\n",
        "    # Transform A/B train and valid\n",
        "    Xa_c_tr = tfidf_char.transform(pa_tr.iloc[tr_idx])\n",
        "    Xb_c_tr = tfidf_char.transform(pb_tr.iloc[tr_idx])\n",
        "    Xa_w_tr = tfidf_word.transform(pa_tr.iloc[tr_idx])\n",
        "    Xb_w_tr = tfidf_word.transform(pb_tr.iloc[tr_idx])\n",
        "    Xa_c_va = tfidf_char.transform(pa_tr.iloc[va_idx])\n",
        "    Xb_c_va = tfidf_char.transform(pb_tr.iloc[va_idx])\n",
        "    Xa_w_va = tfidf_word.transform(pa_tr.iloc[va_idx])\n",
        "    Xb_w_va = tfidf_word.transform(pb_tr.iloc[va_idx])\n",
        "\n",
        "    # Prompt sims using word TF-IDF\n",
        "    Xp_w_tr = tfidf_word.transform(prompt_tr.iloc[tr_idx])\n",
        "    Xp_w_va = tfidf_word.transform(prompt_tr.iloc[va_idx])\n",
        "    sim_b_tr = cosine_rows(Xp_w_tr, Xb_w_tr)\n",
        "    sim_a_tr = cosine_rows(Xp_w_tr, Xa_w_tr)\n",
        "    sim_b_va = cosine_rows(Xp_w_va, Xb_w_va)\n",
        "    sim_a_va = cosine_rows(Xp_w_va, Xa_w_va)\n",
        "    sim_diff_tr = sp.csr_matrix((sim_b_tr - sim_a_tr).reshape(-1,1))\n",
        "    sim_diff_va = sp.csr_matrix((sim_b_va - sim_a_va).reshape(-1,1))\n",
        "\n",
        "    # Response-to-response similarity (symmetric, tie-friendly) using word TF-IDF\n",
        "    cos_ab_tr = sp.csr_matrix(cosine_rows(Xa_w_tr, Xb_w_tr).reshape(-1,1))\n",
        "    cos_ab_va = sp.csr_matrix(cosine_rows(Xa_w_va, Xb_w_va).reshape(-1,1))\n",
        "\n",
        "    # Numeric diffs: diff, abs diff, and sum (all cheap scalars)\n",
        "    A_tr = A_counts[tr_idx]; B_tr = B_counts[tr_idx]\n",
        "    A_va = A_counts[va_idx]; B_va = B_counts[va_idx]\n",
        "    diff_tr = (B_tr - A_tr).astype(np.float32)\n",
        "    diff_va = (B_va - A_va).astype(np.float32)\n",
        "    adiff_tr = np.abs(diff_tr).astype(np.float32)\n",
        "    adiff_va = np.abs(diff_va).astype(np.float32)\n",
        "    sum_tr = (A_tr + B_tr).astype(np.float32)\n",
        "    sum_va = (A_va + B_va).astype(np.float32)\n",
        "    num_tr = sp.csr_matrix(np.hstack([diff_tr, adiff_tr, sum_tr]))\n",
        "    num_va = sp.csr_matrix(np.hstack([diff_va, adiff_va, sum_va]))\n",
        "\n",
        "    # Final sparse stacks: anti-symmetric TF-IDF diffs + sims + numeric blocks\n",
        "    X_tr = sp.hstack([Xb_c_tr - Xa_c_tr, Xb_w_tr - Xa_w_tr, sim_diff_tr, cos_ab_tr, num_tr], format='csr')\n",
        "    X_va = sp.hstack([Xb_c_va - Xa_c_va, Xb_w_va - Xa_w_va, sim_diff_va, cos_ab_va, num_va], format='csr')\n",
        "\n",
        "    # Model (faster tol)\n",
        "    t_fit = perf_counter()\n",
        "    clf = LogisticRegression(multi_class='multinomial', solver='saga', C=2.0, max_iter=1000, tol=1e-3, n_jobs=-1, verbose=0)\n",
        "    clf.fit(X_tr, y[tr_idx])\n",
        "    log(f\"Fold {fold} model fit in {perf_counter()-t_fit:.1f}s\")\n",
        "    oof_fold = clf.predict_proba(X_va).astype(np.float32)\n",
        "    oof[va_idx] = oof_fold\n",
        "    ll = log_loss(y[va_idx], oof_fold, labels=[0,1,2])\n",
        "    log(f\"Fold {fold} logloss={ll:.5f} elapsed={perf_counter()-t0:.1f}s\")\n",
        "\n",
        "    # Test transform and predict for this fold\n",
        "    Xa_c_te = tfidf_char.transform(ra_tr_te)\n",
        "    Xb_c_te = tfidf_char.transform(rb_tr_te)\n",
        "    Xa_w_te = tfidf_word.transform(ra_tr_te)\n",
        "    Xb_w_te = tfidf_word.transform(rb_tr_te)\n",
        "    Xp_w_te = tfidf_word.transform(prompt_tr_te)\n",
        "    sim_b_te = cosine_rows(Xp_w_te, Xb_w_te)\n",
        "    sim_a_te = cosine_rows(Xp_w_te, Xa_w_te)\n",
        "    sim_diff_te = sp.csr_matrix((sim_b_te - sim_a_te).reshape(-1,1))\n",
        "    cos_ab_te = sp.csr_matrix(cosine_rows(Xa_w_te, Xb_w_te).reshape(-1,1))\n",
        "    diff_te = (B_counts_te - A_counts_te).astype(np.float32)\n",
        "    adiff_te = np.abs(diff_te).astype(np.float32)\n",
        "    sum_te = (A_counts_te + B_counts_te).astype(np.float32)\n",
        "    num_te = sp.csr_matrix(np.hstack([diff_te, adiff_te, sum_te]))\n",
        "    X_te = sp.hstack([Xb_c_te - Xa_c_te, Xb_w_te - Xa_w_te, sim_diff_te, cos_ab_te, num_te], format='csr')\n",
        "    test_pred += clf.predict_proba(X_te).astype(np.float32) / cv.n_splits\n",
        "\n",
        "# OOF logloss\n",
        "oof_ll = log_loss(y, oof, labels=[0,1,2])\n",
        "log(f\"OOF logloss={oof_ll:.5f}; total elapsed={perf_counter()-start_all:.1f}s\")\n",
        "\n",
        "# Build submission\n",
        "sub = pd.DataFrame({\n",
        "    'id': test['id'].values,\n",
        "    'winner_model_a': test_pred[:,0],\n",
        "    'winner_model_b': test_pred[:,1],\n",
        "    'winner_tie': test_pred[:,2],\n",
        "})\n",
        "# Probability hygiene: clip and renormalize\n",
        "eps = 1e-15\n",
        "probs = sub[['winner_model_a','winner_model_b','winner_tie']].values\n",
        "probs = np.clip(probs, eps, 1 - eps)\n",
        "probs /= probs.sum(axis=1, keepdims=True)\n",
        "sub[['winner_model_a','winner_model_b','winner_tie']] = probs\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "log('Wrote submission.csv')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T21:17:35Z] Precomputed counts in 9.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T21:17:44Z] Fold 0 start: tr=41174 va=10555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T21:20:37Z] Fold 0 vectorizers fit in 172.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T21:48:29Z] Fold 0 model fit in 1470.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T21:48:29Z] Fold 0 logloss=1.07755 elapsed=1845.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T21:48:52Z] Fold 1 start: tr=41276 va=10453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T21:51:47Z] Fold 1 vectorizers fit in 174.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T22:19:37Z] Fold 1 model fit in 1469.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T22:19:37Z] Fold 1 logloss=1.07522 elapsed=1845.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T22:20:00Z] Fold 2 start: tr=41451 va=10278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T22:22:53Z] Fold 2 vectorizers fit in 173.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T22:50:24Z] Fold 2 model fit in 1451.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T22:50:24Z] Fold 2 logloss=1.07808 elapsed=1824.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T22:50:47Z] Fold 3 start: tr=41598 va=10131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T22:53:44Z] Fold 3 vectorizers fit in 176.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T23:18:56Z] Fold 3 model fit in 1311.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T23:18:56Z] Fold 3 logloss=1.07603 elapsed=1688.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T23:19:18Z] Fold 4 start: tr=41417 va=10312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T23:22:14Z] Fold 4 vectorizers fit in 175.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T23:47:33Z] Fold 4 model fit in 1318.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T23:47:33Z] Fold 4 logloss=1.07832 elapsed=1694.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T23:47:55Z] OOF logloss=1.07704; total elapsed=9020.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-24T23:47:55Z] Wrote submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "14159a80-e9cd-4079-89f0-0b14a9bb7759",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Post-hoc multiclass temperature scaling calibration on OOF; apply to test_pred and rewrite submission\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "def apply_temp_scaling(probs: np.ndarray, T: float) -> np.ndarray:\n",
        "    # Prob-power temperature scaling: p_i^(1/T) and renormalize per row\n",
        "    eps = 1e-15\n",
        "    P = np.clip(probs, eps, 1 - eps).astype(np.float64)\n",
        "    P_pow = np.power(P, 1.0 / max(T, 1e-6))\n",
        "    P_pow /= P_pow.sum(axis=1, keepdims=True)\n",
        "    return P_pow.astype(np.float32)\n",
        "\n",
        "def find_best_T(oof_probs: np.ndarray, y_true: np.ndarray) -> float:\n",
        "    # Optimize T > 0 by 1D search on log T\n",
        "    def nll_from_logT(logT: float) -> float:\n",
        "        T = float(np.exp(logT))\n",
        "        P = apply_temp_scaling(oof_probs, T)\n",
        "        return log_loss(y_true, P, labels=[0,1,2])\n",
        "    # Coarse grid over logT in [-2.0, 2.0]\n",
        "    grid = np.linspace(-2.0, 2.0, 41)\n",
        "    vals = [nll_from_logT(g) for g in grid]\n",
        "    best_idx = int(np.argmin(vals))\n",
        "    best_logT = grid[best_idx]\n",
        "    best_val = vals[best_idx]\n",
        "    # Local refine around best\n",
        "    for _ in range(3):\n",
        "        lo = max(-5.0, best_logT - 0.5)\n",
        "        hi = min(5.0, best_logT + 0.5)\n",
        "        grid = np.linspace(lo, hi, 21)\n",
        "        vals = [nll_from_logT(g) for g in grid]\n",
        "        best_idx = int(np.argmin(vals))\n",
        "        best_logT = grid[best_idx]\n",
        "        best_val = vals[best_idx]\n",
        "    return float(np.exp(best_logT))\n",
        "\n",
        "assert 'oof' in globals() and 'y' in globals() and 'test_pred' in globals(), 'Run training cell first to define oof, y, test_pred'\n",
        "base_oof_ll = log_loss(y, oof, labels=[0,1,2])\n",
        "print(f'Base OOF logloss (uncalibrated): {base_oof_ll:.6f}')\n",
        "T_opt = find_best_T(oof, y)\n",
        "print(f'Optimal temperature T: {T_opt:.4f}')\n",
        "oof_cal = apply_temp_scaling(oof, T_opt)\n",
        "cal_oof_ll = log_loss(y, oof_cal, labels=[0,1,2])\n",
        "print(f'Calibrated OOF logloss: {cal_oof_ll:.6f}')\n",
        "\n",
        "# Apply to test_pred and rewrite submission.csv\n",
        "test_cal = apply_temp_scaling(test_pred, T_opt)\n",
        "sub = pd.DataFrame({\n",
        "    'id': pd.read_csv('test.csv')['id'].values,\n",
        "    'winner_model_a': test_cal[:,0],\n",
        "    'winner_model_b': test_cal[:,1],\n",
        "    'winner_tie': test_cal[:,2],\n",
        "})\n",
        "eps = 1e-15\n",
        "probs = sub[['winner_model_a','winner_model_b','winner_tie']].values\n",
        "probs = np.clip(probs, eps, 1 - eps)\n",
        "probs /= probs.sum(axis=1, keepdims=True)\n",
        "sub[['winner_model_a','winner_model_b','winner_tie']] = probs\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Rewrote submission.csv with temperature-scaled probabilities')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base OOF logloss (uncalibrated): 1.077041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal temperature T: 0.7047\nCalibrated OOF logloss: 1.075130\nRewrote submission.csv with temperature-scaled probabilities\n"
          ]
        }
      ]
    },
    {
      "id": "44ce31f4-6a64-43a0-9708-9d853058dffc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Baseline v3: Faster SGDClassifier, richer features, per-fold isotonic calibration\n",
        "import json, re, time\n",
        "from time import perf_counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from hashlib import blake2b\n",
        "from scipy import sparse as sp\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.calibration import IsotonicRegression\n",
        "\n",
        "def log(msg):\n",
        "    from datetime import datetime\n",
        "    print(f\"[{datetime.utcnow().isoformat(timespec='seconds')}Z] {msg}\", flush=True)\n",
        "\n",
        "def norm_prompt_for_group(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    t = s.strip()\n",
        "    # Try JSON list parsing (multi-turn); fallback to simple cleanup\n",
        "    try:\n",
        "        obj = json.loads(t)\n",
        "        if isinstance(obj, list):\n",
        "            t = ' [TURN] '.join(map(str, obj))\n",
        "        elif isinstance(obj, str):\n",
        "            t = obj\n",
        "    except Exception:\n",
        "        pass\n",
        "    t = t.replace('\\r', ' ').replace('\\n', ' ')\n",
        "    t = ' '.join(t.split())\n",
        "    return t.lower()\n",
        "\n",
        "def hhash(*parts: str, nbytes: int = 8) -> int:\n",
        "    h = blake2b(digest_size=nbytes)\n",
        "    for p in parts:\n",
        "        if p is None:\n",
        "            p = ''\n",
        "        if not isinstance(p, str):\n",
        "            p = str(p)\n",
        "        h.update(p.encode('utf-8', errors='ignore'))\n",
        "        h.update(b'|')\n",
        "    return int.from_bytes(h.digest(), 'little', signed=False)\n",
        "\n",
        "def truncate_head_tail(s: str, head: int = 4000, tail: int = 1000) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    if len(s) <= head + tail:\n",
        "        return s\n",
        "    return s[:head] + s[-tail:]\n",
        "\n",
        "# Regexes and counters\n",
        "re_url = re.compile(r'https?://|www\\.')\n",
        "re_listline = re.compile(r'(?m)^(?:\\s*[-*\\u2022])')\n",
        "re_digit = re.compile(r'\\d')\n",
        "re_codefence = re.compile(r'```')\n",
        "re_quote = re.compile(r'\"|\\u201c|\\u201d|\\'')\n",
        "re_refusal = re.compile(r\"\\b(i\\s+cannot|i\\s+can\\'t|i\\s+cant|sorry|apologize|unable|policy|safety|as an ai)\\b\", re.I)\n",
        "re_letter = re.compile(r'[A-Za-z]')\n",
        "re_upper = re.compile(r'[A-Z]')\n",
        "re_punct = re.compile(r'[\\!\\?\\.,;:\\-\\(\\)\\[\\]\\{\\}\\\"\\'\\`\\~\\/\\\\]')\n",
        "\n",
        "FEATS = ['loglen_char','loglen_word','url','newline','qmark','exclam','listmark','digit','code','quote','refusal',\n",
        "         'letters','uppers','punct']\n",
        "\n",
        "def basic_counts(s: str):\n",
        "    if not isinstance(s, str):\n",
        "        s = ''\n",
        "    letters = len(re_letter.findall(s))\n",
        "    uppers = len(re_upper.findall(s))\n",
        "    punct = len(re_punct.findall(s))\n",
        "    return {\n",
        "        'loglen_char': np.log1p(len(s)),\n",
        "        'loglen_word': np.log1p(len(s.split())),\n",
        "        'url': len(re_url.findall(s)),\n",
        "        'newline': s.count('\\n'),\n",
        "        'qmark': s.count('?'),\n",
        "        'exclam': s.count('!'),\n",
        "        'listmark': len(re_listline.findall(s)),\n",
        "        'digit': len(re_digit.findall(s)),\n",
        "        'code': len(re_codefence.findall(s)),\n",
        "        'quote': len(re_quote.findall(s)),\n",
        "        'refusal': len(re_refusal.findall(s)),\n",
        "        'letters': letters,\n",
        "        'uppers': uppers,\n",
        "        'punct': punct,\n",
        "    }\n",
        "\n",
        "def counts_array(texts):\n",
        "    n = len(texts)\n",
        "    M = np.zeros((n, len(FEATS)), dtype=np.float32)\n",
        "    for i, s in enumerate(texts):\n",
        "        c = basic_counts(s)\n",
        "        for j, f in enumerate(FEATS):\n",
        "            M[i, j] = c[f]\n",
        "    return M\n",
        "\n",
        "def cosine_rows(X, Y):\n",
        "    return np.asarray(X.multiply(Y).sum(axis=1)).ravel()\n",
        "\n",
        "def clean_proba(P):\n",
        "    P = np.asarray(P, dtype=np.float64)\n",
        "    P = np.nan_to_num(P, nan=1.0/3.0, posinf=1.0/3.0, neginf=1e-15)\n",
        "    P = np.clip(P, 0.0, 1.0)\n",
        "    rs = P.sum(axis=1, keepdims=True)\n",
        "    rs[rs == 0] = 1.0\n",
        "    P = P / rs\n",
        "    return P\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "y_cols = ['winner_model_a','winner_model_b','winner_tie']\n",
        "y = train[y_cols].values.argmax(axis=1)\n",
        "\n",
        "# Text preprocessing\n",
        "prompt_tr = train['prompt'].astype(str).map(truncate_head_tail)\n",
        "pa_tr = train['response_a'].astype(str).map(truncate_head_tail)\n",
        "pb_tr = train['response_b'].astype(str).map(truncate_head_tail)\n",
        "prompt_tr_te = test['prompt'].astype(str).map(truncate_head_tail)\n",
        "ra_tr_te = test['response_a'].astype(str).map(truncate_head_tail)\n",
        "rb_tr_te = test['response_b'].astype(str).map(truncate_head_tail)\n",
        "\n",
        "# Precompute counts and simple ratios\n",
        "t0 = perf_counter()\n",
        "A_counts = counts_array(pa_tr.tolist())\n",
        "B_counts = counts_array(pb_tr.tolist())\n",
        "A_counts_te = counts_array(ra_tr_te.tolist())\n",
        "B_counts_te = counts_array(rb_tr_te.tolist())\n",
        "log(f\"Counts computed in {perf_counter()-t0:.1f}s\")\n",
        "\n",
        "def build_scalar_blocks(Ac, Bc):\n",
        "    # diff, abs diff, sum as before\n",
        "    diff = (Bc - Ac).astype(np.float32)\n",
        "    adiff = np.abs(diff).astype(np.float32)\n",
        "    summ = (Ac + Bc).astype(np.float32)\n",
        "    # ratios (avoid div-by-zero)\n",
        "    eps = 1e-6\n",
        "    # len ratios using first two features: loglen_char/loglen_word are logs; better use raw letters/word counts\n",
        "    letters_A = Ac[:, FEATS.index('letters')]; letters_B = Bc[:, FEATS.index('letters')]\n",
        "    words_A = np.expm1(Ac[:, FEATS.index('loglen_word')]); words_B = np.expm1(Bc[:, FEATS.index('loglen_word')])\n",
        "    len_ratio = ((letters_B + eps) / (letters_A + eps)).reshape(-1,1).astype(np.float32)\n",
        "    word_ratio = ((words_B + eps) / (words_A + eps)).reshape(-1,1).astype(np.float32)\n",
        "    # caps ratio and punct density\n",
        "    upp_A = Ac[:, FEATS.index('uppers')]; upp_B = Bc[:, FEATS.index('uppers')]\n",
        "    let_A = letters_A; let_B = letters_B\n",
        "    caps_ratio_A = ((upp_A + eps)/(let_A + eps)).reshape(-1,1)\n",
        "    caps_ratio_B = ((upp_B + eps)/(let_B + eps)).reshape(-1,1)\n",
        "    caps_diff = (caps_ratio_B - caps_ratio_A).astype(np.float32)\n",
        "    caps_adiff = np.abs(caps_diff).astype(np.float32)\n",
        "    punct_A = Ac[:, FEATS.index('punct')]; punct_B = Bc[:, FEATS.index('punct')]\n",
        "    # use char length approx from letters + punct + digits (+ others ignored)\n",
        "    approx_len_A = letters_A + punct_A + Ac[:, FEATS.index('digit')] + 1.0\n",
        "    approx_len_B = letters_B + punct_B + Bc[:, FEATS.index('digit')] + 1.0\n",
        "    pden_A = (punct_A / approx_len_A).reshape(-1,1)\n",
        "    pden_B = (punct_B / approx_len_B).reshape(-1,1)\n",
        "    pden_diff = (pden_B - pden_A).astype(np.float32)\n",
        "    pden_adiff = np.abs(pden_diff).astype(np.float32)\n",
        "    ratios = np.hstack([len_ratio, word_ratio, caps_diff, caps_adiff, pden_diff, pden_adiff]).astype(np.float32)\n",
        "    return sp.csr_matrix(np.hstack([diff, adiff, summ, ratios]))\n",
        "\n",
        "# Grouping by robust prompt parse\n",
        "groups = train['prompt'].map(norm_prompt_for_group).map(lambda s: hhash(s))\n",
        "\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof = np.zeros((len(train), 3), dtype=np.float32)\n",
        "test_pred = np.zeros((len(test), 3), dtype=np.float32)\n",
        "\n",
        "start_all = perf_counter()\n",
        "for fold, (tr_idx, va_idx) in enumerate(cv.split(train, y, groups=groups)):\n",
        "    t_fold = perf_counter()\n",
        "    log(f\"Fold {fold} start tr={len(tr_idx)} va={len(va_idx)}\")\n",
        "    resp_tr_corpus = pd.concat([pa_tr.iloc[tr_idx], pb_tr.iloc[tr_idx]], axis=0).tolist()\n",
        "    # TF-IDF with min_df=3 and tighter caps; char 3-5, word 1-2\n",
        "    t_vec = perf_counter()\n",
        "    tfidf_char = TfidfVectorizer(analyzer='char', ngram_range=(3,5), min_df=3, max_features=150000,\n",
        "                                 sublinear_tf=True, dtype=np.float32, norm='l2')\n",
        "    _ = tfidf_char.fit_transform(resp_tr_corpus)\n",
        "    tfidf_word = TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=3, max_features=150000,\n",
        "                                 sublinear_tf=True, dtype=np.float32, lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\", norm='l2')\n",
        "    _ = tfidf_word.fit_transform(resp_tr_corpus)\n",
        "    log(f\"Fold {fold} vec fit {perf_counter()-t_vec:.1f}s\")\n",
        "\n",
        "    # Transform A/B\n",
        "    Xa_c_tr = tfidf_char.transform(pa_tr.iloc[tr_idx]); Xb_c_tr = tfidf_char.transform(pb_tr.iloc[tr_idx])\n",
        "    Xa_w_tr = tfidf_word.transform(pa_tr.iloc[tr_idx]); Xb_w_tr = tfidf_word.transform(pb_tr.iloc[tr_idx])\n",
        "    Xa_c_va = tfidf_char.transform(pa_tr.iloc[va_idx]); Xb_c_va = tfidf_char.transform(pb_tr.iloc[va_idx])\n",
        "    Xa_w_va = tfidf_word.transform(pa_tr.iloc[va_idx]); Xb_w_va = tfidf_word.transform(pb_tr.iloc[va_idx])\n",
        "\n",
        "    # Prompt sims (word TF-IDF)\n",
        "    Xp_w_tr = tfidf_word.transform(prompt_tr.iloc[tr_idx])\n",
        "    Xp_w_va = tfidf_word.transform(prompt_tr.iloc[va_idx])\n",
        "    sim_pa_tr = cosine_rows(Xp_w_tr, Xa_w_tr)\n",
        "    sim_pb_tr = cosine_rows(Xp_w_tr, Xb_w_tr)\n",
        "    sim_pa_va = cosine_rows(Xp_w_va, Xa_w_va)\n",
        "    sim_pb_va = cosine_rows(Xp_w_va, Xb_w_va)\n",
        "    sim_diff_tr = sp.csr_matrix((sim_pb_tr - sim_pa_tr).reshape(-1,1))\n",
        "    sim_diff_va = sp.csr_matrix((sim_pb_va - sim_pa_va).reshape(-1,1))\n",
        "    cos_pa_tr = sp.csr_matrix(sim_pa_tr.reshape(-1,1))\n",
        "    cos_pb_tr = sp.csr_matrix(sim_pb_tr.reshape(-1,1))\n",
        "    cos_pa_va = sp.csr_matrix(sim_pa_va.reshape(-1,1))\n",
        "    cos_pb_va = sp.csr_matrix(sim_pb_va.reshape(-1,1))\n",
        "    # Response-to-response sim\n",
        "    cos_ab_tr = sp.csr_matrix(cosine_rows(Xa_w_tr, Xb_w_tr).reshape(-1,1))\n",
        "    cos_ab_va = sp.csr_matrix(cosine_rows(Xa_w_va, Xb_w_va).reshape(-1,1))\n",
        "\n",
        "    # Scalars\n",
        "    num_tr = build_scalar_blocks(A_counts[tr_idx], B_counts[tr_idx])\n",
        "    num_va = build_scalar_blocks(A_counts[va_idx], B_counts[va_idx])\n",
        "\n",
        "    # Sparse stacks: anti-sym char+word diffs; add |B-A| for word only; add sims and scalars\n",
        "    w_diff_tr = (Xb_w_tr - Xa_w_tr)\n",
        "    w_adiff_tr = abs(w_diff_tr)\n",
        "    w_diff_va = (Xb_w_va - Xa_w_va)\n",
        "    w_adiff_va = abs(w_diff_va)\n",
        "    X_tr = sp.hstack([Xb_c_tr - Xa_c_tr, w_diff_tr, w_adiff_tr, sim_diff_tr, cos_ab_tr, cos_pa_tr, cos_pb_tr, num_tr], format='csr')\n",
        "    X_va = sp.hstack([Xb_c_va - Xa_c_va, w_diff_va, w_adiff_va, sim_diff_va, cos_ab_va, cos_pa_va, cos_pb_va, num_va], format='csr')\n",
        "\n",
        "    # Model: fast SGDClassifier with early stopping\n",
        "    t_fit = perf_counter()\n",
        "    clf = SGDClassifier(loss='log_loss', alpha=1e-4, penalty='l2',\n",
        "                        early_stopping=True, n_iter_no_change=5, validation_fraction=0.1,\n",
        "                        max_iter=1000, tol=1e-3, random_state=42, n_jobs=-1)\n",
        "    clf.fit(X_tr, y[tr_idx])\n",
        "    log(f\"Fold {fold} SGD fit {perf_counter()-t_fit:.1f}s\")\n",
        "\n",
        "    # Predict val and calibrate per class using isotonic (OVR style)\n",
        "    p_va = clf.predict_proba(X_va)\n",
        "    p_va = clean_proba(p_va)\n",
        "    y_va = y[va_idx]\n",
        "    p_va_cal = np.zeros_like(p_va, dtype=np.float64)\n",
        "    iso_models = []\n",
        "    for c in range(3):\n",
        "        # Guard against degenerate folds or NaNs\n",
        "        x_c = p_va[:, c].astype(np.float64)\n",
        "        if np.any(np.isnan(x_c)) or np.any(np.isinf(x_c)) or (y_va == c).sum() == 0 or (y_va == c).sum() == len(y_va):\n",
        "            iso_models.append(None)\n",
        "            p_va_cal[:, c] = x_c\n",
        "            continue\n",
        "        ir = IsotonicRegression(out_of_bounds='clip', y_min=0.0, y_max=1.0)\n",
        "        ir.fit(x_c, (y_va == c).astype(np.float64))\n",
        "        iso_models.append(ir)\n",
        "        p_va_cal[:, c] = ir.predict(x_c)\n",
        "    # clip and renorm\n",
        "    eps = 1e-15\n",
        "    p_va_cal = np.clip(p_va_cal, eps, 1 - eps)\n",
        "    p_va_cal /= p_va_cal.sum(axis=1, keepdims=True)\n",
        "    oof[va_idx] = p_va_cal.astype(np.float32)\n",
        "    ll = log_loss(y_va, p_va_cal, labels=[0,1,2])\n",
        "    log(f\"Fold {fold} OOF-cal logloss={ll:.5f} total fold {perf_counter()-t_fold:.1f}s\")\n",
        "\n",
        "    # Test transform and predict for this fold\n",
        "    Xa_c_te = tfidf_char.transform(ra_tr_te); Xb_c_te = tfidf_char.transform(rb_tr_te)\n",
        "    Xa_w_te = tfidf_word.transform(ra_tr_te); Xb_w_te = tfidf_word.transform(rb_tr_te)\n",
        "    Xp_w_te = tfidf_word.transform(prompt_tr_te)\n",
        "    sim_pa_te = cosine_rows(Xp_w_te, Xa_w_te)\n",
        "    sim_pb_te = cosine_rows(Xp_w_te, Xb_w_te)\n",
        "    sim_diff_te = sp.csr_matrix((sim_pb_te - sim_pa_te).reshape(-1,1))\n",
        "    cos_ab_te = sp.csr_matrix(cosine_rows(Xa_w_te, Xb_w_te).reshape(-1,1))\n",
        "    cos_pa_te = sp.csr_matrix(sim_pa_te.reshape(-1,1))\n",
        "    cos_pb_te = sp.csr_matrix(sim_pb_te.reshape(-1,1))\n",
        "    num_te = build_scalar_blocks(A_counts_te, B_counts_te)\n",
        "    w_diff_te = (Xb_w_te - Xa_w_te); w_adiff_te = abs(w_diff_te)\n",
        "    X_te = sp.hstack([Xb_c_te - Xa_c_te, w_diff_te, w_adiff_te, sim_diff_te, cos_ab_te, cos_pa_te, cos_pb_te, num_te], format='csr')\n",
        "    p_te = clf.predict_proba(X_te)\n",
        "    p_te = clean_proba(p_te)\n",
        "    # apply isotonic per class\n",
        "    p_te_cal = np.zeros_like(p_te, dtype=np.float64)\n",
        "    for c in range(3):\n",
        "        if iso_models[c] is None:\n",
        "            p_te_cal[:, c] = p_te[:, c]\n",
        "        else:\n",
        "            p_te_cal[:, c] = iso_models[c].predict(p_te[:, c])\n",
        "    p_te_cal = np.clip(p_te_cal, eps, 1 - eps)\n",
        "    p_te_cal /= p_te_cal.sum(axis=1, keepdims=True)\n",
        "    test_pred += (p_te_cal.astype(np.float32) / cv.n_splits)\n",
        "\n",
        "# Final OOF\n",
        "oof_ll = log_loss(y, oof, labels=[0,1,2])\n",
        "log(f\"v3 OOF logloss={oof_ll:.5f}; total elapsed={perf_counter()-start_all:.1f}s\")\n",
        "\n",
        "# Write submission\n",
        "sub = pd.DataFrame({\n",
        "    'id': test['id'].values,\n",
        "    'winner_model_a': test_pred[:,0],\n",
        "    'winner_model_b': test_pred[:,1],\n",
        "    'winner_tie': test_pred[:,2],\n",
        "})\n",
        "eps = 1e-15\n",
        "probs = sub[['winner_model_a','winner_model_b','winner_tie']].values\n",
        "probs = np.clip(probs, eps, 1 - eps)\n",
        "probs /= probs.sum(axis=1, keepdims=True)\n",
        "sub[['winner_model_a','winner_model_b','winner_tie']] = probs\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "log('Wrote submission.csv (v3 calibrated)')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:05:51Z] Counts computed in 19.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:06:01Z] Fold 0 start tr=41206 va=10523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:07:57Z] Fold 0 vec fit 116.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:10:23Z] Fold 0 SGD fit 3.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:10:23Z] Fold 0 OOF-cal logloss=1.08409 total fold 262.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py:403: RuntimeWarning: invalid value encountered in divide\n  prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:10:40Z] Fold 1 start tr=41466 va=10263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py:403: RuntimeWarning: invalid value encountered in divide\n  prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:12:34Z] Fold 1 vec fit 114.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:14:58Z] Fold 1 SGD fit 2.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:14:58Z] Fold 1 OOF-cal logloss=1.08711 total fold 258.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py:403: RuntimeWarning: invalid value encountered in divide\n  prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:15:14Z] Fold 2 start tr=41493 va=10236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py:403: RuntimeWarning: invalid value encountered in divide\n  prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:17:10Z] Fold 2 vec fit 116.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:19:38Z] Fold 2 SGD fit 2.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:19:38Z] Fold 2 OOF-cal logloss=1.09287 total fold 264.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py:403: RuntimeWarning: invalid value encountered in divide\n  prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:19:55Z] Fold 3 start tr=41248 va=10481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py:403: RuntimeWarning: invalid value encountered in divide\n  prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:21:52Z] Fold 3 vec fit 117.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:24:17Z] Fold 3 SGD fit 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:24:17Z] Fold 3 OOF-cal logloss=1.08772 total fold 262.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py:403: RuntimeWarning: invalid value encountered in divide\n  prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:24:33Z] Fold 4 start tr=41503 va=10226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py:403: RuntimeWarning: invalid value encountered in divide\n  prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:26:29Z] Fold 4 vec fit 116.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:28:56Z] Fold 4 SGD fit 3.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:28:56Z] Fold 4 OOF-cal logloss=1.08548 total fold 262.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py:403: RuntimeWarning: invalid value encountered in divide\n  prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:29:12Z] v3 OOF logloss=1.08744; total elapsed=1400.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-25T00:29:12Z] Wrote submission.csv (v3 calibrated)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py:403: RuntimeWarning: invalid value encountered in divide\n  prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n"
          ]
        }
      ]
    },
    {
      "id": "0f7b395e-cd08-48ac-bb78-465be60c28bc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# GPU setup: install cu121 torch stack and transformers; sanity check GPU\n",
        "import os, sys, subprocess, shutil, time\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print('>', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "# Uninstall any stray torch stacks\n",
        "for pkg in ('torch','torchvision','torchaudio'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "\n",
        "# Clean common stray site dirs\n",
        "for d in (\n",
        "    '/app/.pip-target/torch',\n",
        "    '/app/.pip-target/torchvision',\n",
        "    '/app/.pip-target/torchaudio',\n",
        "    '/app/.pip-target/torch-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torch-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.23.0.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.4.1.dist-info',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d, flush=True)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# Install exact cu121 torch stack\n",
        "pip('install',\n",
        "    '--index-url', 'https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url', 'https://pypi.org/simple',\n",
        "    'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1')\n",
        "\n",
        "# Constraints file to freeze torch versions\n",
        "Path('constraints.txt').write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "\n",
        "# Install HF stack\n",
        "pip('install', '-c', 'constraints.txt',\n",
        "    'transformers==4.44.2', 'accelerate==0.34.2', 'datasets==2.21.0', 'evaluate==0.4.2',\n",
        "    'sentencepiece', '--upgrade-strategy', 'only-if-needed')\n",
        "\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'CUDA build:', getattr(torch.version, 'cuda', None))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    raise SystemExit('CUDA not available; cannot train transformer')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torch as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchvision as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> install --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchaudio as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 446.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 25.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 229.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 241.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 81.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 539.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 508.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 110.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 408.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 187.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 516.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 513.0 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 175.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 293.4 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 294.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 231.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 377.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 520.5 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 225.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 531.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 124.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 208.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 515.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> install -c constraints.txt transformers==4.44.2 accelerate==0.34.2 datasets==2.21.0 evaluate==0.4.2 sentencepiece --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.5/9.5 MB 86.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate==0.34.2\n  Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 324.4/324.4 KB 470.9 MB/s eta 0:00:00\nCollecting datasets==2.21.0\n  Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 527.3/527.3 KB 513.5 MB/s eta 0:00:00\nCollecting evaluate==0.4.2\n  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 84.1/84.1 KB 445.6 MB/s eta 0:00:00\nCollecting sentencepiece\n  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 197.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers<0.20,>=0.19\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.6/3.6 MB 284.9 MB/s eta 0:00:00\nCollecting packaging>=20.0\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 380.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex!=2019.12.17\n  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 KB 344.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.17\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 243.4 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 424.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors>=0.4.1\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 467.6 MB/s eta 0:00:00\nCollecting tqdm>=4.27\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 449.7 MB/s eta 0:00:00\nCollecting huggingface-hub<1.0,>=0.23.2\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 563.3/563.3 KB 307.9 MB/s eta 0:00:00\nCollecting pyyaml>=5.1\n  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 763.0/763.0 KB 565.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psutil\n  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 291.2/291.2 KB 550.2 MB/s eta 0:00:00\nCollecting torch>=1.10.0\n  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.1/797.1 MB 26.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow>=15.0.0\n  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 42.8/42.8 MB 500.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aiohttp\n  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.7/1.7 MB 470.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas\n  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.4/12.4 MB 478.6 MB/s eta 0:00:00\nCollecting xxhash\n  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 194.8/194.8 KB 489.7 MB/s eta 0:00:00\nCollecting multiprocess\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 144.5/144.5 KB 371.4 MB/s eta 0:00:00\nCollecting dill<0.3.9,>=0.3.0\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 116.3/116.3 KB 436.1 MB/s eta 0:00:00\nCollecting fsspec[http]<=2024.6.1,>=2023.1.0\n  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 177.6/177.6 KB 529.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting propcache>=0.2.0\n  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 213.5/213.5 KB 455.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting multidict<7.0,>=4.5\n  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 246.7/246.7 KB 489.8 MB/s eta 0:00:00\nCollecting aiohappyeyeballs>=2.5.0\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yarl<2.0,>=1.17.0\n  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 349.0/349.0 KB 550.9 MB/s eta 0:00:00\nCollecting attrs>=17.3.0\n  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 63.8/63.8 KB 456.2 MB/s eta 0:00:00\nCollecting aiosignal>=1.4.0\n  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 235.3/235.3 KB 529.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=3.7.4.3\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 337.7 MB/s eta 0:00:00\nCollecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 412.1 MB/s eta 0:00:00\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 474.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 521.6 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 411.0 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 511.2 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 220.8 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 214.2 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 77.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 160.9 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 202.1 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 220.6 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 111.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 151.4 MB/s eta 0:00:00\nCollecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 290.8 MB/s eta 0:00:00\nCollecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 108.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 572.9 MB/s eta 0:00:00\nCollecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 444.8 MB/s eta 0:00:00\nCollecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 562.0 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 207.5 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 458.4 MB/s eta 0:00:00\nCollecting nvidia-nvjitlink-cu12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 240.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting multiprocess\n  Downloading multiprocess-0.70.17-py311-none-any.whl (144 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 144.3/144.3 KB 304.8 MB/s eta 0:00:00\n  Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 143.5/143.5 KB 504.6 MB/s eta 0:00:00\nCollecting tzdata>=2022.7\n  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 347.8/347.8 KB 526.4 MB/s eta 0:00:00\nCollecting python-dateutil>=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.9/229.9 KB 529.6 MB/s eta 0:00:00\nCollecting pytz>=2020.1\n  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 509.2/509.2 KB 533.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting six>=1.5\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nCollecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 565.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, typing-extensions, tqdm, sympy, six, sentencepiece, safetensors, regex, pyyaml, pyarrow, psutil, propcache, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, MarkupSafe, idna, hf-xet, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, triton, requests, python-dateutil, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jinja2, aiosignal, pandas, nvidia-cusolver-cu12, huggingface-hub, aiohttp, torch, tokenizers, transformers, datasets, accelerate, evaluate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 accelerate-0.34.2 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 certifi-2025.8.3 charset_normalizer-3.4.3 datasets-2.21.0 dill-0.3.8 evaluate-0.4.2 filelock-3.19.1 frozenlist-1.7.0 fsspec-2024.6.1 hf-xet-1.1.10 huggingface-hub-0.35.1 idna-3.10 jinja2-3.1.6 mpmath-1.3.0 multidict-6.6.4 multiprocess-0.70.16 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 packaging-25.0 pandas-2.3.2 propcache-0.3.2 psutil-7.1.0 pyarrow-21.0.0 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.2 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 sentencepiece-0.2.1 six-1.17.0 sympy-1.14.0 tokenizers-0.19.1 torch-2.4.1 tqdm-4.67.1 transformers-4.44.2 triton-3.0.0 typing-extensions-4.15.0 tzdata-2025.2 urllib3-2.5.0 xxhash-3.5.0 yarl-1.20.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.4.5.107.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.1.0.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/MarkupSafe-3.0.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.1.3.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.0.2.54.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.2.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.20.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.9.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.4.1+cu121 CUDA build: 12.1\nCUDA available: True\nGPU: NVIDIA A10-24Q\n"
          ]
        }
      ]
    },
    {
      "id": "90a85745-c00e-4cca-9523-67e56c854abd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cross-encoder v2: DeBERTa-v3-large, 3-fold x 2 seeds, swap aug + symmetric inference, bf16 + grad checkpointing\n",
        "import os, json, math, random, time\n",
        "from time import perf_counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from hashlib import blake2b\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import log_loss\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
        ")\n",
        "\n",
        "def log(msg):\n",
        "    from datetime import datetime\n",
        "    print(f\"[{datetime.utcnow().isoformat(timespec='seconds')}Z] {msg}\", flush=True)\n",
        "\n",
        "# Performance/precision knobs\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = os.environ.get('PYTORCH_CUDA_ALLOC_CONF','') or 'expandable_segments:True'\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def norm_prompt_for_group(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    t = s.strip()\n",
        "    try:\n",
        "        obj = json.loads(t)\n",
        "        if isinstance(obj, list):\n",
        "            t = ' [TURN] '.join(map(str, obj))\n",
        "        elif isinstance(obj, str):\n",
        "            t = obj\n",
        "    except Exception:\n",
        "        pass\n",
        "    t = t.replace('\\r', ' ').replace('\\n', ' ')\n",
        "    t = ' '.join(t.split())\n",
        "    return t.lower()\n",
        "\n",
        "def hhash(*parts: str, nbytes: int = 8) -> int:\n",
        "    h = blake2b(digest_size=nbytes)\n",
        "    for p in parts:\n",
        "        if p is None:\n",
        "            p = ''\n",
        "        if not isinstance(p, str):\n",
        "            p = str(p)\n",
        "        h.update(p.encode('utf-8', errors='ignore')); h.update(b'|')\n",
        "    return int.from_bytes(h.digest(), 'little', signed=False)\n",
        "\n",
        "def truncate_head_tail_text(s: str, head: int, tail: int) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    if len(s) <= head + tail:\n",
        "        return s\n",
        "    return s[:head] + s[-tail:]\n",
        "\n",
        "def build_df(train_csv='train.csv', test_csv='test.csv', head_tail=(3000, 1000), prompt_ht=(1200, 400)):\n",
        "    train = pd.read_csv(train_csv)\n",
        "    test = pd.read_csv(test_csv)\n",
        "    y_cols = ['winner_model_a','winner_model_b','winner_tie']\n",
        "    y = train[y_cols].values.argmax(axis=1)\n",
        "    ph, pt = prompt_ht\n",
        "    h, t = head_tail\n",
        "    tr_prompt = train['prompt'].astype(str).map(lambda s: truncate_head_tail_text(s, ph, pt))\n",
        "    tr_a = train['response_a'].astype(str).map(lambda s: truncate_head_tail_text(s, h, t))\n",
        "    tr_b = train['response_b'].astype(str).map(lambda s: truncate_head_tail_text(s, h, t))\n",
        "    te_prompt = test['prompt'].astype(str).map(lambda s: truncate_head_tail_text(s, ph, pt))\n",
        "    te_a = test['response_a'].astype(str).map(lambda s: truncate_head_tail_text(s, h, t))\n",
        "    te_b = test['response_b'].astype(str).map(lambda s: truncate_head_tail_text(s, h, t))\n",
        "    df_train = pd.DataFrame({\n",
        "        'id': train['id'].values,\n",
        "        'prompt': tr_prompt,\n",
        "        'a': tr_a,\n",
        "        'b': tr_b,\n",
        "        'label': y,\n",
        "        'group': train['prompt'].map(norm_prompt_for_group).map(lambda s: hhash(s))\n",
        "    })\n",
        "    df_test = pd.DataFrame({\n",
        "        'id': test['id'].values,\n",
        "        'prompt': te_prompt,\n",
        "        'a': te_a,\n",
        "        'b': te_b\n",
        "    })\n",
        "    return df_train, df_test\n",
        "\n",
        "# Input template with explicit instruction and separators\n",
        "def make_input(prompt, a, b, sep_token='[SEP]'):\n",
        "    return (f\"{sep_token} Instruction: decide which response is better overall (A, B, or Tie).\\n\"\n",
        "            f\"{sep_token} Prompt:\\n{prompt}\\n\"\n",
        "            f\"{sep_token} Response A:\\n{a}\\n\"\n",
        "            f\"{sep_token} Response B:\\n{b}\")\n",
        "\n",
        "# Config\n",
        "model_name = 'microsoft/deberta-v3-large'  # fallback to 'microsoft/deberta-v3-base' if OOM\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "sep_tok = tokenizer.sep_token or '[SEP]'\n",
        "\n",
        "# Tokenization: dynamic padding, truncate to model limit (512 for DeBERTa v3); string template + budgets above\n",
        "def tokenize_function(examples, max_length=512):\n",
        "    return tokenizer(examples['text'], truncation=True, max_length=max_length, padding=False)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
        "    return {'logloss': float(log_loss(labels, probs, labels=[0,1,2]))}\n",
        "\n",
        "# Build dataframes\n",
        "df_train, df_test = build_df()\n",
        "labels = df_train['label'].values\n",
        "groups = df_train['group'].values\n",
        "\n",
        "# Pre-build test datasets (orig and swapped) once; reuse across folds/seeds\n",
        "df_te_orig = df_test.copy()\n",
        "df_te_orig['text'] = [make_input(p, a, b, sep_tok) for p,a,b in zip(df_te_orig['prompt'], df_te_orig['a'], df_te_orig['b'])]\n",
        "ds_te_orig = Dataset.from_pandas(df_te_orig[['text']])\n",
        "ds_te_orig = ds_te_orig.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "\n",
        "df_te_swap = df_test.copy()\n",
        "df_te_swap['text'] = [make_input(p, b, a, sep_tok) for p,a,b in zip(df_te_swap['prompt'], df_te_swap['a'], df_te_swap['b'])]\n",
        "ds_te_swap = Dataset.from_pandas(df_te_swap[['text']])\n",
        "ds_te_swap = ds_te_swap.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "\n",
        "# Swap augmentation\n",
        "def augment_swap(df):\n",
        "    df_sw = df.copy()\n",
        "    df_sw['a'], df_sw['b'] = df['b'], df['a']\n",
        "    lab = df['label'].values.copy()\n",
        "    lab_sw = lab.copy()\n",
        "    lab_sw[lab == 0] = 1\n",
        "    lab_sw[lab == 1] = 0\n",
        "    df_sw['label'] = lab_sw\n",
        "    return pd.concat([df, df_sw], axis=0, ignore_index=True)\n",
        "\n",
        "seeds = [42, 2025]\n",
        "n_folds = 3\n",
        "skf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
        "\n",
        "all_oof_logits = np.zeros((len(df_train), 3), dtype=np.float64)\n",
        "test_pred_logits = np.zeros((len(df_test), 3), dtype=np.float64)\n",
        "\n",
        "start_all = perf_counter()\n",
        "for seed in seeds:\n",
        "    log(f\"Seed {seed} run start\")\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    oof_logits_seed = np.zeros((len(df_train), 3), dtype=np.float64)\n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(df_train, labels, groups=groups)):\n",
        "        log(f\"XEnc Fold {fold} (seed {seed}) start tr={len(tr_idx)} va={len(va_idx)}\")\n",
        "        df_tr = df_train.iloc[tr_idx].reset_index(drop=True)\n",
        "        df_va = df_train.iloc[va_idx].reset_index(drop=True)\n",
        "        df_tr_aug = augment_swap(df_tr)\n",
        "        # Build texts\n",
        "        df_tr_aug['text'] = [make_input(p, a, b, sep_tok) for p,a,b in zip(df_tr_aug['prompt'], df_tr_aug['a'], df_tr_aug['b'])]\n",
        "        df_va['text'] = [make_input(p, a, b, sep_tok) for p,a,b in zip(df_va['prompt'], df_va['a'], df_va['b'])]\n",
        "        ds_tr = Dataset.from_pandas(df_tr_aug[['text','label']])\n",
        "        ds_va = Dataset.from_pandas(df_va[['text','label']])\n",
        "        ds_tr = ds_tr.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "        ds_va = ds_va.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "        # Regularization and memory\n",
        "        try:\n",
        "            model.gradient_checkpointing_enable()\n",
        "        except Exception:\n",
        "            pass\n",
        "        if hasattr(model, 'config'):\n",
        "            try:\n",
        "                model.config.use_cache = False\n",
        "                if hasattr(model.config, 'hidden_dropout_prob'):\n",
        "                    model.config.hidden_dropout_prob = 0.1\n",
        "                if hasattr(model.config, 'attention_probs_dropout_prob'):\n",
        "                    model.config.attention_probs_dropout_prob = 0.1\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        args = TrainingArguments(\n",
        "            output_dir=f'ce_fold{fold}_seed{seed}',\n",
        "            learning_rate=1.5e-5,\n",
        "            weight_decay=0.01,\n",
        "            lr_scheduler_type='cosine',\n",
        "            warmup_ratio=0.06,\n",
        "            num_train_epochs=2,\n",
        "            per_device_train_batch_size=4,\n",
        "            per_device_eval_batch_size=16,\n",
        "            gradient_accumulation_steps=8,  # eff batch ~32\n",
        "            bf16=True, fp16=False,\n",
        "            label_smoothing_factor=0.05,\n",
        "            logging_steps=50,\n",
        "            evaluation_strategy='epoch',\n",
        "            save_strategy='no',\n",
        "            report_to=[],\n",
        "            dataloader_num_workers=4,\n",
        "            dataloader_pin_memory=True,\n",
        "            optim='adamw_torch',\n",
        "            seed=seed,\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            train_dataset=ds_tr,\n",
        "            eval_dataset=ds_va,\n",
        "            data_collator=data_collator,\n",
        "            tokenizer=tokenizer,\n",
        "            compute_metrics=compute_metrics\n",
        "        )\n",
        "        t0 = perf_counter()\n",
        "        trainer.train()\n",
        "        log(f\"Fold {fold} (seed {seed}) train done in {perf_counter()-t0:.1f}s\")\n",
        "\n",
        "        # Predict val\n",
        "        preds = trainer.predict(ds_va).predictions\n",
        "        oof_probs = torch.softmax(torch.tensor(preds), dim=-1).numpy().astype(np.float64)\n",
        "        oof_logits_seed[va_idx] = preds.astype(np.float64)\n",
        "        ll = log_loss(df_va['label'].values, oof_probs, labels=[0,1,2])\n",
        "        log(f\"Fold {fold} (seed {seed}) val logloss={ll:.5f}\")\n",
        "\n",
        "        # Test-time symmetric inference (reuse pre-tokenized ds_te_orig/ds_te_swap)\n",
        "        logits_orig = trainer.predict(ds_te_orig).predictions.astype(np.float64)\n",
        "        logits_sw = trainer.predict(ds_te_swap).predictions.astype(np.float64)\n",
        "        logits_back = logits_sw.copy()\n",
        "        logits_back[:,0], logits_back[:,1] = logits_sw[:,1], logits_sw[:,0]\n",
        "        logits_avg = (logits_orig + logits_back) / 2.0\n",
        "        test_pred_logits += logits_avg / (n_folds * len(seeds))\n",
        "\n",
        "    # accumulate OOF logits (average over seeds later)\n",
        "    all_oof_logits += oof_logits_seed / len(seeds)\n",
        "\n",
        "oof_probs_final = torch.softmax(torch.tensor(all_oof_logits), dim=-1).numpy()\n",
        "oof_ll = log_loss(df_train['label'].values, oof_probs_final, labels=[0,1,2])\n",
        "log(f\"XEnc OOF logloss={oof_ll:.5f}; total elapsed={perf_counter()-start_all:.1f}s\")\n",
        "\n",
        "# Build submission from softmax of averaged logits\n",
        "test_probs = torch.softmax(torch.tensor(test_pred_logits), dim=-1).numpy()\n",
        "sub = pd.DataFrame({\n",
        "    'id': df_test['id'].values,\n",
        "    'winner_model_a': test_probs[:,0],\n",
        "    'winner_model_b': test_probs[:,1],\n",
        "    'winner_tie': test_probs[:,2],\n",
        "})\n",
        "eps = 1e-15\n",
        "P = sub[['winner_model_a','winner_model_b','winner_tie']].values\n",
        "P = np.clip(P, eps, 1-eps); P /= P.sum(axis=1, keepdims=True)\n",
        "sub[['winner_model_a','winner_model_b','winner_tie']] = P\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "log('Wrote submission.csv (cross-encoder v2, no temp scaling)')\n",
        "\n",
        "# Save OOF logits for later temperature scaling\n",
        "np.save('oof_logits.npy', all_oof_logits.astype(np.float32))\n",
        "np.save('test_logits.npy', test_pred_logits.astype(np.float32))\n",
        "log('Saved oof_logits.npy and test_logits.npy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMap:   0%|          | 0/5748 [00:00<?, ? examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMap:  17%|\u2588\u258b        | 1000/5748 [00:00<00:01, 3169.28 examples/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rMap:  35%|\u2588\u2588\u2588\u258d      | 2000/5748 [00:00<00:01, 3393.22 examples/s]"
          ]
        }
      ]
    },
    {
      "id": "daa80a0f-1465-44f6-9482-f9a227c1891b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Post-training: temperature scaling on saved logits and write calibrated submission\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import log_loss\n",
        "from time import perf_counter\n",
        "\n",
        "def log(msg):\n",
        "    from datetime import datetime\n",
        "    print(f\"[{datetime.utcnow().isoformat(timespec='seconds')}Z] {msg}\", flush=True)\n",
        "\n",
        "def softmax(logits):\n",
        "    x = logits - logits.max(axis=1, keepdims=True)\n",
        "    ex = np.exp(x)\n",
        "    return ex / ex.sum(axis=1, keepdims=True)\n",
        "\n",
        "def find_best_temperature(oof_logits: np.ndarray, y_true: np.ndarray) -> float:\n",
        "    # Optimize a single scalar T on logits: probs = softmax(logits / T)\n",
        "    def nll_from_logT(logT: float) -> float:\n",
        "        T = float(np.exp(logT))\n",
        "        P = softmax(oof_logits / max(T, 1e-6))\n",
        "        return log_loss(y_true, P, labels=[0,1,2])\n",
        "    grid = np.linspace(-2.0, 2.0, 41)\n",
        "    vals = [nll_from_logT(g) for g in grid]\n",
        "    best_logT = float(grid[int(np.argmin(vals))])\n",
        "    for _ in range(3):\n",
        "        lo = max(-5.0, best_logT - 0.5); hi = min(5.0, best_logT + 0.5)\n",
        "        grid = np.linspace(lo, hi, 31)\n",
        "        vals = [nll_from_logT(g) for g in grid]\n",
        "        best_logT = float(grid[int(np.argmin(vals))])\n",
        "    return float(np.exp(best_logT))\n",
        "\n",
        "t0 = perf_counter()\n",
        "oof_logits = np.load('oof_logits.npy')\n",
        "test_logits = np.load('test_logits.npy')\n",
        "y = pd.read_csv('train.csv')[['winner_model_a','winner_model_b','winner_tie']].values.argmax(axis=1)\n",
        "oof_probs = softmax(oof_logits)\n",
        "base_ll = log_loss(y, oof_probs, labels=[0,1,2])\n",
        "log(f\"Uncalibrated OOF logloss from saved logits: {base_ll:.6f}\")\n",
        "T = find_best_temperature(oof_logits, y)\n",
        "log(f\"Optimal temperature T (logit scaling): {T:.4f}\")\n",
        "oof_probs_cal = softmax(oof_logits / T)\n",
        "cal_ll = log_loss(y, oof_probs_cal, labels=[0,1,2])\n",
        "log(f\"Calibrated OOF logloss: {cal_ll:.6f}\")\n",
        "\n",
        "# Apply to test logits and write calibrated submission\n",
        "test_probs_cal = softmax(test_logits / T).astype(np.float64)\n",
        "eps = 1e-15\n",
        "test_probs_cal = np.clip(test_probs_cal, eps, 1 - eps)\n",
        "test_probs_cal /= test_probs_cal.sum(axis=1, keepdims=True)\n",
        "test_ids = pd.read_csv('test.csv')['id'].values\n",
        "sub = pd.DataFrame({\n",
        "    'id': test_ids,\n",
        "    'winner_model_a': test_probs_cal[:,0],\n",
        "    'winner_model_b': test_probs_cal[:,1],\n",
        "    'winner_tie': test_probs_cal[:,2],\n",
        "})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "sub.to_csv('submission_calibrated.csv', index=False)\n",
        "log(f\"Wrote submission.csv and submission_calibrated.csv with temperature scaling; elapsed {perf_counter()-t0:.1f}s\")\n",
        "with open('calibration_T.txt','w') as f: f.write(str(T))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}