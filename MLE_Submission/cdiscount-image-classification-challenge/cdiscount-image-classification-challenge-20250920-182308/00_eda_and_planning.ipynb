{
  "cells": [
    {
      "id": "1deaa878-f614-43ef-b2eb-9a5caf4f0421",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: Zero-Shot CLIP Baseline for Cdiscount\n",
        "\n",
        "Objectives:\n",
        "- Ship a working baseline ASAP that generates submission.csv.\n",
        "- Use GPU-accelerated zero-shot image classification (CLIP) with category_names.csv since train data is not present.\n",
        "\n",
        "Rationale:\n",
        "- Repository includes test.bson, sample_submission.csv, and category_names.csv, but no train images/labels.\n",
        "- Zero-shot CLIP can map image embeddings to text label embeddings of categories.\n",
        "- This avoids training time and leverages GPU for fast throughput.\n",
        "\n",
        "Milestones:\n",
        "1) Environment + GPU check\n",
        "2) Data inspection: read test.bson, confirm schema (product_id, images)\n",
        "3) Build category text prompts from category_names.csv\n",
        "4) Load CLIP (open_clip or OpenAI clip) on GPU\n",
        "5) Encode category prompts (cache text embeddings)\n",
        "6) Iterate over test.bson in streaming batches:\n",
        "   - Decode images (PIL), preprocess\n",
        "   - Encode images with CLIP\n",
        "   - Compute similarities to category text embeddings\n",
        "   - Argmax to get predicted category_id\n",
        "   - Use majority vote across images per product_id\n",
        "7) Write submission.csv and validate format vs sample_submission.csv\n",
        "8) Time/perf logging per N batches; subsample smoke test first\n",
        "\n",
        "Risk/Alternatives:\n",
        "- Zero-shot accuracy may be limited with >5k classes; may still place well in this benchmark.\n",
        "- If slow, reduce model size (e.g., ViT-B/16) and use fp16.\n",
        "\n",
        "Next:\n",
        "- Ask experts to confirm zero-shot CLIP viability and any prompt engineering tips.\n",
        "- Then implement GPU check + BSON reader + CLIP pipeline with careful logging."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "4d36d4b5-a80c-4717-8907-6213bb6b2578",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup: inventory files, search for train data, and GPU check/install deps\n",
        "import os, sys, subprocess, time, json, shutil, gc\n",
        "from pathlib import Path\n",
        "\n",
        "# Set CPU threading env BEFORE importing torch\n",
        "os.environ.setdefault('OMP_NUM_THREADS', '24')\n",
        "os.environ.setdefault('MKL_NUM_THREADS', '24')\n",
        "os.environ.setdefault('OPENBLAS_NUM_THREADS', '24')\n",
        "\n",
        "print('CWD:', os.getcwd())\n",
        "print('Files:', os.listdir())\n",
        "\n",
        "# Search for train data quickly (sanity check)\n",
        "import subprocess\n",
        "try:\n",
        "    out = subprocess.check_output(['bash','-lc','set -o pipefail; find / -name \"*train.bson*\" 2>/dev/null | head -n 20'], text=True)\n",
        "    print('Potential train files found:\\n', out)\n",
        "except Exception as e:\n",
        "    print('find train.bson error:', e)\n",
        "\n",
        "# Ensure key packages; install torch if missing\n",
        "def ensure(pkg, pip_name=None, extra_args=None):\n",
        "    pip_name = pip_name or pkg\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "        print(f'OK: {pkg} already installed')\n",
        "    except Exception:\n",
        "        cmd = [sys.executable, '-m', 'pip', 'install', pip_name]\n",
        "        if extra_args:\n",
        "            cmd += extra_args\n",
        "        print('Installing', pip_name)\n",
        "        subprocess.check_call(cmd)\n",
        "\n",
        "ensure('torch', 'torch')\n",
        "ensure('pandas', 'pandas')\n",
        "ensure('numpy', 'numpy')\n",
        "ensure('PIL', 'Pillow')\n",
        "ensure('bson', 'pymongo')  # bson is provided by pymongo\n",
        "ensure('open_clip', 'open_clip_torch')\n",
        "\n",
        "import torch\n",
        "print(f'GPU Available: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU Name: {torch.cuda.get_device_name(0)}')\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    print(f'GPU Memory: {props.total_memory / 1024**3:.1f} GB')\n",
        "\n",
        "import pandas as pd\n",
        "cat_df = pd.read_csv('category_names.csv')\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "print('category_names.csv shape:', cat_df.shape)\n",
        "print('sample_submission.csv shape:', ss.shape)\n",
        "print(cat_df.head(3))\n",
        "print(ss.head(3))\n",
        "print('Unique _id in sample_submission:', ss['_id'].nunique())\n",
        "\n",
        "print('Setup complete.')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CWD: /var/lib/simon/agent_run_states/cdiscount-image-classification-challenge-20250920-182308\nFiles: ['cache', 'test.bson', 'sample_submission.csv', 'category_names.csv', 'docker_run.log', 'requirements.txt', 'submission.csv', 'agent_metadata', 'task.txt', '00_eda_and_planning.ipynb', 'description.md']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "find train.bson error: Command '['bash', '-lc', 'set -o pipefail; find / -name \"*train.bson*\" 2>/dev/null | head -n 20']' returned non-zero exit status 1.\nOK: torch already installed\nOK: pandas already installed\nOK: numpy already installed\nOK: PIL already installed\nOK: bson already installed\nOK: open_clip already installed\nGPU Available: False\ncategory_names.csv shape: (5270, 4)\nsample_submission.csv shape: (706990, 2)\n   category_id              category_level1     category_level2  \\\n0   1000021794        ABONNEMENT / SERVICES      CARTE PREPAYEE   \n1   1000012764  AMENAGEMENT URBAIN - VOIRIE  AMENAGEMENT URBAIN   \n2   1000012776  AMENAGEMENT URBAIN - VOIRIE  AMENAGEMENT URBAIN   \n\n             category_level3  \n0  CARTE PREPAYEE MULTIMEDIA  \n1                ABRI FUMEUR  \n2      ABRI VELO - ABRI MOTO  \n   _id  category_id\n0    6   1000010653\n1    7   1000010653\n2   12   1000010653\nUnique _id in sample_submission: 706990\nSetup complete.\n"
          ]
        }
      ]
    },
    {
      "id": "fd1be06a-b0d7-450d-8de8-b71c418c3c87",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build class list, prompts, load CLIP, and encode text embeddings (cacheable) + hierarchy maps\n",
        "import pandas as pd, numpy as np, os, re, torch, open_clip, time, json\n",
        "from pathlib import Path\n",
        "\n",
        "# CPU-optimized settings (GPU unavailable): RN50 + more prompts\n",
        "SUBSAMPLE_CLASSES = None  # encode all classes\n",
        "\n",
        "cat_df = pd.read_csv('category_names.csv')\n",
        "assert 'category_id' in cat_df.columns, 'category_id column missing in category_names.csv'\n",
        "\n",
        "# Use provided hierarchy columns if present\n",
        "lvl1_col = 'category_level1' if 'category_level1' in cat_df.columns else None\n",
        "lvl3_col = None\n",
        "for c in ['category_level3', 'category_name', 'name', 'title']:\n",
        "    if c in cat_df.columns:\n",
        "        lvl3_col = c\n",
        "        break\n",
        "assert lvl3_col is not None, 'No usable category name (level3) column found'\n",
        "\n",
        "def clean_label(s):\n",
        "    s = str(s)\n",
        "    s = re.split(r'[>/,|]', s)[-1]\n",
        "    s = s.strip()\n",
        "    return s if s else 'produit'\n",
        "\n",
        "# Level-3 (final classes)\n",
        "class_ids_all = cat_df['category_id'].astype(int).tolist()\n",
        "class_names_raw_all = cat_df[lvl3_col].astype(str).tolist()\n",
        "class_names_all = [clean_label(x) for x in class_names_raw_all]\n",
        "if SUBSAMPLE_CLASSES is not None:\n",
        "    class_ids = class_ids_all[:SUBSAMPLE_CLASSES]\n",
        "    class_names = class_names_all[:SUBSAMPLE_CLASSES]\n",
        "else:\n",
        "    class_ids = class_ids_all\n",
        "    class_names = class_names_all\n",
        "num_classes = len(class_ids)\n",
        "print(f'Classes used (L3): {num_classes} / {len(class_ids_all)}; sample: {class_ids[:3]} -> {class_names[:3]}')\n",
        "\n",
        "# Level-1 (superclasses)\n",
        "if lvl1_col is not None:\n",
        "    lvl1_all = cat_df[lvl1_col].astype(str).tolist()\n",
        "    lvl1_names_unique = sorted(set(lvl1_all))\n",
        "else:\n",
        "    lvl1_names_unique = []\n",
        "print('Level1 classes:', len(lvl1_names_unique))\n",
        "\n",
        "# Prompt templates (FR + EN) - 9 templates\n",
        "fr_tmps = [\n",
        "    'une photo de {}',\n",
        "    'une photo produit de {}',\n",
        "    'une image de {}',\n",
        "    'photo d\\u2019un {}',\n",
        "    '{} (produit)'\n",
        "]\n",
        "en_tmps = [\n",
        "    'a product photo of a {}',\n",
        "    'a catalog photo of a {}',\n",
        "    'an image of a {}',\n",
        "    'a {}'\n",
        "]\n",
        "templates = fr_tmps + en_tmps\n",
        "print(f'Using {len(templates)} templates')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device:', device)\n",
        "# Faster CPU model\n",
        "model_name = 'RN50'\n",
        "pretrained_tag = 'openai'  # available tags for RN50: ['openai','yfcc15m','cc12m']\n",
        "print('Loading open_clip model:', model_name, pretrained_tag)\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained_tag, device=device)\n",
        "tokenizer = open_clip.get_tokenizer(model_name)\n",
        "model.eval()\n",
        "try:\n",
        "    torch.set_num_threads(12)\n",
        "    torch.set_num_interop_threads(1)\n",
        "    torch.backends.mkldnn.enabled = True\n",
        "    print('torch threads:', torch.get_num_threads(), 'interop:', torch.get_num_interop_threads(), 'mkldnn:', torch.backends.mkldnn.enabled)\n",
        "except Exception as e:\n",
        "    print('Could not set torch threading:', e)\n",
        "\n",
        "# Encode and cache text embeddings helpers\n",
        "cache_dir = Path('cache')\n",
        "cache_dir.mkdir(exist_ok=True)\n",
        "\n",
        "def encode_labels_to_embeds(labels, cache_key):\n",
        "    text_cache_path = cache_dir / f'text_embeds_{model_name}_{pretrained_tag}_{len(templates)}tmps_{cache_key}.npy'\n",
        "    order_cache_path = cache_dir / f'labels_order_{model_name}_{cache_key}.json'\n",
        "    if text_cache_path.exists() and order_cache_path.exists():\n",
        "        try:\n",
        "            embeds = np.load(text_cache_path)\n",
        "            cached_labels = json.loads(order_cache_path.read_text())\n",
        "            if cached_labels == labels:\n",
        "                print(f'Loaded cached text embeddings for {cache_key}:', embeds.shape)\n",
        "                t = torch.from_numpy(embeds).to(device)\n",
        "                return t / t.norm(dim=-1, keepdim=True)\n",
        "            else:\n",
        "                print('Label order changed; recomputing for', cache_key)\n",
        "        except Exception as e:\n",
        "            print('Cache load failed for', cache_key, e)\n",
        "    with torch.no_grad():\n",
        "        all_embeds = []\n",
        "        start = time.time()\n",
        "        for idx, name in enumerate(labels):\n",
        "            texts = [t.format(name) for t in templates]\n",
        "            text_tokens = tokenizer(texts).to(device)\n",
        "            tf = model.encode_text(text_tokens)\n",
        "            tf = tf / tf.norm(dim=-1, keepdim=True)\n",
        "            cls_embed = tf.mean(dim=0, keepdim=True)\n",
        "            cls_embed = cls_embed / cls_embed.norm(dim=-1, keepdim=True)\n",
        "            all_embeds.append(cls_embed)\n",
        "            if (idx+1) % 500 == 0 or (idx+1) == len(labels):\n",
        "                print(f'Encoded {idx+1}/{len(labels)} labels for {cache_key} in {time.time()-start:.1f}s')\n",
        "        embeds = torch.cat(all_embeds, dim=0).float().cpu().numpy()\n",
        "    np.save(text_cache_path, embeds)\n",
        "    order_cache_path.write_text(json.dumps(labels))\n",
        "    t = torch.from_numpy(embeds).to(device)\n",
        "    return t / t.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# Build Level1 -> Level3 index map\n",
        "lvl1_to_children_idx = {}  # level1 index -> list of L3 indices\n",
        "lvl1_names = lvl1_names_unique\n",
        "lvl1_name_to_idx = {n:i for i,n in enumerate(lvl1_names)}\n",
        "if lvl1_names:\n",
        "    # Map based on cat_df rows to indices in our class_ids list\n",
        "    l3_id_to_pos = {cid:i for i,cid in enumerate(class_ids)}\n",
        "    for _, row in cat_df.iterrows():\n",
        "        cid = int(row['category_id'])\n",
        "        if cid not in l3_id_to_pos:\n",
        "            continue\n",
        "        l3_pos = l3_id_to_pos[cid]\n",
        "        l1_name = str(row[lvl1_col]) if lvl1_col else None\n",
        "        if l1_name is None:\n",
        "            continue\n",
        "        l1_idx = lvl1_name_to_idx[l1_name]\n",
        "        lvl1_to_children_idx.setdefault(l1_idx, []).append(l3_pos)\n",
        "    print('Built Level1->Level3 child map for', len(lvl1_to_children_idx), 'Level1 nodes')\n",
        "\n",
        "# Encode text embeddings for Level3 (final) and Level1 (coarse) labels\n",
        "text_embeds_l3_t = encode_labels_to_embeds(class_names, cache_key=f'L3_{num_classes}cls')\n",
        "if lvl1_names:\n",
        "    text_embeds_l1_t = encode_labels_to_embeds(lvl1_names, cache_key=f'L1_{len(lvl1_names)}cls')\n",
        "else:\n",
        "    text_embeds_l1_t = None\n",
        "\n",
        "# Make variables available for inference cells\n",
        "text_embeds_t = text_embeds_l3_t  # backwards compat\n",
        "print('Final: L3 embeds:', tuple(text_embeds_l3_t.shape), 'L1 embeds:', None if text_embeds_l1_t is None else tuple(text_embeds_l1_t.shape))\n",
        "print('Done text encoding.')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes used (L3): 5270 / 5270; sample: [1000021794, 1000012764, 1000012776] -> ['CARTE PREPAYEE MULTIMEDIA', 'ABRI FUMEUR', 'ABRI VELO - ABRI MOTO']\nLevel1 classes: 49\nUsing 9 templates\nDevice: cpu\nLoading open_clip model: RN50 openai\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch threads: 12 interop: 1 mkldnn: True\nBuilt Level1->Level3 child map for 49 Level1 nodes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 500/5270 labels for L3_5270cls in 36.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 1000/5270 labels for L3_5270cls in 72.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 1500/5270 labels for L3_5270cls in 108.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 2000/5270 labels for L3_5270cls in 143.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 2500/5270 labels for L3_5270cls in 179.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 3000/5270 labels for L3_5270cls in 215.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 3500/5270 labels for L3_5270cls in 251.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 4000/5270 labels for L3_5270cls in 288.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 4500/5270 labels for L3_5270cls in 323.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 5000/5270 labels for L3_5270cls in 359.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 5270/5270 labels for L3_5270cls in 378.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 49/49 labels for L1_49cls in 3.5s\nFinal: L3 embeds: (5270, 1024) L1 embeds: (49, 1024)\nDone text encoding.\n"
          ]
        }
      ]
    },
    {
      "id": "290044d0-4ebf-477b-b398-737a17b28e34",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inference: stream test.bson, encode images, score vs text (hierarchical with L1 top-k), and build submission\n",
        "import bson\n",
        "from io import BytesIO\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import pandas as pd, numpy as np, torch, time, sys, collections\n",
        "\n",
        "# CPU-friendly knobs\n",
        "SUBSAMPLE_PRODUCTS = None   # full run\n",
        "USE_TTA_HFLIP = False       # keep False for CPU\n",
        "BATCH_IMAGES = 256          # tune {192,256,320,384}\n",
        "L1_TOPK = 2                 # beam size at Level-1\n",
        "\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "ss_ids = ss['_id'].tolist()\n",
        "print('Sample submission size:', len(ss_ids))\n",
        "\n",
        "def pil_from_bytes(b):\n",
        "    try:\n",
        "        return Image.open(BytesIO(b)).convert('RGB')\n",
        "    except UnidentifiedImageError:\n",
        "        return None\n",
        "\n",
        "# Batch encode helper\n",
        "def flush_batch(img_tensors, owners):\n",
        "    if not img_tensors:\n",
        "        return {}\n",
        "    batch = torch.stack(img_tensors).to(device)\n",
        "    with torch.no_grad():\n",
        "        feats = model.encode_image(batch)\n",
        "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "    prod_feats = {}\n",
        "    for feat, pid in zip(feats, owners):  # one image per product in our pipeline\n",
        "        prod_feats[pid] = feat / feat.norm()\n",
        "    return prod_feats\n",
        "\n",
        "def hierarchical_score_topk(prod_feats_dict):\n",
        "    # Per-product scoring with L1 top-k beam, then score union of children\n",
        "    out = {}\n",
        "    for pid, feat in prod_feats_dict.items():\n",
        "        if text_embeds_l1_t is not None and len(lvl1_to_children_idx) > 0:\n",
        "            with torch.no_grad():\n",
        "                sims_l1 = (feat @ text_embeds_l1_t.T)  # [L1]\n",
        "                topk_vals, topk_idx = torch.topk(sims_l1, k=min(L1_TOPK, sims_l1.shape[0]))\n",
        "            child_set = []\n",
        "            for l1i in topk_idx.tolist():\n",
        "                child_set.extend(lvl1_to_children_idx.get(int(l1i), []))\n",
        "            if not child_set:\n",
        "                # flat fallback\n",
        "                with torch.no_grad():\n",
        "                    sims = feat @ text_embeds_t.T\n",
        "                    ti = int(torch.argmax(sims).item())\n",
        "                out[pid] = int(class_ids[ti])\n",
        "            else:\n",
        "                # dedup children\n",
        "                child_set = list(dict.fromkeys(child_set))\n",
        "                child_mat = text_embeds_t[child_set]  # [K, D]\n",
        "                with torch.no_grad():\n",
        "                    sims_c = feat @ child_mat.T  # [K]\n",
        "                    cp = int(torch.argmax(sims_c).item())\n",
        "                cls_pos = child_set[cp]\n",
        "                out[pid] = int(class_ids[cls_pos])\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                sims = feat @ text_embeds_t.T\n",
        "                ti = int(torch.argmax(sims).item())\n",
        "            out[pid] = int(class_ids[ti])\n",
        "    return out\n",
        "\n",
        "# Threading (ensure high CPU throughput)\n",
        "try:\n",
        "    torch.set_num_threads(24)\n",
        "    torch.set_num_interop_threads(1)\n",
        "    torch.backends.mkldnn.enabled = True\n",
        "    print('torch threads:', torch.get_num_threads(), 'interop:', torch.get_num_interop_threads())\n",
        "except Exception as e:\n",
        "    print('Thread config failed:', e)\n",
        "\n",
        "pred_map = {}  # product_id -> category_id\n",
        "fallback_cat = int(class_ids[0])\n",
        "start = time.time()\n",
        "img_tensors, owners = [], []\n",
        "img_count = 0\n",
        "predicted_so_far = 0\n",
        "log_every_preds = 2000\n",
        "\n",
        "with open('test.bson', 'rb') as f:\n",
        "    for doc in bson.decode_file_iter(f):\n",
        "        pid = int(doc['_id'])\n",
        "        pics = doc.get('imgs') or doc.get('pictures') or doc.get('images') or []\n",
        "        # Use only the largest image by byte size per product for speed\n",
        "        best_b = None\n",
        "        best_sz = -1\n",
        "        for p in pics:\n",
        "            b = p.get('picture') or p.get('img') or p.get('data')\n",
        "            if b is None:\n",
        "                continue\n",
        "            sz = len(b) if hasattr(b, '__len__') else 0\n",
        "            if sz > best_sz:\n",
        "                best_sz = sz\n",
        "                best_b = b\n",
        "        if best_b is not None:\n",
        "            im = pil_from_bytes(best_b)\n",
        "            if im is not None:\n",
        "                img_tensors.append(preprocess(im))\n",
        "                owners.append(pid)\n",
        "                img_count += 1\n",
        "\n",
        "        if len(img_tensors) >= BATCH_IMAGES:\n",
        "            prod_feats = flush_batch(img_tensors, owners)\n",
        "            img_tensors.clear(); owners.clear()\n",
        "            if prod_feats:\n",
        "                preds_chunk = hierarchical_score_topk(prod_feats)\n",
        "                pred_map.update(preds_chunk)\n",
        "                predicted_so_far = len(pred_map)\n",
        "                if predicted_so_far % log_every_preds == 0:\n",
        "                    dt = time.time() - start\n",
        "                    ips = img_count / max(dt, 1e-6)\n",
        "                    print(f'Pred products: {predicted_so_far}, imgs processed: {img_count}, elapsed: {dt:.1f}s, imgs/s: {ips:.1f}', flush=True)\n",
        "                if SUBSAMPLE_PRODUCTS is not None and predicted_so_far >= SUBSAMPLE_PRODUCTS:\n",
        "                    print(f'Stopping early after {predicted_so_far} products (profile)')\n",
        "                    break\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "if img_tensors:\n",
        "    prod_feats = flush_batch(img_tensors, owners)\n",
        "    if prod_feats:\n",
        "        preds_chunk = hierarchical_score_topk(prod_feats)\n",
        "        pred_map.update(preds_chunk)\n",
        "\n",
        "dt = time.time() - start\n",
        "ips = img_count / max(dt, 1e-6)\n",
        "print(f'Total predicted products: {len(pred_map)}; images processed: {img_count}; elapsed: {dt:.1f}s; imgs/s: {ips:.1f}')\n",
        "\n",
        "# Build submission in sample order; fill missing with fallback (should be minimal)\n",
        "preds = [pred_map.get(pid, fallback_cat) for pid in ss_ids]\n",
        "sub = pd.DataFrame({'_id': ss_ids, 'category_id': preds})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Wrote submission.csv with shape:', sub.shape)\n",
        "print(sub.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample submission size: 706990\nThread config failed: Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called\n"
          ]
        }
      ]
    },
    {
      "id": "f51e2034-f676-4360-9877-dc29dbb016a5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick scan: count products and images in test.bson, estimate runtime\n",
        "import bson, time, os\n",
        "from io import BytesIO\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "path = 'test.bson'\n",
        "size_gb = os.path.getsize(path) / (1024**3)\n",
        "print(f'test.bson size: {size_gb:.3f} GB')\n",
        "\n",
        "start = time.time()\n",
        "n_prod = 0\n",
        "n_imgs = 0\n",
        "bad_imgs = 0\n",
        "sample_limit = 2000  # stop early to estimate throughput\n",
        "t0 = time.time()\n",
        "with open(path, 'rb') as f:\n",
        "    for doc in bson.decode_file_iter(f):\n",
        "        n_prod += 1\n",
        "        pics = doc.get('imgs') or []\n",
        "        n_imgs += len(pics)\n",
        "        if n_prod % 10000 == 0:\n",
        "            dt = time.time() - start\n",
        "            print(f'Counted {n_prod} products, {n_imgs} images in {dt:.1f}s')\n",
        "        if sample_limit and n_prod >= sample_limit:\n",
        "            break\n",
        "dt = time.time() - start\n",
        "pps = n_prod / max(dt, 1e-6)\n",
        "ips = n_imgs / max(dt, 1e-6)\n",
        "print(f'Sampled {n_prod} products, {n_imgs} images in {dt:.2f}s  (products/s ~ {pps:.1f}, images/s ~ {ips:.1f})')\n",
        "print('Note: This is scan speed only (no model). Use to estimate total product count feasibility.')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test.bson size: 5.814 GB\nSampled 2000 products, 3242 images in 0.01s  (products/s ~ 201950.2, images/s ~ 327361.3)\nNote: This is scan speed only (no model). Use to estimate total product count feasibility.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}