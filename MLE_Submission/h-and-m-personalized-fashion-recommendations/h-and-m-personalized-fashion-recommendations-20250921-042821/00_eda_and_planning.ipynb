{
  "cells": [
    {
      "id": "208ebc16-f531-4942-b73e-fceb484f1c24",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# H&M Personalized Fashion Recommendations \u2014 Plan to Medal\n",
        "\n",
        "Objective: Achieve MAP@12 \u2265 0.02394 (bronze+) within 24h by building a strong, fast, GPU-accelerated recommender with robust temporal CV.\n",
        "\n",
        "Key constraints and approach:\n",
        "- Use GPU (A10) for any heavy modeling (XGBoost/CatBoost, FAISS).\n",
        "- Lock temporal CV mirroring test week. Avoid leakage.\n",
        "- Fast, proven candidate generation (co-visitation + popularity + simple content) \u2192 train a re-ranker.\n",
        "- Iterate on features with ablations; cache intermediates.\n",
        "\n",
        "Data expectations (Kaggle H&M dataset):\n",
        "- transactions_train.csv, customers.csv, articles.csv, sample_submission.csv, images/.\n",
        "- In this repo, images/ exist; locate CSVs (root or subdirs). If not present, locate prepared parquet/csv artifacts.\n",
        "\n",
        "Validation protocol:\n",
        "- Time-based split: last week(s) as validation target. Standard practice:\n",
        "  - Train range: up to 2020-09-15\n",
        "  - Valid target: 2020-09-16 to 2020-09-22 (one week) predicting next-week purchases per customer.\n",
        "- Reproduce LB-like CV by creating validation queries for customers with activity up to cutoff and labels in next week.\n",
        "- Metrics: MAP@12; use exact submission formatting for validation scoring.\n",
        "\n",
        "System checks (immediately):\n",
        "- Verify GPU availability.\n",
        "- Inspect directory structure; confirm presence/format of data artifacts.\n",
        "- If large CSVs: convert to feather/parquet once; load with dtypes to save RAM.\n",
        "\n",
        "Baseline (fast) deliverable (target \u2265 0.020 CV):\n",
        "1) Popularity per week (last-N days) and overall popularity fallback.\n",
        "2) Co-visitation matrix (item-item) using last 3-4 weeks window; generate top-100 candidates/customer.\n",
        "3) Blend candidates: co-visitation (weights by recency) + global recent popularity.\n",
        "4) Rank by blended scores; produce 12 per customer.\n",
        "\n",
        "Reranker (to push to medal range):\n",
        "- Model: XGBoost (GPU) or CatBoost (GPU).\n",
        "- Training data: (customer, candidate article, label if bought in target week).\n",
        "- Features:\n",
        "  Customer:\n",
        "  - Age, FN, Active, club_member_status, fashion_news_frequency, avg basket size, recency (days since last purchase), frequency counts, price spend stats.\n",
        "  Article:\n",
        "  - product_type_no, product_group_name, graphical_appearance_no, color_group, perceived_colour, section, garment_group, price, season/year, text length if description available.\n",
        "  Interactions:\n",
        "  - Days since customer last bought this article/product_type/product_group.\n",
        "  - Customer x article co-vis scores (from co-vis), session co-counts, recency-weighted counts.\n",
        "  - Similarity via simple TF-IDF of product_type/colour or target-encoding means.\n",
        "  - Price relativity: article price vs customer's median price.\n",
        "- Negative sampling: for each customer, sample from candidate pool not purchased in target window.\n",
        "- Loss: binary logistic; predict probability; rank top-12.\n",
        "\n",
        "Candidate generation details:\n",
        "- Build multiple co-vis matrices:\n",
        "  1) Standard co-vis (pairs within customer sessions/window).\n",
        "  2) Time-decayed co-vis (exponential decay).\n",
        "  3) Buy2buy only (pairs where both are purchases).\n",
        "- Optionally add simple content candidates:\n",
        "  - Top items per product_type within last week.\n",
        "  - FAISS on image embeddings (later, if time permits) to fill long-tail.\n",
        "- Cap candidates per source (e.g., 30 + 30 + 30 + popularity 50 \u2192 unique top ~150).\n",
        "\n",
        "Efficiency & caching:\n",
        "- Persist co-vis matrices and candidate lists to parquet.\n",
        "- Persist features matrices (train/valid/test) and predictions for quick iteration.\n",
        "- Log timings per step; subsample to debug.\n",
        "\n",
        "Milestones and Expert reviews:\n",
        "M1) Repo scan + GPU check + Data load plan (request expert review).\n",
        "M2) Temporal CV + baseline popularity and co-vis candidates; local CV MAP@12 (expert review).\n",
        "M3) Reranker v1 features + XGB GPU; OOF MAP@12 and speed (expert review).\n",
        "M4) Feature ablations (recency, price-rel, TE), tune early stopping; ensure stability with seeds (expert review).\n",
        "M5) Final train on full window, generate submission.csv, sanity-check top-12 formatting.\n",
        "\n",
        "Risk controls:\n",
        "- Prevent leakage: candidates/feats only from train window.\n",
        "- Handle cold-start customers: fallback to recent popularity by age/club segment, else global top-12.\n",
        "- Memory: cast dtypes (int32/int16, category), chunk reads.\n",
        "\n",
        "Next steps (in order):\n",
        "1) System/GPU check and list files.\n",
        "2) Load metadata heads; infer dtypes; plan parquet caching.\n",
        "3) Implement CV splitter and local MAP@12 evaluator.\n",
        "4) Build fast baseline candidates (co-vis + popularity) and produce a working submission.\n",
        "5) Add reranker features and train GPU model.\n",
        "6) Iterate features; finalize and submit."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "959cd29e-8948-4888-815e-a9c1971f9def",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, json, time, shutil, glob\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "print('=== System & GPU Check ===')\n",
        "start=time.time()\n",
        "def run(cmd):\n",
        "    try:\n",
        "        out = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT, text=True)\n",
        "        print(out)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(e.output)\n",
        "\n",
        "print('Python:', sys.version)\n",
        "print('CWD:', os.getcwd())\n",
        "print('GPU env vars CUDA_VISIBLE_DEVICES=', os.environ.get('CUDA_VISIBLE_DEVICES'))\n",
        "print('\\n[nvidia-smi]')\n",
        "run('nvidia-smi || true')\n",
        "\n",
        "print('\\n[Check GPU via PyTorch import if available]')\n",
        "try:\n",
        "    import torch\n",
        "    print('Torch version:', torch.__version__)\n",
        "    print('GPU Available:', torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print('GPU Count:', torch.cuda.device_count())\n",
        "        print('GPU Name:', torch.cuda.get_device_name(0))\n",
        "        props = torch.cuda.get_device_properties(0)\n",
        "        print(f'GPU Memory: {props.total_memory/1024**3:.1f} GB')\n",
        "except Exception as e:\n",
        "    print('Torch not available or failed to import:', e)\n",
        "\n",
        "print('\\n=== Scan for data artifacts (csv/parquet/feather) ===')\n",
        "root = Path('.')\n",
        "patterns = ['**/*.csv','**/*.parquet','**/*.feather']\n",
        "found = []\n",
        "for pat in patterns:\n",
        "    # limit to first 500 hits for performance\n",
        "    for p in root.glob(pat):\n",
        "        # skip very deep image trees\n",
        "        if 'images' in p.parts:\n",
        "            continue\n",
        "        try:\n",
        "            size = p.stat().st_size\n",
        "        except Exception:\n",
        "            size = -1\n",
        "        found.append((str(p), size))\n",
        "found.sort(key=lambda x: (-x[1], x[0]))\n",
        "for path,size in found[:50]:\n",
        "    print(f'{size/1024/1024:8.1f} MB  {path}')\n",
        "print(f'Total data-like files found: {len(found)}')\n",
        "\n",
        "print('\\n=== List top-level entries ===')\n",
        "for p in sorted(root.iterdir()):\n",
        "    try:\n",
        "        info = f'{p.name}/' if p.is_dir() else p.name\n",
        "        size = p.stat().st_size if p.is_file() else sum(f.stat().st_size for f in p.rglob('*') if f.is_file())\n",
        "        print(f'{size/1024/1024:8.1f} MB  {info}')\n",
        "    except Exception as e:\n",
        "        print(p, e)\n",
        "\n",
        "print(f'Completed in {time.time()-start:.1f}s')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== System & GPU Check ===\nPython: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\nCWD: /var/lib/simon/agent_run_states/h-and-m-personalized-fashion-recommendations-20250921-042821\nGPU env vars CUDA_VISIBLE_DEVICES= None\n\n[nvidia-smi]\nSun Sep 21 04:48:45 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     182MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\n\n[Check GPU via PyTorch import if available]\nTorch not available or failed to import: No module named 'torch'\n\n=== Scan for data artifacts (csv/parquet/feather) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  3268.5 MB  transactions_train.csv\n   257.8 MB  sample_submission.csv\n   257.8 MB  submission.csv\n   197.5 MB  customers.csv\n    34.5 MB  articles.csv\nTotal data-like files found: 5\n\n=== List top-level entries ===\n     0.0 MB  00_eda_and_planning.ipynb\n     0.1 MB  agent_metadata/\n    34.5 MB  articles.csv\n   197.5 MB  customers.csv\n     0.0 MB  description.md\n     0.0 MB  docker_run.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 29141.5 MB  images/\n     0.0 MB  requirements.txt\n   257.8 MB  sample_submission.csv\n   257.8 MB  submission.csv\n     0.0 MB  task.txt\n  3268.5 MB  transactions_train.csv\nCompleted in 1.7s\n"
          ]
        }
      ]
    },
    {
      "id": "230f5ef4-46bc-477a-80a1-602a3e0b9ed6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install deps and convert CSV -> Parquet with optimized dtypes\n",
        "import sys, subprocess, time, os\n",
        "t0=time.time()\n",
        "def pip_install(pkgs):\n",
        "    print('Installing:', ' '.join(pkgs), flush=True)\n",
        "    cmd=[sys.executable, '-m', 'pip', 'install', '-q'] + pkgs\n",
        "    res = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    print(res.stdout)\n",
        "\n",
        "need = ['polars', 'xgboost', 'catboost']\n",
        "pip_install(need)\n",
        "\n",
        "import polars as pl\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path('.')\n",
        "out_dir = Path('parquet'); out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print('Converting CSV to Parquet with downcast dtypes...', flush=True)\n",
        "\n",
        "# Transactions: keep core columns; cast compact dtypes. Keep all rows; filter price>0 later in features/labels.\n",
        "tx_csv = data_dir / 'transactions_train.csv'\n",
        "tx_parq = out_dir / 'transactions_train.parquet'\n",
        "if not tx_parq.exists():\n",
        "    t=time.time()\n",
        "    dtypes = {\n",
        "        't_dat': pl.Utf8,\n",
        "        'customer_id': pl.Utf8,\n",
        "        'article_id': pl.Int64,\n",
        "        'price': pl.Float64,\n",
        "        'sales_channel_id': pl.Int64,\n",
        "    }\n",
        "    print('Reading transactions lazily...', flush=True)\n",
        "    ldf = pl.scan_csv(str(tx_csv), schema_overrides=dtypes).select([\n",
        "        pl.col('t_dat'),\n",
        "        pl.col('customer_id'),\n",
        "        pl.col('article_id').cast(pl.Int32),\n",
        "        pl.col('price').cast(pl.Float32),\n",
        "        pl.col('sales_channel_id').cast(pl.Int8),\n",
        "    ]).with_columns([\n",
        "        pl.col('t_dat').str.strptime(pl.Date, format='%Y-%m-%d', strict=False).alias('t_dat'),\n",
        "        # customer_id stays as string; we can later map to int codes per pipeline step\n",
        "    ])\n",
        "    print('Writing transactions to Parquet...', flush=True)\n",
        "    ldf.collect(streaming=True).write_parquet(str(tx_parq))\n",
        "    print(f'transactions -> {tx_parq} done in {time.time()-t:.1f}s')\n",
        "else:\n",
        "    print('Skipping transactions; Parquet exists')\n",
        "\n",
        "# Customers\n",
        "cust_csv = data_dir / 'customers.csv'\n",
        "cust_parq = out_dir / 'customers.parquet'\n",
        "if not cust_parq.exists():\n",
        "    t=time.time()\n",
        "    print('Reading customers...', flush=True)\n",
        "    ldf = pl.scan_csv(str(cust_csv)).with_columns([\n",
        "        pl.col('FN').cast(pl.Int8, strict=False),\n",
        "        pl.col('Active').cast(pl.Int8, strict=False),\n",
        "        pl.col('age').cast(pl.Int16, strict=False),\n",
        "    ])\n",
        "    print('Writing customers to Parquet...', flush=True)\n",
        "    ldf.collect(streaming=True).write_parquet(str(cust_parq))\n",
        "    print(f'customers -> {cust_parq} done in {time.time()-t:.1f}s')\n",
        "else:\n",
        "    print('Skipping customers; Parquet exists')\n",
        "\n",
        "# Articles\n",
        "art_csv = data_dir / 'articles.csv'\n",
        "art_parq = out_dir / 'articles.parquet'\n",
        "if not art_parq.exists():\n",
        "    t=time.time()\n",
        "    print('Reading articles...', flush=True)\n",
        "    ldf = pl.scan_csv(str(art_csv))\n",
        "    print('Writing articles to Parquet...', flush=True)\n",
        "    ldf.collect(streaming=True).write_parquet(str(art_parq))\n",
        "    print(f'articles -> {art_parq} done in {time.time()-t:.1f}s')\n",
        "else:\n",
        "    print('Skipping articles; Parquet exists')\n",
        "\n",
        "print(f'All conversions completed in {time.time()-t0:.1f}s', flush=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing: polars xgboost catboost\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Target directory /app/.pip-target/catboost-1.2.8.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/catboost already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/matplotlib-3.10.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpl_toolkits already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/matplotlib already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pylab.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pandas-2.3.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pandas already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/xgboost-3.0.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/xgboost.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/xgboost already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/contourpy-1.3.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/contourpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/plotly-6.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/plotly already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/_plotly_utils already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/python_dateutil-2.9.0.post0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/dateutil already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy-1.16.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/cycler-0.12.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/cycler already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fonttools-4.60.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fontTools already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/graphviz-0.21.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/graphviz already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/kiwisolver-1.4.9.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/kiwisolver already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/narwhals-2.5.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/narwhals already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.28.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging-25.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/polars already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/polars-1.33.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pyparsing-3.2.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pyparsing already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/six-1.17.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/six.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tzdata-2025.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tzdata already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pytz-2025.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pytz already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/etc already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n\nConverting CSV to Parquet with downcast dtypes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading transactions lazily...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing transactions to Parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_50/1707250167.py:45: DeprecationWarning: the `streaming` parameter was deprecated in 1.25.0; use `engine` instead.\n  ldf.collect(streaming=True).write_parquet(str(tx_parq))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transactions -> parquet/transactions_train.parquet done in 7.9s\nReading customers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing customers to Parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_50/1707250167.py:62: DeprecationWarning: the `streaming` parameter was deprecated in 1.25.0; use `engine` instead.\n  ldf.collect(streaming=True).write_parquet(str(cust_parq))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "customers -> parquet/customers.parquet done in 1.3s\nReading articles...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing articles to Parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "articles -> parquet/articles.parquet done in 0.1s\nAll conversions completed in 32.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_50/1707250167.py:75: DeprecationWarning: the `streaming` parameter was deprecated in 1.25.0; use `engine` instead.\n  ldf.collect(streaming=True).write_parquet(str(art_parq))\n"
          ]
        }
      ]
    },
    {
      "id": "3bd08c9b-93ff-48f8-983d-e11759773d04",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CV dates, MAP@12 scorer, and helpers (no execution yet)\n",
        "from datetime import date, timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Fixed CV window matching LB-like split\n",
        "TRAIN_END = date(2020, 9, 15)\n",
        "VAL_START = date(2020, 9, 16)\n",
        "VAL_END   = date(2020, 9, 22)\n",
        "\n",
        "def map12_score(y_true_dict, y_pred_dict):\n",
        "    # y_true_dict: {customer_id: set(article_ids)}\n",
        "    # y_pred_dict: {customer_id: [article_ids ordered length<=12]}\n",
        "    K = 12\n",
        "    aps = []\n",
        "    for cid, preds in y_pred_dict.items():\n",
        "        labels = y_true_dict.get(cid, set())\n",
        "        if not isinstance(labels, set):\n",
        "            labels = set(labels)\n",
        "        hit = 0\n",
        "        prec_sum = 0.0\n",
        "        for i, aid in enumerate(preds[:K]):\n",
        "            if aid in labels:\n",
        "                hit += 1\n",
        "                prec_sum += hit / (i + 1)\n",
        "        if len(labels) == 0:\n",
        "            aps.append(0.0)  # customers with no purchases contribute 0 as per competition\n",
        "        else:\n",
        "            aps.append(prec_sum / min(len(labels), K))\n",
        "    return float(np.mean(aps)) if aps else 0.0\n",
        "\n",
        "def ensure_12(preds_list, filler_list):\n",
        "    out = []\n",
        "    seen = set()\n",
        "    for a in preds_list:\n",
        "        if a not in seen:\n",
        "            out.append(a); seen.add(a)\n",
        "        if len(out) == 12:\n",
        "            return out\n",
        "    for a in filler_list:\n",
        "        if a not in seen:\n",
        "            out.append(a); seen.add(a)\n",
        "        if len(out) == 12:\n",
        "            break\n",
        "    if len(out) < 12:\n",
        "        out += [filler_list[0]] * (12 - len(out))\n",
        "    return out\n",
        "\n",
        "def load_sample_customers(sample_path='sample_submission.csv') -> pd.Series:\n",
        "    ss = pd.read_csv(sample_path, usecols=['customer_id'])\n",
        "    return ss['customer_id']\n",
        "\n",
        "def read_transactions_parquet(path='parquet/transactions_train.parquet', cols=None):\n",
        "    import polars as pl\n",
        "    cols = cols or ['t_dat','customer_id','article_id','price','sales_channel_id']\n",
        "    return pl.read_parquet(path, columns=cols)\n",
        "\n",
        "def compute_global_popularity(pl_df, start_date, end_date, topk=20000):\n",
        "    # pl_df: Polars DataFrame with t_dat (Date), article_id (int), price (float32)\n",
        "    import polars as pl\n",
        "    df = pl_df.filter((pl.col('t_dat') >= pl.lit(start_date)) & (pl.col('t_dat') <= pl.lit(end_date)) & (pl.col('price') > 0))\n",
        "    pop = (df.group_by('article_id').len().sort('len', descending=True).select('article_id').head(topk))\n",
        "    return pop['article_id'].to_list()\n",
        "\n",
        "def to_submission_format(pred_dict, out_path='submission.csv'):\n",
        "    # pred_dict: {customer_id: [aid,...12]} aids are ints\n",
        "    rows = []\n",
        "    for cid, aids in pred_dict.items():\n",
        "        preds_str = ' '.join([str(a).zfill(10) for a in aids[:12]])\n",
        "        rows.append((cid, preds_str))\n",
        "    sub = pd.DataFrame(rows, columns=['customer_id','prediction'])\n",
        "    sub.to_csv(out_path, index=False)\n",
        "    return out_path\n",
        "\n",
        "print('Helpers ready: MAP@12 scorer, ensure_12, global popularity, submission writer.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "46bfec18-5b2b-4cca-bbe4-7fa1585744df",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fast baseline: recent 7-day popularity -> submission.csv\n",
        "import time\n",
        "t0=time.time()\n",
        "import polars as pl\n",
        "\n",
        "parq_tx = 'parquet/transactions_train.parquet'\n",
        "print('Loading transactions parquet (columns: t_dat, article_id, price)...', flush=True)\n",
        "tx = pl.read_parquet(parq_tx, columns=['t_dat','article_id','price'])\n",
        "max_day = tx.select(pl.col('t_dat').max()).item()\n",
        "start_day = max_day - pl.duration(days=6)\n",
        "print(f'Max date in data: {max_day}; Popularity window: {start_day}..{max_day}', flush=True)\n",
        "\n",
        "tx7 = tx.filter((pl.col('t_dat') >= start_day) & (pl.col('t_dat') <= max_day) & (pl.col('price') > 0))\n",
        "pop7 = (tx7.group_by('article_id').len().sort('len', descending=True).select('article_id'))\n",
        "topN = pop7['article_id'].to_list()\n",
        "if not topN:\n",
        "    # fallback if window empty (shouldn't happen)\n",
        "    pop_all = tx.filter(pl.col('price') > 0).group_by('article_id').len().sort('len', descending=True).select('article_id')\n",
        "    topN = pop_all['article_id'].to_list()\n",
        "top12 = [int(a) for a in topN[:12]]\n",
        "print('Top12 articles:', top12, flush=True)\n",
        "\n",
        "import pandas as pd\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "def ensure_12_fill(aids, filler):\n",
        "    out=[]; seen=set()\n",
        "    for a in aids:\n",
        "        if a not in seen:\n",
        "            out.append(a); seen.add(a)\n",
        "        if len(out)==12: break\n",
        "    i=0\n",
        "    while len(out)<12:\n",
        "        a=filler[i % len(filler)]\n",
        "        if a not in seen:\n",
        "            out.append(a); seen.add(a)\n",
        "        i+=1\n",
        "    return out\n",
        "\n",
        "pred_str = ' '.join([str(a).zfill(10) for a in ensure_12_fill(top12, topN[:1000])])\n",
        "ss['prediction'] = pred_str  # same string for all customers as a smoke baseline\n",
        "out_path = 'submission.csv'\n",
        "ss.to_csv(out_path, index=False)\n",
        "print(f'Wrote baseline submission to {out_path} in {time.time()-t0:.1f}s; size={Path(out_path).stat().st_size/1024/1024:.2f} MB', flush=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading transactions parquet (columns: t_dat, article_id, price)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max date in data: 2020-09-14; Popularity window: [(2020-09-14) - (6d.alias(\"duration\"))]..2020-09-14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top12 articles: [909370001, 865799006, 918522001, 448509014, 751471001, 924243001, 918292001, 762846027, 863646001, 809238001, 715624001, 673677002]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote baseline submission to submission.csv in 4.5s; size=257.76 MB\n"
          ]
        }
      ]
    },
    {
      "id": "783d3765-81bd-4e8d-a390-4f426b6bdda2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Temporal CV: popularity-only baseline MAP@12\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def map12_score(y_true_dict, y_pred_dict):\n",
        "    K = 12\n",
        "    aps = []\n",
        "    for cid, preds in y_pred_dict.items():\n",
        "        labels = y_true_dict.get(cid, set())\n",
        "        if not isinstance(labels, set):\n",
        "            labels = set(labels)\n",
        "        hit = 0\n",
        "        prec_sum = 0.0\n",
        "        for i, aid in enumerate(preds[:K]):\n",
        "            if aid in labels:\n",
        "                hit += 1\n",
        "                prec_sum += hit / (i + 1)\n",
        "        if len(labels) == 0:\n",
        "            aps.append(0.0)\n",
        "        else:\n",
        "            aps.append(prec_sum / min(len(labels), K))\n",
        "    return float(np.mean(aps)) if aps else 0.0\n",
        "\n",
        "t0=time.time()\n",
        "tx = pl.read_parquet('parquet/transactions_train.parquet', columns=['t_dat','customer_id','article_id','price'])\n",
        "max_day = tx.select(pl.col('t_dat').max()).item()  # python date\n",
        "VAL_END = max_day\n",
        "VAL_START = VAL_END - timedelta(days=6)\n",
        "TRAIN_END = VAL_START - timedelta(days=1)\n",
        "print(f'CV windows -> TRAIN_END: {TRAIN_END}, VAL: {VAL_START}..{VAL_END}', flush=True)\n",
        "\n",
        "# Build labels for validation week (price>0), dedup customer-article in week\n",
        "val_df = tx.filter((pl.col('t_dat') >= pl.lit(VAL_START)) & (pl.col('t_dat') <= pl.lit(VAL_END)) & (pl.col('price') > 0))\n",
        "val_pairs = val_df.select(['customer_id','article_id']).unique(maintain_order=True)\n",
        "labels = {}\n",
        "for cid, grp in val_pairs.to_pandas().groupby('customer_id'):\n",
        "    labels[cid] = set(grp['article_id'].astype(int).tolist())\n",
        "print('Validation customers with purchases:', len(labels))\n",
        "\n",
        "# Popularity computed on last 7 days of training window\n",
        "hist_start = TRAIN_END - timedelta(days=6)\n",
        "hist_end = TRAIN_END\n",
        "print(f'Popularity window for training: {hist_start}..{hist_end}', flush=True)\n",
        "hist = tx.filter((pl.col('t_dat') >= pl.lit(hist_start)) & (pl.col('t_dat') <= pl.lit(hist_end)) & (pl.col('price') > 0))\n",
        "pop = hist.group_by('article_id').len().sort('len', descending=True).select('article_id')\n",
        "pop_list = [int(a) for a in pop['article_id'].to_list()]\n",
        "fallback = pop_list[:1000] if len(pop_list)>=12 else (tx.filter(pl.col('price')>0).group_by('article_id').len().sort('len', descending=True).select('article_id')['article_id'].to_list())\n",
        "fallback = [int(a) for a in fallback][:1000]\n",
        "top12 = fallback[:12]\n",
        "\n",
        "# Predictions for all customers in sample_submission\n",
        "ss_cust = pd.read_csv('sample_submission.csv', usecols=['customer_id'])['customer_id']\n",
        "pred_dict = {}\n",
        "for cid in ss_cust.values:\n",
        "    pred_dict[cid] = top12  # same list for baseline\n",
        "\n",
        "# Score MAP@12\n",
        "score = map12_score(labels, pred_dict)\n",
        "print(f'CV MAP@12 (popularity-only): {score:.5f}  | elapsed {time.time()-t0:.1f}s')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV windows -> TRAIN_END: 2020-09-07, VAL: 2020-09-08..2020-09-14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation customers with purchases: 74575\nPopularity window for training: 2020-09-01..2020-09-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV MAP@12 (popularity-only): 0.00037  | elapsed 6.0s\n"
          ]
        }
      ]
    },
    {
      "id": "11897ff9-6ffd-403e-8711-00dbee279732",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Temporal CV: Repurchase + popularity baseline\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def map12_score(y_true_dict, y_pred_dict):\n",
        "    K = 12\n",
        "    aps = []\n",
        "    for cid, preds in y_pred_dict.items():\n",
        "        labels = y_true_dict.get(cid, set())\n",
        "        if not isinstance(labels, set):\n",
        "            labels = set(labels)\n",
        "        hit = 0\n",
        "        prec_sum = 0.0\n",
        "        for i, aid in enumerate(preds[:K]):\n",
        "            if aid in labels:\n",
        "                hit += 1\n",
        "                prec_sum += hit / (i + 1)\n",
        "        if len(labels) == 0:\n",
        "            aps.append(0.0)\n",
        "        else:\n",
        "            aps.append(prec_sum / min(len(labels), K))\n",
        "    return float(np.mean(aps)) if aps else 0.0\n",
        "\n",
        "t0=time.time()\n",
        "tx = pl.read_parquet('parquet/transactions_train.parquet', columns=['t_dat','customer_id','article_id','price'])\n",
        "max_day = tx.select(pl.col('t_dat').max()).item()\n",
        "VAL_END = max_day\n",
        "VAL_START = VAL_END - timedelta(days=6)\n",
        "TRAIN_END = VAL_START - timedelta(days=1)\n",
        "print(f'Windows -> TRAIN_END: {TRAIN_END}, VAL: {VAL_START}..{VAL_END}', flush=True)\n",
        "\n",
        "# Labels for validation (price>0), unique per customer-article within the week\n",
        "val_df = tx.filter((pl.col('t_dat') >= pl.lit(VAL_START)) & (pl.col('t_dat') <= pl.lit(VAL_END)) & (pl.col('price') > 0))\n",
        "val_pairs = val_df.select(['customer_id','article_id']).unique(maintain_order=True).to_pandas()\n",
        "labels = {cid: set(g.article_id.astype(int).tolist()) for cid, g in val_pairs.groupby('customer_id')}\n",
        "print('Validation customers with purchases:', len(labels))\n",
        "\n",
        "# Training slice (last 12 weeks) for repurchase + popularity\n",
        "train_start = TRAIN_END - timedelta(days=7*12)\n",
        "hist_start = TRAIN_END - timedelta(days=6)\n",
        "hist_end = TRAIN_END\n",
        "print(f'Train slice: {train_start}..{TRAIN_END} | Pop window: {hist_start}..{hist_end}', flush=True)\n",
        "train_tx = tx.filter((pl.col('t_dat') >= pl.lit(train_start)) & (pl.col('t_dat') <= pl.lit(TRAIN_END)) & (pl.col('price') > 0))\n",
        "\n",
        "# Global popularity (fallback)\n",
        "pop = train_tx.filter((pl.col('t_dat') >= pl.lit(hist_start)) & (pl.col('t_dat') <= pl.lit(hist_end))).group_by('article_id').len().sort('len', descending=True).select('article_id')\n",
        "pop_list = [int(a) for a in pop['article_id'].to_list()]\n",
        "if len(pop_list) < 12:\n",
        "    pop_all = train_tx.group_by('article_id').len().sort('len', descending=True).select('article_id')\n",
        "    pop_list = [int(a) for a in pop_all['article_id'].to_list()]\n",
        "fallback = pop_list[:5000]\n",
        "\n",
        "# Repurchase candidates: last 20 distinct items per customer ordered by recency\n",
        "train_tx_pd = train_tx.select(['customer_id','article_id','t_dat']).to_pandas()\n",
        "train_tx_pd['article_id'] = train_tx_pd['article_id'].astype(int)\n",
        "train_tx_pd = train_tx_pd.sort_values(['customer_id','t_dat'], ascending=[True, False])\n",
        "repurchase = {}\n",
        "for cid, g in train_tx_pd.groupby('customer_id', sort=False):\n",
        "    seen=set(); lst=[]\n",
        "    for aid in g['article_id'].values:\n",
        "        if aid not in seen:\n",
        "            lst.append(aid); seen.add(aid)\n",
        "            if len(lst) >= 20:\n",
        "                break\n",
        "    repurchase[cid]=lst\n",
        "print('Repurchase dict size:', len(repurchase))\n",
        "\n",
        "# Build predictions for all customers in sample_submission\n",
        "ss_cust = pd.read_csv('sample_submission.csv', usecols=['customer_id'])['customer_id']\n",
        "pred = {}\n",
        "for cid in ss_cust.values:\n",
        "    base = repurchase.get(cid, [])\n",
        "    # fill to 12 with popularity without duplicates\n",
        "    seen=set(base)\n",
        "    for a in fallback:\n",
        "        if len(base) >= 12: break\n",
        "        if a not in seen:\n",
        "            base.append(a); seen.add(a)\n",
        "    if len(base) < 12 and fallback:\n",
        "        base += [fallback[0]] * (12-len(base))\n",
        "    pred[cid] = base[:12]\n",
        "\n",
        "cv = map12_score(labels, pred)\n",
        "print(f'CV MAP@12 (repurchase + pop): {cv:.5f} | elapsed {time.time()-t0:.1f}s', flush=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Windows -> TRAIN_END: 2020-09-07, VAL: 2020-09-08..2020-09-14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation customers with purchases: 74575\nTrain slice: 2020-06-15..2020-09-07 | Pop window: 2020-09-01..2020-09-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repurchase dict size: 523693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV MAP@12 (repurchase + pop): 0.00131 | elapsed 26.7s\n"
          ]
        }
      ]
    },
    {
      "id": "a9ca08c1-4825-4ffd-b245-e9548701b4a4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Co-visitation (buy2buy + decayed) candidates + CV MAP@12\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def map12_score(y_true_dict, y_pred_dict):\n",
        "    K = 12\n",
        "    aps = []; \n",
        "    for cid, preds in y_pred_dict.items():\n",
        "        labels = y_true_dict.get(cid, set())\n",
        "        if not isinstance(labels, set):\n",
        "            labels = set(labels)\n",
        "        hit = 0; prec_sum = 0.0\n",
        "        for i, aid in enumerate(preds[:K]):\n",
        "            if aid in labels:\n",
        "                hit += 1; prec_sum += hit / (i + 1)\n",
        "        aps.append(0.0 if len(labels)==0 else (prec_sum / min(len(labels), K)))\n",
        "    return float(np.mean(aps)) if aps else 0.0\n",
        "\n",
        "t0=time.time()\n",
        "tx = pl.read_parquet('parquet/transactions_train.parquet', columns=['t_dat','customer_id','article_id','price'])\n",
        "max_day = tx.select(pl.col('t_dat').max()).item()\n",
        "VAL_END = max_day\n",
        "VAL_START = VAL_END - timedelta(days=6)\n",
        "TRAIN_END = VAL_START - timedelta(days=1)\n",
        "print(f'Windows -> TRAIN_END: {TRAIN_END}, VAL: {VAL_START}..{VAL_END}', flush=True)\n",
        "\n",
        "# Labels in validation week\n",
        "val_df = tx.filter((pl.col('t_dat') >= pl.lit(VAL_START)) & (pl.col('t_dat') <= pl.lit(VAL_END)) & (pl.col('price') > 0))\n",
        "val_pairs = val_df.select(['customer_id','article_id']).unique(maintain_order=True).to_pandas()\n",
        "labels = {cid: set(g.article_id.astype(int).tolist()) for cid, g in val_pairs.groupby('customer_id')}\n",
        "print('Validation customers with purchases:', len(labels))\n",
        "\n",
        "# Training slice for co-vis (last 10 weeks), price>0 only\n",
        "train_start = TRAIN_END - timedelta(days=7*10)\n",
        "train_tx = tx.filter((pl.col('t_dat') >= pl.lit(train_start)) & (pl.col('t_dat') <= pl.lit(TRAIN_END)) & (pl.col('price') > 0))\n",
        "print(f'Train slice for covis: {train_start}..{TRAIN_END}  | rows={train_tx.height}', flush=True)\n",
        "\n",
        "# Global popularity fallback (last 7 days of train window)\n",
        "hist_start = TRAIN_END - timedelta(days=6)\n",
        "hist_end = TRAIN_END\n",
        "pop = train_tx.filter((pl.col('t_dat') >= pl.lit(hist_start)) & (pl.col('t_dat') <= pl.lit(hist_end))).group_by('article_id').len().sort('len', descending=True).select('article_id')\n",
        "pop_list = [int(a) for a in pop['article_id'].to_list()]\n",
        "if len(pop_list) < 12:\n",
        "    pop_all = train_tx.group_by('article_id').len().sort('len', descending=True).select('article_id')\n",
        "    pop_list = [int(a) for a in pop_all['article_id'].to_list()]\n",
        "fallback_pop = pop_list[:20000]\n",
        "\n",
        "# Build customer-day baskets (cap 30 items/day), compute recency weight\n",
        "print('Building co-vis pairs...', flush=True)\n",
        "cov_t0 = time.time()\n",
        "baskets = (\n",
        "    train_tx.select([\n",
        "        pl.col('customer_id'),\n",
        "        pl.col('t_dat'),\n",
        "        pl.col('article_id').alias('aid')\n",
        "    ])\n",
        "    .group_by(['customer_id','t_dat'])\n",
        "    .agg(pl.col('aid').head(30).alias('items'))\n",
        ")\n",
        "print('Baskets:', baskets.height, 'elapsed', f'{time.time()-cov_t0:.1f}s', flush=True)\n",
        "\n",
        "# Prepare for self-join\n",
        "cov_t1 = time.time()\n",
        "b1 = baskets.explode('items').rename({'items':'aid_i'})\n",
        "b2 = baskets.explode('items').rename({'items':'aid_j'})\n",
        "# Join on same customer_id & t_dat to get unordered pairs, drop self-pairs\n",
        "pairs = (\n",
        "    b1.join(b2, on=['customer_id','t_dat'])\n",
        "      .filter(pl.col('aid_i') != pl.col('aid_j'))\n",
        ")\n",
        "# Weight by recency: w = 0.9^(days_ago/7)\n",
        "days_ago = (pl.lit(TRAIN_END) - pl.col('t_dat')).dt.total_days().cast(pl.Float32)\n",
        "pairs = pairs.with_columns((0.9 ** (days_ago / 7.0)).alias('w'))\n",
        "print('Pairs rows:', pairs.height, 'elapsed', f'{time.time()-cov_t1:.1f}s', flush=True)\n",
        "\n",
        "# Aggregate scores for aid_i -> aid_j\n",
        "cov_t2 = time.time()\n",
        "covis = pairs.group_by(['aid_i','aid_j']).agg(pl.col('w').sum().alias('score'))\n",
        "print('Covis pairs unique:', covis.height, 'elapsed', f'{time.time()-cov_t2:.1f}s', flush=True)\n",
        "\n",
        "# Keep top 100 neighbors per aid_i\n",
        "cov_t3 = time.time()\n",
        "covis_pd = covis.sort(['aid_i','score'], descending=[False, True]).to_pandas()\n",
        "top_neighbors = {}; \n",
        "for (aid_i), grp in covis_pd.groupby('aid_i', sort=False):\n",
        "    g = grp.nlargest(100, columns='score')\n",
        "    top_neighbors[int(aid_i)] = list(zip(g['aid_j'].astype(int).tolist(), g['score'].astype(float).tolist()))\n",
        "print('Neighbors built for aids:', len(top_neighbors), 'elapsed', f'{time.time()-cov_t3:.1f}s', flush=True)\n",
        "\n",
        "# Build customer recent history (last 20 distinct aids up to TRAIN_END)\n",
        "hist_t0 = time.time()\n",
        "cust_hist_pd = (\n",
        "    train_tx.select(['customer_id','article_id','t_dat'])\n",
        "            .sort(['customer_id','t_dat'], descending=[False, True])\n",
        "            .to_pandas()\n",
        ")\n",
        "cust_hist_pd['article_id'] = cust_hist_pd['article_id'].astype(int)\n",
        "cust_recent = {}\n",
        "for cid, g in cust_hist_pd.groupby('customer_id', sort=False):\n",
        "    seen=set(); lst=[]\n",
        "    for aid in g['article_id'].values:\n",
        "        if aid not in seen:\n",
        "            lst.append(aid); seen.add(aid)\n",
        "            if len(lst) >= 20: break\n",
        "    cust_recent[cid] = lst\n",
        "print('Customer histories:', len(cust_recent), 'elapsed', f'{time.time()-hist_t0:.1f}s', flush=True)\n",
        "\n",
        "# Generate candidates per customer by expanding neighbors of recent items\n",
        "ss_cust = pd.read_csv('sample_submission.csv', usecols=['customer_id'])['customer_id']\n",
        "pred = {}\n",
        "gen_t0 = time.time()\n",
        "for idx, cid in enumerate(ss_cust.values):\n",
        "    base = []; seen = set()\n",
        "    # repurchases first (strong recency bias)\n",
        "    for a in cust_recent.get(cid, [])[:20]:\n",
        "        if a not in seen:\n",
        "            base.append(a); seen.add(a)\n",
        "    # expand via neighbors\n",
        "    for a in cust_recent.get(cid, [])[:10]:\n",
        "        neigh = top_neighbors.get(a, [])\n",
        "        for nb, sc in neigh[:30]:\n",
        "            if nb not in seen:\n",
        "                base.append(nb); seen.add(nb)\n",
        "            if len(base) >= 150:\n",
        "                break\n",
        "        if len(base) >= 150:\n",
        "            break\n",
        "    # fill with popularity\n",
        "    for a in fallback_pop:\n",
        "        if len(base) >= 12: break\n",
        "        if a not in seen:\n",
        "            base.append(a); seen.add(a)\n",
        "    if len(base) < 12 and fallback_pop:\n",
        "        base += [fallback_pop[0]] * (12-len(base))\n",
        "    pred[cid] = base[:12]\n",
        "    if idx % 200000 == 0:\n",
        "        print(f'Built preds for {idx} customers...', flush=True)\n",
        "print('Candidate generation + preds done in', f'{time.time()-gen_t0:.1f}s', flush=True)\n",
        "\n",
        "# Score MAP@12\n",
        "cv = map12_score(labels, pred)\n",
        "print(f'CV MAP@12 (covis buy2buy decayed + repurchase + pop): {cv:.5f} | total elapsed {time.time()-t0:.1f}s', flush=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Windows -> TRAIN_END: 2020-09-07, VAL: 2020-09-08..2020-09-14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation customers with purchases: 74575\nTrain slice for covis: 2020-06-29..2020-09-07  | rows=2969342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building co-vis pairs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baskets: 902963 elapsed 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pairs rows: 14812350 elapsed 0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Covis pairs unique: 8308274 elapsed 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neighbors built for aids: 37954 elapsed 20.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer histories: 450514 elapsed 12.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built preds for 0 customers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built preds for 200000 customers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built preds for 400000 customers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built preds for 600000 customers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built preds for 800000 customers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built preds for 1000000 customers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built preds for 1200000 customers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Candidate generation + preds done in 15.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV MAP@12 (covis buy2buy decayed + repurchase + pop): 0.00137 | total elapsed 56.1s\n"
          ]
        }
      ]
    },
    {
      "id": "5f738b88-cee5-4609-8a01-2bde1bd8c9c8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Co-vis scored blend (short-decayed + long-nondecayed) with ACTIVE/ALL MAP and recall diagnostics (DAILY RAW, no norm, no sym)\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import time, os, pickle\n",
        "\n",
        "def map12_all_and_active(y_true_dict, y_pred_dict):\n",
        "    K=12\n",
        "    # ALL\n",
        "    aps_all=[]\n",
        "    for cid, preds in y_pred_dict.items():\n",
        "        labels = y_true_dict.get(cid, set())\n",
        "        if not isinstance(labels, set): labels=set(labels)\n",
        "        hit=0; prec=0.0\n",
        "        for i,a in enumerate(preds[:K]):\n",
        "            if a in labels:\n",
        "                hit+=1; prec+=hit/(i+1)\n",
        "        aps_all.append(0.0 if len(labels)==0 else (prec/min(len(labels),K)))\n",
        "    all_map = float(np.mean(aps_all)) if aps_all else 0.0\n",
        "    # ACTIVE\n",
        "    aps_act=[]\n",
        "    for cid, labels in y_true_dict.items():\n",
        "        preds = y_pred_dict.get(cid, [])\n",
        "        hit=0; prec=0.0\n",
        "        for i,a in enumerate(preds[:K]):\n",
        "            if a in labels:\n",
        "                hit+=1; prec+=hit/(i+1)\n",
        "        aps_act.append(prec/min(len(labels),K))\n",
        "    act_map = float(np.mean(aps_act)) if aps_act else 0.0\n",
        "    return all_map, act_map\n",
        "\n",
        "def recall_at_k_active(y_true_dict, y_pred_dict, k=100):\n",
        "    hits=0; total=0\n",
        "    for cid, labels in y_true_dict.items():\n",
        "        total += len(labels)\n",
        "        if total==0: continue\n",
        "        preds = set(y_pred_dict.get(cid, [])[:k])\n",
        "        hits += sum(1 for a in labels if a in preds)\n",
        "    return hits/total if total>0 else 0.0\n",
        "\n",
        "t0=time.time()\n",
        "tx = pl.read_parquet('parquet/transactions_train.parquet', columns=['t_dat','customer_id','article_id','price'])\n",
        "max_day = tx.select(pl.col('t_dat').max()).item()\n",
        "VAL_END = max_day\n",
        "VAL_START = VAL_END - timedelta(days=6)\n",
        "TRAIN_END = VAL_START - timedelta(days=1)\n",
        "print(f'Windows -> TRAIN_END: {TRAIN_END}, VAL: {VAL_START}..{VAL_END}', flush=True)\n",
        "\n",
        "# Labels\n",
        "val_df = tx.filter((pl.col('t_dat') >= pl.lit(VAL_START)) & (pl.col('t_dat') <= pl.lit(VAL_END)) & (pl.col('price') > 0))\n",
        "val_pairs = val_df.select(['customer_id','article_id']).unique(maintain_order=True).to_pandas()\n",
        "labels = {cid: set(g.article_id.astype(int).tolist()) for cid, g in val_pairs.groupby('customer_id')}\n",
        "print('Active customers in VAL:', len(labels))\n",
        "\n",
        "# Windows\n",
        "short_start = TRAIN_END - timedelta(days=7*6)\n",
        "long_start  = TRAIN_END - timedelta(days=7*12)\n",
        "ultra_start = TRAIN_END - timedelta(days=7*2)\n",
        "print(f'Co-vis ultra: {ultra_start}..{TRAIN_END} | short: {short_start}..{TRAIN_END} | long: {long_start}..{TRAIN_END}', flush=True)\n",
        "\n",
        "# Popularity fallback (last 7 days of train window)\n",
        "hist_start = TRAIN_END - timedelta(days=6)\n",
        "hist_end = TRAIN_END\n",
        "train_tx_all = tx.filter((pl.col('t_dat') >= pl.lit(long_start)) & (pl.col('t_dat') <= pl.lit(TRAIN_END)) & (pl.col('price') > 0))\n",
        "pop = train_tx_all.filter((pl.col('t_dat') >= pl.lit(hist_start)) & (pl.col('t_dat') <= pl.lit(hist_end))).group_by('article_id').len().sort('len', descending=True).select('article_id')\n",
        "fallback_pop = [int(a) for a in pop['article_id'].to_list()][:20000]\n",
        "if len(fallback_pop) < 12:\n",
        "    pop_all = train_tx_all.group_by('article_id').len().sort('len', descending=True).select('article_id')\n",
        "    fallback_pop = [int(a) for a in pop_all['article_id'].to_list()][:20000]\n",
        "\n",
        "def build_covis_daily_raw(start_date, end_date, decay=None, topk=200):\n",
        "    t=time.time()\n",
        "    df = tx.filter((pl.col('t_dat') >= pl.lit(start_date)) & (pl.col('t_dat') <= pl.lit(end_date)) & (pl.col('price') > 0))\n",
        "    # daily baskets: unique items per customer-day, cap 30 (preserve order for stability)\n",
        "    baskets = (df.select([pl.col('customer_id'), pl.col('t_dat'), pl.col('article_id').alias('aid')])\n",
        "                 .group_by(['customer_id','t_dat'])\n",
        "                 .agg(pl.col('aid').unique(maintain_order=True).head(30).alias('items')))\n",
        "    b1 = baskets.explode('items').rename({'items':'aid_i'})\n",
        "    b2 = baskets.explode('items').rename({'items':'aid_j'})\n",
        "    pairs = (b1.join(b2, on=['customer_id','t_dat']).filter(pl.col('aid_i') != pl.col('aid_j')))\n",
        "    # recency weight on day\n",
        "    if decay is None:\n",
        "        pairs = pairs.with_columns(pl.lit(1.0).alias('w'))\n",
        "    else:\n",
        "        days_ago = (pl.lit(end_date) - pl.col('t_dat')).dt.total_days().cast(pl.Float32)\n",
        "        pairs = pairs.with_columns((decay ** (days_ago / 7.0)).alias('w'))\n",
        "    covis = pairs.group_by(['aid_i','aid_j']).agg(pl.col('w').sum().alias('score'))\n",
        "    covis = covis.filter(pl.col('score') > 1e-9)\n",
        "    covis_pd = covis.sort(['aid_i','score'], descending=[False, True]).to_pandas()\n",
        "    top = {}\n",
        "    for aid_i, grp in covis_pd.groupby('aid_i', sort=False):\n",
        "        g = grp.nlargest(topk, columns='score')\n",
        "        top[int(aid_i)] = dict(zip(g['aid_j'].astype(int).values, g['score'].astype(float).values))\n",
        "    print(f'Built DAILY RAW covis for {len(top)} aids in {time.time()-t:.1f}s  [{start_date}..{end_date}] topk={topk}')\n",
        "    return top\n",
        "\n",
        "cache_dir='cache'; os.makedirs(cache_dir, exist_ok=True)\n",
        "topk_items = 200\n",
        "short_cache = os.path.join(cache_dir, f'covis_short_daily_raw_{short_start}_{TRAIN_END}_top{topk_items}.pkl')\n",
        "long_cache  = os.path.join(cache_dir, f'covis_long_daily_raw_{long_start}_{TRAIN_END}_top{topk_items}.pkl')\n",
        "ultra_cache = os.path.join(cache_dir, f'covis_ultra_daily_raw_{ultra_start}_{TRAIN_END}_top{topk_items}.pkl')\n",
        "\n",
        "if os.path.exists(ultra_cache):\n",
        "    with open(ultra_cache,'rb') as f: covis_ultra = pickle.load(f); print('Loaded covis_ultra from cache')\n",
        "else:\n",
        "    covis_ultra = build_covis_daily_raw(ultra_start, TRAIN_END, decay=0.9, topk=topk_items)\n",
        "    with open(ultra_cache,'wb') as f: pickle.dump(covis_ultra, f)\n",
        "if os.path.exists(short_cache):\n",
        "    with open(short_cache,'rb') as f: covis_short = pickle.load(f); print('Loaded covis_short from cache')\n",
        "else:\n",
        "    covis_short = build_covis_daily_raw(short_start, TRAIN_END, decay=0.9, topk=topk_items)\n",
        "    with open(short_cache,'wb') as f: pickle.dump(covis_short, f)\n",
        "if os.path.exists(long_cache):\n",
        "    with open(long_cache,'rb') as f: covis_long = pickle.load(f); print('Loaded covis_long from cache')\n",
        "else:\n",
        "    covis_long  = build_covis_daily_raw(long_start, TRAIN_END, decay=None, topk=topk_items)\n",
        "    with open(long_cache,'wb') as f: pickle.dump(covis_long, f)\n",
        "\n",
        "# Customer recent distinct history (last 20) from long window\n",
        "t_hist=time.time()\n",
        "hist_pd = (train_tx_all.select(['customer_id','article_id','t_dat'])\n",
        "           .sort(['customer_id','t_dat'], descending=[False, True])\n",
        "           .to_pandas())\n",
        "hist_pd['article_id']=hist_pd['article_id'].astype(int)\n",
        "cust_recent={}\n",
        "for cid, g in hist_pd.groupby('customer_id', sort=False):\n",
        "    seen=set(); lst=[]\n",
        "    for aid in g['article_id'].values:\n",
        "        if aid not in seen:\n",
        "            lst.append(aid); seen.add(aid)\n",
        "            if len(lst)>=20: break\n",
        "    cust_recent[cid]=lst\n",
        "print('Customer histories:', len(cust_recent), 'built in', f'{time.time()-t_hist:.1f}s', flush=True)\n",
        "\n",
        "# Scored aggregation per customer (blend ultra/short/long + strong repeat)\n",
        "w_ultra = 0.20\n",
        "w_short = 0.70\n",
        "w_long  = 0.10\n",
        "repeat_self_base = 8.0  # multiplied by 0.8^p\n",
        "repeat_mult_if_in_hist = 1.6  # multiplicative\n",
        "per_seed_neigh_cap = 100\n",
        "candidate_pool_cap = 400\n",
        "\n",
        "ss_cust = pd.read_csv('sample_submission.csv', usecols=['customer_id'])['customer_id']\n",
        "pred12 = {}\n",
        "pred100 = {}  # for recall@100 diagnostics\n",
        "t_pred=time.time()\n",
        "for idx, cid in enumerate(ss_cust.values):\n",
        "    hist = cust_recent.get(cid, [])\n",
        "    cand_score = {}\n",
        "    hist_set = set(hist)\n",
        "    # seed direct repurchases with strong prior, decayed by position\n",
        "    for p, h in enumerate(hist[:20]):\n",
        "        w_hist = (0.8 ** p)\n",
        "        cand_score[h] = cand_score.get(h, 0.0) + repeat_self_base * w_hist\n",
        "    # neighbor expansion, per-seed cap\n",
        "    for p, h in enumerate(hist[:20]):\n",
        "        w_hist = (0.8 ** p)\n",
        "        nu = covis_ultra.get(h, {})\n",
        "        ns = covis_short.get(h, {})\n",
        "        nl = covis_long.get(h, {})\n",
        "        if nu or ns or nl:\n",
        "            cnt=0\n",
        "            keys = set(nu.keys()) | set(ns.keys()) | set(nl.keys())\n",
        "            # precompute blend to sort efficiently\n",
        "            def blend(x):\n",
        "                return w_ultra*nu.get(x,0.0) + w_short*ns.get(x,0.0) + w_long*nl.get(x,0.0)\n",
        "            for nb in sorted(keys, key=lambda x: -blend(x)):\n",
        "                sc = blend(nb)\n",
        "                if sc <= 0: continue\n",
        "                cand_score[nb] = cand_score.get(nb, 0.0) + w_hist * sc\n",
        "                cnt+=1\n",
        "                if cnt>=per_seed_neigh_cap: break\n",
        "    # multiplicative boost for items in recent history\n",
        "    for nb in list(cand_score.keys()):\n",
        "        if nb in hist_set:\n",
        "            cand_score[nb] *= repeat_mult_if_in_hist\n",
        "    # rank and cap candidate pools\n",
        "    ranked = sorted(cand_score.items(), key=lambda x: -x[1])\n",
        "    base_candidates = [nb for nb,_ in ranked[:candidate_pool_cap]]\n",
        "    # fill with popularity to ensure at least 12/100 unique\n",
        "    seen=set(base_candidates)\n",
        "    filled = list(base_candidates)\n",
        "    for a in fallback_pop:\n",
        "        if len(filled) >= 100: break\n",
        "        if a not in seen:\n",
        "            filled.append(a); seen.add(a)\n",
        "    if len(filled) < 100 and fallback_pop:\n",
        "        filled += [fallback_pop[0]] * (100-len(filled))\n",
        "    pred100[cid] = filled[:100]\n",
        "    pred12[cid] = filled[:12]\n",
        "    if idx % 200000 == 0:\n",
        "        print(f'Preds built for {idx} customers...', flush=True)\n",
        "print('Predictions built in', f'{time.time()-t_pred:.1f}s', flush=True)\n",
        "\n",
        "all_map, act_map = map12_all_and_active(labels, pred12)\n",
        "recall100 = recall_at_k_active(labels, pred100, k=100)\n",
        "recall12 = recall_at_k_active(labels, pred12, k=12)\n",
        "print(f'MAP@12 ALL: {all_map:.5f} | ACTIVE: {act_map:.5f} | ACTIVE recall@100: {recall100:.3f} | ACTIVE recall@12: {recall12:.3f} | total elapsed {time.time()-t0:.1f}s', flush=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Windows -> TRAIN_END: 2020-09-07, VAL: 2020-09-08..2020-09-14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Active customers in VAL: 74575\nCo-vis ultra: 2020-08-24..2020-09-07 | short: 2020-07-27..2020-09-07 | long: 2020-06-15..2020-09-07\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}