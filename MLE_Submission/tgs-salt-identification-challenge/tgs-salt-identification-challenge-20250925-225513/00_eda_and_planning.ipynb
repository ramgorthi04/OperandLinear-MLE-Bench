{
  "cells": [
    {
      "id": "4950c070-5d30-4d18-9b8d-a2a8e9869b7b",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: TGS Salt Identification Challenge\n",
        "\n",
        "Objectives:\n",
        "- Establish GPU-ready environment and fast baseline segmentation pipeline.\n",
        "- Robust CV with folds saved; produce OOF + test predictions.\n",
        "- Target Dice/BCE-trained U-Net with ImageNet encoder; TTA + postprocessing.\n",
        "\n",
        "Data & Metric:\n",
        "- Images: 101x101 grayscale; Masks same size. depths.csv provides scalar feature.\n",
        "- Metric: mean precision at IoU thresholds (0.5..0.95).\n",
        "\n",
        "Validation:\n",
        "- 5-fold StratifiedKFold by mask coverage buckets (e.g., 0, (0,0.1], ..., (0.9,1]).\n",
        "- Deterministic seeds; save folds indices to disk.\n",
        "\n",
        "Baseline Model:\n",
        "- PyTorch + timm: U-Net (or FPN/DeepLabV3) with ResNet34/50 encoder.\n",
        "- Input: pad to 128x128; 2 channels [image, depth_norm] or concat depth via broadcast.\n",
        "- Loss: BCEWithLogits + Dice (e.g., 0.5/0.5).\n",
        "- Optimizer: AdamW; Scheduler: Cosine or OneCycle. Mixed precision.\n",
        "- Augment: flips, shifts, slight rotate, brightness/contrast, elastic (light).\n",
        "\n",
        "Inference:\n",
        "- TTA: hflip/vflip (4x) average logits.\n",
        "- Postprocess: sigmoid -> threshold tuning on OOF; remove small objects, fill holes.\n",
        "\n",
        "Milestones:\n",
        "1) Env check (GPU), install torch/cu121 + libs.\n",
        "2) EDA: verify files, shapes, coverage distribution; leak check.\n",
        "3) Data pipeline + folds saving.\n",
        "4) Baseline train 5-10 epochs to verify; get OOF score proxy (Dice).\n",
        "5) Full training 30-50 epochs with early stopping; log per-fold times.\n",
        "6) TTA + postproc threshold search; generate submission.csv.\n",
        "7) If time: larger encoder, blend seeds/models.\n",
        "\n",
        "We will request expert review at: plan, after EDA, after baseline, after tuning, and before final submit."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "5562e57e-bf0b-4e2f-a996-aca102b7a279",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, time, json, shutil\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "print('=== GPU CHECK: nvidia-smi ===', flush=True)\n",
        "try:\n",
        "    out = subprocess.run(['bash','-lc','nvidia-smi || true'], capture_output=True, text=True, check=False)\n",
        "    print(out.stdout)\n",
        "except Exception as e:\n",
        "    print('nvidia-smi error:', e)\n",
        "\n",
        "print('=== Torch CUDA Sanity ===', flush=True)\n",
        "try:\n",
        "    import torch\n",
        "    print('torch:', torch.__version__, 'CUDA build:', getattr(torch.version,'cuda',None))\n",
        "    print('CUDA available:', torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print('GPU:', torch.cuda.get_device_name(0))\n",
        "except Exception as e:\n",
        "    print('Torch not available or error:', e)\n",
        "\n",
        "print('=== List files ===', flush=True)\n",
        "for p in sorted(Path('.').iterdir()):\n",
        "    try:\n",
        "        print(p, '->', 'dir' if p.is_dir() else p.stat().st_size, 'bytes' if p.is_file() else '')\n",
        "    except Exception:\n",
        "        print(p)\n",
        "\n",
        "print('=== Peek CSVs ===', flush=True)\n",
        "for csv in ['train.csv','depths.csv','sample_submission.csv']:\n",
        "    if Path(csv).exists():\n",
        "        df = pd.read_csv(csv)\n",
        "        print(csv, df.shape)\n",
        "        print(df.head(3))\n",
        "\n",
        "print('=== Train/Test folders ===', flush=True)\n",
        "for d in ['train','test']:\n",
        "    p = Path(d)\n",
        "    if p.exists():\n",
        "        imgs = list(p.glob('images/*.png'))[:3]\n",
        "        masks = list(p.glob('masks/*.png'))[:3] if (p/'masks').exists() else []\n",
        "        print(d, 'images sample:', [str(x.name) for x in imgs])\n",
        "        print(d, 'masks sample:', [str(x.name) for x in masks])\n",
        "\n",
        "print('ENV CHECK COMPLETE', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "85440998-6e97-4b3d-884e-3d625db4f289",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, shutil, time\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print('> pip', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "print('=== Install CUDA 12.1 Torch stack ===', flush=True)\n",
        "for pkg in ('torch','torchvision','torchaudio'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "for d in (\n",
        "    '/app/.pip-target/torch',\n",
        "    '/app/.pip-target/torchvision',\n",
        "    '/app/.pip-target/torchaudio',\n",
        "    '/app/.pip-target/torch-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.19.1.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchgen',\n",
        "    '/app/.pip-target/functorch',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d, flush=True)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "pip('install',\n",
        "    '--index-url','https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url','https://pypi.org/simple',\n",
        "    'torch==2.4.1','torchvision==0.19.1','torchaudio==2.4.1')\n",
        "\n",
        "Path('constraints.txt').write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "\n",
        "print('=== Install segmentation deps (honor constraints) ===', flush=True)\n",
        "pip('install','-c','constraints.txt',\n",
        "    'segmentation-models-pytorch==0.3.3',\n",
        "    'timm',\n",
        "    'albumentations',\n",
        "    'opencv-python-headless',\n",
        "    'scikit-image',\n",
        "    'scikit-learn',\n",
        "    'scipy',\n",
        "    'pandas',\n",
        "    'numpy',\n",
        "    '--upgrade-strategy','only-if-needed')\n",
        "\n",
        "print('=== Sanity check torch/CUDA ===', flush=True)\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'built CUDA:', getattr(torch.version,'cuda',None), flush=True)\n",
        "print('CUDA available:', torch.cuda.is_available(), flush=True)\n",
        "assert str(getattr(torch.version,'cuda','')).startswith('12.1'), f'Wrong CUDA build: {torch.version.cuda}'\n",
        "assert torch.cuda.is_available(), 'CUDA not available after install'\n",
        "print('GPU:', torch.cuda.get_device_name(0), flush=True)\n",
        "print('INSTALL COMPLETE', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1f5c053e-0459-4a19-a141-8466b2c8ba68",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Folds + utilities: coverage bins, depth norm stats, RLE, pad/crop\n",
        "import os, gc, math, time, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "DATA_DIR = Path('.')\n",
        "TRAIN_IMG_DIR = DATA_DIR/'train/images'\n",
        "TRAIN_MASK_DIR = DATA_DIR/'train/masks'\n",
        "TEST_IMG_DIR = DATA_DIR/'test/images'\n",
        "OUT_DIR = DATA_DIR/'out'; OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def read_gray(path):\n",
        "    img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        raise FileNotFoundError(path)\n",
        "    return img\n",
        "\n",
        "def reflect_pad_to_128(img):\n",
        "    # img: HxW (101x101)\n",
        "    h, w = img.shape[:2]\n",
        "    assert h == 101 and w == 101, f'Unexpected shape: {img.shape}'\n",
        "    pad_top = (128 - h) // 2\n",
        "    pad_bottom = 128 - h - pad_top\n",
        "    pad_left = (128 - w) // 2\n",
        "    pad_right = 128 - w - pad_left\n",
        "    return cv2.copyMakeBorder(img, pad_top, pad_bottom, pad_left, pad_right, borderType=cv2.BORDER_REFLECT_101)\n",
        "\n",
        "def crop_center_101(img):\n",
        "    # img: 128x128 -> center crop back to 101x101\n",
        "    h, w = img.shape[:2]\n",
        "    assert h == 128 and w == 128, f'Unexpected shape: {img.shape}'\n",
        "    s = 101\n",
        "    y0 = (h - s)//2\n",
        "    x0 = (w - s)//2\n",
        "    return img[y0:y0+s, x0:x0+s]\n",
        "\n",
        "def rle_encode(mask):\n",
        "    # mask: 2D binary (H,W) 0/1; TGS expects column-major flatten\n",
        "    pixels = mask.T.flatten()\n",
        "    # 1-indexed runs\n",
        "    runs = []\n",
        "    prev = -2\n",
        "    for i, val in enumerate(pixels, start=1):\n",
        "        if val and (i > 1 and pixels[i-2] == 0):\n",
        "            runs.append(i)\n",
        "        if val and (i == len(pixels) or pixels[i-1] == 0):\n",
        "            runs.append(i - (runs[-1] if runs else i) + 1)\n",
        "    return ' '.join(map(str, runs))\n",
        "\n",
        "def coverage_of_mask(mask):\n",
        "    return float(mask.sum()) / float(mask.size)\n",
        "\n",
        "print('Scanning train ids...', flush=True)\n",
        "train_ids = sorted([p.stem for p in TRAIN_IMG_DIR.glob('*.png')])\n",
        "print('Train count:', len(train_ids), flush=True)\n",
        "\n",
        "print('Load depths...', flush=True)\n",
        "depths = pd.read_csv(DATA_DIR/'depths.csv')\n",
        "depths = depths.set_index('id').reindex(train_ids)\n",
        "z_vals = depths['z'].values.astype(np.float32)\n",
        "z_min, z_max = float(np.nanmin(z_vals)), float(np.nanmax(z_vals))\n",
        "print('Depth z_min/z_max:', z_min, z_max, flush=True)\n",
        "json.dump({'z_min': z_min, 'z_max': z_max}, open(OUT_DIR/'depth_norm.json','w'))\n",
        "\n",
        "print('Compute coverage for stratification...', flush=True)\n",
        "coverages = []\n",
        "t0 = time.time()\n",
        "for i, tid in enumerate(train_ids):\n",
        "    m = read_gray(TRAIN_MASK_DIR/f'{tid}.png')\n",
        "    m = (m>127).astype(np.uint8)\n",
        "    coverages.append(coverage_of_mask(m))\n",
        "    if (i+1)%500==0:\n",
        "        print(f'  processed {i+1}/{len(train_ids)} in {time.time()-t0:.1f}s', flush=True)\n",
        "coverages = np.array(coverages, dtype=np.float32)\n",
        "\n",
        "print('Build stratification bins...', flush=True)\n",
        "bins = np.digitize(coverages, bins=np.linspace(0.0, 1.0, 11), right=True)\n",
        "empty_bin = (coverages == 0.0).astype(int)  # ensure empties separation\n",
        "y_strat = bins + 100*empty_bin  # combine\n",
        "\n",
        "folds_csv = OUT_DIR/'folds.csv'\n",
        "if folds_csv.exists():\n",
        "    print('folds.csv exists; will overwrite to ensure determinism', flush=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rows = []\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(train_ids, y_strat)):\n",
        "    for idx in val_idx:\n",
        "        rows.append({'id': train_ids[idx], 'fold': fold})\n",
        "folds_df = pd.DataFrame(rows).set_index('id').loc[train_ids].reset_index()\n",
        "folds_df.to_csv(folds_csv, index=False)\n",
        "print('Saved folds to', folds_csv, flush=True)\n",
        "print(folds_df['fold'].value_counts().sort_index())\n",
        "\n",
        "print('Done folds/utilities setup.', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "d0f0107a-0343-462e-abf4-7a47c085b9f5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training: SMP UNet-ResNet34, 2-channel (image + depth), 5-fold CV, AMP, OneCycle, HFlip TTA\n",
        "import os, time, math, json, gc\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import albumentations as A\n",
        "import segmentation_models_pytorch as smp\n",
        "from skimage.morphology import remove_small_objects\n",
        "from scipy.ndimage import binary_fill_holes\n",
        "\n",
        "DATA_DIR = Path('.')\n",
        "TRAIN_IMG_DIR = DATA_DIR/'train/images'\n",
        "TRAIN_MASK_DIR = DATA_DIR/'train/masks'\n",
        "TEST_IMG_DIR = DATA_DIR/'test/images'\n",
        "OUT_DIR = DATA_DIR/'out'\n",
        "OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Reuse helpers from previous cell by redefining if not in scope\n",
        "def read_gray(path):\n",
        "    img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None: raise FileNotFoundError(path)\n",
        "    return img\n",
        "def reflect_pad_to_128(img):\n",
        "    h,w = img.shape[:2]; assert (h,w)==(101,101)\n",
        "    pt=(128-h)//2; pb=128-h-pt; pl=(128-w)//2; pr=128-w-pl\n",
        "    return cv2.copyMakeBorder(img, pt, pb, pl, pr, cv2.BORDER_REFLECT_101)\n",
        "def crop_center_101(img):\n",
        "    h,w = img.shape[:2]; assert (h,w)==(128,128)\n",
        "    s=101; y0=(h-s)//2; x0=(w-s)//2; return img[y0:y0+s, x0:x0+s]\n",
        "def rle_encode(mask):\n",
        "    pixels = mask.T.flatten()\n",
        "    runs=[]\n",
        "    for i in range(1, len(pixels)+1):\n",
        "        if pixels[i-1] and (i==1 or pixels[i-2]==0):\n",
        "            runs.append(i)\n",
        "        if pixels[i-1] and (i==len(pixels) or pixels[i]==0):\n",
        "            runs.append(i - runs[-1] + 1)\n",
        "    return ' '.join(map(str, runs))\n",
        "\n",
        "def tgs_metric(y_true, y_pred):\n",
        "    # y_true, y_pred: (N, H, W) bool arrays\n",
        "    thresholds = np.arange(0.5, 1.0, 0.05)\n",
        "    scores=[]\n",
        "    for t in thresholds:\n",
        "        inter = (y_true & y_pred).sum(axis=(1,2)).astype(np.float32)\n",
        "        union = (y_true | y_pred).sum(axis=(1,2)).astype(np.float32)\n",
        "        iou = np.where(union>0, inter/union, (y_pred.sum(axis=(1,2))==0))\n",
        "        scores.append((iou>t).mean())\n",
        "    return np.mean(scores)\n",
        "\n",
        "class SaltDataset(Dataset):\n",
        "    def __init__(self, ids, depths_df, z_min, z_max, aug=None, is_train=True):\n",
        "        self.ids = ids\n",
        "        self.depths = depths_df\n",
        "        self.z_min = z_min; self.z_max = z_max\n",
        "        self.aug = aug\n",
        "        self.is_train = is_train\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        _id = self.ids[idx]\n",
        "        img = read_gray(TRAIN_IMG_DIR/f'{_id}.png') if self.is_train else read_gray(TEST_IMG_DIR/f'{_id}.png')\n",
        "        img = reflect_pad_to_128(img).astype(np.float32)/255.0\n",
        "        if self.is_train:\n",
        "            mask = read_gray(TRAIN_MASK_DIR/f'{_id}.png')\n",
        "            mask = reflect_pad_to_128(mask)\n",
        "            mask = (mask>127).astype(np.float32)\n",
        "        else:\n",
        "            mask = np.zeros_like(img, dtype=np.float32)\n",
        "        # depth channel\n",
        "        z = float(self.depths.loc[_id, 'z']) if _id in self.depths.index else float(self.depths.loc[_id])\n",
        "        z_norm = (z - self.z_min) / max(1e-6, (self.z_max - self.z_min))\n",
        "        depth_ch = np.full_like(img, z_norm, dtype=np.float32)\n",
        "        img2 = np.stack([img, depth_ch], axis=0)  # (2,128,128)\n",
        "        if self.aug is not None:\n",
        "            data = {'image': img2.transpose(1,2,0), 'mask': mask}\n",
        "            data = self.aug(**data)\n",
        "            im = data['image'].transpose(2,0,1)\n",
        "            mk = data['mask']\n",
        "        else:\n",
        "            im = img2; mk = mask\n",
        "        return torch.from_numpy(im).float(), torch.from_numpy(mk[None]).float(), _id\n",
        "\n",
        "def get_augs():\n",
        "    train_tfms = A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=10, border_mode=cv2.BORDER_REFLECT_101, p=0.7),\n",
        "        A.GridDistortion(num_steps=3, distort_limit=0.05, border_mode=cv2.BORDER_REFLECT_101, p=0.2),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
        "    ])\n",
        "    val_tfms = A.Compose([])\n",
        "    return train_tfms, val_tfms\n",
        "\n",
        "def bce_dice_loss():\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "    def dice_loss(logits, targets, eps=1e-6):\n",
        "        probs = torch.sigmoid(logits)\n",
        "        num = 2*(probs*targets).sum(dim=(2,3))\n",
        "        den = (probs*probs + targets*targets).sum(dim=(2,3)) + eps\n",
        "        dice = 1 - (num/den)\n",
        "        return dice.mean()\n",
        "    def loss_fn(logits, targets):\n",
        "        return 0.5*bce(logits, targets) + 0.5*dice_loss(logits, targets)\n",
        "    return loss_fn\n",
        "\n",
        "def train_fold(fold, train_ids, val_ids, depths_df, z_min, z_max, epochs=45, batch_size=64, max_lr=1e-3):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    train_tfms, val_tfms = get_augs()\n",
        "    ds_tr = SaltDataset(train_ids, depths_df, z_min, z_max, aug=train_tfms, is_train=True)\n",
        "    ds_va = SaltDataset(val_ids, depths_df, z_min, z_max, aug=val_tfms, is_train=True)\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    model = smp.Unet(encoder_name='resnet34', encoder_weights='imagenet', in_channels=2, classes=1, activation=None)\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=1e-4)\n",
        "    steps_per_epoch = max(1, math.ceil(len(ds_tr)/batch_size))\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=steps_per_epoch, pct_start=0.3, div_factor=10.0, final_div_factor=10.0)\n",
        "    scaler = GradScaler(enabled=(device=='cuda'))\n",
        "    loss_fn = bce_dice_loss()\n",
        "\n",
        "    best_dice = -1.0; best_path = OUT_DIR/f'ckpt_fold{fold}.pth'\n",
        "    t_start = time.time()\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train(); tr_loss=0.0; t0=time.time()\n",
        "        for it,(x,y,_) in enumerate(dl_tr):\n",
        "            x=x.to(device, non_blocking=True); y=y.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast(enabled=(device=='cuda')):\n",
        "                logits = model(x)\n",
        "                loss = loss_fn(logits, y)\n",
        "            scaler.scale(loss).step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            tr_loss += loss.item()*x.size(0)\n",
        "            if (it+1)%50==0: print(f'[fold {fold}] epoch {epoch} iter {it+1}/{len(dl_tr)} elapsed {time.time()-t0:.1f}s', flush=True)\n",
        "        tr_loss/=len(ds_tr)\n",
        "        # Val\n",
        "        model.eval(); dices=[]\n",
        "        with torch.no_grad():\n",
        "            for x,y,_ in dl_va:\n",
        "                x=x.to(device); y=y.to(device)\n",
        "                logits = model(x)\n",
        "                probs = torch.sigmoid(logits)\n",
        "                # threshold 0.5 dice\n",
        "                pred = (probs>0.5).float()\n",
        "                num = 2*(pred*y).sum(dim=(2,3))\n",
        "                den = (pred.sum(dim=(2,3)) + y.sum(dim=(2,3)) + 1e-6)\n",
        "                dices.append((num/den).detach().cpu().numpy())\n",
        "        val_dice = float(np.concatenate(dices).mean())\n",
        "        print(f'[fold {fold}] epoch {epoch}/{epochs} tr_loss {tr_loss:.4f} val_dice {val_dice:.4f} epoch_time {time.time()-t0:.1f}s total {time.time()-t_start:.1f}s', flush=True)\n",
        "        if val_dice>best_dice:\n",
        "            best_dice=val_dice\n",
        "            torch.save({'model': model.state_dict(), 'dice': best_dice}, best_path)\n",
        "    print(f'[fold {fold}] best_dice {best_dice:.4f} saved {best_path}', flush=True)\n",
        "    return str(best_path), best_dice\n",
        "\n",
        "def infer_fold(fold, ckpt_path, val_ids, test_ids, depths_df, z_min, z_max, batch_size=64, tta_hflip=True):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    val_ds = SaltDataset(val_ids, depths_df, z_min, z_max, aug=A.Compose([]), is_train=True)\n",
        "    test_ds = SaltDataset(test_ids, depths_df, z_min, z_max, aug=A.Compose([]), is_train=False)\n",
        "    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    model = smp.Unet(encoder_name='resnet34', encoder_weights=None, in_channels=2, classes=1, activation=None)\n",
        "    sd = torch.load(ckpt_path, map_location='cpu')['model']\n",
        "    model.load_state_dict(sd, strict=True); model.to(device); model.eval()\n",
        "    # VAL logits\n",
        "    val_logits=[]; val_order=[]\n",
        "    with torch.no_grad():\n",
        "        for x,_,ids in val_dl:\n",
        "            x = x.to(device)\n",
        "            logits = model(x)\n",
        "            if tta_hflip:\n",
        "                x2 = torch.flip(x, dims=[-1])\n",
        "                logits2 = model(x2)\n",
        "                logits2 = torch.flip(logits2, dims=[-1])\n",
        "                logits = 0.5*(logits + logits2)\n",
        "            val_logits.append(logits.float().cpu().numpy())\n",
        "            val_order.extend(ids)\n",
        "    val_logits = np.concatenate(val_logits, axis=0)[:,0]  # (N,128,128)\n",
        "    # TEST logits\n",
        "    test_logits=[]; test_order=[]\n",
        "    with torch.no_grad():\n",
        "        for x,_,ids in test_dl:\n",
        "            x = x.to(device)\n",
        "            logits = model(x)\n",
        "            if tta_hflip:\n",
        "                x2 = torch.flip(x, dims=[-1])\n",
        "                logits2 = model(x2)\n",
        "                logits2 = torch.flip(logits2, dims=[-1])\n",
        "                logits = 0.5*(logits + logits2)\n",
        "            test_logits.append(logits.float().cpu().numpy())\n",
        "            test_order.extend(ids)\n",
        "    test_logits = np.concatenate(test_logits, axis=0)[:,0]\n",
        "    np.save(OUT_DIR/f'val_logits_fold{fold}.npy', val_logits)\n",
        "    np.save(OUT_DIR/f'test_logits_fold{fold}.npy', test_logits)\n",
        "    pd.Series(val_order).to_csv(OUT_DIR/f'val_ids_fold{fold}.csv', index=False, header=False)\n",
        "    pd.Series(test_order).to_csv(OUT_DIR/f'test_ids_fold{fold}.csv', index=False, header=False)\n",
        "    print(f'[fold {fold}] saved val/test logits', flush=True)\n",
        "    return val_order, val_logits, test_order, test_logits\n",
        "\n",
        "# Orchestrate 5-fold train + infer\n",
        "folds_df = pd.read_csv(OUT_DIR/'folds.csv')\n",
        "train_ids_all = folds_df['id'].tolist()\n",
        "fold_by_id = dict(zip(folds_df['id'], folds_df['fold']))\n",
        "depths = pd.read_csv(DATA_DIR/'depths.csv').set_index('id')\n",
        "z_stats = json.load(open(OUT_DIR/'depth_norm.json')) if (OUT_DIR/'depth_norm.json').exists() else None\n",
        "if z_stats is None:\n",
        "    z_vals = depths.loc[train_ids_all, 'z'].values.astype(np.float32)\n",
        "    z_stats = {'z_min': float(np.min(z_vals)), 'z_max': float(np.max(z_vals))}\n",
        "z_min, z_max = z_stats['z_min'], z_stats['z_max']\n",
        "test_ids_all = sorted([p.stem for p in TEST_IMG_DIR.glob('*.png')])\n",
        "\n",
        "all_val_ids=[]; all_val_logits=[]; test_logits_folds=[]\n",
        "for fold in range(5):\n",
        "    tr_ids = [i for i in train_ids_all if fold_by_id[i]!=fold]\n",
        "    va_ids = [i for i in train_ids_all if fold_by_id[i]==fold]\n",
        "    print(f'=== Fold {fold}: train {len(tr_ids)} val {len(va_ids)} ===', flush=True)\n",
        "    ckpt_path, best = train_fold(fold, tr_ids, va_ids, depths, z_min, z_max, epochs=45, batch_size=64, max_lr=1e-3)\n",
        "    va_order, va_logits, te_order, te_logits = infer_fold(fold, ckpt_path, va_ids, test_ids_all, depths, z_min, z_max, batch_size=64, tta_hflip=True)\n",
        "    all_val_ids.extend(va_order); all_val_logits.append(va_logits); test_logits_folds.append(te_logits)\n",
        "    # free gpu cache\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "all_val_logits = np.concatenate(all_val_logits, axis=0)\n",
        "oof_df = pd.DataFrame({'id': all_val_ids})\n",
        "oof_df['logit_idx'] = np.arange(len(oof_df))\n",
        "np.save(OUT_DIR/'oof_logits.npy', all_val_logits)\n",
        "oof_df.to_csv(OUT_DIR/'oof_order.csv', index=False)\n",
        "print('Saved OOF logits and order', flush=True)\n",
        "\n",
        "# Build OOF probs aligned to ground truth and compute tuning\n",
        "gt_masks=[]; oof_probs=[]\n",
        "for _id, logit_idx in zip(oof_df['id'].tolist(), oof_df['logit_idx'].tolist()):\n",
        "    mask = read_gray(TRAIN_MASK_DIR/f'{_id}.png'); mask = reflect_pad_to_128(mask); mask = (mask>127)\n",
        "    gt_masks.append(mask)\n",
        "    oof_probs.append(1/(1+np.exp(-all_val_logits[logit_idx])))\n",
        "gt_masks = np.stack(gt_masks, axis=0)\n",
        "oof_probs = np.stack(oof_probs, axis=0)\n",
        "\n",
        "def postprocess(prob, thr, min_size):\n",
        "    m = prob>thr\n",
        "    m = remove_small_objects(m, min_size=min_size) if m.any() else m\n",
        "    m = binary_fill_holes(m)\n",
        "    return m\n",
        "\n",
        "ths = np.linspace(0.3, 0.7, 9)\n",
        "mins = [5,10,20,30,40,50,75,100]\n",
        "best_score=-1; best_thr=0.5; best_min=0\n",
        "for thr in ths:\n",
        "    for ms in mins:\n",
        "        preds = np.stack([postprocess(p, thr, ms) for p in oof_probs], axis=0)\n",
        "        score = tgs_metric(gt_masks, preds)\n",
        "        if score>best_score:\n",
        "            best_score=score; best_thr=thr; best_min=ms\n",
        "print(f'Tuned on OOF: best mp-IoU {best_score:.5f} @ thr {best_thr:.3f}, min_size {best_min}', flush=True)\n",
        "\n",
        "# Average test logits across folds and generate submission\n",
        "test_logits_folds = np.stack(test_logits_folds, axis=0)  # (5,N,128,128)\n",
        "test_logits_mean = test_logits_folds.mean(axis=0)\n",
        "test_probs = 1/(1+np.exp(-test_logits_mean))\n",
        "\n",
        "sub_ids = test_ids_all\n",
        "rles=[]\n",
        "for prob, _id in zip(test_probs, sub_ids):\n",
        "    # crop to 101x101 before RLE\n",
        "    prob_101 = crop_center_101((prob*255).astype(np.uint8)) / 255.0\n",
        "    mask = postprocess(prob_101, best_thr, best_min)\n",
        "    rles.append(rle_encode(mask.astype(np.uint8)))\n",
        "sub = pd.DataFrame({'id': sub_ids, 'rle_mask': rles})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "5d2b1f26-bd31-4944-a5af-879084f59938",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, traceback\n",
        "import cv2; cv2.setNumThreads(0)\n",
        "print('Start incremental import debug', flush=True)\n",
        "try:\n",
        "    import torch\n",
        "    print('OK: torch', torch.__version__, 'cuda avail', torch.cuda.is_available(), flush=True)\n",
        "except Exception as e:\n",
        "    print('FAIL: torch'); traceback.print_exc()\n",
        "\n",
        "try:\n",
        "    import torchvision\n",
        "    print('OK: torchvision', torchvision.__version__, flush=True)\n",
        "except Exception as e:\n",
        "    print('FAIL: torchvision'); traceback.print_exc()\n",
        "\n",
        "try:\n",
        "    import timm\n",
        "    print('OK: timm', getattr(timm, '__version__', 'unknown'), flush=True)\n",
        "except Exception as e:\n",
        "    print('FAIL: timm'); traceback.print_exc()\n",
        "\n",
        "try:\n",
        "    import segmentation_models_pytorch as smp\n",
        "    print('OK: smp', getattr(smp, '__version__', 'unknown'), flush=True)\n",
        "    m = smp.Unet(encoder_name='resnet34', encoder_weights=None, in_channels=2, classes=1, activation=None)\n",
        "    print('OK: created UNet model', sum(p.numel() for p in m.parameters())//1000, 'K params', flush=True)\n",
        "except Exception as e:\n",
        "    print('FAIL: smp'); traceback.print_exc()\n",
        "print('Import debug done', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "6dff1cc3-39df-4a64-9dc8-e2b3ec8613e5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys, subprocess, os\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print('> pip', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "print('=== Pinning library versions to stabilize imports (per expert advice) ===', flush=True)\n",
        "Path('constraints.txt').write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "pip('install','-c','constraints.txt',\n",
        "    'timm==0.9.12',\n",
        "    'albumentations==1.4.3',\n",
        "    'opencv-python-headless==4.10.0.84',\n",
        "    'scikit-image==0.22.0',\n",
        "    'scikit-learn==1.3.2',\n",
        "    'scipy==1.11.4',\n",
        "    'numpy==1.26.4',\n",
        "    '--upgrade','--upgrade-strategy','only-if-needed')\n",
        "print('Re-run incremental import after pinning in next cell.', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}