{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb04fa4a",
   "metadata": {},
   "source": [
    "\n",
    "# Dog Breed Identification â€” Final Submission Pipeline\n",
    "This notebook is a clean, linear, and self-contained pipeline that builds the final submission.csv from validated artifacts:\n",
    "- Calibrated baseline (embedding linear head) logits and temperature\n",
    "- Optimized kNN geometric blend hyperparameters (K, tau, lambda)\n",
    "- OOF-calibrated temperature for the blended model\n",
    "\n",
    "Inputs expected in CWD:\n",
    "- labels.csv, sample_submission.csv, train_image_meta.csv, test_image_meta.csv\n",
    "- oof_probs.npy, test_logits.npy, temperatures.json (baseline calibration)\n",
    "- train_embeds.npy, test_embeds.npy (L2-normalized at use-time)\n",
    "- fold_assignments.csv (5-fold StratifiedGroupKFold)\n",
    "- knn_blend_config.json (with best K, tau, lambda); falls back to hard-coded best if missing\n",
    "\n",
    "Output:\n",
    "- submission.csv (final calibrated, kNN-blended probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8312294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, pandas as pd, json, shutil\n",
    "from pathlib import Path\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "def softmax_np(x):\n",
    "    m = x.max(axis=1, keepdims=True)\n",
    "    ex = np.exp(x - m)\n",
    "    return ex / ex.sum(axis=1, keepdims=True)\n",
    "\n",
    "def row_normalize(p, eps=1e-12):\n",
    "    p = np.clip(p, eps, 1.0)\n",
    "    p /= p.sum(axis=1, keepdims=True)\n",
    "    return p\n",
    "\n",
    "def fit_temperature_from_probs(P, y_true, device='cpu'):\n",
    "    x = torch.from_numpy(np.log(np.clip(P, 1e-12, 1.0))).to(device)\n",
    "    y_t = torch.from_numpy(y_true).long().to(device)\n",
    "    T = torch.tensor(1.0, dtype=torch.float32, requires_grad=True, device=device)\n",
    "    nll = torch.nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.LBFGS([T], lr=0.5, max_iter=100, line_search_fn='strong_wolfe')\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        logits = x / torch.clamp(T, min=1e-3)\n",
    "        loss = nll(logits, y_t)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    opt.step(closure)\n",
    "    return float(T.detach().cpu().clamp_min(1e-3).item())\n",
    "\n",
    "# 1) Rebuild calibrated baseline test probs and promote to submission.csv\n",
    "sample_df = pd.read_csv('sample_submission.csv')\n",
    "classes = [c for c in sample_df.columns if c != 'id']\n",
    "test_meta = pd.read_csv('test_image_meta.csv')\n",
    "test_ids = test_meta['id'].tolist()\n",
    "baseline_logits_path = Path('test_logits.npy')\n",
    "temps_path = Path('temperatures.json')\n",
    "assert baseline_logits_path.exists(), 'test_logits.npy missing; regenerate baseline artifacts first.'\n",
    "logits = np.load(baseline_logits_path)\n",
    "T_base = 1.0\n",
    "if temps_path.exists():\n",
    "    try:\n",
    "        T_base = float(json.load(open(temps_path))['global_T'])\n",
    "    except Exception:\n",
    "        T_base = 1.0\n",
    "probs_base_test = row_normalize(softmax_np(logits / max(1e-3, T_base)))\n",
    "sub_base = pd.DataFrame(probs_base_test, columns=classes)\n",
    "sub_base.insert(0, 'id', test_ids)\n",
    "sub_base.to_csv('submission_baseline.csv', index=False)\n",
    "shutil.copyfile('submission_baseline.csv', 'submission.csv')\n",
    "print('Baseline rebuilt and promoted: submission_baseline.csv -> submission.csv')\n",
    "\n",
    "# 2) Load optimized kNN config (fallback to validated best if missing)\n",
    "cfg_path = Path('knn_blend_config.json')\n",
    "if cfg_path.exists():\n",
    "    cfg = json.load(open(cfg_path))\n",
    "    K = int(cfg.get('K', 300))\n",
    "    tau = float(cfg.get('tau', 0.02))\n",
    "    lam = float(cfg.get('lambda', 0.59))\n",
    "else:\n",
    "    K, tau, lam = 300, 0.02, 0.59\n",
    "print(f'Using optimized kNN config: K={K}, tau={tau}, lambda={lam}')\n",
    "\n",
    "# 3) Recompute OOF kNN with (K,tau), blend with baseline OOF, and fit global temperature T\n",
    "labels_df = pd.read_csv('labels.csv')\n",
    "breed_to_idx = {b:i for i,b in enumerate(classes)}\n",
    "y = labels_df['breed'].map(breed_to_idx).values.astype(np.int64)\n",
    "folds = pd.read_csv('fold_assignments.csv').set_index('id').loc[labels_df['id'], 'fold'].values.astype(int)\n",
    "n_folds = int(folds.max() + 1)\n",
    "oof_base = np.load('oof_probs.npy').astype(np.float64)\n",
    "\n",
    "train_embeds_raw = np.load('train_embeds.npy').astype(np.float32)\n",
    "train_ids_order = pd.read_csv('train_image_meta.csv')['id'].tolist()\n",
    "id_to_pos = {id_: i for i, id_ in enumerate(train_ids_order)}\n",
    "reindex = np.array([id_to_pos[id_] for id_ in labels_df['id']], dtype=np.int64)\n",
    "X = train_embeds_raw[reindex]\n",
    "X /= (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "oof_knn = np.zeros_like(oof_base, dtype=np.float64)\n",
    "for f in range(n_folds):\n",
    "    tr = np.where(folds != f)[0]\n",
    "    va = np.where(folds == f)[0]\n",
    "    index = faiss.IndexFlatIP(X.shape[1])\n",
    "    index.add(X[tr].astype('float32'))\n",
    "    sims, idxs = index.search(X[va].astype('float32'), min(K, len(tr)))\n",
    "    C = len(classes)\n",
    "    for i in range(len(va)):\n",
    "        s = sims[i]\n",
    "        ids = idxs[i].astype(int)\n",
    "        lbls = y[tr][ids]\n",
    "        w = np.exp(s / max(1e-8, tau))\n",
    "        w = w / max(1e-12, w.sum())\n",
    "        row = np.full(C, 1e-12, dtype=np.float64)\n",
    "        for c, ww in zip(lbls, w):\n",
    "            row[int(c)] += float(ww)\n",
    "        row /= row.sum()\n",
    "        oof_knn[va[i]] = row\n",
    "\n",
    "P_oof = row_normalize((oof_base ** max(0.0, 1.0 - lam)) * (oof_knn ** lam))\n",
    "T = fit_temperature_from_probs(P_oof, y)\n",
    "with open('temperatures_knn_blend.json','w') as f:\n",
    "    json.dump({'global_T': float(T), 'K': int(K), 'tau': float(tau), 'lambda': float(lam)}, f)\n",
    "print(f'OOF calibration complete: T={T:.4f} (lambda={lam}, K={K}, tau={tau})')\n",
    "\n",
    "# 4) Build test kNN with (K,tau), blend with baseline test probs, apply T, and write submission.csv\n",
    "test_embeds_raw = np.load('test_embeds.npy').astype(np.float32)\n",
    "X_test = test_embeds_raw\n",
    "X_test /= (np.linalg.norm(X_test, axis=1, keepdims=True) + 1e-12)\n",
    "index_full = faiss.IndexFlatIP(X.shape[1])\n",
    "index_full.add(X.astype('float32'))\n",
    "sims_te, idxs_te = index_full.search(X_test.astype('float32'), min(K, X.shape[0]))\n",
    "C = len(classes)\n",
    "test_knn = np.full((X_test.shape[0], C), 1e-12, dtype=np.float64)\n",
    "w = np.exp(sims_te / max(1e-8, tau))\n",
    "w = w / np.clip(w.sum(axis=1, keepdims=True), 1e-12, None)\n",
    "for i in range(X_test.shape[0]):\n",
    "    lbls = y[idxs_te[i].astype(int)]\n",
    "    for c, ww in zip(lbls, w[i]):\n",
    "        test_knn[i, int(c)] += float(ww)\n",
    "    test_knn[i] /= test_knn[i].sum()\n",
    "\n",
    "sub_base_aligned = pd.read_csv('submission.csv')\n",
    "assert list(sub_base_aligned.columns)[1:] == classes\n",
    "P_base_test = sub_base_aligned[classes].to_numpy(dtype=np.float64)\n",
    "\n",
    "P_blend = row_normalize((P_base_test ** max(0.0, 1.0 - lam)) * (test_knn ** lam))\n",
    "P_test_cal = row_normalize(np.power(P_blend, 1.0 / max(1e-3, T)))\n",
    "\n",
    "sub_out = pd.DataFrame(P_test_cal, columns=classes)\n",
    "sub_out.insert(0, 'id', sub_base_aligned['id'])\n",
    "sub_out.to_csv('submission.csv', index=False)\n",
    "print('Final submission.csv written: baseline-aligned geometric kNN blend + OOF-calibrated temperature applied.')\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
