{
  "cells": [
    {
      "id": "1f85220d-fe32-4bfb-b008-499961ef2af3",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan to WIN A MEDAL\n",
        "\n",
        "## High-level strategy\n",
        "- Quickly inspect provided artifacts (train2.zip, test2.zip, sampleSubmission.csv).\n",
        "- Identify training labels and feature representation (likely audio features or precomputed features).\n",
        "- Build a strong baseline fast: stratified CV + LightGBM/XGBoost or an audio CNN if features are spectrograms already provided.\n",
        "- Iterate with feature cleaning, normalization, target leakage checks, and robust CV aligned with competition splits.\n",
        "- Ensembling/stacking if multiple feature sets exist inside zips.\n",
        "- Submit early baseline, then iterate to improve AUC with calibrated thresholds/ensembles.\n",
        "\n",
        "## Milestones\n",
        "1) Data discovery and schema mapping.\n",
        "2) Baseline model with proper CV, log AUC.\n",
        "3) Feature engineering/selection or data augmentation if images/spectrograms.\n",
        "4) Model tuning and ensembling.\n",
        "5) Generate submission.csv and validate format.\n",
        "\n",
        "## Logging and efficiency\n",
        "- Print progress and timing per fold.\n",
        "- Use early stopping and reasonable params first.\n",
        "- Avoid long training without expert check.\n",
        "\n",
        "Next: explore files, unzip train/test, preview contents and schema."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "bc8c4e9e-ef34-4349-a2ea-b98e729a6a55",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, zipfile, pandas as pd, time, json, sys, glob\n",
        "from pathlib import Path\n",
        "print('CWD:', os.getcwd())\n",
        "print('Dir listing:', os.listdir())\n",
        "\n",
        "print('\\nPreview sampleSubmission.csv:')\n",
        "sample = pd.read_csv('sampleSubmission.csv')\n",
        "print(sample.shape)\n",
        "print(sample.head())\n",
        "\n",
        "def inspect_zip(zpath):\n",
        "    print(f'\\nInspecting {zpath} ...')\n",
        "    with zipfile.ZipFile(zpath, 'r') as z:\n",
        "        names = z.namelist()\n",
        "        print('Total files:', len(names))\n",
        "        for n in names[:20]:\n",
        "            print(' -', n)\n",
        "        # find potential csv/labels/features\n",
        "        csvs = [n for n in names if n.lower().endswith('.csv')]\n",
        "        print('CSV files in zip:', csvs[:10])\n",
        "inspect_zip('train2.zip')\n",
        "inspect_zip('test2.zip')\n",
        "\n",
        "# Unzip into folders if not already\n",
        "def safe_extract(zpath, outdir):\n",
        "    out = Path(outdir)\n",
        "    if out.exists() and any(out.iterdir()):\n",
        "        print(f'{outdir} already extracted, skipping')\n",
        "        return\n",
        "    out.mkdir(parents=True, exist_ok=True)\n",
        "    t0=time.time()\n",
        "    with zipfile.ZipFile(zpath, 'r') as z:\n",
        "        z.extractall(outdir)\n",
        "    print(f'Extracted {zpath} -> {outdir} in {time.time()-t0:.2f}s')\n",
        "\n",
        "safe_extract('train2.zip', 'train2')\n",
        "safe_extract('test2.zip', 'test2')\n",
        "\n",
        "# Walk directories to understand schema\n",
        "def walk_dir(d, depth=2):\n",
        "    d = Path(d)\n",
        "    print(f'\\nWalking {d} (depth={depth})')\n",
        "    for root, dirs, files in os.walk(d):\n",
        "        rel = Path(root).relative_to(d)\n",
        "        lvl = len(rel.parts)\n",
        "        print(root, 'dirs:', len(dirs), 'files:', len(files))\n",
        "        # show a few sample files\n",
        "        for f in files[:10]:\n",
        "            print('  file:', f)\n",
        "        if lvl >= depth:\n",
        "            # don't descend further\n",
        "            dirs[:] = []\n",
        "walk_dir('train2', depth=3)\n",
        "walk_dir('test2', depth=3)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CWD: /app/agent_run_states/the-icml-2013-whale-challenge-right-whale-redux-spray-20250911-045804\nDir listing: ['test2.zip', 'train2.zip', 'main.ipynb', 'agent_metadata', 'requirements.txt', 'task.txt', 'sampleSubmission.csv', 'docker_run.log', 'description.md']\n\nPreview sampleSubmission.csv:\n(25149, 2)\n                                clip  probability\n0  20090330_000000_015s2ms_Test0.aif            0\n1  20090330_000000_020s0ms_Test1.aif            0\n2  20090330_000000_025s8ms_Test2.aif            0\n3  20090330_000000_028s0ms_Test3.aif            0\n4  20090330_000000_035s4ms_Test4.aif            0\n\nInspecting train2.zip ...\nTotal files: 22693\n - train2/\n - train2/20090328_121500_44292s1ms_TRAIN3930_0.aif\n - train2/20090328_040000_14754s3ms_TRAIN1536_0.aif\n - train2/20090329_190000_68809s7ms_TRAIN20079_0.aif\n - train2/20090328_170000_61317s6ms_TRAIN5011_0.aif\n - train2/20090329_050000_18012s6ms_TRAIN11800_0.aif\n - train2/20090329_191500_69780s9ms_TRAIN20291_0.aif\n - train2/20090328_113000_41563s5ms_TRAIN3815_0.aif\n - train2/20090328_194500_71180s5ms_TRAIN5818_0.aif\n - train2/20090328_101500_37143s2ms_TRAIN3463_0.aif\n - train2/20090329_181500_66414s2ms_TRAIN19609_0.aif\n - train2/20090329_154500_56865s7ms_TRAIN16987_0.aif\n - train2/20090328_211500_76538s5ms_TRAIN6564_0.aif\n - train2/20090329_141500_51382s3ms_TRAIN15732_0.aif\n - train2/20090328_094500_35176s5ms_TRAIN3112_0.aif\n - train2/20090329_161500_58532s0ms_TRAIN17444_0.aif\n - train2/20090329_094500_35429s8ms_TRAIN13596_0.aif\n - train2/20090329_151500_55778s8ms_TRAIN16675_0.aif\n - train2/20090328_143000_52458s8ms_TRAIN4318_0.aif\n - train2/20090329_053000_20477s3ms_TRAIN12128_0.aif\nCSV files in zip: []\n\nInspecting test2.zip ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 25150\n - test2/\n - test2/20090330_211500_76911s3ms_Test9103.aif\n - test2/20090331_093000_34786s9ms_Test16890.aif\n - test2/20090331_164500_60302s0ms_Test20173.aif\n - test2/20090331_061500_23065s5ms_Test15089.aif\n - test2/20090330_103000_38310s6ms_Test4068.aif\n - test2/20090331_173000_63548s4ms_Test20737.aif\n - test2/20090331_204500_74792s1ms_Test22617.aif\n - test2/20090330_184500_67999s0ms_Test7115.aif\n - test2/20090330_203000_74285s8ms_Test8581.aif\n - test2/20090331_054500_21339s8ms_Test14792.aif\n - test2/20090330_211500_76623s1ms_Test9047.aif\n - test2/20090331_181500_66381s2ms_Test21309.aif\n - test2/20090330_183000_66957s4ms_Test7017.aif\n - test2/20090330_144500_53767s4ms_Test5595.aif\n - test2/20090331_201500_73585s2ms_Test22419.aif\n - test2/20090330_191500_70183s0ms_Test7551.aif\n - test2/20090331_020000_7418s6ms_Test12846.aif\n - test2/20090330_191500_70086s1ms_Test7521.aif\n - test2/20090331_000000_056s1ms_Test11109.aif\nCSV files in zip: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted train2.zip -> train2 in 3.66s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted test2.zip -> test2 in 4.10s\n\nWalking train2 (depth=3)\ntrain2 dirs: 1 files: 0\ntrain2/train2 dirs: 0 files: 22692\n  file: 20090329_224500_82193s1ms_TRAIN22168_0.aif\n  file: 20090329_181500_66249s3ms_TRAIN19586_0.aif\n  file: 20090329_191500_70052s7ms_TRAIN20346_0.aif\n  file: 20090328_143000_52703s4ms_TRAIN4326_0.aif\n  file: 20090329_153000_55933s0ms_TRAIN16724_0.aif\n  file: 20090329_040000_14652s0ms_TRAIN11313_0.aif\n  file: 20090329_191500_69741s8ms_TRAIN20283_0.aif\n  file: 20090329_041500_15461s1ms_TRAIN11470_0.aif\n  file: 20090328_093000_34253s1ms_TRAIN3035_0.aif\n  file: 20090328_203000_74619s7ms_TRAIN6196_0.aif\n\nWalking test2 (depth=3)\ntest2 dirs: 1 files: 0\ntest2/test2 dirs: 0 files: 25149\n  file: 20090330_021500_8988s9ms_Test893.aif\n  file: 20090331_211500_77375s4ms_Test23172.aif\n  file: 20090330_001500_1374s9ms_Test107.aif\n  file: 20090330_210000_76001s4ms_Test8920.aif\n  file: 20090330_003000_2023s7ms_Test215.aif\n  file: 20090330_211500_76703s8ms_Test9057.aif\n  file: 20090331_064500_24618s3ms_Test15260.aif\n  file: 20090331_014500_6481s4ms_Test12641.aif\n  file: 20090331_053000_20648s8ms_Test14694.aif\n  file: 20090331_141500_51610s7ms_Test18958.aif\n"
          ]
        }
      ]
    },
    {
      "id": "e35adae3-6ffd-4ed4-a9b9-13bee0234a33",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import re, numpy as np, pandas as pd, os, glob, time\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "t0 = time.time()\n",
        "train_files = sorted(glob.glob('train2/train2/*.aif'))\n",
        "test_files = sorted(glob.glob('test2/test2/*.aif'))\n",
        "print('Found train files:', len(train_files), 'test files:', len(test_files))\n",
        "\n",
        "pat_train = re.compile(r'(?P<date>\\d{8})_(?P<hh>\\d{2})(?P<mm>\\d{2})(?P<ss>\\d{2})_(?P<s>\\d+)s(?P<ms>\\d+)ms_.*_(?P<label>[01])\\.aif$')\n",
        "pat_test = re.compile(r'(?P<date>\\d{8})_(?P<hh>\\d{2})(?P<mm>\\d{2})(?P<ss>\\d{2})_(?P<s>\\d+)s(?P<ms>\\d+)ms_.*\\.aif$')\n",
        "\n",
        "def parse_train(fp):\n",
        "    name = os.path.basename(fp)\n",
        "    m = pat_train.search(name)\n",
        "    if not m:\n",
        "        return None\n",
        "    d = m.groupdict()\n",
        "    return {\n",
        "        'clip': name,\n",
        "        'date': int(d['date']),\n",
        "        'hour': int(d['hh']),\n",
        "        'minute': int(d['mm']),\n",
        "        'second': int(d['ss']),\n",
        "        'seg_seconds': int(d['s']),\n",
        "        'seg_ms': int(d['ms']),\n",
        "        'label': int(d['label'])\n",
        "    }\n",
        "\n",
        "def parse_test(fp):\n",
        "    name = os.path.basename(fp)\n",
        "    m = pat_test.search(name)\n",
        "    if not m:\n",
        "        return None\n",
        "    d = m.groupdict()\n",
        "    return {\n",
        "        'clip': name,\n",
        "        'date': int(d['date']),\n",
        "        'hour': int(d['hh']),\n",
        "        'minute': int(d['mm']),\n",
        "        'second': int(d['ss']),\n",
        "        'seg_seconds': int(d['s']),\n",
        "        'seg_ms': int(d['ms'])\n",
        "    }\n",
        "\n",
        "t1 = time.time()\n",
        "train_rows = [r for r in map(parse_train, train_files) if r is not None]\n",
        "test_rows = [r for r in map(parse_test, test_files) if r is not None]\n",
        "print(f'Parsed train rows: {len(train_rows)}, test rows: {len(test_rows)} in {time.time()-t1:.2f}s')\n",
        "\n",
        "train_df = pd.DataFrame(train_rows)\n",
        "test_df = pd.DataFrame(test_rows)\n",
        "print(train_df.head())\n",
        "print(train_df['label'].value_counts())\n",
        "\n",
        "# Basic time-based features\n",
        "def add_features(df):\n",
        "    df = df.copy()\n",
        "    df['tod_sec'] = df['hour']*3600 + df['minute']*60 + df['second']\n",
        "    df['seg_len_ms'] = df['seg_seconds']*1000 + df['seg_ms']\n",
        "    df['hour_sin'] = np.sin(2*np.pi*df['hour']/24)\n",
        "    df['hour_cos'] = np.cos(2*np.pi*df['hour']/24)\n",
        "    df['minute_sin'] = np.sin(2*np.pi*df['minute']/60)\n",
        "    df['minute_cos'] = np.cos(2*np.pi*df['minute']/60)\n",
        "    return df\n",
        "\n",
        "train_fe = add_features(train_df)\n",
        "test_fe = add_features(test_df)\n",
        "feats = ['tod_sec','seg_len_ms','hour','minute','second','seg_seconds','seg_ms','hour_sin','hour_cos','minute_sin','minute_cos']\n",
        "\n",
        "# CV baseline with logistic regression\n",
        "X = train_fe[feats].values\n",
        "y = train_fe['label'].values\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof = np.zeros(len(train_fe))\n",
        "fold = 0\n",
        "for tr, va in skf.split(X, y):\n",
        "    fold += 1\n",
        "    print(f'Fold {fold} start (n_tr={len(tr)}, n_va={len(va)})')\n",
        "    tfs = time.time()\n",
        "    model = make_pipeline(StandardScaler(), LogisticRegression(max_iter=200, n_jobs=8, solver='lbfgs'))\n",
        "    model.fit(X[tr], y[tr])\n",
        "    oof[va] = model.predict_proba(X[va])[:,1]\n",
        "    print(f'Fold {fold} done in {time.time()-tfs:.2f}s')\n",
        "auc = roc_auc_score(y, oof)\n",
        "print('OOF AUC:', auc)\n",
        "\n",
        "# Train final model on all data and predict test\n",
        "final_model = make_pipeline(StandardScaler(), LogisticRegression(max_iter=200, n_jobs=8, solver='lbfgs'))\n",
        "final_model.fit(X, y)\n",
        "test_pred = final_model.predict_proba(test_fe[feats].values)[:,1]\n",
        "\n",
        "# Build submission per sampleSubmission order\n",
        "sub_template = pd.read_csv('sampleSubmission.csv')\n",
        "sub_map = pd.Series(test_pred, index=test_fe['clip']).to_dict()\n",
        "sub = sub_template.copy()\n",
        "miss = 0\n",
        "vals = []\n",
        "for c in sub['clip']:\n",
        "    v = sub_map.get(c)\n",
        "    if v is None:\n",
        "        miss += 1\n",
        "        v = 0.5\n",
        "    vals.append(v)\n",
        "sub['probability'] = vals\n",
        "print('Missing mapped clips:', miss)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Wrote submission.csv with shape', sub.shape, 'in', f'{time.time()-t0:.2f}s')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found train files: 22692 test files: 25149\nParsed train rows: 22692, test rows: 25149 in 0.15s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                   clip      date  hour  minute  second  \\\n0  20090328_000000_002s3ms_TRAIN0_0.aif  20090328     0       0       0   \n1  20090328_000000_010s8ms_TRAIN1_0.aif  20090328     0       0       0   \n2  20090328_000000_021s6ms_TRAIN2_0.aif  20090328     0       0       0   \n3  20090328_000000_059s0ms_TRAIN3_0.aif  20090328     0       0       0   \n4  20090328_000000_068s4ms_TRAIN4_0.aif  20090328     0       0       0   \n\n   seg_seconds  seg_ms  label  \n0            2       3      0  \n1           10       8      0  \n2           21       6      0  \n3           59       0      0  \n4           68       4      0  \nlabel\n0    20426\n1     2266\nName: count, dtype: int64\nFold 1 start (n_tr=18153, n_va=4539)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 done in 1.33s\nFold 2 start (n_tr=18153, n_va=4539)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 done in 1.07s\nFold 3 start (n_tr=18154, n_va=4538)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 done in 1.06s\nFold 4 start (n_tr=18154, n_va=4538)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 done in 1.10s\nOOF AUC: 0.57517519163097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing mapped clips: 0\nWrote submission.csv with shape (25149, 2) in 7.34s\n"
          ]
        }
      ]
    },
    {
      "id": "29d146ba-bef5-4b3b-8def-889240d4596a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Audio feature extraction + LGBM with grouped CV\n",
        "import os, sys, time, glob, re, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "t0_all = time.time()\n",
        "\n",
        "# Install deps if missing\n",
        "def ensure(pkg, import_name=None):\n",
        "    import importlib\n",
        "    try:\n",
        "        importlib.import_module(import_name or pkg)\n",
        "    except Exception as e:\n",
        "        print(f'Installing {pkg} ...')\n",
        "        os.system(f'{sys.executable} -m pip install -q {pkg}')\n",
        "ensure('soundfile')\n",
        "ensure('librosa')\n",
        "ensure('joblib')\n",
        "ensure('scipy')\n",
        "ensure('lightgbm')\n",
        "\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "from joblib import Parallel, delayed\n",
        "from scipy.signal import butter, filtfilt\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load parsed filename metadata from previous cell's train_df/test_df if available, else parse again\n",
        "if 'train_df' not in globals() or 'test_df' not in globals():\n",
        "    train_files = sorted(glob.glob('train2/train2/*.aif'))\n",
        "    test_files = sorted(glob.glob('test2/test2/*.aif'))\n",
        "    pat_train = re.compile(r'(?P<date>\\d{8})_(?P<hh>\\d{2})(?P<mm>\\d{2})(?P<ss>\\d{2})_(?P<s>\\d+)s(?P<ms>\\d+)ms_.*_(?P<label>[01])\\.aif$')\n",
        "    pat_test = re.compile(r'(?P<date>\\d{8})_(?P<hh>\\d{2})(?P<mm>\\d{2})(?P<ss>\\d{2})_(?P<s>\\d+)s(?P<ms>\\d+)ms_.*\\.aif$')\n",
        "    def parse_train(fp):\n",
        "        name = os.path.basename(fp)\n",
        "        m = pat_train.search(name)\n",
        "        if not m: return None\n",
        "        d = m.groupdict()\n",
        "        return {'clip': name, 'date': int(d['date']), 'hour': int(d['hh']), 'minute': int(d['mm']), 'second': int(d['ss']), 'label': int(d['label'])}\n",
        "    def parse_test(fp):\n",
        "        name = os.path.basename(fp)\n",
        "        m = pat_test.search(name)\n",
        "        if not m: return None\n",
        "        d = m.groupdict()\n",
        "        return {'clip': name, 'date': int(d['date']), 'hour': int(d['hh']), 'minute': int(d['mm']), 'second': int(d['ss'])}\n",
        "    train_rows = [r for r in map(parse_train, train_files) if r]\n",
        "    test_rows = [r for r in map(parse_test, test_files) if r]\n",
        "    train_df = pd.DataFrame(train_rows)\n",
        "    test_df = pd.DataFrame(test_rows)\n",
        "else:\n",
        "    train_files = sorted(glob.glob('train2/train2/*.aif'))\n",
        "    test_files = sorted(glob.glob('test2/test2/*.aif'))\n",
        "\n",
        "print(f'Train clips: {len(train_files)}, Test clips: {len(test_files)}')\n",
        "\n",
        "# Audio params\n",
        "SR = 2000\n",
        "TARGET_LEN = 2 * SR  # 2 seconds\n",
        "N_FFT = 512\n",
        "HOP = 128\n",
        "WIN = 256\n",
        "N_MELS = 64\n",
        "FMIN = 20\n",
        "FMAX = 1000\n",
        "BP_LO = 50\n",
        "BP_HI = 400\n",
        "b, a = butter(4, [BP_LO/(SR/2), BP_HI/(SR/2)], btype='band')\n",
        "\n",
        "mel_freqs = librosa.mel_frequencies(n_mels=N_MELS, fmin=FMIN, fmax=FMAX)\n",
        "low_idx = (mel_freqs >= 50) & (mel_freqs <= 400)\n",
        "\n",
        "def load_audio(fp):\n",
        "    y, orig_sr = sf.read(fp, dtype='float32')\n",
        "    if y.ndim > 1:\n",
        "        y = y.mean(axis=1)\n",
        "    if orig_sr != SR:\n",
        "        y = librosa.resample(y, orig_sr=orig_sr, target_sr=SR, res_type='kaiser_fast')\n",
        "    if len(y) < TARGET_LEN:\n",
        "        y = np.pad(y, (0, TARGET_LEN - len(y)))\n",
        "    else:\n",
        "        y = y[:TARGET_LEN]\n",
        "    # Band-pass filter\n",
        "    try:\n",
        "        y = filtfilt(b, a, y)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return y\n",
        "\n",
        "def feature_dict_from_audio(y):\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=SR, n_fft=N_FFT, hop_length=HOP, win_length=WIN, n_mels=N_MELS, fmin=FMIN, fmax=FMAX, power=2.0)\n",
        "    L = np.log1p(S)\n",
        "    feats = {}\n",
        "    # per-mel stats (mean, std, max, p90)\n",
        "    feats.update({f'mel_mean_{i}': float(L[i].mean()) for i in range(L.shape[0])})\n",
        "    feats.update({f'mel_std_{i}': float(L[i].std()) for i in range(L.shape[0])})\n",
        "    feats.update({f'mel_max_{i}': float(L[i].max()) for i in range(L.shape[0])})\n",
        "    feats.update({f'mel_p90_{i}': float(np.percentile(L[i], 90)) for i in range(L.shape[0])})\n",
        "    # MFCCs + deltas\n",
        "    mfcc = librosa.feature.mfcc(S=librosa.power_to_db(S + 1e-12), sr=SR, n_mfcc=20)\n",
        "    dm = librosa.feature.delta(mfcc)\n",
        "    feats.update({f'mfcc_mean_{i}': float(mfcc[i].mean()) for i in range(mfcc.shape[0])})\n",
        "    feats.update({f'mfcc_std_{i}': float(mfcc[i].std()) for i in range(mfcc.shape[0])})\n",
        "    feats.update({f'dmfcc_mean_{i}': float(dm[i].mean()) for i in range(dm.shape[0])})\n",
        "    feats.update({f'dmfcc_std_{i}': float(dm[i].std()) for i in range(dm.shape[0])})\n",
        "    # Spectral + energy stats\n",
        "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=SR, n_fft=N_FFT, hop_length=HOP).squeeze()\n",
        "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=SR, n_fft=N_FFT, hop_length=HOP).squeeze()\n",
        "    spec_roll = librosa.feature.spectral_rolloff(y=y, sr=SR, roll_percent=0.80, n_fft=N_FFT, hop_length=HOP).squeeze()\n",
        "    flat = librosa.feature.spectral_flatness(y=y, n_fft=N_FFT, hop_length=HOP).squeeze()\n",
        "    rms = librosa.feature.rms(y=y, frame_length=WIN, hop_length=HOP).squeeze()\n",
        "    zcr = librosa.feature.zero_crossing_rate(y=y, frame_length=WIN, hop_length=HOP).squeeze()\n",
        "    def add_stat(name, arr):\n",
        "        feats[f'{name}_mean'] = float(arr.mean())\n",
        "        feats[f'{name}_std'] = float(arr.std())\n",
        "        feats[f'{name}_max'] = float(arr.max())\n",
        "        feats[f'{name}_p90'] = float(np.percentile(arr, 90))\n",
        "    for name, arr in [('centroid', spec_cent), ('bandwidth', spec_bw), ('rolloff', spec_roll), ('flatness', flat), ('rms', rms), ('zcr', zcr)]:\n",
        "        add_stat(name, arr)\n",
        "    # low-band energy metrics\n",
        "    low_band = L[low_idx].mean(axis=0)\n",
        "    full_band = L.mean(axis=0)\n",
        "    feats['low_full_ratio_mean'] = float(low_band.mean() / (full_band.mean() + 1e-8))\n",
        "    feats['low_p90'] = float(np.percentile(low_band, 90))\n",
        "    feats['low_peak_to_mean'] = float(low_band.max() / (low_band.mean() + 1e-8))\n",
        "    thr = float(np.percentile(low_band, 75))\n",
        "    mask = (low_band > thr).astype(np.int32)\n",
        "    # run-lengths of >thr\n",
        "    if mask.size:\n",
        "        idx = np.where(np.r_[True, mask==0, True])[0]\n",
        "        runs = np.diff(idx) - 1\n",
        "        runs = runs[runs > 0]\n",
        "        feats['low_run_max'] = float(runs.max() if runs.size else 0.0)\n",
        "        feats['low_run_count'] = float((mask[1:] > mask[:-1]).sum())\n",
        "    else:\n",
        "        feats['low_run_max'] = 0.0\n",
        "        feats['low_run_count'] = 0.0\n",
        "    return feats\n",
        "\n",
        "def process_clip(fp):\n",
        "    try:\n",
        "        y = load_audio(fp)\n",
        "        feats = feature_dict_from_audio(y)\n",
        "        feats['clip'] = os.path.basename(fp)\n",
        "        return feats\n",
        "    except Exception as e:\n",
        "        return {'clip': os.path.basename(fp), 'error': str(e)}\n",
        "\n",
        "feat_train_path = Path('features_train.parquet')\n",
        "feat_test_path = Path('features_test.parquet')\n",
        "\n",
        "def extract_or_load_features(files, out_path, tag):\n",
        "    if out_path.exists():\n",
        "        print(f'Loading cached {tag} features from {out_path} ...')\n",
        "        return pd.read_parquet(out_path)\n",
        "    n = len(files)\n",
        "    print(f'Extracting {tag} features for {n} files ...')\n",
        "    t0 = time.time()\n",
        "    res = Parallel(n_jobs=-1, batch_size=16, prefer='processes')(delayed(process_clip)(fp) for fp in files)\n",
        "    df = pd.DataFrame(res)\n",
        "    if 'error' in df.columns:\n",
        "        errs = df['error'].notnull().sum()\n",
        "        print(f'{tag} errors: {errs}')\n",
        "    df.to_parquet(out_path, index=False)\n",
        "    print(f'Saved {tag} features to {out_path} in {time.time()-t0:.1f}s, shape={df.shape}')\n",
        "    return df\n",
        "\n",
        "train_feat_df = extract_or_load_features(train_files, feat_train_path, 'train')\n",
        "test_feat_df = extract_or_load_features(test_files, feat_test_path, 'test')\n",
        "\n",
        "# Merge labels\n",
        "train_feat_df = train_feat_df.merge(train_df[['clip','label','date','hour']], on='clip', how='left')\n",
        "test_feat_df = test_feat_df.merge(test_df[['clip','date','hour']], on='clip', how='left')\n",
        "print('Train features shape:', train_feat_df.shape, 'Test features shape:', test_feat_df.shape)\n",
        "\n",
        "# Prepare features list\n",
        "drop_cols = {'clip','label','date','hour','error'}\n",
        "features = [c for c in train_feat_df.columns if c not in drop_cols]\n",
        "print('N features:', len(features))\n",
        "\n",
        "# Grouped CV by date+hour to reduce leakage\n",
        "groups = train_feat_df['date'].astype(str) + '_' + train_feat_df['hour'].astype(int).astype(str).str.zfill(2)\n",
        "y = train_feat_df['label'].astype(int).values\n",
        "\n",
        "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof = np.zeros(len(train_feat_df), dtype=np.float32)\n",
        "models = []\n",
        "# compute global class imbalance for scale_pos_weight\n",
        "pos = (y == 1).sum()\n",
        "neg = (y == 0).sum()\n",
        "scale_pos_weight = float(neg) / max(1.0, float(pos))\n",
        "params = dict(objective='binary', metric='auc', learning_rate=0.05, num_leaves=64, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=1, min_data_in_leaf=50, verbosity=-1, scale_pos_weight=scale_pos_weight)\n",
        "\n",
        "for i, (tr, va) in enumerate(cv.split(train_feat_df[features], y, groups), 1):\n",
        "    tfs = time.time()\n",
        "    print(f'Fold {i} start: tr={len(tr)} va={len(va)}')\n",
        "    dtr = lgb.Dataset(train_feat_df.loc[tr, features], label=y[tr])\n",
        "    dva = lgb.Dataset(train_feat_df.loc[va, features], label=y[va])\n",
        "    callbacks = [lgb.early_stopping(200), lgb.log_evaluation(200)]\n",
        "    model = lgb.train(params, dtr, num_boost_round=3000, valid_sets=[dva], callbacks=callbacks)\n",
        "    oof[va] = model.predict(train_feat_df.loc[va, features], num_iteration=model.best_iteration)\n",
        "    models.append(model)\n",
        "    print(f'Fold {i} done in {time.time()-tfs:.1f}s, best_iter={model.best_iteration}')\n",
        "auc = roc_auc_score(y, oof)\n",
        "print('OOF AUC:', auc)\n",
        "\n",
        "# Train final model on all data (use median best_iter * 1.1)\n",
        "best_iters = [m.best_iteration for m in models]\n",
        "final_boost_round = int(np.median(best_iters) * 1.1) if best_iters else 1000\n",
        "print('Final boost round:', final_boost_round)\n",
        "final_train = lgb.Dataset(train_feat_df[features], label=y)\n",
        "final_model = lgb.train(params, final_train, num_boost_round=final_boost_round, valid_sets=[final_train])\n",
        "test_pred = final_model.predict(test_feat_df[features])\n",
        "\n",
        "# Build submission\n",
        "sub = pd.read_csv('sampleSubmission.csv')\n",
        "pred_map = pd.Series(test_pred, index=test_feat_df['clip']).to_dict()\n",
        "sub['probability'] = sub['clip'].map(pred_map).fillna(0.5)\n",
        "miss = int(sub['probability'].isna().sum())\n",
        "print('Missing mapped clips:', miss)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Wrote submission.csv', sub.shape, 'Total time:', f'{time.time()-t0_all:.1f}s')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train clips: 22692, Test clips: 25149\nLoading cached train features from features_train.parquet ...\nLoading cached test features from features_test.parquet ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train features shape: (22692, 369) Test features shape: (25149, 368)\nN features: 365\nFold 1 start: tr=19139 va=3553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's auc: 0.932028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's auc: 0.933794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's auc: 0.933641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[440]\tvalid_0's auc: 0.934492\nFold 1 done in 12.8s, best_iter=440\nFold 2 start: tr=18184 va=4508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's auc: 0.949883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's auc: 0.951055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's auc: 0.952004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's auc: 0.952088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[694]\tvalid_0's auc: 0.952866\nFold 2 done in 12.9s, best_iter=694\nFold 3 start: tr=18115 va=4577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's auc: 0.941456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's auc: 0.944947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's auc: 0.945568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[524]\tvalid_0's auc: 0.946663\nFold 3 done in 9.9s, best_iter=524\nFold 4 start: tr=16960 va=5732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's auc: 0.929178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's auc: 0.932521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's auc: 0.934217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's auc: 0.934686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's auc: 0.935368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's auc: 0.935898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's auc: 0.935997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[1229]\tvalid_0's auc: 0.936037\nFold 4 done in 18.7s, best_iter=1229\nFold 5 start: tr=18370 va=4322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 200 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's auc: 0.940522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's auc: 0.943225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's auc: 0.944721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping, best iteration is:\n[598]\tvalid_0's auc: 0.944864\nFold 5 done in 11.7s, best_iter=598\nOOF AUC: 0.9303386521116115\nFinal boost round: 657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing mapped clips: 0\nWrote submission.csv (25149, 2) Total time: 74.5s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}