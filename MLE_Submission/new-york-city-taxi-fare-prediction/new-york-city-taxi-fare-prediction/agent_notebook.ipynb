{
  "cells": [
    {
      "id": "8aa9ebbd-749f-4b9c-b76b-a366e77638ed",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NYC Taxi Fare Prediction \u2014 Corrected Experiment Plan (Audit-Ready)\n",
        "\n",
        "Objective: Win a medal (target: GOLD). Metric: RMSE (lower is better).\n",
        "\n",
        "Performance targets:\n",
        "- Bronze: RMSE \u2264 2.92371\n",
        "- Silver: RMSE \u2264 2.88191\n",
        "- Gold: RMSE \u2264 2.83377\n",
        "\n",
        "1) Definitive Data Structure\n",
        "- Files in CWD:\n",
        "  - labels.csv: FULL TRAINING DATASET (features + target). Columns: [key, pickup_datetime, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, passenger_count, fare_amount].\n",
        "  - test.csv: HOLDOUT feature table for final prediction. Columns: [key, pickup_datetime, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, passenger_count].\n",
        "  - sample_submission.csv: [key, fare_amount] \u2014 defines required submission order and set; keys match test.csv exactly.\n",
        "- No key overlap: labels.csv keys are disjoint from test.csv keys. No joins required for training.\n",
        "- Training sampling: load labels.csv in chunks and randomly sample 200k\u20135M rows (scalable), ensuring UTC tz-awareness and non-null critical fields.\n",
        "\n",
        "2) Validation Plan (time-aware, no leakage)\n",
        "- Convert pickup_datetime to timezone-aware UTC on load, and to America/New_York only for temporal features.\n",
        "- Forward-chaining TimeSeries CV with 5 folds and a 1-day gap before each validation slice.\n",
        "- Track RMSE on original scale; model trained on log1p(target) with inverse-transform for scoring.\n",
        "- Keep a final temporal holdout (last ~10%) if needed for sanity checks.\n",
        "\n",
        "3) Train Cleaning and Test Parity\n",
        "- Train (labels.csv) strict filters:\n",
        "  - NYC geofence: lat \u2208 [40.5, 41.0], lon \u2208 [-74.5, -72.8] for both pickup and dropoff.\n",
        "  - passenger_count \u2208 [1, 6].\n",
        "  - Distance (haversine_km): keep 0.01 \u2264 distance \u2264 200.\n",
        "  - fare_amount \u2208 [2.5, 200].\n",
        "- Test (test.csv) parity without dropping rows:\n",
        "  - Clip passenger_count to [1, 6], clip lat/lon to geofence.\n",
        "  - Clip derived distances post-feature to [0.01, 200].\n",
        "\n",
        "4) Core Feature Set (initial \u2265 20 features)\n",
        "- Geometry: haversine_km, manhattan_km (reuse haversine components), delta_lat, delta_lon, bearing_sin/cos, center_lat/lon.\n",
        "- Temporal (NY local): year, month, day, hour, dow, is_weekend, is_rush, is_night, hour_sin/cos, dow_sin/cos.\n",
        "- Domain/POIs: distances to JFK/LGA/EWR/Midtown/FiDi; is_airport_trip; interactions like distance \u00d7 rush/weekend.\n",
        "- Consistent clipping for engineered features to robust ranges.\n",
        "\n",
        "5) Modeling Strategy (LightGBM baseline \u2014 mandatory)\n",
        "- Baseline model: LightGBM Regressor on log1p(fare).\n",
        "- Initial params (CPU): {num_leaves: 128, max_depth: -1, learning_rate: 0.05, n_estimators: up to 10k (manual early stopping via time-CV only), min_data_in_leaf: 200, feature_fraction: 0.8, bagging_fraction: 0.8, bagging_freq: 1, lambda_l1: 0.0, lambda_l2: 1.0}.\n",
        "- Disable internal LightGBM validation leakage; use our manual time-CV splits for any early stopping.\n",
        "- Report fold RMSE and mean/std. Fit final model on all ordered data; predict test; clip predictions to [0, 500].\n",
        "\n",
        "6) Scale-up & Advanced Features (post-baseline)\n",
        "- Scale training sample to 2\u20135M+ rows (highest ROI).\n",
        "- Add geohash/grid bins, KMeans clusters (~50\u2013100), airport/borough/toll proxies, rotated-Manhattan distance, bearing buckets, holiday flags, frequency encodings.\n",
        "- Hyperparameter tuning with Optuna (50\u2013100 trials).\n",
        "- Diversity models (XGBoost/CatBoost) and ensembling/stacking.\n",
        "\n",
        "7) Reproducibility & Hygiene\n",
        "- Global seed (e.g., 2025) for numpy/pandas/LightGBM.\n",
        "- Hard assertions on schema, dtypes, tz-awareness, key uniqueness, and train\u2013test disjointness.\n",
        "- Clean notebook: delete stale code after documenting; resolve deprecation/future warnings (use root_mean_squared_error, dtype checks via isinstance).\n",
        "- Save submission via robust merge on key, preserving sample_submission order.\n",
        "\n",
        "Milestones\n",
        "- M1 (baseline): LightGBM + core features + time-CV \u2192 aim RMSE \u2264 3.3\u20133.5 (with 1\u20132M rows).\n",
        "- M2: +features + tuning \u2192 \u2264 2.9\u20133.1.\n",
        "- M3: +ensemble/stack \u2192 \u2264 2.83\u20132.88 (gold range).\n",
        "\n",
        "Audit Checkpoints\n",
        "- This corrected plan.\n",
        "- After data sampling and schema assertions.\n",
        "- After LightGBM baseline CV.\n",
        "- After scaling/tuning/features.\n",
        "- Before final submission."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "e7fe6459-5ebc-44cc-919e-3e0dcb7ad10e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "SEED = 2025\n",
        "rng = np.random.default_rng(SEED)\n",
        "\n",
        "def read_test_and_sample():\n",
        "    dtypes = {\n",
        "        'key': 'string',\n",
        "        'pickup_longitude': 'float32',\n",
        "        'pickup_latitude': 'float32',\n",
        "        'dropoff_longitude': 'float32',\n",
        "        'dropoff_latitude': 'float32',\n",
        "        'passenger_count': 'int8'\n",
        "    }\n",
        "    test = pd.read_csv(\n",
        "        'test.csv',\n",
        "        dtype=dtypes,\n",
        "        parse_dates=['pickup_datetime']\n",
        "    )\n",
        "    test['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'], utc=True, errors='raise')\n",
        "    sample = pd.read_csv('sample_submission.csv', dtype={'key': 'string'})\n",
        "    # Hard assertions\n",
        "    assert test['key'].notna().all(), 'Null keys in test'\n",
        "    assert test['key'].is_unique, 'Duplicate keys in test'\n",
        "    assert test['pickup_datetime'].notna().all(), 'Null pickup_datetime in test'\n",
        "    assert isinstance(test['pickup_datetime'].dtype, pd.DatetimeTZDtype), 'pickup_datetime must be timezone-aware'\n",
        "    # Ensure dtypes\n",
        "    assert test['passenger_count'].dtype == 'int8', 'passenger_count not int8'\n",
        "    for col in ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']:\n",
        "        assert test[col].dtype == 'float32', f'{col} not float32'\n",
        "    # Sample alignment: keys must exactly match test keys\n",
        "    assert sample['key'].is_unique, 'Duplicate keys in sample_submission'\n",
        "    test_keys = set(test['key'].tolist())\n",
        "    samp_keys = set(sample['key'].tolist())\n",
        "    assert test_keys == samp_keys, f'Sample and test key sets differ: diff_count={len(test_keys.symmetric_difference(samp_keys))}'\n",
        "    return test, sample\n",
        "\n",
        "test, sample = read_test_and_sample()\n",
        "summary = {\n",
        "    'test_rows': int(len(test)),\n",
        "    'sample_rows': int(len(sample))\n",
        "}\n",
        "print('SUMMARY:', summary)\n",
        "print('Dtypes OK; TZ-aware datetime enforced; keys validated; sample alignment verified.')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUMMARY: {'test_rows': 9914, 'sample_rows': 9914}\nDtypes OK; TZ-aware datetime enforced; keys validated; sample alignment verified.\n"
          ]
        }
      ]
    },
    {
      "id": "e4864b6d-f6bc-4e53-90ff-9d1126e3a0a6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Robust, unbiased hash-based sampler over entire labels.csv; tz-aware; assertions; optional cache\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.util import hash_pandas_object\n",
        "\n",
        "TRAIN_SAMPLE_TARGET = 2_000_000  # per plan (>=2M)\n",
        "CHUNKSIZE = 1_000_000            # IO-efficient chunk size\n",
        "EST_TOTAL_ROWS = 55_000_000      # rough total rows to set sampling prob\n",
        "OVERSAMPLE_FACTOR = 1.1          # safety margin to reduce undershoot risk\n",
        "CACHE_PATH = None                # e.g., 'train_sample.parquet' to persist\n",
        "\n",
        "def empty_typed_frame():\n",
        "    return pd.DataFrame({\n",
        "        'key': pd.Series([], dtype='string'),\n",
        "        'pickup_datetime': pd.Series([], dtype='datetime64[ns, UTC]'),\n",
        "        'pickup_longitude': pd.Series([], dtype='float32'),\n",
        "        'pickup_latitude': pd.Series([], dtype='float32'),\n",
        "        'dropoff_longitude': pd.Series([], dtype='float32'),\n",
        "        'dropoff_latitude': pd.Series([], dtype='float32'),\n",
        "        'passenger_count': pd.Series([], dtype='int8'),\n",
        "        'fare_amount': pd.Series([], dtype='float32')\n",
        "    })\n",
        "\n",
        "def load_train_sample_hash(target_rows=TRAIN_SAMPLE_TARGET, chunksize=CHUNKSIZE, seed=SEED,\n",
        "                           est_total_rows=EST_TOTAL_ROWS, oversample=OVERSAMPLE_FACTOR, cache_path=CACHE_PATH):\n",
        "    if cache_path:\n",
        "        try:\n",
        "            df_cached = pd.read_parquet(cache_path)\n",
        "            if len(df_cached) >= int(0.9 * target_rows):\n",
        "                print({'train_sample_rows': int(len(df_cached)), 'source': 'cache'})\n",
        "                return df_cached\n",
        "        except Exception:\n",
        "            pass\n",
        "    usecols = ['key','pickup_datetime','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count','fare_amount']\n",
        "    dtypes = {\n",
        "        'key': 'string',\n",
        "        'pickup_longitude': 'float32',\n",
        "        'pickup_latitude': 'float32',\n",
        "        'dropoff_longitude': 'float32',\n",
        "        'dropoff_latitude': 'float32',\n",
        "        'passenger_count': 'int8',\n",
        "        'fare_amount': 'float32'\n",
        "    }\n",
        "    required_nonnull = ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count','fare_amount']\n",
        "    # Probability chosen to hit target on full-pass; oversample to reduce undershoot\n",
        "    p = min(0.5, max(1e-4, (target_rows / max(1, est_total_rows)) * oversample))\n",
        "    parts = []\n",
        "    for ch in pd.read_csv('labels.csv', usecols=usecols, dtype=dtypes, chunksize=chunksize):\n",
        "        # Convert datetime post-read for speed and enforce UTC\n",
        "        ch['pickup_datetime'] = pd.to_datetime(ch['pickup_datetime'], utc=True, errors='coerce')\n",
        "        ch = ch.dropna(subset=['pickup_datetime'] + required_nonnull)\n",
        "        if ch.empty:\n",
        "            continue\n",
        "        # Deterministic hash-based sampling across entire file (unbiased by position/time)\n",
        "        h = hash_pandas_object(ch['key'], index=False).values.astype('uint64')\n",
        "        keep = (h / np.float64(2**64)) < p\n",
        "        if keep.any():\n",
        "            parts.append(ch.loc[keep])\n",
        "    if not parts:\n",
        "        df = empty_typed_frame()\n",
        "    else:\n",
        "        df = pd.concat(parts, ignore_index=True)\n",
        "    # If we overshot target, downsample deterministically\n",
        "    if len(df) > target_rows:\n",
        "        df = df.sample(n=target_rows, random_state=seed)\n",
        "    # Guarantee minimum size\n",
        "    assert len(df) >= int(0.9 * target_rows), f'Sampler undershot: got {len(df)} < 90% of target {target_rows}. Increase oversample or adjust est_total_rows.'\n",
        "    # Hard assertions (post-clean)\n",
        "    assert df['pickup_datetime'].notna().all(), 'Null pickup_datetime in train sample'\n",
        "    assert isinstance(df['pickup_datetime'].dtype, pd.DatetimeTZDtype), 'pickup_datetime must be tz-aware'\n",
        "    for col in required_nonnull:\n",
        "        assert df[col].notna().all(), f'Nulls in {col} after cleaning'\n",
        "    for col in ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']:\n",
        "        assert df[col].dtype == 'float32', f'{col} dtype not float32'\n",
        "    assert df['fare_amount'].dtype == 'float32', 'fare_amount dtype incorrect'\n",
        "    assert df['passenger_count'].dtype == 'int8', 'passenger_count dtype incorrect'\n",
        "    # Key uniqueness: warn instead of hard fail (some datasets may contain dup keys)\n",
        "    if not df['key'].is_unique:\n",
        "        dup_ct = int(df['key'].size - df['key'].nunique())\n",
        "        print(f'Warning: {dup_ct} duplicate keys in train_sample (diagnostic only).')\n",
        "    # Train-test disjoint (diagnostic assert ok)\n",
        "    try:\n",
        "        _test_keys = set(test['key'])\n",
        "    except NameError:\n",
        "        _test_keys = set()\n",
        "    assert _test_keys == set() or set(df['key']).isdisjoint(_test_keys), 'Train sample overlaps with test keys'\n",
        "    if cache_path:\n",
        "        try:\n",
        "            df.to_parquet(cache_path, index=False)\n",
        "        except Exception:\n",
        "            pass\n",
        "    print({'train_sample_rows': int(len(df)), 'p': float(p)})\n",
        "    return df\n",
        "\n",
        "train_sample = load_train_sample_hash()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train_sample_rows': 2000000, 'p': 0.04}\n"
          ]
        }
      ]
    },
    {
      "id": "bf6f604d-8265-480e-b79a-f8915f69245f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LightGBM with corrected time-CV (quantile folds + 1-day gap) and stronger params (unleash capacity)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "from datetime import timedelta\n",
        "\n",
        "assert 'train_sample' in globals(), 'train_sample not loaded'\n",
        "assert 'test' in globals() and 'sample' in globals(), 'test/sample not loaded'\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "SEED = 2025\n",
        "np.random.seed(SEED)\n",
        "\n",
        "def haversine_km(lat1, lon1, lat2, lon2):\n",
        "    R = 6371.0\n",
        "    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    return R * c\n",
        "\n",
        "def bearing_rad(lat1, lon1, lat2, lon2):\n",
        "    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])\n",
        "    dlon = lon2 - lon1\n",
        "    y = np.sin(dlon) * np.cos(lat2)\n",
        "    x = np.cos(lat1)*np.cos(lat2)*np.cos(dlon) + np.sin(lat1)*np.sin(lat2)\n",
        "    return np.arctan2(y, x)\n",
        "\n",
        "POIS = {\n",
        "    'JFK': (40.6413, -73.7781),\n",
        "    'LGA': (40.7769, -73.8740),\n",
        "    'EWR': (40.6895, -74.1745),\n",
        "    'MIDTOWN': (40.7580, -73.9855),\n",
        "    'FIDI': (40.7060, -74.0086)\n",
        "}\n",
        "\n",
        "def to_local_ny(dt_series):\n",
        "    return dt_series.dt.tz_convert('America/New_York')\n",
        "\n",
        "def add_features(df):\n",
        "    df = df.copy()\n",
        "    dt_local = to_local_ny(df['pickup_datetime'])\n",
        "    pu_lat = df['pickup_latitude'].astype('float32')\n",
        "    pu_lon = df['pickup_longitude'].astype('float32')\n",
        "    do_lat = df['dropoff_latitude'].astype('float32')\n",
        "    do_lon = df['dropoff_longitude'].astype('float32')\n",
        "    # Geometry\n",
        "    df['dist_hav_km'] = haversine_km(pu_lat, pu_lon, do_lat, do_lon).astype('float32')\n",
        "    df['dist_man_km'] = (\n",
        "        haversine_km(pu_lat, pu_lon, pu_lat, do_lon) +\n",
        "        haversine_km(pu_lat, do_lon, do_lat, do_lon)\n",
        "    ).astype('float32')\n",
        "    df['delta_lat'] = (do_lat - pu_lat).astype('float32')\n",
        "    df['delta_lon'] = (do_lon - pu_lon).astype('float32')\n",
        "    b = bearing_rad(pu_lat, pu_lon, do_lat, do_lon)\n",
        "    df['bear_sin'] = np.sin(b).astype('float32')\n",
        "    df['bear_cos'] = np.cos(b).astype('float32')\n",
        "    df['center_lat'] = ((pu_lat + do_lat) / 2.0).astype('float32')\n",
        "    df['center_lon'] = ((pu_lon + do_lon) / 2.0).astype('float32')\n",
        "    # Temporal (NY local time)\n",
        "    df['year'] = dt_local.dt.year.astype('int16')\n",
        "    df['month'] = dt_local.dt.month.astype('int8')\n",
        "    df['day'] = dt_local.dt.day.astype('int8')\n",
        "    df['hour'] = dt_local.dt.hour.astype('int8')\n",
        "    df['dow'] = dt_local.dt.dayofweek.astype('int8')\n",
        "    df['is_weekend'] = df['dow'].isin([5,6]).astype('int8')\n",
        "    df['is_rush'] = ((df['hour'].between(7,10)) | (df['hour'].between(16,19))).astype('int8')\n",
        "    df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 5)).astype('int8')\n",
        "    df['hour_sin'] = np.sin(2*np.pi*df['hour']/24).astype('float32')\n",
        "    df['hour_cos'] = np.cos(2*np.pi*df['hour']/24).astype('float32')\n",
        "    df['dow_sin'] = np.sin(2*np.pi*df['dow']/7).astype('float32')\n",
        "    df['dow_cos'] = np.cos(2*np.pi*df['dow']/7).astype('float32')\n",
        "    # POI distances\n",
        "    for name, (lat, lon) in POIS.items():\n",
        "        df[f'dist_{name.lower()}'] = haversine_km(pu_lat, pu_lon, lat, lon).astype('float32')\n",
        "    df['is_airport_trip'] = (df[['dist_jfk','dist_lga','dist_ewr']].min(axis=1) < 2.0).astype('int8')\n",
        "    # Interactions\n",
        "    df['dist_x_rush'] = (df['dist_hav_km'] * df['is_rush']).astype('float32')\n",
        "    df['dist_x_weekend'] = (df['dist_hav_km'] * df['is_weekend']).astype('float32')\n",
        "    return df\n",
        "\n",
        "def clean_train(df):\n",
        "    df = df.copy()\n",
        "    m = (df['pickup_latitude'].between(40.5, 41.0) &\n",
        "         df['pickup_longitude'].between(-74.5, -72.8) &\n",
        "         df['dropoff_latitude'].between(40.5, 41.0) &\n",
        "         df['dropoff_longitude'].between(-74.5, -72.8))\n",
        "    m &= df['passenger_count'].between(1, 6)\n",
        "    dist = haversine_km(df['pickup_latitude'], df['pickup_longitude'], df['dropoff_latitude'], df['dropoff_longitude'])\n",
        "    m &= pd.Series(dist).between(0.01, 200).values\n",
        "    m &= df['fare_amount'].between(2.5, 200)\n",
        "    return df.loc[m].copy()\n",
        "\n",
        "def clip_test(df):\n",
        "    df = df.copy()\n",
        "    df['passenger_count'] = df['passenger_count'].clip(lower=1, upper=6).astype('int8')\n",
        "    df['pickup_latitude'] = df['pickup_latitude'].clip(lower=40.5, upper=41.0).astype('float32')\n",
        "    df['dropoff_latitude'] = df['dropoff_latitude'].clip(lower=40.5, upper=41.0).astype('float32')\n",
        "    df['pickup_longitude'] = df['pickup_longitude'].clip(lower=-74.5, upper=-72.8).astype('float32')\n",
        "    df['dropoff_longitude'] = df['dropoff_longitude'].clip(lower=-74.5, upper=-72.8).astype('float32')\n",
        "    return df\n",
        "\n",
        "# Prepare datasets with parity\n",
        "train_df = clean_train(train_sample)\n",
        "test_df = clip_test(test)\n",
        "\n",
        "train_df = add_features(train_df)\n",
        "test_df = add_features(test_df)\n",
        "\n",
        "# Post-feature clipping on test to match train domain for derived distances\n",
        "for col in ['dist_hav_km', 'dist_man_km']:\n",
        "    test_df[col] = test_df[col].clip(lower=0.01, upper=200).astype('float32')\n",
        "\n",
        "feature_cols = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'year','month','day','hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi','is_airport_trip','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "\n",
        "train_df = train_df.dropna(subset=feature_cols + ['fare_amount']).copy()\n",
        "test_df[feature_cols] = test_df[feature_cols].fillna(0)\n",
        "\n",
        "# Order by pickup time for time-series CV\n",
        "train_df = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "\n",
        "# Create matrices\n",
        "X_all = train_df[feature_cols].astype('float32').values\n",
        "y_all = np.log1p(train_df['fare_amount'].astype('float32').values)\n",
        "dt_all = train_df['pickup_datetime']  # tz-aware\n",
        "X_test = test_df[feature_cols].astype('float32').values\n",
        "\n",
        "def make_time_folds_quantile(dt, n_folds=3, gap_days=1):\n",
        "    \"\"\"\n",
        "    Build exactly n_folds non-empty validation windows using array_split on indices:\n",
        "    - Split indices into (warmup + n_folds) contiguous chunks.\n",
        "    - Each fold uses chunk k (1..K) as validation, and all rows strictly before (v_start - gap) as train.\n",
        "    - If gap removes all train for earliest fold, fallback to all indices before val start (no leakage within val).\n",
        "    Uses int64 timestamps and searchsorted for robust gap cut.\n",
        "    \"\"\"\n",
        "    assert len(dt) > 0\n",
        "    n = len(dt)\n",
        "    idx = np.arange(n, dtype=int)\n",
        "    chunks = np.array_split(idx, n_folds + 1)  # 0: warmup, 1..K: folds\n",
        "    ts = dt.astype('int64').to_numpy()  # ns since epoch, sorted ascending\n",
        "    gap_ns = np.int64(gap_days * 24 * 3600 * 10**9)\n",
        "    folds = []\n",
        "    for k in range(1, n_folds + 1):\n",
        "        val_idx = chunks[k]\n",
        "        if val_idx.size == 0:\n",
        "            continue\n",
        "        s = int(val_idx[0])\n",
        "        v_start_ns = ts[s]\n",
        "        cutoff = v_start_ns - gap_ns\n",
        "        train_end = int(np.searchsorted(ts, cutoff, side='left'))  # positions strictly before cutoff\n",
        "        if train_end == 0:\n",
        "            train_end = s  # shrink gap only if needed to ensure non-empty train\n",
        "        if train_end == 0:\n",
        "            continue\n",
        "        train_idx = np.arange(0, train_end, dtype=int)\n",
        "        folds.append((train_idx, val_idx))\n",
        "    assert len(folds) == n_folds, f'Expected {n_folds} folds, got {len(folds)}'\n",
        "    return folds\n",
        "\n",
        "K = 3\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "# LightGBM stronger params to reduce underfitting; rely on early stopping\n",
        "lgb_params = dict(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=10000,\n",
        "    num_leaves=256,\n",
        "    max_depth=-1,\n",
        "    min_data_in_leaf=100,\n",
        "    feature_fraction=0.8,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=1,\n",
        "    max_bin=255,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=1.0,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses = []\n",
        "best_iters = []\n",
        "\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "    X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "    model = lgb.LGBMRegressor(**lgb_params)\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False), lgb.log_evaluation(period=50)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    y_true_val = np.expm1(y_va)\n",
        "    rmse = float(root_mean_squared_error(y_true_val, y_pred_val))\n",
        "    cv_rmses.append(rmse)\n",
        "    best_iters.append(int(model.best_iteration_ if model.best_iteration_ is not None else model.n_estimators))\n",
        "    print(f'Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})')\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses))\n",
        "cv_std = float(np.std(cv_rmses))\n",
        "best_iter_final = int(np.clip(int(np.median(best_iters)), 200, 10000))\n",
        "print({'cv_rmse_mean': round(cv_mean, 6), 'cv_rmse_std': round(cv_std, 6), 'best_iter_final': best_iter_final})\n",
        "\n",
        "# Fit final model on all ordered data with chosen iterations\n",
        "final_model = lgb.LGBMRegressor(**{**lgb_params, 'n_estimators': best_iter_final})\n",
        "final_model.fit(X_all, y_all)\n",
        "\n",
        "test_pred = np.expm1(final_model.predict(X_test)).astype('float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "\n",
        "pred_df = pd.DataFrame({'key': test_df['key'].astype('string'), 'fare_amount': test_pred})\n",
        "sub = sample[['key']].merge(pred_df, on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0], 'Submission key alignment error'\n",
        "assert sub['fare_amount'].notna().all(), 'Missing predictions for some keys'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape:', sub.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalid_0's rmse: 0.217772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.211975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.210341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.209268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.208753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.208603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[350]\tvalid_0's rmse: 0.208558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.208604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.208581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.208573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[550]\tvalid_0's rmse: 0.208589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: RMSE=3.40236, best_iter=389 (train_n=480373, val_n=481134)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalid_0's rmse: 0.242087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.228094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.224096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.222233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.22136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.220764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[350]\tvalid_0's rmse: 0.220321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.21998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.219742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.219602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[550]\tvalid_0's rmse: 0.219477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.219348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[650]\tvalid_0's rmse: 0.219169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\tvalid_0's rmse: 0.219103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.218999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.218918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[850]\tvalid_0's rmse: 0.218836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.218833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[950]\tvalid_0's rmse: 0.218781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.218736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.218707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\tvalid_0's rmse: 0.218696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1150]\tvalid_0's rmse: 0.21866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.218684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1250]\tvalid_0's rmse: 0.218645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1300]\tvalid_0's rmse: 0.218656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.218652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.218589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1450]\tvalid_0's rmse: 0.218591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.218565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1550]\tvalid_0's rmse: 0.218573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.21859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.218598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1700]\tvalid_0's rmse: 0.218641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: RMSE=3.86595, best_iter=1523 (train_n=961323, val_n=481134)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalid_0's rmse: 0.219922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.202177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.197403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.194709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.193111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.192227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[350]\tvalid_0's rmse: 0.191598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.191059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.190737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.19048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[550]\tvalid_0's rmse: 0.190193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.189983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[650]\tvalid_0's rmse: 0.189859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\tvalid_0's rmse: 0.189689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.189581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.189454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[850]\tvalid_0's rmse: 0.18936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.189304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[950]\tvalid_0's rmse: 0.189228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.189178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.189121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\tvalid_0's rmse: 0.189043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1150]\tvalid_0's rmse: 0.189011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.188989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1250]\tvalid_0's rmse: 0.18898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1300]\tvalid_0's rmse: 0.188948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.188925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.188887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1450]\tvalid_0's rmse: 0.18887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.188846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1550]\tvalid_0's rmse: 0.188827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.188819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.18882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1700]\tvalid_0's rmse: 0.188811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1750]\tvalid_0's rmse: 0.18878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.188771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1850]\tvalid_0's rmse: 0.188796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1900]\tvalid_0's rmse: 0.188783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1950]\tvalid_0's rmse: 0.188789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: RMSE=3.48195, best_iter=1787 (train_n=1442667, val_n=481133)\n{'cv_rmse_mean': 3.583418, 'cv_rmse_std': 0.202406, 'best_iter_final': 1523}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with shape: (9914, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "1b309578-0af3-42d6-9859-58a131edb286",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnosis: Investigate CV instability (Fold-wise time ranges, distributions, holiday impact) and prepare for 5-fold CV\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar as USCal\n",
        "\n",
        "assert 'train_df' in globals() and 'train_df' is not None, 'train_df missing'\n",
        "assert 'dt_all' in globals(), 'dt_all missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'fold generator missing'\n",
        "\n",
        "def fold_diagnostics(dt, df_full, folds, name):\n",
        "    print(f'=== Diagnostics for {name} ===')\n",
        "    # Prepare holiday calendar on NY local dates\n",
        "    dt_local = df_full['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    date_min = dt_local.min().date()\n",
        "    date_max = dt_local.max().date()\n",
        "    cal = USCal()\n",
        "    hol = cal.holidays(start=pd.Timestamp(date_min), end=pd.Timestamp(date_max))\n",
        "    hol_dates = set(pd.to_datetime(hol).date)\n",
        "    results = []\n",
        "    for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "        va_dt = dt.iloc[va_idx]\n",
        "        va_local = va_dt.dt.tz_convert('America/New_York')\n",
        "        v_start, v_end = va_local.min(), va_local.max()\n",
        "        va_part = df_full.iloc[va_idx]\n",
        "        # Basic distributions\n",
        "        fare_desc = va_part['fare_amount'].describe() if 'fare_amount' in va_part.columns else None\n",
        "        dist_desc = va_part['dist_hav_km'].describe() if 'dist_hav_km' in va_part.columns else None\n",
        "        pc_counts = va_part['passenger_count'].value_counts(normalize=True).sort_index()\n",
        "        # Holiday share\n",
        "        va_dates = va_local.dt.date\n",
        "        hol_share = float(np.mean(va_dates.isin(hol_dates)))\n",
        "        dow_counts = va_local.dt.dayofweek.value_counts(normalize=True).sort_index()\n",
        "        month_counts = va_local.dt.month.value_counts(normalize=True).sort_index()\n",
        "        print({\n",
        "            'fold': i,\n",
        "            'val_n': int(len(va_idx)),\n",
        "            'val_range_local': f\"{v_start} -> {v_end}\",\n",
        "            'holiday_share': round(hol_share, 4),\n",
        "            'fare_mean': round(float(fare_desc['mean']) if fare_desc is not None else float('nan'), 4),\n",
        "            'fare_p95': round(float(va_part['fare_amount'].quantile(0.95)) if 'fare_amount' in va_part.columns else float('nan'), 4),\n",
        "            'dist_hav_mean': round(float(dist_desc['mean']) if dist_desc is not None else float('nan'), 4),\n",
        "            'dist_hav_p95': round(float(va_part['dist_hav_km'].quantile(0.95)) if 'dist_hav_km' in va_part.columns else float('nan'), 4),\n",
        "        })\n",
        "        results.append({\n",
        "            'fold': i,\n",
        "            'v_start': v_start,\n",
        "            'v_end': v_end,\n",
        "            'holiday_share': hol_share,\n",
        "            'pc_dist': pc_counts.to_dict(),\n",
        "            'dow_dist': dow_counts.to_dict(),\n",
        "            'month_dist': month_counts.to_dict(),\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# Ensure features exist for diagnostics (they were added before modeling)\n",
        "assert 'dist_hav_km' in train_df.columns, 'Expected dist_hav_km in train_df'\n",
        "\n",
        "# Rebuild 3-folds used earlier and also prepare 5-folds for robustness\n",
        "folds_3 = make_time_folds_quantile(dt_all, n_folds=3, gap_days=1)\n",
        "diag3 = fold_diagnostics(dt_all, train_df, folds_3, name='3-fold CV (current)')\n",
        "\n",
        "folds_5 = make_time_folds_quantile(dt_all, n_folds=5, gap_days=1)\n",
        "diag5 = fold_diagnostics(dt_all, train_df, folds_5, name='5-fold CV (proposed)')\n",
        "\n",
        "# Parity check note: ensure train-side clipping for derived distances in future modeling (currently only test is clipped)\n",
        "train_clip_check = {\n",
        "    'train_dist_hav_min': float(train_df['dist_hav_km'].min()),\n",
        "    'train_dist_hav_max': float(train_df['dist_hav_km'].max()),\n",
        "    'train_dist_man_min': float(train_df['dist_man_km'].min()),\n",
        "    'train_dist_man_max': float(train_df['dist_man_km'].max()),\n",
        "}\n",
        "print('Train distance ranges (pre-clip):', train_clip_check)\n",
        "print('Next step: clip train dist_hav_km and dist_man_km to [0.01, 200] to match test, add holiday/day-of-year features, and switch to 5-fold CV.')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:6: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n<>:6: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n/tmp/ipykernel_44/3270079139.py:6: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  assert 'train_df' in globals() and 'train_df' is not None, 'train_df missing'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Diagnostics for 3-fold CV (current) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fold': 1, 'val_n': 481134, 'val_range_local': '2010-08-15 10:20:00-04:00 -> 2012-03-23 06:40:51-04:00', 'holiday_share': 0.0234, 'fare_mean': 10.3199, 'fare_p95': 26.9, 'dist_hav_mean': 3.3214, 'dist_hav_p95': 9.8415}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fold': 2, 'val_n': 481134, 'val_range_local': '2012-03-23 06:41:00-04:00 -> 2013-10-22 18:06:00-04:00', 'holiday_share': 0.017, 'fare_mean': 11.9459, 'fare_p95': 32.0, 'dist_hav_mean': 3.428, 'dist_hav_p95': 10.2621}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fold': 3, 'val_n': 481133, 'val_range_local': '2013-10-22 18:07:37-04:00 -> 2015-06-30 19:59:25-04:00', 'holiday_share': 0.0196, 'fare_mean': 12.8364, 'fare_p95': 35.5, 'dist_hav_mean': 3.4225, 'dist_hav_p95': 10.496}\n=== Diagnostics for 5-fold CV (proposed) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fold': 1, 'val_n': 320756, 'val_range_local': '2010-01-27 12:25:28-05:00 -> 2011-03-05 12:35:00-05:00', 'holiday_share': 0.0214, 'fare_mean': 10.0924, 'fare_p95': 26.1, 'dist_hav_mean': 3.2518, 'dist_hav_p95': 9.6344}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fold': 2, 'val_n': 320756, 'val_range_local': '2011-03-05 12:35:04-05:00 -> 2012-03-23 06:40:51-04:00', 'holiday_share': 0.0189, 'fare_mean': 10.4113, 'fare_p95': 27.3, 'dist_hav_mean': 3.3645, 'dist_hav_p95': 9.9602}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fold': 3, 'val_n': 320756, 'val_range_local': '2012-03-23 06:41:00-04:00 -> 2013-04-09 03:24:00-04:00', 'holiday_share': 0.0182, 'fare_mean': 11.5625, 'fare_p95': 30.5, 'dist_hav_mean': 3.4024, 'dist_hav_p95': 10.1674}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fold': 4, 'val_n': 320756, 'val_range_local': '2013-04-09 03:27:00-04:00 -> 2014-05-03 16:48:00-04:00', 'holiday_share': 0.0181, 'fare_mean': 12.6008, 'fare_p95': 34.33, 'dist_hav_mean': 3.4203, 'dist_hav_p95': 10.3212}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fold': 5, 'val_n': 320755, 'val_range_local': '2014-05-03 16:50:37-04:00 -> 2015-06-30 19:59:25-04:00', 'holiday_share': 0.0187, 'fare_mean': 13.0101, 'fare_p95': 36.33, 'dist_hav_mean': 3.453, 'dist_hav_p95': 10.6888}\nTrain distance ranges (pre-clip): {'train_dist_hav_min': 0.010023146867752075, 'train_dist_hav_max': 91.0614013671875, 'train_dist_man_min': 0.01025301218032837, 'train_dist_man_max': 101.6641845703125}\nNext step: clip train dist_hav_km and dist_man_km to [0.01, 200] to match test, add holiday/day-of-year features, and switch to 5-fold CV.\n"
          ]
        }
      ]
    },
    {
      "id": "9d5baa60-5d05-48c8-bc0a-75e08a82b6be",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stability pass: parity clipping on train, add holiday/day-of-year features, switch to 5-fold CV\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar as USCal\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals(), 'Expected prepared train_df/test_df'\n",
        "assert 'make_time_folds_quantile' in globals(), 'fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "def add_holiday_and_doy(df):\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    # Day-of-year cyclical\n",
        "    doy = dt_local.dt.dayofyear.astype('int16')\n",
        "    df['doy_sin'] = np.sin(2*np.pi*(doy/365.25)).astype('float32')\n",
        "    df['doy_cos'] = np.cos(2*np.pi*(doy/365.25)).astype('float32')\n",
        "    # US Federal Holidays flag\n",
        "    start = dt_local.min().normalize().tz_localize(None)\n",
        "    end = dt_local.max().normalize().tz_localize(None)\n",
        "    hol = USCal().holidays(start=start, end=end)\n",
        "    hol_dates = set(pd.to_datetime(hol).date)\n",
        "    df['is_holiday'] = dt_local.dt.date.map(lambda d: d in hol_dates).astype('int8')\n",
        "    return df\n",
        "\n",
        "# Train-side parity clipping for derived distances\n",
        "for col in ['dist_hav_km', 'dist_man_km']:\n",
        "    if col in train_df.columns:\n",
        "        train_df[col] = train_df[col].clip(lower=0.01, upper=200).astype('float32')\n",
        "\n",
        "# Add stability features\n",
        "train_df = add_holiday_and_doy(train_df)\n",
        "test_df = add_holiday_and_doy(test_df)\n",
        "\n",
        "# Features (extended)\n",
        "feature_cols = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'year','month','day','hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi','is_airport_trip','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "\n",
        "# Clean NaNs just in case\n",
        "train_df = train_df.dropna(subset=feature_cols + ['fare_amount']).copy()\n",
        "test_df[feature_cols] = test_df[feature_cols].fillna(0)\n",
        "\n",
        "# Order by time and build matrices\n",
        "train_df = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "X_all = train_df[feature_cols].astype('float32').values\n",
        "y_all = np.log1p(train_df['fare_amount'].astype('float32').values)\n",
        "dt_all = train_df['pickup_datetime']\n",
        "X_test = test_df[feature_cols].astype('float32').values\n",
        "\n",
        "# 5-fold CV with 1-day gap\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "SEED = 2025\n",
        "lgb_params = dict(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=12000,\n",
        "    num_leaves=256,\n",
        "    max_depth=-1,\n",
        "    min_data_in_leaf=100,\n",
        "    feature_fraction=0.8,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=1,\n",
        "    max_bin=255,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=1.0,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "    X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "    model = lgb.LGBMRegressor(**lgb_params)\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False), lgb.log_evaluation(period=100)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    y_true_val = np.expm1(y_va)\n",
        "    rmse = float(root_mean_squared_error(y_true_val, y_pred_val))\n",
        "    cv_rmses.append(rmse)\n",
        "    best_iters.append(int(model.best_iteration_ if model.best_iteration_ is not None else model.n_estimators))\n",
        "    print(f'Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})')\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses))\n",
        "cv_std = float(np.std(cv_rmses))\n",
        "best_iter_final = int(np.clip(int(np.median(best_iters)), 300, 12000))\n",
        "print({'cv_rmse_mean': round(cv_mean, 6), 'cv_rmse_std': round(cv_std, 6), 'best_iter_final': best_iter_final})\n",
        "\n",
        "# Fit final model and write submission\n",
        "final_model = lgb.LGBMRegressor(**{**lgb_params, 'n_estimators': best_iter_final})\n",
        "final_model.fit(X_all, y_all)\n",
        "test_pred = np.expm1(final_model.predict(X_test)).astype('float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "pred_df = pd.DataFrame({'key': test_df['key'].astype('string'), 'fare_amount': test_pred})\n",
        "sub = sample[['key']].merge(pred_df, on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape:', sub.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.234393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.231866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.231498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.231514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.231644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.231863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: RMSE=3.71100, best_iter=357 (train_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.193662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.191594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.191712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.191552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.191791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.192006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: RMSE=3.19618, best_iter=388 (train_n=640523, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.216781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.21133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.21015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.20956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.20926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.20898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\tvalid_0's rmse: 0.208798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.208703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.208656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.208607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\tvalid_0's rmse: 0.208536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.208525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1300]\tvalid_0's rmse: 0.208514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.208542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.208575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: RMSE=3.61429, best_iter=1232 (train_n=961323, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.209217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.201428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.199304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.19818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.197609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.197272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\tvalid_0's rmse: 0.197031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.196821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.196684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.196571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\tvalid_0's rmse: 0.196447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.196375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1300]\tvalid_0's rmse: 0.196293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.196247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.196189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.196144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1700]\tvalid_0's rmse: 0.196128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.196117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1900]\tvalid_0's rmse: 0.19609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.196095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.196093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\tvalid_0's rmse: 0.196101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2300]\tvalid_0's rmse: 0.196103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.196109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: RMSE=3.55101, best_iter=2136 (train_n=1282234, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.192223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.183929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.181241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.179948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.179229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.178694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\tvalid_0's rmse: 0.178406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.17823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.177993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.177886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\tvalid_0's rmse: 0.17779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.177687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1300]\tvalid_0's rmse: 0.17763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.177611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.177527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.177509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1700]\tvalid_0's rmse: 0.177494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.177463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1900]\tvalid_0's rmse: 0.177458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.17744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.177417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\tvalid_0's rmse: 0.177429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2300]\tvalid_0's rmse: 0.177411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.177402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2500]\tvalid_0's rmse: 0.177416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2600]\tvalid_0's rmse: 0.177426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2700]\tvalid_0's rmse: 0.177414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5: RMSE=3.28215, best_iter=2404 (train_n=1602868, val_n=320755)\n{'cv_rmse_mean': 3.470925, 'cv_rmse_std': 0.197849, 'best_iter_final': 1232}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with shape: (9914, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "289e16fa-b2f9-467f-933c-af760fe4d6d7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Feature sprint: spatial bins + frequency encodings (leakage-safe), dropoff POIs, continuous trend; 5-fold time-CV\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals(), 'train_df/test_df not prepared'\n",
        "assert 'make_time_folds_quantile' in globals(), 'fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# 1) Add dropoff-side POIs and robust airport flag\n",
        "def add_dropoff_pois(df):\n",
        "    df = df.copy()\n",
        "    pu_lat = df['pickup_latitude'].astype('float32'); pu_lon = df['pickup_longitude'].astype('float32')\n",
        "    do_lat = df['dropoff_latitude'].astype('float32'); do_lon = df['dropoff_longitude'].astype('float32')\n",
        "    # Dropoff distances\n",
        "    df['do_dist_jfk'] = haversine_km(do_lat, do_lon, POIS['JFK'][0], POIS['JFK'][1]).astype('float32')\n",
        "    df['do_dist_lga'] = haversine_km(do_lat, do_lon, POIS['LGA'][0], POIS['LGA'][1]).astype('float32')\n",
        "    df['do_dist_ewr'] = haversine_km(do_lat, do_lon, POIS['EWR'][0], POIS['EWR'][1]).astype('float32')\n",
        "    df['do_dist_midtown'] = haversine_km(do_lat, do_lon, POIS['MIDTOWN'][0], POIS['MIDTOWN'][1]).astype('float32')\n",
        "    df['do_dist_fidi'] = haversine_km(do_lat, do_lon, POIS['FIDI'][0], POIS['FIDI'][1]).astype('float32')\n",
        "    # Any-airport indicator (pickup or dropoff within 2km of any airport)\n",
        "    min_pu_air = df[['dist_jfk','dist_lga','dist_ewr']].min(axis=1)\n",
        "    min_do_air = df[['do_dist_jfk','do_dist_lga','do_dist_ewr']].min(axis=1)\n",
        "    df['is_airport_trip_any'] = ((min_pu_air < 2.0) | (min_do_air < 2.0)).astype('int8')\n",
        "    return df\n",
        "\n",
        "train_df = add_dropoff_pois(train_df)\n",
        "test_df = add_dropoff_pois(test_df)\n",
        "\n",
        "# 2) Continuous time trend (days since start) in NY local time\n",
        "def add_time_trend(df, global_start):\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    df['days_since_start'] = (dt_local.view('int64') - global_start) / np.float64(24*3600*1e9)\n",
        "    df['days_since_start'] = df['days_since_start'].astype('float32')\n",
        "    return df\n",
        "\n",
        "dt_local_all = train_df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "global_start_ns = int(dt_local_all.min().value)\n",
        "train_df = add_time_trend(train_df, global_start_ns)\n",
        "test_df = add_time_trend(test_df, global_start_ns)\n",
        "\n",
        "# 3) Spatial discretization: 500m-ish grid bins (approx via 0.005 degrees)\n",
        "def add_spatial_bins(df, res=0.005):\n",
        "    df = df.copy()\n",
        "    pu_lat_bin = np.floor(df['pickup_latitude'] / res).astype('int32')\n",
        "    pu_lon_bin = np.floor(df['pickup_longitude'] / res).astype('int32')\n",
        "    do_lat_bin = np.floor(df['dropoff_latitude'] / res).astype('int32')\n",
        "    do_lon_bin = np.floor(df['dropoff_longitude'] / res).astype('int32')\n",
        "    df['pu_bin'] = (pu_lat_bin.astype(str) + '_' + pu_lon_bin.astype(str))\n",
        "    df['do_bin'] = (do_lat_bin.astype(str) + '_' + do_lon_bin.astype(str))\n",
        "    df['pair_bin'] = (df['pu_bin'] + '|' + df['do_bin'])\n",
        "    return df\n",
        "\n",
        "train_df = add_spatial_bins(train_df)\n",
        "test_df = add_spatial_bins(test_df)\n",
        "\n",
        "# 4) Optional clusters (MiniBatchKMeans for speed) on a 200k sample\n",
        "def add_clusters(train, test, k=80, sample_n=200_000, seed=2025):\n",
        "    coords = train[['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude']].astype('float32')\n",
        "    # Fit pickup and dropoff clusters separately for clarity\n",
        "    pu_coords = train[['pickup_latitude','pickup_longitude']].astype('float32')\n",
        "    do_coords = train[['dropoff_latitude','dropoff_longitude']].astype('float32')\n",
        "    if len(train) > sample_n:\n",
        "        samp_idx = np.random.default_rng(seed).choice(len(train), size=sample_n, replace=False)\n",
        "        pu_fit = pu_coords.iloc[samp_idx]\n",
        "        do_fit = do_coords.iloc[samp_idx]\n",
        "    else:\n",
        "        pu_fit, do_fit = pu_coords, do_coords\n",
        "    pu_km = MiniBatchKMeans(n_clusters=k, random_state=seed, batch_size=10000, n_init=5, max_no_improvement=20)\n",
        "    do_km = MiniBatchKMeans(n_clusters=k, random_state=seed+1, batch_size=10000, n_init=5, max_no_improvement=20)\n",
        "    pu_km.fit(pu_fit.values)\n",
        "    do_km.fit(do_fit.values)\n",
        "    train = train.copy(); test = test.copy()\n",
        "    train['pu_cl'] = pu_km.predict(pu_coords.values).astype('int32')\n",
        "    train['do_cl'] = do_km.predict(do_coords.values).astype('int32')\n",
        "    test['pu_cl'] = pu_km.predict(test[['pickup_latitude','pickup_longitude']].astype('float32').values).astype('int32')\n",
        "    test['do_cl'] = do_km.predict(test[['dropoff_latitude','dropoff_longitude']].astype('float32').values).astype('int32')\n",
        "    train['same_cl'] = (train['pu_cl'] == train['do_cl']).astype('int8')\n",
        "    test['same_cl'] = (test['pu_cl'] == test['do_cl']).astype('int8')\n",
        "    return train, test\n",
        "\n",
        "train_df, test_df = add_clusters(train_df, test_df, k=80)\n",
        "\n",
        "# 5) Prepare base numeric features\n",
        "base_features = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'year','month','day','hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday','days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi','do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend',\n",
        "    'pu_cl','do_cl','same_cl'\n",
        "]\n",
        "\n",
        "# Clean NaNs\n",
        "train_df = train_df.dropna(subset=base_features + ['fare_amount']).reset_index(drop=True)\n",
        "test_df[base_features] = test_df[base_features].fillna(0)\n",
        "\n",
        "# Order by time\n",
        "train_df = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "dt_all = train_df['pickup_datetime']\n",
        "y_all = np.log1p(train_df['fare_amount'].astype('float32').values)\n",
        "\n",
        "# 6) 5-fold CV with leakage-safe frequency encodings computed per-fold\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "def build_fold_matrices(df, tr_idx, va_idx):\n",
        "    # Compute frequency encodings on training subset only\n",
        "    tr = df.iloc[tr_idx]\n",
        "    va = df.iloc[va_idx]\n",
        "    pu_cnt_map = tr['pu_bin'].value_counts().astype('int32')\n",
        "    do_cnt_map = tr['do_bin'].value_counts().astype('int32')\n",
        "    pair_cnt_map = tr['pair_bin'].value_counts().astype('int32')\n",
        "    # Map counts to both train and val (safe; no target)\n",
        "    df_enc = df.copy()\n",
        "    df_enc['pu_cnt'] = df_enc['pu_bin'].map(pu_cnt_map).fillna(0).astype('int32')\n",
        "    df_enc['do_cnt'] = df_enc['do_bin'].map(do_cnt_map).fillna(0).astype('int32')\n",
        "    df_enc['pair_cnt'] = df_enc['pair_bin'].map(pair_cnt_map).fillna(0).astype('int32')\n",
        "    # Feature matrix\n",
        "    use_cols = base_features + ['pu_cnt','do_cnt','pair_cnt']\n",
        "    X_tr = df_enc.iloc[tr_idx][use_cols].astype('float32').values\n",
        "    X_va = df_enc.iloc[va_idx][use_cols].astype('float32').values\n",
        "    return X_tr, X_va, use_cols\n",
        "\n",
        "SEED = 2025\n",
        "lgb_params = dict(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=16000,\n",
        "    num_leaves=384,\n",
        "    max_depth=-1,\n",
        "    min_data_in_leaf=100,\n",
        "    feature_fraction=0.85,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=1,\n",
        "    max_bin=255,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=1.0,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    X_tr, X_va, use_cols = build_fold_matrices(train_df, tr_idx, va_idx)\n",
        "    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n",
        "    model = lgb.LGBMRegressor(**lgb_params)\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False), lgb.log_evaluation(period=150)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    y_true_val = np.expm1(y_va)\n",
        "    rmse = float(root_mean_squared_error(y_true_val, y_pred_val))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    print(f'Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})')\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_final = int(np.clip(int(np.median(best_iters)), 600, 16000))\n",
        "print({'cv_rmse_mean': round(cv_mean, 6), 'cv_rmse_std': round(cv_std, 6), 'best_iter_final': best_iter_final})\n",
        "\n",
        "# 7) Fit final model on all data: build frequency encodings using full train, then predict test\n",
        "pu_cnt_full = train_df['pu_bin'].value_counts().astype('int32')\n",
        "do_cnt_full = train_df['do_bin'].value_counts().astype('int32')\n",
        "pair_cnt_full = train_df['pair_bin'].value_counts().astype('int32')\n",
        "\n",
        "train_full = train_df.copy()\n",
        "train_full['pu_cnt'] = train_full['pu_bin'].map(pu_cnt_full).fillna(0).astype('int32')\n",
        "train_full['do_cnt'] = train_full['do_bin'].map(do_cnt_full).fillna(0).astype('int32')\n",
        "train_full['pair_cnt'] = train_full['pair_bin'].map(pair_cnt_full).fillna(0).astype('int32')\n",
        "\n",
        "test_full = test_df.copy()\n",
        "test_full['pu_cnt'] = test_full['pu_bin'].map(pu_cnt_full).fillna(0).astype('int32')\n",
        "test_full['do_cnt'] = test_full['do_bin'].map(do_cnt_full).fillna(0).astype('int32')\n",
        "test_full['pair_cnt'] = test_full['pair_bin'].map(pair_cnt_full).fillna(0).astype('int32')\n",
        "\n",
        "final_features = use_cols\n",
        "X_full = train_full[final_features].astype('float32').values\n",
        "X_test = test_full[final_features].astype('float32').values\n",
        "\n",
        "final_model = lgb.LGBMRegressor(**{**lgb_params, 'n_estimators': best_iter_final})\n",
        "final_model.fit(X_full, y_all)\n",
        "test_pred = np.expm1(final_model.predict(X_test)).astype('float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "pred_df = pd.DataFrame({'key': test_df['key'].astype('string'), 'fare_amount': test_pred})\n",
        "sub = sample[['key']].merge(pred_df, on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape:', sub.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_44/2785190460.py:40: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n  df['days_since_start'] = (dt_local.view('int64') - global_start) / np.float64(24*3600*1e9)\n/tmp/ipykernel_44/2785190460.py:40: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n  df['days_since_start'] = (dt_local.view('int64') - global_start) / np.float64(24*3600*1e9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.231592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.230974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.231527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: RMSE=3.72799, best_iter=251 (train_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.19299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.191751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.192054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: RMSE=3.24314, best_iter=271 (train_n=640523, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.211039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.208184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.207952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.207591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.207656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.207543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.207556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: RMSE=3.58950, best_iter=781 (train_n=961323, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.200274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.196358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.195354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.194925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.194694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.194651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.194604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.19459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.194658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: RMSE=3.50795, best_iter=1082 (train_n=1282234, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.183133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.17814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.177058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.176579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.176387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.176238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.176232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.176264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5: RMSE=3.21542, best_iter=1029 (train_n=1602868, val_n=320755)\n{'cv_rmse_mean': 3.456801, 'cv_rmse_std': 0.198841, 'best_iter_final': 781}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with shape: (9914, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "e84a8d76-b0d3-480e-941b-7f203baf1464",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Mandatory rework (fast): stabilize TE \u2014 drop raw cluster IDs, add pu/do/pair log-counts, ablate te_pair, stronger regularization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals(), 'train_df/test_df not prepared'\n",
        "assert 'make_time_folds_quantile' in globals(), 'fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# 1) Time trend (warning-safe)\n",
        "def add_time_trend_fixed(df, global_start_ns):\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    ns = dt_local.astype('int64').values\n",
        "    df['days_since_start'] = (ns - np.int64(global_start_ns)) / np.float64(24*3600*1e9)\n",
        "    return df.astype({'days_since_start':'float32'})\n",
        "\n",
        "dt_local_all = train_df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "global_start_ns = int(dt_local_all.min().value)\n",
        "if 'days_since_start' not in train_df.columns:\n",
        "    train_df = add_time_trend_fixed(train_df, global_start_ns)\n",
        "    test_df = add_time_trend_fixed(test_df, global_start_ns)\n",
        "\n",
        "# 2) Rotated-Manhattan in km\n",
        "def add_rot_manhattan(df, theta_deg=29.0):\n",
        "    df = df.copy()\n",
        "    lat_rad = np.deg2rad(df['pickup_latitude'].astype('float32'))\n",
        "    k_lat = 111_000.0\n",
        "    k_lon = (np.cos(lat_rad).astype('float32') * 111_000.0).astype('float32')\n",
        "    dx = (df['delta_lon'].astype('float32') * k_lon).astype('float32')\n",
        "    dy = (df['delta_lat'].astype('float32') * k_lat).astype('float32')\n",
        "    th = np.deg2rad(np.float32(theta_deg))\n",
        "    c, s = np.cos(th), np.sin(th)\n",
        "    r1 = np.abs(dx*c + dy*s); r2 = np.abs(-dx*s + dy*c)\n",
        "    df['rot_manh_km'] = ((r1 + r2) / 1000.0).astype('float32')\n",
        "    return df\n",
        "\n",
        "if 'rot_manh_km' not in train_df.columns:\n",
        "    train_df = add_rot_manhattan(train_df)\n",
        "    test_df = add_rot_manhattan(test_df)\n",
        "\n",
        "# 3) Ensure fast spatial bins\n",
        "def ensure_spatial_bins(df, res=0.005):\n",
        "    if all(c in df.columns for c in ['pu_bin','do_bin','pair_bin']):\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    pu_lat_bin = np.floor(df['pickup_latitude'] / res).astype('int32')\n",
        "    pu_lon_bin = np.floor(df['pickup_longitude'] / res).astype('int32')\n",
        "    do_lat_bin = np.floor(df['dropoff_latitude'] / res).astype('int32')\n",
        "    do_lon_bin = np.floor(df['dropoff_longitude'] / res).astype('int32')\n",
        "    df['pu_bin'] = (pu_lat_bin.astype(str) + '_' + pu_lon_bin.astype(str))\n",
        "    df['do_bin'] = (do_lat_bin.astype(str) + '_' + do_lon_bin.astype(str))\n",
        "    df['pair_bin'] = (df['pu_bin'] + '|' + df['do_bin'])\n",
        "    return df\n",
        "\n",
        "train_df = ensure_spatial_bins(train_df)\n",
        "test_df = ensure_spatial_bins(test_df)\n",
        "\n",
        "# 4) Smoothed target encoding helper (use moderate smoothing)\n",
        "def target_encode_smooth(tr_ser, tr_target, va_ser, m=200.0, prior=None):\n",
        "    if prior is None:\n",
        "        prior = float(tr_target.mean())\n",
        "    g = pd.DataFrame({'k': tr_ser.astype('object'), 'y': tr_target}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict()\n",
        "    tr_enc = tr_ser.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    va_enc = va_ser.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return tr_enc, va_enc, prior, mp\n",
        "\n",
        "# 5) Base numeric features \u2014 drop raw cluster IDs to avoid fighting TE; keep same_cl only if present\n",
        "base_num = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'year','month','day','hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday','days_since_start','rot_manh_km',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'same_cl' in train_df.columns: base_num.append('same_cl')\n",
        "\n",
        "need_cols = base_num + ['fare_amount']\n",
        "train_df = train_df.dropna(subset=need_cols).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "test_df[base_num] = test_df[base_num].fillna(0)\n",
        "dt_all = train_df['pickup_datetime']\n",
        "\n",
        "# 6) 5-fold CV with per-fold TE (pu_bin, do_bin only) + log-counts (pu/do/pair) from train-only maps\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "SEED = 2025\n",
        "lgb_params = dict(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    learning_rate=0.04,\n",
        "    n_estimators=25000,\n",
        "    num_leaves=256,\n",
        "    max_depth=-1,\n",
        "    min_data_in_leaf=200,\n",
        "    feature_fraction=0.8,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=1,\n",
        "    max_bin=255,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=10.0,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_df.iloc[tr_idx].copy(); va = train_df.iloc[va_idx].copy()\n",
        "    # targets (log1p)\n",
        "    y_tr_log = np.log1p(tr['fare_amount'].astype('float32').values); y_va_log = np.log1p(va['fare_amount'].astype('float32').values)\n",
        "    prior = float(y_tr_log.mean()); m = 200.0\n",
        "    # TE on pu/do only (ablate te_pair_bin for stability)\n",
        "    for col in ['pu_bin','do_bin']:\n",
        "        tr_enc, va_enc, _, _ = target_encode_smooth(tr[col], y_tr_log, va[col], m=m, prior=prior)\n",
        "        tr[f'te_{col}'] = tr_enc; va[f'te_{col}'] = va_enc\n",
        "    # Leakage-safe counts from train-only\n",
        "    pu_vc = tr['pu_bin'].astype('object').value_counts().to_dict()\n",
        "    do_vc = tr['do_bin'].astype('object').value_counts().to_dict()\n",
        "    pair_vc = tr['pair_bin'].astype('object').value_counts().to_dict()\n",
        "    tr['log_pu_cnt'] = np.log1p(tr['pu_bin'].astype('object').map(pu_vc).fillna(0).astype('int32')).astype('float32')\n",
        "    va['log_pu_cnt'] = np.log1p(va['pu_bin'].astype('object').map(pu_vc).fillna(0).astype('int32')).astype('float32')\n",
        "    tr['log_do_cnt'] = np.log1p(tr['do_bin'].astype('object').map(do_vc).fillna(0).astype('int32')).astype('float32')\n",
        "    va['log_do_cnt'] = np.log1p(va['do_bin'].astype('object').map(do_vc).fillna(0).astype('int32')).astype('float32')\n",
        "    tr['log_pair_cnt'] = np.log1p(tr['pair_bin'].astype('object').map(pair_vc).fillna(0).astype('int32')).astype('float32')\n",
        "    va['log_pair_cnt'] = np.log1p(va['pair_bin'].astype('object').map(pair_vc).fillna(0).astype('int32')).astype('float32')\n",
        "    # Assemble\n",
        "    use_cols = base_num + ['te_pu_bin','te_do_bin','log_pu_cnt','log_do_cnt','log_pair_cnt']\n",
        "    X_tr = tr[use_cols].astype('float32'); X_va = va[use_cols].astype('float32')\n",
        "    assert np.isfinite(X_tr.values).all() and np.isfinite(X_va.values).all(), 'Non-finite values in features'\n",
        "    model = lgb.LGBMRegressor(**lgb_params)\n",
        "    model.fit(\n",
        "        X_tr, y_tr_log,\n",
        "        eval_set=[(X_va, y_va_log)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=500, verbose=False), lgb.log_evaluation(period=200)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va_log), y_pred_val))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    print(f'Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})')\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_final = int(np.clip(int(np.median(best_iters)), 1200, 25000))\n",
        "print({'cv_rmse_mean': round(cv_mean, 6), 'cv_rmse_std': round(cv_std, 6), 'best_iter_final': best_iter_final})\n",
        "\n",
        "# 7) Final model: fit TE on full train (pu/do) + log-counts; predict test\n",
        "full = train_df.copy(); tst = test_df.copy()\n",
        "y_full_log = np.log1p(full['fare_amount'].astype('float32').values)\n",
        "prior_full = float(y_full_log.mean()); m = 200.0\n",
        "for col in ['pu_bin','do_bin']:\n",
        "    tr_enc, va_enc, _, _ = target_encode_smooth(full[col], y_full_log, tst[col], m=m, prior=prior_full)\n",
        "    full[f'te_{col}'] = tr_enc; tst[f'te_{col}'] = va_enc\n",
        "pu_vc_full = full['pu_bin'].astype('object').value_counts().to_dict()\n",
        "do_vc_full = full['do_bin'].astype('object').value_counts().to_dict()\n",
        "pair_vc_full = full['pair_bin'].astype('object').value_counts().to_dict()\n",
        "full['log_pu_cnt'] = np.log1p(full['pu_bin'].astype('object').map(pu_vc_full).fillna(0).astype('int32')).astype('float32')\n",
        "tst['log_pu_cnt'] = np.log1p(tst['pu_bin'].astype('object').map(pu_vc_full).fillna(0).astype('int32')).astype('float32')\n",
        "full['log_do_cnt'] = np.log1p(full['do_bin'].astype('object').map(do_vc_full).fillna(0).astype('int32')).astype('float32')\n",
        "tst['log_do_cnt'] = np.log1p(tst['do_bin'].astype('object').map(do_vc_full).fillna(0).astype('int32')).astype('float32')\n",
        "full['log_pair_cnt'] = np.log1p(full['pair_bin'].astype('object').map(pair_vc_full).fillna(0).astype('int32')).astype('float32')\n",
        "tst['log_pair_cnt'] = np.log1p(tst['pair_bin'].astype('object').map(pair_vc_full).fillna(0).astype('int32')).astype('float32')\n",
        "final_use = base_num + ['te_pu_bin','te_do_bin','log_pu_cnt','log_do_cnt','log_pair_cnt']\n",
        "X_full = full[final_use].astype('float32'); X_test = tst[final_use].astype('float32')\n",
        "assert np.isfinite(X_full.values).all() and np.isfinite(X_test.values).all(), 'Non-finite values in final features'\n",
        "final_model = lgb.LGBMRegressor(**{**lgb_params, 'n_estimators': best_iter_final})\n",
        "final_model.fit(X_full, y_full_log)\n",
        "test_pred = np.expm1(final_model.predict(X_test)).astype('float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "pred_df = pd.DataFrame({'key': test_df['key'].astype('string'), 'fare_amount': test_pred})\n",
        "sub = sample[['key']].merge(pred_df, on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape:', sub.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.232369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.23242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.233228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: RMSE=3.79261, best_iter=256 (train_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.189598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.189246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.189645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: RMSE=3.36168, best_iter=268 (train_n=640523, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.211846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.2098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.209486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.209322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.209366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.209441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.209475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: RMSE=3.76216, best_iter=929 (train_n=961323, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.200908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.197461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.196715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.196321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.196153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.196115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.196141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.196177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.196172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: RMSE=3.73814, best_iter=1448 (train_n=1282234, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.1841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.180154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.179176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.178836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.178555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.178449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.17848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.178509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5: RMSE=3.51287, best_iter=1262 (train_n=1602868, val_n=320755)\n{'cv_rmse_mean': 3.633491, 'cv_rmse_std': 0.168083, 'best_iter_final': 1200}\n"
          ]
        }
      ]
    },
    {
      "id": "d8309902-037b-494a-a1b2-57d7512fa920",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Debug TE pipeline: inspect per-fold target encodings and feature matrix for Fold 1\n",
        "import numpy as np, pandas as pd\n",
        "assert 'train_df' in globals() and 'make_time_folds_quantile' in globals(), 'Prereqs missing'\n",
        "\n",
        "# Ensure required columns exist\n",
        "for c in ['pu_bin','do_bin','pair_bin','fare_amount','pickup_datetime']:\n",
        "    assert c in train_df.columns, f'Missing {c} in train_df'\n",
        "\n",
        "dt_all_dbg = train_df['pickup_datetime']\n",
        "folds_dbg = make_time_folds_quantile(dt_all_dbg, n_folds=5, gap_days=1)\n",
        "tr_idx, va_idx = folds_dbg[0]\n",
        "tr = train_df.iloc[tr_idx].copy(); va = train_df.iloc[va_idx].copy()\n",
        "y_tr_log = np.log1p(tr['fare_amount'].astype('float32').values)\n",
        "prior = float(y_tr_log.mean()); m = 1000.0\n",
        "\n",
        "def te_map(series_tr, target_tr, m=1000.0, prior=None):\n",
        "    if prior is None:\n",
        "        prior = float(target_tr.mean())\n",
        "    g = pd.DataFrame({'k': series_tr.astype('object'), 'y': target_tr}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    return smooth.to_dict(), prior\n",
        "\n",
        "# Build maps on train only\n",
        "maps = {}\n",
        "for col in ['pu_bin','do_bin','pair_bin'] + ([c for c in ['pu_cl','do_cl'] if c in tr.columns]):\n",
        "    mp, pr = te_map(tr[col], y_tr_log, m=m, prior=prior)\n",
        "    maps[col] = (mp, pr)\n",
        "\n",
        "# Apply maps\n",
        "te_stats = []\n",
        "for col in maps:\n",
        "    mp, pr = maps[col]\n",
        "    tr_enc = tr[col].astype('object').map(mp).fillna(pr).astype('float32')\n",
        "    va_enc = va[col].astype('object').map(mp).fillna(pr).astype('float32')\n",
        "    te_stats.append({\n",
        "        'col': col,\n",
        "        'tr_na_rate': float(1.0 - tr[col].astype('object').map(mp).notna().mean()),\n",
        "        'va_na_rate': float(1.0 - va[col].astype('object').map(mp).notna().mean()),\n",
        "        'tr_mean': float(tr_enc.mean()),\n",
        "        'va_mean': float(va_enc.mean()),\n",
        "        'tr_std': float(tr_enc.std()),\n",
        "        'va_std': float(va_enc.std())\n",
        "    })\n",
        "\n",
        "print('TE diagnostics (Fold 1): prior=', prior)\n",
        "for row in te_stats:\n",
        "    print(row)\n",
        "\n",
        "# Check feature finiteness and variance for Fold 1 assembled features\n",
        "base_num = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'year','month','day','hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday'\n",
        "]\n",
        "if 'days_since_start' in train_df.columns: base_num.append('days_since_start')\n",
        "if 'rot_manh_km' in train_df.columns: base_num.append('rot_manh_km')\n",
        "for c in ['dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi','do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi','is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend']:\n",
        "    if c in train_df.columns: base_num.append(c)\n",
        "for c in ['pu_cl','do_cl','same_cl']:\n",
        "    if c in train_df.columns: base_num.append(c)\n",
        "\n",
        "# Assemble TE features\n",
        "for col in maps:\n",
        "    mp, pr = maps[col]\n",
        "    tr[f'te_{col}'] = tr[col].astype('object').map(mp).fillna(pr).astype('float32')\n",
        "    va[f'te_{col}'] = va[col].astype('object').map(mp).fillna(pr).astype('float32')\n",
        "tr['log_pair_cnt'] = np.log1p(tr['pair_bin'].astype('object').map(tr['pair_bin'].astype('object').value_counts()).fillna(0).astype('int32')).astype('float32')\n",
        "va['log_pair_cnt'] = np.log1p(va['pair_bin'].astype('object').map(tr['pair_bin'].astype('object').value_counts()).fillna(0).astype('int32')).astype('float32')\n",
        "use_cols = base_num + [c for c in tr.columns if c.startswith('te_')] + ['log_pair_cnt']\n",
        "X_tr = tr[use_cols].astype('float32')\n",
        "X_va = va[use_cols].astype('float32')\n",
        "print('Shapes:', X_tr.shape, X_va.shape, 'n_use_cols=', len(use_cols))\n",
        "print('Finite checks:', np.isfinite(X_tr.values).all(), np.isfinite(X_va.values).all())\n",
        "print('Variance (first 10 cols):', X_tr.var().head(10).to_dict())\n",
        "print('Any zero-variance cols:', bool((X_tr.var() == 0).any()))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TE diagnostics (Fold 1): prior= 2.243056058883667\n{'col': 'pu_bin', 'tr_na_rate': 0.0, 'va_na_rate': 0.00333898664405341, 'tr_mean': 2.2265536785125732, 'va_mean': 2.227857828140259, 'tr_std': 0.11589936912059784, 'va_std': 0.11819490790367126}\n{'col': 'do_bin', 'tr_na_rate': 0.0, 'va_na_rate': 0.00381598473606104, 'tr_mean': 2.20108699798584, 'va_mean': 2.2015466690063477, 'tr_std': 0.0922887846827507, 'va_std': 0.09205456078052521}\n{'col': 'pair_bin', 'tr_na_rate': 0.0, 'va_na_rate': 0.09091645986357233, 'tr_mean': 2.2360243797302246, 'va_mean': 2.236215591430664, 'tr_std': 0.015584036707878113, 'va_std': 0.01518352422863245}\n{'col': 'pu_cl', 'tr_na_rate': 0.0, 'va_na_rate': 0.0, 'tr_mean': 2.233232021331787, 'va_mean': 2.2348179817199707, 'tr_std': 0.1591968834400177, 'va_std': 0.16106157004833221}\n{'col': 'do_cl', 'tr_na_rate': 0.0, 'va_na_rate': 3.117634588267748e-06, 'tr_mean': 2.2207770347595215, 'va_mean': 2.2219908237457275, 'tr_std': 0.1709526628255844, 'va_std': 0.17199507355690002}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (319908, 53) (320756, 53) n_use_cols= 53\nFinite checks: True True\nVariance (first 10 cols): {'pickup_longitude': 0.0010823372285813093, 'pickup_latitude': 0.0006756684742867947, 'dropoff_longitude': 0.0011220808373764157, 'dropoff_latitude': 0.0009095306741073728, 'passenger_count': 1.5728309154510498, 'dist_hav_km': 11.514139175415039, 'dist_man_km': 19.904155731201172, 'delta_lat': 0.0009210002608597279, 'delta_lon': 0.001471993513405323, 'bear_sin': 2.5747036147549807e-07}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Any zero-variance cols: False\n"
          ]
        }
      ]
    },
    {
      "id": "8fb19e14-c10a-488a-88c5-620aa0a62dd1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick A/B: robust objective (huber) on stable feature set from Cell 5 to test impact on CV std\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'feature_cols' in globals(), 'Prereqs missing (train_df/feature_cols)'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Rebuild matrices from the simpler, previously stable feature set\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "X_all_h = train_ord[feature_cols].astype('float32').values\n",
        "y_all_h = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "dt_all_h = train_ord['pickup_datetime']\n",
        "\n",
        "# 5-fold time CV with 1-day gap\n",
        "K = 5\n",
        "folds_h = make_time_folds_quantile(dt_all_h, n_folds=K, gap_days=1)\n",
        "assert len(folds_h) == K, f'Expected {K} folds, got {len(folds_h)}'\n",
        "\n",
        "SEED = 2025\n",
        "lgb_params_huber = dict(\n",
        "    objective='huber',\n",
        "    metric='rmse',\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=16000,\n",
        "    num_leaves=256,\n",
        "    max_depth=-1,\n",
        "    min_data_in_leaf=120,\n",
        "    feature_fraction=0.8,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=1,\n",
        "    max_bin=255,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=2.0,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds_h, 1):\n",
        "    X_tr, y_tr = X_all_h[tr_idx], y_all_h[tr_idx]\n",
        "    X_va, y_va = X_all_h[va_idx], y_all_h[va_idx]\n",
        "    model = lgb.LGBMRegressor(**lgb_params_huber)\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False), lgb.log_evaluation(period=150)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    y_true_val = np.expm1(y_va)\n",
        "    rmse = float(root_mean_squared_error(y_true_val, y_pred_val))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    print(f'[huber] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})')\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_final = int(np.clip(int(np.median(best_iters)), 400, 16000))\n",
        "print({'cv_rmse_mean': round(cv_mean, 6), 'cv_rmse_std': round(cv_std, 6), 'best_iter_final': best_iter_final, 'objective': 'huber'})"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.233055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.231346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.231142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.231246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[huber] Fold 1: RMSE=3.68608, best_iter=431 (train_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.190609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.190214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.190117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.190738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[huber] Fold 2: RMSE=3.16536, best_iter=395 (train_n=640523, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.213345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.209824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.208807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.208317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.208066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.207899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.207796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.207754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.207802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.207733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.207761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.207793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[huber] Fold 3: RMSE=3.58556, best_iter=1512 (train_n=961323, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.20438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.199264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.197697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.197006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.196583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.196287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.196071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.195913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.195818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.195742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.195669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.19562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1950]\tvalid_0's rmse: 0.195613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.195597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2250]\tvalid_0's rmse: 0.195581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.19561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[huber] Fold 4: RMSE=3.51703, best_iter=2245 (train_n=1282234, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.186709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.180568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.178788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.178003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.17744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.177071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.176827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.176683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.176574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.176521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.176469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.176441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1950]\tvalid_0's rmse: 0.176425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.17641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[huber] Fold 5: RMSE=3.21017, best_iter=1912 (train_n=1602868, val_n=320755)\n{'cv_rmse_mean': 3.432841, 'cv_rmse_std': 0.207688, 'best_iter_final': 1512, 'objective': 'huber'}\n"
          ]
        }
      ]
    },
    {
      "id": "e5ed69f6-b0d5-4f6f-a781-cadb7b8aa152",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 1 (Stability): No Target Encoding, per-fold frequency encodings, fare policy feature, monotone constraint on time trend\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals(), 'train_df/test_df missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# 0) Ensure time trend and spatial bins exist\n",
        "def ensure_days_since_start(df, global_start_ns):\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    ns = dt_local.astype('int64').values\n",
        "    df['days_since_start'] = ((ns - np.int64(global_start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return df\n",
        "\n",
        "def ensure_spatial_bins(df, res=0.005):\n",
        "    if all(c in df.columns for c in ['pu_bin','do_bin','pair_bin']):\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    pu_lat_bin = np.floor(df['pickup_latitude'] / res).astype('int32')\n",
        "    pu_lon_bin = np.floor(df['pickup_longitude'] / res).astype('int32')\n",
        "    do_lat_bin = np.floor(df['dropoff_latitude'] / res).astype('int32')\n",
        "    do_lon_bin = np.floor(df['dropoff_longitude'] / res).astype('int32')\n",
        "    df['pu_bin'] = (pu_lat_bin.astype(str) + '_' + pu_lon_bin.astype(str))\n",
        "    df['do_bin'] = (do_lat_bin.astype(str) + '_' + do_lon_bin.astype(str))\n",
        "    df['pair_bin'] = (df['pu_bin'] + '|' + df['do_bin'])\n",
        "    return df\n",
        "\n",
        "dt_local_all = train_df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "global_start_ns = int(dt_local_all.min().value)\n",
        "if 'days_since_start' not in train_df.columns:\n",
        "    train_df = ensure_days_since_start(train_df, global_start_ns)\n",
        "    test_df = ensure_days_since_start(test_df, global_start_ns)\n",
        "train_df = ensure_spatial_bins(train_df)\n",
        "test_df = ensure_spatial_bins(test_df)\n",
        "\n",
        "# 1) Fare policy feature (NYC fare hike around 2012-09-04 local time) + simple interactions\n",
        "def add_fare_policy(df):\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    df['after_hike'] = (dt_local >= cutoff).astype('int8')\n",
        "    # interactions (keep small for stability)\n",
        "    if 'dist_hav_km' in df.columns:\n",
        "        df['dist_x_after_hike'] = (df['dist_hav_km'] * df['after_hike']).astype('float32')\n",
        "    else:\n",
        "        df['dist_x_after_hike'] = np.float32(0.0)\n",
        "    df['pc_x_after_hike'] = (df['passenger_count'].astype('float32') * df['after_hike']).astype('float32')\n",
        "    return df\n",
        "\n",
        "train_df = add_fare_policy(train_df)\n",
        "test_df = add_fare_policy(test_df)\n",
        "\n",
        "# 2) Core numeric features (no TE). Use robust, already-engineered set.\n",
        "core_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'year','month','day','hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday','days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_df.columns: core_feats.append('rot_manh_km')\n",
        "policy_feats = ['after_hike','dist_x_after_hike','pc_x_after_hike']\n",
        "\n",
        "# 3) Build 5-fold time CV with 1-day gap and per-fold frequency encodings (log-counts) for pu/do/pair bins\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "def build_fold_mats(df, tr_idx, va_idx):\n",
        "    tr = df.iloc[tr_idx].copy()\n",
        "    va = df.iloc[va_idx].copy()\n",
        "    pu_cnt = tr['pu_bin'].astype('object').value_counts()\n",
        "    do_cnt = tr['do_bin'].astype('object').value_counts()\n",
        "    pair_cnt = tr['pair_bin'].astype('object').value_counts()\n",
        "    for d in (tr, va):\n",
        "        d['log_pu_cnt'] = np.log1p(d['pu_bin'].astype('object').map(pu_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_do_cnt'] = np.log1p(d['do_bin'].astype('object').map(do_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_pair_cnt'] = np.log1p(d['pair_bin'].astype('object').map(pair_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "    use_cols = core_feats + policy_feats + ['log_pu_cnt','log_do_cnt','log_pair_cnt']\n",
        "    X_tr = tr[use_cols].astype('float32').values\n",
        "    X_va = va[use_cols].astype('float32').values\n",
        "    return X_tr, X_va, use_cols\n",
        "\n",
        "SEED = 2025\n",
        "lgb_params = dict(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=16000,\n",
        "    num_leaves=256,\n",
        "    max_depth=-1,\n",
        "    min_data_in_leaf=120,\n",
        "    feature_fraction=0.85,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=1,\n",
        "    max_bin=255,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=2.0,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "final_use_cols = None\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    X_tr, X_va, use_cols = build_fold_mats(train_ord, tr_idx, va_idx)\n",
        "    y_tr, y_va = y_all_log[tr_idx], y_all_log[va_idx]\n",
        "    # Monotonicity: +1 on days_since_start, 0 elsewhere\n",
        "    mono = [0] * len(use_cols)\n",
        "    if 'days_since_start' in use_cols:\n",
        "        mono[use_cols.index('days_since_start')] = 1\n",
        "    model = lgb.LGBMRegressor(**{**lgb_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False), lgb.log_evaluation(period=150)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    y_true_val = np.expm1(y_va)\n",
        "    rmse = float(root_mean_squared_error(y_true_val, y_pred_val))\n",
        "    cv_rmses.append(rmse)\n",
        "    best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    final_use_cols = use_cols\n",
        "    print(f'[Stability Phase] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})')\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_final = int(np.clip(int(np.median(best_iters)), 500, lgb_params['n_estimators']))\n",
        "print({'cv_rmse_mean': round(cv_mean, 6), 'cv_rmse_std': round(cv_std, 6), 'best_iter_final': best_iter_final, 'note': 'No TE; per-fold freq enc; fare policy; monotone days_since_start'})\n",
        "\n",
        "# 4) Train final model on full data with full-data frequency maps; predict test and save submission\n",
        "pu_cnt_full = train_ord['pu_bin'].astype('object').value_counts()\n",
        "do_cnt_full = train_ord['do_bin'].astype('object').value_counts()\n",
        "pair_cnt_full = train_ord['pair_bin'].astype('object').value_counts()\n",
        "\n",
        "train_full = train_ord.copy()\n",
        "test_full = test_df.copy()\n",
        "for d in (train_full, test_full):\n",
        "    d['log_pu_cnt'] = np.log1p(d['pu_bin'].astype('object').map(pu_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "    d['log_do_cnt'] = np.log1p(d['do_bin'].astype('object').map(do_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "    d['log_pair_cnt'] = np.log1p(d['pair_bin'].astype('object').map(pair_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "\n",
        "use_cols = final_use_cols\n",
        "X_full = train_full[use_cols].astype('float32').values\n",
        "y_full = y_all_log\n",
        "X_test = test_full[use_cols].astype('float32').values\n",
        "mono = [0] * len(use_cols)\n",
        "if 'days_since_start' in use_cols:\n",
        "    mono[use_cols.index('days_since_start')] = 1\n",
        "final_model = lgb.LGBMRegressor(**{**lgb_params, 'n_estimators': best_iter_final, 'monotone_constraints': mono})\n",
        "final_model.fit(X_full, y_full)\n",
        "test_pred = np.expm1(final_model.predict(X_test)).astype('float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "pred_df = pd.DataFrame({'key': test_df['key'].astype('string'), 'fare_amount': test_pred})\n",
        "sub = sample[['key']].merge(pred_df, on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape:', sub.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.231743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.230688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.230989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.231295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stability Phase] Fold 1: RMSE=3.68317, best_iter=306 (train_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.187459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.186754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.186616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.187017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stability Phase] Fold 2: RMSE=3.05045, best_iter=407 (train_n=640523, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.211124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.207833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.207165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.206658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.206078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.205662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.205571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.205574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.205615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.205616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stability Phase] Fold 3: RMSE=3.55499, best_iter=1227 (train_n=961323, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.200765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.196497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.195444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.194967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.194673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.194511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.194424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.194333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.194275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.194296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.194248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.194355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stability Phase] Fold 4: RMSE=3.49205, best_iter=1629 (train_n=1282234, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.184215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.178949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.177555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.176899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.176472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.176277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.176231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.176156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.176195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stability Phase] Fold 5: RMSE=3.19196, best_iter=1157 (train_n=1602868, val_n=320755)\n{'cv_rmse_mean': 3.394525, 'cv_rmse_std': 0.235795, 'best_iter_final': 1157, 'note': 'No TE; per-fold freq enc; fare policy; monotone days_since_start'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with shape: (9914, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "5753e5b9-fad2-4c7b-9d71-42aaf9746b24",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 1 \u2014 Targeted Iteration for Stability: piecewise time trend, airport-hike interaction, per-fold freq encodings (no monotone to avoid slowdown)\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals(), 'train_df/test_df missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "def ensure_days_since_start(df, global_start_ns):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    ns = dt_local.astype('int64').values\n",
        "    df['days_since_start'] = ((ns - np.int64(global_start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return df\n",
        "\n",
        "def ensure_spatial_bins(df, res=0.005):\n",
        "    if all(c in df.columns for c in ['pu_bin','do_bin','pair_bin']): return df\n",
        "    df = df.copy()\n",
        "    pu_lat_bin = np.floor(df['pickup_latitude'] / res).astype('int32')\n",
        "    pu_lon_bin = np.floor(df['pickup_longitude'] / res).astype('int32')\n",
        "    do_lat_bin = np.floor(df['dropoff_latitude'] / res).astype('int32')\n",
        "    do_lon_bin = np.floor(df['dropoff_longitude'] / res).astype('int32')\n",
        "    df['pu_bin'] = (pu_lat_bin.astype(str) + '_' + pu_lon_bin.astype(str))\n",
        "    df['do_bin'] = (do_lat_bin.astype(str) + '_' + do_lon_bin.astype(str))\n",
        "    df['pair_bin'] = (df['pu_bin'] + '|' + df['do_bin'])\n",
        "    return df\n",
        "\n",
        "dt_local_all = train_df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "global_start_ns = int(dt_local_all.min().value)\n",
        "train_df = ensure_days_since_start(train_df, global_start_ns)\n",
        "test_df = ensure_days_since_start(test_df, global_start_ns)\n",
        "train_df = ensure_spatial_bins(train_df)\n",
        "test_df = ensure_spatial_bins(test_df)\n",
        "\n",
        "def add_policy_and_interactions(df):\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    df['after_hike'] = (dt_local >= cutoff).astype('int8')\n",
        "    df['days_since_start_x_after_hike'] = (df['days_since_start'] * df['after_hike']).astype('float32')\n",
        "    if 'dist_hav_km' in df.columns:\n",
        "        df['dist_x_after_hike'] = (df['dist_hav_km'] * df['after_hike']).astype('float32')\n",
        "    else:\n",
        "        df['dist_x_after_hike'] = np.float32(0.0)\n",
        "    if 'is_airport_trip_any' not in df.columns:\n",
        "        if all(c in df.columns for c in ['dist_jfk','dist_lga','dist_ewr']):\n",
        "            df['is_airport_trip_any'] = (df[['dist_jfk','dist_lga','dist_ewr']].min(axis=1) < 2.0).astype('int8')\n",
        "        else:\n",
        "            df['is_airport_trip_any'] = df.get('is_airport_trip', pd.Series(0, index=df.index)).astype('int8')\n",
        "    df['is_airport_trip_any_after_hike'] = (df['is_airport_trip_any'] * df['after_hike']).astype('float32')\n",
        "    return df\n",
        "\n",
        "train_df = add_policy_and_interactions(train_df)\n",
        "test_df = add_policy_and_interactions(test_df)\n",
        "\n",
        "core_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'year','month','day','hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday','days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_df.columns: core_feats.append('rot_manh_km')\n",
        "policy_feats = ['after_hike','days_since_start_x_after_hike','dist_x_after_hike','is_airport_trip_any_after_hike']\n",
        "\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "def build_fold_mats(df, tr_idx, va_idx):\n",
        "    tr = df.iloc[tr_idx].copy(); va = df.iloc[va_idx].copy()\n",
        "    pu_cnt = tr['pu_bin'].astype('object').value_counts()\n",
        "    do_cnt = tr['do_bin'].astype('object').value_counts()\n",
        "    pair_cnt = tr['pair_bin'].astype('object').value_counts()\n",
        "    for d in (tr, va):\n",
        "        d['log_pu_cnt'] = np.log1p(d['pu_bin'].astype('object').map(pu_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_do_cnt'] = np.log1p(d['do_bin'].astype('object').map(do_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_pair_cnt'] = np.log1p(d['pair_bin'].astype('object').map(pair_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "    use_cols = core_feats + policy_feats + ['log_pu_cnt','log_do_cnt','log_pair_cnt']\n",
        "    X_tr = tr[use_cols].astype('float32').values; X_va = va[use_cols].astype('float32').values\n",
        "    return X_tr, X_va, use_cols\n",
        "\n",
        "SEED = 2025\n",
        "lgb_params = dict(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    learning_rate=0.08,\n",
        "    n_estimators=1200,\n",
        "    num_leaves=32,\n",
        "    max_depth=-1,\n",
        "    min_data_in_leaf=2000,\n",
        "    feature_fraction=0.6,\n",
        "    bagging_fraction=0.7,\n",
        "    bagging_freq=1,\n",
        "    max_bin=31,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=40.0,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "final_use_cols = None\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    print(f'Starting fold {i}/{K}...')\n",
        "    X_tr, X_va, use_cols = build_fold_mats(train_ord, tr_idx, va_idx)\n",
        "    y_tr, y_va = y_all_log[tr_idx], y_all_log[va_idx]\n",
        "    model = lgb.LGBMRegressor(**lgb_params)\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False), lgb.log_evaluation(period=10)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    y_true_val = np.expm1(y_va)\n",
        "    rmse = float(root_mean_squared_error(y_true_val, y_pred_val))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators)); final_use_cols = use_cols\n",
        "    print(f'[Stability Iter FAST] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})')\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_final = int(np.clip(int(np.median(best_iters)), 100, lgb_params['n_estimators']))\n",
        "print({'cv_rmse_mean': round(cv_mean, 6), 'cv_rmse_std': round(cv_std, 6), 'best_iter_final': best_iter_final, 'note': 'piecewise time; airport-hike; per-fold freq; no monotone (speed)'} )\n",
        "\n",
        "# Train final model and predict test\n",
        "pu_cnt_full = train_ord['pu_bin'].astype('object').value_counts()\n",
        "do_cnt_full = train_ord['do_bin'].astype('object').value_counts()\n",
        "pair_cnt_full = train_ord['pair_bin'].astype('object').value_counts()\n",
        "train_full = train_ord.copy(); test_full = test_df.copy()\n",
        "for d in (train_full, test_full):\n",
        "    d['log_pu_cnt'] = np.log1p(d['pu_bin'].astype('object').map(pu_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "    d['log_do_cnt'] = np.log1p(d['do_bin'].astype('object').map(do_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "    d['log_pair_cnt'] = np.log1p(d['pair_bin'].astype('object').map(pair_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "use_cols = final_use_cols\n",
        "X_full = train_full[use_cols].astype('float32').values; y_full = y_all_log\n",
        "X_test = test_full[use_cols].astype('float32').values\n",
        "final_model = lgb.LGBMRegressor(**{**lgb_params, 'n_estimators': best_iter_final})\n",
        "final_model.fit(X_full, y_full)\n",
        "test_pred = np.expm1(final_model.predict(X_test)).astype('float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "pred_df = pd.DataFrame({'key': test_df['key'].astype('string'), 'fare_amount': test_pred})\n",
        "sub = sample[['key']].merge(pred_df, on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape:', sub.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fold 1/5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10]\tvalid_0's rmse: 0.312364\n[20]\tvalid_0's rmse: 0.258756\n[30]\tvalid_0's rmse: 0.246197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[40]\tvalid_0's rmse: 0.242903\n[50]\tvalid_0's rmse: 0.24146\n[60]\tvalid_0's rmse: 0.240405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[70]\tvalid_0's rmse: 0.239623\n[80]\tvalid_0's rmse: 0.238997\n[90]\tvalid_0's rmse: 0.238343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.23779\n[110]\tvalid_0's rmse: 0.237198\n[120]\tvalid_0's rmse: 0.236821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[130]\tvalid_0's rmse: 0.236571\n[140]\tvalid_0's rmse: 0.236225\n[150]\tvalid_0's rmse: 0.235945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[160]\tvalid_0's rmse: 0.235696\n[170]\tvalid_0's rmse: 0.235428\n[180]\tvalid_0's rmse: 0.235196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[190]\tvalid_0's rmse: 0.235026\n[200]\tvalid_0's rmse: 0.23484\n[210]\tvalid_0's rmse: 0.234743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[220]\tvalid_0's rmse: 0.234586\n[230]\tvalid_0's rmse: 0.234457\n[240]\tvalid_0's rmse: 0.234374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.234237\n[260]\tvalid_0's rmse: 0.234164\n[270]\tvalid_0's rmse: 0.234084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[280]\tvalid_0's rmse: 0.23403\n[290]\tvalid_0's rmse: 0.23395\n[300]\tvalid_0's rmse: 0.233761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[310]\tvalid_0's rmse: 0.23366\n[320]\tvalid_0's rmse: 0.233615\n[330]\tvalid_0's rmse: 0.233558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[340]\tvalid_0's rmse: 0.233467\n[350]\tvalid_0's rmse: 0.233431\n[360]\tvalid_0's rmse: 0.233395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[370]\tvalid_0's rmse: 0.233331\n[380]\tvalid_0's rmse: 0.233249\n[390]\tvalid_0's rmse: 0.233203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.233126\n[410]\tvalid_0's rmse: 0.233068\n[420]\tvalid_0's rmse: 0.233012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[430]\tvalid_0's rmse: 0.232961\n[440]\tvalid_0's rmse: 0.232923\n[450]\tvalid_0's rmse: 0.232881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[460]\tvalid_0's rmse: 0.232828\n[470]\tvalid_0's rmse: 0.232773\n[480]\tvalid_0's rmse: 0.232731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[490]\tvalid_0's rmse: 0.232684\n[500]\tvalid_0's rmse: 0.232631\n[510]\tvalid_0's rmse: 0.232625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[520]\tvalid_0's rmse: 0.232584\n[530]\tvalid_0's rmse: 0.232549\n[540]\tvalid_0's rmse: 0.232518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[550]\tvalid_0's rmse: 0.232495\n[560]\tvalid_0's rmse: 0.232444\n[570]\tvalid_0's rmse: 0.232425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[580]\tvalid_0's rmse: 0.232392\n[590]\tvalid_0's rmse: 0.232422\n[600]\tvalid_0's rmse: 0.232397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[610]\tvalid_0's rmse: 0.232351\n[620]\tvalid_0's rmse: 0.232305\n[630]\tvalid_0's rmse: 0.232273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[640]\tvalid_0's rmse: 0.232234\n[650]\tvalid_0's rmse: 0.232217\n[660]\tvalid_0's rmse: 0.232202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[670]\tvalid_0's rmse: 0.232174\n[680]\tvalid_0's rmse: 0.232146\n[690]\tvalid_0's rmse: 0.232134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\tvalid_0's rmse: 0.232111\n[710]\tvalid_0's rmse: 0.232095\n[720]\tvalid_0's rmse: 0.232082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[730]\tvalid_0's rmse: 0.232058\n[740]\tvalid_0's rmse: 0.232022\n[750]\tvalid_0's rmse: 0.232011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[760]\tvalid_0's rmse: 0.232012\n[770]\tvalid_0's rmse: 0.232038\n[780]\tvalid_0's rmse: 0.232042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[790]\tvalid_0's rmse: 0.232028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stability Iter FAST] Fold 1: RMSE=3.73128, best_iter=745 (train_n=319908, val_n=320756)\nStarting fold 2/5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10]\tvalid_0's rmse: 0.298428\n[20]\tvalid_0's rmse: 0.228591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30]\tvalid_0's rmse: 0.206987\n[40]\tvalid_0's rmse: 0.198029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalid_0's rmse: 0.194519\n[60]\tvalid_0's rmse: 0.191986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[70]\tvalid_0's rmse: 0.190204\n[80]\tvalid_0's rmse: 0.188663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[90]\tvalid_0's rmse: 0.187907\n[100]\tvalid_0's rmse: 0.18719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[110]\tvalid_0's rmse: 0.186246\n[120]\tvalid_0's rmse: 0.185978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[130]\tvalid_0's rmse: 0.185428\n[140]\tvalid_0's rmse: 0.185316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.184929\n[160]\tvalid_0's rmse: 0.184748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[170]\tvalid_0's rmse: 0.184471\n[180]\tvalid_0's rmse: 0.184161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[190]\tvalid_0's rmse: 0.183795\n[200]\tvalid_0's rmse: 0.183441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[210]\tvalid_0's rmse: 0.183271\n[220]\tvalid_0's rmse: 0.183134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[230]\tvalid_0's rmse: 0.182948\n[240]\tvalid_0's rmse: 0.182882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.182689\n[260]\tvalid_0's rmse: 0.182446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[270]\tvalid_0's rmse: 0.182312\n[280]\tvalid_0's rmse: 0.182517\n[290]\tvalid_0's rmse: 0.18239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.182395\n[310]\tvalid_0's rmse: 0.182287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[320]\tvalid_0's rmse: 0.182168\n[330]\tvalid_0's rmse: 0.182165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[340]\tvalid_0's rmse: 0.182244\n[350]\tvalid_0's rmse: 0.182163\n[360]\tvalid_0's rmse: 0.182121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[370]\tvalid_0's rmse: 0.182014\n[380]\tvalid_0's rmse: 0.182036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[390]\tvalid_0's rmse: 0.181995\n[400]\tvalid_0's rmse: 0.181923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[410]\tvalid_0's rmse: 0.181925\n[420]\tvalid_0's rmse: 0.181843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[430]\tvalid_0's rmse: 0.181848\n[440]\tvalid_0's rmse: 0.18175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.181625\n[460]\tvalid_0's rmse: 0.18153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[470]\tvalid_0's rmse: 0.181513\n[480]\tvalid_0's rmse: 0.181465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[490]\tvalid_0's rmse: 0.181565\n[500]\tvalid_0's rmse: 0.181498\n[510]\tvalid_0's rmse: 0.181468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[520]\tvalid_0's rmse: 0.181524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stability Iter FAST] Fold 2: RMSE=3.05041, best_iter=478 (train_n=640523, val_n=320756)\nStarting fold 3/5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10]\tvalid_0's rmse: 0.333515\n[20]\tvalid_0's rmse: 0.264044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30]\tvalid_0's rmse: 0.242557\n[40]\tvalid_0's rmse: 0.234073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalid_0's rmse: 0.229793\n[60]\tvalid_0's rmse: 0.226995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[70]\tvalid_0's rmse: 0.225117\n[80]\tvalid_0's rmse: 0.223741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[90]\tvalid_0's rmse: 0.222562\n[100]\tvalid_0's rmse: 0.221396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[110]\tvalid_0's rmse: 0.220479\n[120]\tvalid_0's rmse: 0.219732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[130]\tvalid_0's rmse: 0.219057\n[140]\tvalid_0's rmse: 0.218316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.217804\n[160]\tvalid_0's rmse: 0.217308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[170]\tvalid_0's rmse: 0.216892\n[180]\tvalid_0's rmse: 0.216511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[190]\tvalid_0's rmse: 0.216188\n[200]\tvalid_0's rmse: 0.215949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[210]\tvalid_0's rmse: 0.215653\n[220]\tvalid_0's rmse: 0.215399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[230]\tvalid_0's rmse: 0.215243\n[240]\tvalid_0's rmse: 0.215046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.214791\n[260]\tvalid_0's rmse: 0.214617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[270]\tvalid_0's rmse: 0.214528\n[280]\tvalid_0's rmse: 0.214336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290]\tvalid_0's rmse: 0.214205\n[300]\tvalid_0's rmse: 0.214034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[310]\tvalid_0's rmse: 0.21394\n[320]\tvalid_0's rmse: 0.21386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[330]\tvalid_0's rmse: 0.213708\n[340]\tvalid_0's rmse: 0.213614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[350]\tvalid_0's rmse: 0.213546\n[360]\tvalid_0's rmse: 0.213434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[370]\tvalid_0's rmse: 0.213286\n[380]\tvalid_0's rmse: 0.213146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[390]\tvalid_0's rmse: 0.213068\n[400]\tvalid_0's rmse: 0.212957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[410]\tvalid_0's rmse: 0.212918\n[420]\tvalid_0's rmse: 0.212846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[430]\tvalid_0's rmse: 0.212774\n[440]\tvalid_0's rmse: 0.212708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.212689\n[460]\tvalid_0's rmse: 0.212589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[470]\tvalid_0's rmse: 0.212547\n[480]\tvalid_0's rmse: 0.212434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[490]\tvalid_0's rmse: 0.212343\n[500]\tvalid_0's rmse: 0.212288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[510]\tvalid_0's rmse: 0.212257\n[520]\tvalid_0's rmse: 0.212193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[530]\tvalid_0's rmse: 0.212148\n[540]\tvalid_0's rmse: 0.212038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[550]\tvalid_0's rmse: 0.211889\n[560]\tvalid_0's rmse: 0.211767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[570]\tvalid_0's rmse: 0.211654\n[580]\tvalid_0's rmse: 0.211548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[590]\tvalid_0's rmse: 0.211497\n[600]\tvalid_0's rmse: 0.211378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[610]\tvalid_0's rmse: 0.211366\n[620]\tvalid_0's rmse: 0.211328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[630]\tvalid_0's rmse: 0.211335\n[640]\tvalid_0's rmse: 0.211283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[650]\tvalid_0's rmse: 0.21128\n[660]\tvalid_0's rmse: 0.211208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[670]\tvalid_0's rmse: 0.211169\n[680]\tvalid_0's rmse: 0.211082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[690]\tvalid_0's rmse: 0.211031\n[700]\tvalid_0's rmse: 0.210991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[710]\tvalid_0's rmse: 0.210979\n[720]\tvalid_0's rmse: 0.210933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[730]\tvalid_0's rmse: 0.210921\n[740]\tvalid_0's rmse: 0.210884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.210842\n[760]\tvalid_0's rmse: 0.210825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[770]\tvalid_0's rmse: 0.210705\n[780]\tvalid_0's rmse: 0.210698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[790]\tvalid_0's rmse: 0.210679\n[800]\tvalid_0's rmse: 0.210663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[810]\tvalid_0's rmse: 0.210614\n[820]\tvalid_0's rmse: 0.210588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[830]\tvalid_0's rmse: 0.210601\n[840]\tvalid_0's rmse: 0.210576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[850]\tvalid_0's rmse: 0.210566\n[860]\tvalid_0's rmse: 0.210554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[870]\tvalid_0's rmse: 0.210535\n[880]\tvalid_0's rmse: 0.210483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[890]\tvalid_0's rmse: 0.210486\n[900]\tvalid_0's rmse: 0.210415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[910]\tvalid_0's rmse: 0.21029\n[920]\tvalid_0's rmse: 0.210204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[930]\tvalid_0's rmse: 0.210201\n[940]\tvalid_0's rmse: 0.2102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[950]\tvalid_0's rmse: 0.210131\n[960]\tvalid_0's rmse: 0.210082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[970]\tvalid_0's rmse: 0.210032\n[980]\tvalid_0's rmse: 0.209993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[990]\tvalid_0's rmse: 0.209957\n[1000]\tvalid_0's rmse: 0.209961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1010]\tvalid_0's rmse: 0.209958\n[1020]\tvalid_0's rmse: 0.209895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1030]\tvalid_0's rmse: 0.209852\n[1040]\tvalid_0's rmse: 0.209849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.209849\n[1060]\tvalid_0's rmse: 0.209808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1070]\tvalid_0's rmse: 0.209786\n[1080]\tvalid_0's rmse: 0.209803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1090]\tvalid_0's rmse: 0.209771\n[1100]\tvalid_0's rmse: 0.20971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1110]\tvalid_0's rmse: 0.209704\n[1120]\tvalid_0's rmse: 0.209702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1130]\tvalid_0's rmse: 0.209705\n[1140]\tvalid_0's rmse: 0.209697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1150]\tvalid_0's rmse: 0.209671\n[1160]\tvalid_0's rmse: 0.209629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1170]\tvalid_0's rmse: 0.209611\n[1180]\tvalid_0's rmse: 0.209601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1190]\tvalid_0's rmse: 0.209613\n[1200]\tvalid_0's rmse: 0.209593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stability Iter FAST] Fold 3: RMSE=3.70670, best_iter=1183 (train_n=961323, val_n=320756)\nStarting fold 4/5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10]\tvalid_0's rmse: 0.343724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20]\tvalid_0's rmse: 0.264035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30]\tvalid_0's rmse: 0.239414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[40]\tvalid_0's rmse: 0.229089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalid_0's rmse: 0.223809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[60]\tvalid_0's rmse: 0.220265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[70]\tvalid_0's rmse: 0.218219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[80]\tvalid_0's rmse: 0.216414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[90]\tvalid_0's rmse: 0.214508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.213321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[110]\tvalid_0's rmse: 0.212097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[120]\tvalid_0's rmse: 0.211315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[130]\tvalid_0's rmse: 0.210456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[140]\tvalid_0's rmse: 0.209704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.208992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[160]\tvalid_0's rmse: 0.208296\n[170]\tvalid_0's rmse: 0.207708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[180]\tvalid_0's rmse: 0.207256\n[190]\tvalid_0's rmse: 0.206875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.20645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[210]\tvalid_0's rmse: 0.206041\n[220]\tvalid_0's rmse: 0.205755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[230]\tvalid_0's rmse: 0.20541\n[240]\tvalid_0's rmse: 0.20518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.204923\n[260]\tvalid_0's rmse: 0.204702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[270]\tvalid_0's rmse: 0.204475\n[280]\tvalid_0's rmse: 0.204332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290]\tvalid_0's rmse: 0.204178\n[300]\tvalid_0's rmse: 0.204038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[310]\tvalid_0's rmse: 0.203831\n[320]\tvalid_0's rmse: 0.203656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[330]\tvalid_0's rmse: 0.203499\n[340]\tvalid_0's rmse: 0.203331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[350]\tvalid_0's rmse: 0.203164\n[360]\tvalid_0's rmse: 0.203017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[370]\tvalid_0's rmse: 0.202881\n[380]\tvalid_0's rmse: 0.202765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[390]\tvalid_0's rmse: 0.202661\n[400]\tvalid_0's rmse: 0.202532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[410]\tvalid_0's rmse: 0.20241\n[420]\tvalid_0's rmse: 0.202301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[430]\tvalid_0's rmse: 0.20217\n[440]\tvalid_0's rmse: 0.202054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.201908\n[460]\tvalid_0's rmse: 0.201816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[470]\tvalid_0's rmse: 0.201723\n[480]\tvalid_0's rmse: 0.20158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[490]\tvalid_0's rmse: 0.20151\n[500]\tvalid_0's rmse: 0.201394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[510]\tvalid_0's rmse: 0.201319\n[520]\tvalid_0's rmse: 0.201265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[530]\tvalid_0's rmse: 0.201186\n[540]\tvalid_0's rmse: 0.201102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[550]\tvalid_0's rmse: 0.201043\n[560]\tvalid_0's rmse: 0.200969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[570]\tvalid_0's rmse: 0.200891\n[580]\tvalid_0's rmse: 0.200828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[590]\tvalid_0's rmse: 0.200762\n[600]\tvalid_0's rmse: 0.200705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[610]\tvalid_0's rmse: 0.200614\n[620]\tvalid_0's rmse: 0.200539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[630]\tvalid_0's rmse: 0.200475\n[640]\tvalid_0's rmse: 0.2004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[650]\tvalid_0's rmse: 0.200339\n[660]\tvalid_0's rmse: 0.200259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[670]\tvalid_0's rmse: 0.200164\n[680]\tvalid_0's rmse: 0.200106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[690]\tvalid_0's rmse: 0.20004\n[700]\tvalid_0's rmse: 0.199981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[710]\tvalid_0's rmse: 0.199948\n[720]\tvalid_0's rmse: 0.199903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[730]\tvalid_0's rmse: 0.199811\n[740]\tvalid_0's rmse: 0.199783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.199732\n[760]\tvalid_0's rmse: 0.199688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[770]\tvalid_0's rmse: 0.199648\n[780]\tvalid_0's rmse: 0.199628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[790]\tvalid_0's rmse: 0.199557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.199511\n[810]\tvalid_0's rmse: 0.199459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[820]\tvalid_0's rmse: 0.199424\n[830]\tvalid_0's rmse: 0.199389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[840]\tvalid_0's rmse: 0.199349\n[850]\tvalid_0's rmse: 0.199311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[860]\tvalid_0's rmse: 0.199267\n[870]\tvalid_0's rmse: 0.199242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[880]\tvalid_0's rmse: 0.199194\n[890]\tvalid_0's rmse: 0.19916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.199119\n[910]\tvalid_0's rmse: 0.199086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[920]\tvalid_0's rmse: 0.199057\n[930]\tvalid_0's rmse: 0.199006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[940]\tvalid_0's rmse: 0.19898\n[950]\tvalid_0's rmse: 0.198955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[960]\tvalid_0's rmse: 0.198926\n[970]\tvalid_0's rmse: 0.198847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[980]\tvalid_0's rmse: 0.198823\n[990]\tvalid_0's rmse: 0.198793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.198761\n[1010]\tvalid_0's rmse: 0.198734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1020]\tvalid_0's rmse: 0.198689\n[1030]\tvalid_0's rmse: 0.198654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1040]\tvalid_0's rmse: 0.198615\n[1050]\tvalid_0's rmse: 0.19858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1060]\tvalid_0's rmse: 0.198567\n[1070]\tvalid_0's rmse: 0.198536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1080]\tvalid_0's rmse: 0.198509\n[1090]\tvalid_0's rmse: 0.198489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1100]\tvalid_0's rmse: 0.198455\n[1110]\tvalid_0's rmse: 0.198438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1120]\tvalid_0's rmse: 0.198422\n[1130]\tvalid_0's rmse: 0.198406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1140]\tvalid_0's rmse: 0.19838\n[1150]\tvalid_0's rmse: 0.198339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1160]\tvalid_0's rmse: 0.198302\n[1170]\tvalid_0's rmse: 0.198262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1180]\tvalid_0's rmse: 0.198242\n[1190]\tvalid_0's rmse: 0.198218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.198209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stability Iter FAST] Fold 4: RMSE=3.61654, best_iter=1200 (train_n=1282234, val_n=320756)\nStarting fold 5/5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10]\tvalid_0's rmse: 0.340881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20]\tvalid_0's rmse: 0.256952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30]\tvalid_0's rmse: 0.228799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[40]\tvalid_0's rmse: 0.217647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalid_0's rmse: 0.211495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[60]\tvalid_0's rmse: 0.207357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[70]\tvalid_0's rmse: 0.204553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[80]\tvalid_0's rmse: 0.202308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[90]\tvalid_0's rmse: 0.200545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.199049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[110]\tvalid_0's rmse: 0.19766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[120]\tvalid_0's rmse: 0.19636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[130]\tvalid_0's rmse: 0.195449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[140]\tvalid_0's rmse: 0.194455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.193743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[160]\tvalid_0's rmse: 0.193062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[170]\tvalid_0's rmse: 0.192528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[180]\tvalid_0's rmse: 0.192006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[190]\tvalid_0's rmse: 0.191518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.19093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[210]\tvalid_0's rmse: 0.19045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[220]\tvalid_0's rmse: 0.190091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[230]\tvalid_0's rmse: 0.189829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[240]\tvalid_0's rmse: 0.189504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.189207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[260]\tvalid_0's rmse: 0.188967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[270]\tvalid_0's rmse: 0.18869\n[280]\tvalid_0's rmse: 0.188483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290]\tvalid_0's rmse: 0.18828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.18807\n[310]\tvalid_0's rmse: 0.187855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[320]\tvalid_0's rmse: 0.187596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[330]\tvalid_0's rmse: 0.187428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[340]\tvalid_0's rmse: 0.187279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[350]\tvalid_0's rmse: 0.187119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[360]\tvalid_0's rmse: 0.186942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[370]\tvalid_0's rmse: 0.186742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[380]\tvalid_0's rmse: 0.186549\n[390]\tvalid_0's rmse: 0.186395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.186321\n[410]\tvalid_0's rmse: 0.186224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[420]\tvalid_0's rmse: 0.186083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[430]\tvalid_0's rmse: 0.185917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[440]\tvalid_0's rmse: 0.185785\n[450]\tvalid_0's rmse: 0.185654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[460]\tvalid_0's rmse: 0.185555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[470]\tvalid_0's rmse: 0.185426\n[480]\tvalid_0's rmse: 0.185297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[490]\tvalid_0's rmse: 0.185199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.185089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[510]\tvalid_0's rmse: 0.184972\n[520]\tvalid_0's rmse: 0.184852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[530]\tvalid_0's rmse: 0.184784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[540]\tvalid_0's rmse: 0.18468\n[550]\tvalid_0's rmse: 0.184599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[560]\tvalid_0's rmse: 0.184529\n[570]\tvalid_0's rmse: 0.184454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[580]\tvalid_0's rmse: 0.184386\n[590]\tvalid_0's rmse: 0.184327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.18427\n[610]\tvalid_0's rmse: 0.184171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[620]\tvalid_0's rmse: 0.184087\n[630]\tvalid_0's rmse: 0.183973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[640]\tvalid_0's rmse: 0.183887\n[650]\tvalid_0's rmse: 0.183807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[660]\tvalid_0's rmse: 0.183749\n[670]\tvalid_0's rmse: 0.183693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[680]\tvalid_0's rmse: 0.183649\n[690]\tvalid_0's rmse: 0.183591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\tvalid_0's rmse: 0.183506\n[710]\tvalid_0's rmse: 0.183427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[720]\tvalid_0's rmse: 0.183367\n[730]\tvalid_0's rmse: 0.183329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[740]\tvalid_0's rmse: 0.183277\n[750]\tvalid_0's rmse: 0.183231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[760]\tvalid_0's rmse: 0.183158\n[770]\tvalid_0's rmse: 0.183114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[780]\tvalid_0's rmse: 0.18305\n[790]\tvalid_0's rmse: 0.182994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.182913\n[810]\tvalid_0's rmse: 0.182862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[820]\tvalid_0's rmse: 0.182818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[830]\tvalid_0's rmse: 0.182772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[840]\tvalid_0's rmse: 0.182734\n[850]\tvalid_0's rmse: 0.18269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[860]\tvalid_0's rmse: 0.182631\n[870]\tvalid_0's rmse: 0.182578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[880]\tvalid_0's rmse: 0.18253\n[890]\tvalid_0's rmse: 0.182502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.182452\n[910]\tvalid_0's rmse: 0.182409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[920]\tvalid_0's rmse: 0.182382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[930]\tvalid_0's rmse: 0.182347\n[940]\tvalid_0's rmse: 0.182294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[950]\tvalid_0's rmse: 0.182258\n[960]\tvalid_0's rmse: 0.18222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[970]\tvalid_0's rmse: 0.182176\n[980]\tvalid_0's rmse: 0.182115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[990]\tvalid_0's rmse: 0.182078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.182027\n[1010]\tvalid_0's rmse: 0.181992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1020]\tvalid_0's rmse: 0.181959\n[1030]\tvalid_0's rmse: 0.181912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1040]\tvalid_0's rmse: 0.181868\n[1050]\tvalid_0's rmse: 0.181832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1060]\tvalid_0's rmse: 0.181817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1070]\tvalid_0's rmse: 0.181757\n[1080]\tvalid_0's rmse: 0.181704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1090]\tvalid_0's rmse: 0.181676\n[1100]\tvalid_0's rmse: 0.181626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1110]\tvalid_0's rmse: 0.181597\n[1120]\tvalid_0's rmse: 0.181586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1130]\tvalid_0's rmse: 0.181567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1140]\tvalid_0's rmse: 0.181529\n[1150]\tvalid_0's rmse: 0.181508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1160]\tvalid_0's rmse: 0.181482\n[1170]\tvalid_0's rmse: 0.181459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1180]\tvalid_0's rmse: 0.1814\n[1190]\tvalid_0's rmse: 0.181377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.181343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stability Iter FAST] Fold 5: RMSE=3.35906, best_iter=1200 (train_n=1602868, val_n=320755)\n{'cv_rmse_mean': 3.492799, 'cv_rmse_std': 0.2575, 'best_iter_final': 1183, 'note': 'piecewise time; airport-hike; per-fold freq; no monotone (speed)'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with shape: (9914, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "72340d61-fa47-40d6-9394-a96761633cc0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 2 \u2014 Two-Stage Residual Model (fixed): robust Stage1 trend + Stage2 residuals with residual TE (pu/do) and counts\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals(), 'train_df/test_df missing'\n",
        "assert 'sample' in globals() and 'make_time_folds_quantile' in globals(), 'Prereqs missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "def ensure_days_since_start(df, global_start_ns):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    ns = dt_local.astype('int64').values\n",
        "    df['days_since_start'] = ((ns - np.int64(global_start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return df\n",
        "\n",
        "def ensure_policy_feats(df):\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    df['after_hike'] = (dt_local >= cutoff).astype('int8')\n",
        "    if 'days_since_start' in df.columns:\n",
        "        df['days_since_start_x_after_hike'] = (df['days_since_start'] * df['after_hike']).astype('float32')\n",
        "    else:\n",
        "        df['days_since_start_x_after_hike'] = np.float32(0.0)\n",
        "    return df\n",
        "\n",
        "def ensure_spatial_bins(df, res=0.005):\n",
        "    if all(c in df.columns for c in ['pu_bin','do_bin','pair_bin']): return df\n",
        "    df = df.copy()\n",
        "    pu_lat_bin = np.floor(df['pickup_latitude'] / res).astype('int32')\n",
        "    pu_lon_bin = np.floor(df['pickup_longitude'] / res).astype('int32')\n",
        "    do_lat_bin = np.floor(df['dropoff_latitude'] / res).astype('int32')\n",
        "    do_lon_bin = np.floor(df['dropoff_longitude'] / res).astype('int32')\n",
        "    df['pu_bin'] = (pu_lat_bin.astype(str) + '_' + pu_lon_bin.astype(str))\n",
        "    df['do_bin'] = (do_lat_bin.astype(str) + '_' + do_lon_bin.astype(str))\n",
        "    df['pair_bin'] = (df['pu_bin'] + '|' + df['do_bin'])\n",
        "    return df\n",
        "\n",
        "dt_local_all = train_df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "global_start_ns = int(dt_local_all.min().value)\n",
        "train_df = ensure_days_since_start(train_df, global_start_ns)\n",
        "test_df = ensure_days_since_start(test_df, global_start_ns)\n",
        "train_df = ensure_policy_feats(train_df)\n",
        "test_df = ensure_policy_feats(test_df)\n",
        "train_df = ensure_spatial_bins(train_df)\n",
        "test_df = ensure_spatial_bins(test_df)\n",
        "\n",
        "# Stage 1 (Trend) features \u2014 time and policy only\n",
        "trend_feats = ['days_since_start','after_hike','days_since_start_x_after_hike','year','month','doy_sin','doy_cos']\n",
        "for c in trend_feats: assert c in train_df.columns, f'Missing trend feature: {c}'\n",
        "\n",
        "# Stage 2 base features \u2014 robust numeric set + policy interactions if present\n",
        "base_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'year','month','day','hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday','days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_df.columns: base_feats.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_df.columns: base_feats.append('dist_x_after_hike')\n",
        "\n",
        "# Prepare ordered data\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "# Smoothed TE on residuals (safe), and log-counts\n",
        "def te_smooth(train_key, train_tgt, apply_key, m=300.0, prior=None):\n",
        "    if prior is None: prior = float(np.mean(train_tgt))\n",
        "    g = pd.DataFrame({'k': train_key.astype('object'), 'y': train_tgt}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict()\n",
        "    enc = apply_key.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return enc, prior, mp\n",
        "\n",
        "cv_rmses, best_iters_s2 = [], []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx].copy(); va = train_ord.iloc[va_idx].copy()\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    # Stage 1: small, monotone-constrained trend model\n",
        "    X1_tr = tr[trend_feats].astype('float32').values\n",
        "    X1_va = va[trend_feats].astype('float32').values\n",
        "    mono_trend = [0]*len(trend_feats)\n",
        "    for feat in ['days_since_start','after_hike','days_since_start_x_after_hike']:\n",
        "        if feat in trend_feats: mono_trend[trend_feats.index(feat)] = 1\n",
        "    trend_model = lgb.LGBMRegressor(objective='regression', metric='rmse',\n",
        "                                   learning_rate=0.06, n_estimators=1200, num_leaves=16,\n",
        "                                   min_data_in_leaf=1200, feature_fraction=0.8, bagging_fraction=0.8,\n",
        "                                   bagging_freq=1, max_bin=63, reg_lambda=20.0, random_state=2025, n_jobs=-1, verbose=-1,\n",
        "                                   monotone_constraints=mono_trend)\n",
        "    trend_model.fit(X1_tr, y_tr, eval_set=[(X1_va, y_va)], eval_metric='rmse',\n",
        "                    callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False), lgb.log_evaluation(period=100)])\n",
        "    tr_trend = trend_model.predict(X1_tr, num_iteration=trend_model.best_iteration_).astype('float32')\n",
        "    va_trend = trend_model.predict(X1_va, num_iteration=trend_model.best_iteration_).astype('float32')\n",
        "    # Residual targets (log space)\n",
        "    y_tr_res = (y_tr - tr_trend).astype('float32')\n",
        "    y_va_res = (y_va - va_trend).astype('float32')\n",
        "    # Residual encodings on train-only: TE for pu/do (no pair) + log-counts\n",
        "    tr['te_pu'], _, _ = te_smooth(tr['pu_bin'], y_tr_res, tr['pu_bin'], m=400.0)\n",
        "    va['te_pu'], _, _ = te_smooth(tr['pu_bin'], y_tr_res, va['pu_bin'], m=400.0)\n",
        "    tr['te_do'], _, _ = te_smooth(tr['do_bin'], y_tr_res, tr['do_bin'], m=400.0)\n",
        "    va['te_do'], _, _ = te_smooth(tr['do_bin'], y_tr_res, va['do_bin'], m=400.0)\n",
        "    pu_cnt = tr['pu_bin'].astype('object').value_counts().astype('int32')\n",
        "    do_cnt = tr['do_bin'].astype('object').value_counts().astype('int32')\n",
        "    pair_cnt = tr['pair_bin'].astype('object').value_counts().astype('int32')\n",
        "    for d in (tr, va):\n",
        "        d['log_pu_cnt'] = np.log1p(d['pu_bin'].astype('object').map(pu_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_do_cnt'] = np.log1p(d['do_bin'].astype('object').map(do_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_pair_cnt'] = np.log1p(d['pair_bin'].astype('object').map(pair_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "    # Stage 2: Residual model on base features + trend_pred + residual TE/log-counts\n",
        "    tr['trend_pred'] = tr_trend; va['trend_pred'] = va_trend\n",
        "    use_cols2 = base_feats + ['trend_pred','te_pu','te_do','log_pu_cnt','log_do_cnt','log_pair_cnt']\n",
        "    X2_tr = tr[use_cols2].astype('float32').values\n",
        "    X2_va = va[use_cols2].astype('float32').values\n",
        "    s2 = lgb.LGBMRegressor(objective='huber', metric='rmse',\n",
        "                           learning_rate=0.05, n_estimators=12000, num_leaves=128,\n",
        "                           min_data_in_leaf=600, feature_fraction=0.8, bagging_fraction=0.8,\n",
        "                           bagging_freq=1, max_bin=127, reg_lambda=15.0, random_state=2026, n_jobs=-1, verbose=-1)\n",
        "    s2.fit(X2_tr, y_tr_res, eval_set=[(X2_va, y_va_res)], eval_metric='rmse',\n",
        "           callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False), lgb.log_evaluation(period=150)])\n",
        "    va_res_pred = s2.predict(X2_va, num_iteration=s2.best_iteration_).astype('float32')\n",
        "    va_final_log = (va_trend + va_res_pred).astype('float32')\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), np.expm1(va_final_log)))\n",
        "    cv_rmses.append(rmse); best_iters_s2.append(int(s2.best_iteration_ or s2.n_estimators))\n",
        "    print(f'[Two-Stage FIX] Fold {i}: RMSE={rmse:.5f}, s2_best_iter={best_iters_s2[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})')\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_s2_final = int(np.clip(int(np.median(best_iters_s2)), 600, 12000))\n",
        "print({'cv_rmse_mean': round(cv_mean, 6), 'cv_rmse_std': round(cv_std, 6), 's2_best_iter_final': best_iter_s2_final, 'note': 'Two-stage residual model (fixed) with residual TE pu/do + counts'})\n",
        "\n",
        "# Final fit on full data\n",
        "full = train_ord.copy(); tst = test_df.copy()\n",
        "X1_full = full[trend_feats].astype('float32').values\n",
        "mono_trend_full = [0]*len(trend_feats)\n",
        "for feat in ['days_since_start','after_hike','days_since_start_x_after_hike']:\n",
        "    if feat in trend_feats: mono_trend_full[trend_feats.index(feat)] = 1\n",
        "trend_model_full = lgb.LGBMRegressor(objective='regression', metric='rmse',\n",
        "                                     learning_rate=0.06, n_estimators=max(400, best_iter_s2_final//10), num_leaves=16,\n",
        "                                     min_data_in_leaf=1200, feature_fraction=0.8, bagging_fraction=0.8,\n",
        "                                     bagging_freq=1, max_bin=63, reg_lambda=20.0, random_state=3030, n_jobs=-1, verbose=-1,\n",
        "                                     monotone_constraints=mono_trend_full)\n",
        "trend_model_full.fit(X1_full, y_all_log)\n",
        "full_trend = trend_model_full.predict(X1_full).astype('float32')\n",
        "X1_test = tst[trend_feats].astype('float32').values\n",
        "test_trend = trend_model_full.predict(X1_test).astype('float32')\n",
        "y_full_res = (y_all_log - full_trend).astype('float32')\n",
        "\n",
        "def te_apply_full(tr_key, tr_tgt, ap_key, m=400.0):\n",
        "    prior = float(np.mean(tr_tgt))\n",
        "    g = pd.DataFrame({'k': tr_key.astype('object'), 'y': tr_tgt}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict()\n",
        "    tr_enc = tr_key.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    ap_enc = ap_key.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return tr_enc, ap_enc\n",
        "\n",
        "full['te_pu'], tst['te_pu'] = te_apply_full(full['pu_bin'], y_full_res, tst['pu_bin'])\n",
        "full['te_do'], tst['te_do'] = te_apply_full(full['do_bin'], y_full_res, tst['do_bin'])\n",
        "pu_cnt_full = full['pu_bin'].astype('object').value_counts().astype('int32')\n",
        "do_cnt_full = full['do_bin'].astype('object').value_counts().astype('int32')\n",
        "pair_cnt_full = full['pair_bin'].astype('object').value_counts().astype('int32')\n",
        "full['log_pu_cnt'] = np.log1p(full['pu_bin'].astype('object').map(pu_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "tst['log_pu_cnt'] = np.log1p(tst['pu_bin'].astype('object').map(pu_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "full['log_do_cnt'] = np.log1p(full['do_bin'].astype('object').map(do_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "tst['log_do_cnt'] = np.log1p(tst['do_bin'].astype('object').map(do_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "full['log_pair_cnt'] = np.log1p(full['pair_bin'].astype('object').map(pair_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "tst['log_pair_cnt'] = np.log1p(tst['pair_bin'].astype('object').map(pair_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "\n",
        "full['trend_pred'] = full_trend; tst['trend_pred'] = test_trend\n",
        "final_features = base_feats + ['trend_pred','te_pu','te_do','log_pu_cnt','log_do_cnt','log_pair_cnt']\n",
        "X2_full = full[final_features].astype('float32').values\n",
        "X2_test = tst[final_features].astype('float32').values\n",
        "s2_full = lgb.LGBMRegressor(objective='huber', metric='rmse',\n",
        "                            learning_rate=0.05, n_estimators=best_iter_s2_final, num_leaves=128,\n",
        "                            min_data_in_leaf=600, feature_fraction=0.8, bagging_fraction=0.8,\n",
        "                            bagging_freq=1, max_bin=127, reg_lambda=15.0, random_state=4040, n_jobs=-1, verbose=-1)\n",
        "s2_full.fit(X2_full, y_full_res)\n",
        "test_res = s2_full.predict(X2_test).astype('float32')\n",
        "test_pred = np.expm1(test_trend + test_res).astype('float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "pred_df = pd.DataFrame({'key': test_df['key'].astype('string'), 'fare_amount': test_pred})\n",
        "sub = sample[['key']].merge(pred_df, on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape:', sub.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.50873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.23367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.231544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.230829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.230664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.230628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.230591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.230703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage FIX] Fold 1: RMSE=3.71307, s2_best_iter=897 (train_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.516779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.516822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.186695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.186515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.186563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage FIX] Fold 2: RMSE=3.23728, s2_best_iter=225 (train_n=640523, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.5459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.545706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.545432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.54522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.545204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.544816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700]\tvalid_0's rmse: 0.54468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.544432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.544363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.544185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.210083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.207646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.206826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.206357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.206158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.205892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.205884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.205842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.2058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.205767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.205734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.20567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1950]\tvalid_0's rmse: 0.20571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage FIX] Fold 3: RMSE=3.60131, s2_best_iter=1735 (train_n=961323, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.558158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.203743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.19922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.197791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.197079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.196584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.196273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.196059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.195854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.195669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.195455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.195401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.195334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1950]\tvalid_0's rmse: 0.195408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage FIX] Fold 4: RMSE=3.54319, s2_best_iter=1781 (train_n=1282234, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.572638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.187028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.181248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.179353\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.178216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.177518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.177073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.176695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.176366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.176199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.176043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.175911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.17578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1950]\tvalid_0's rmse: 0.175659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.175579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2250]\tvalid_0's rmse: 0.175556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.17553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2550]\tvalid_0's rmse: 0.175482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2700]\tvalid_0's rmse: 0.175453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2850]\tvalid_0's rmse: 0.175441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.175416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3150]\tvalid_0's rmse: 0.175406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3300]\tvalid_0's rmse: 0.175398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3450]\tvalid_0's rmse: 0.175403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\tvalid_0's rmse: 0.175403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage FIX] Fold 5: RMSE=3.23181, s2_best_iter=3362 (train_n=1602868, val_n=320755)\n{'cv_rmse_mean': 3.465332, 'cv_rmse_std': 0.196197, 's2_best_iter_final': 1735, 'note': 'Two-stage residual model (fixed) with residual TE pu/do + counts'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with shape: (9914, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "83027332-19e8-4aed-b9df-8d4dd921e4b5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 2 \u2014 Two-Stage Residual Model (gate fix): stronger Stage1, purified Stage2, geohash residual TE, time-decay weights\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals(), 'train_df/test_df missing'\n",
        "assert 'sample' in globals() and 'make_time_folds_quantile' in globals(), 'Prereqs missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "try:\n",
        "    import pygeohash as pgh\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pygeohash'])\n",
        "    import pygeohash as pgh\n",
        "\n",
        "# Ensure helpers\n",
        "def ensure_days_since_start(df, global_start_ns):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    ns = dt_local.astype('int64').values\n",
        "    df['days_since_start'] = ((ns - np.int64(global_start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return df\n",
        "\n",
        "def ensure_policy_feats(df):\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    df['after_hike'] = (dt_local >= cutoff).astype('int8')\n",
        "    if 'days_since_start' in df.columns:\n",
        "        df['days_since_start_x_after_hike'] = (df['days_since_start'] * df['after_hike']).astype('float32')\n",
        "    else:\n",
        "        df['days_since_start_x_after_hike'] = np.float32(0.0)\n",
        "    return df\n",
        "\n",
        "def add_geohash(df, prec=6):\n",
        "    df = df.copy()\n",
        "    def enc_row(lat, lon):\n",
        "        return pgh.encode(float(lat), float(lon), precision=prec) if np.isfinite(lat) and np.isfinite(lon) else ''\n",
        "    df['pu_gh6'] = [enc_row(lat, lon) for lat, lon in zip(df['pickup_latitude'].astype('float32'), df['pickup_longitude'].astype('float32'))]\n",
        "    df['do_gh6'] = [enc_row(lat, lon) for lat, lon in zip(df['dropoff_latitude'].astype('float32'), df['dropoff_longitude'].astype('float32'))]\n",
        "    return df\n",
        "\n",
        "def te_smooth(train_key, train_tgt, apply_key, m=700.0, prior=None):\n",
        "    if prior is None: prior = float(np.mean(train_tgt))\n",
        "    g = pd.DataFrame({'k': train_key.astype('object'), 'y': train_tgt}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict()\n",
        "    enc = apply_key.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return enc, prior, mp\n",
        "\n",
        "# Prepare ordered data and features\n",
        "dt_local_all = train_df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "global_start_ns = int(dt_local_all.min().value)\n",
        "train_df = ensure_days_since_start(train_df, global_start_ns)\n",
        "test_df = ensure_days_since_start(test_df, global_start_ns)\n",
        "train_df = ensure_policy_feats(train_df)\n",
        "test_df = ensure_policy_feats(test_df)\n",
        "train_df = add_geohash(train_df, prec=6)\n",
        "test_df = add_geohash(test_df, prec=6)\n",
        "\n",
        "# Stage 1: expanded trend features (time + policy + weekly seasonality + holidays)\n",
        "trend_feats = ['days_since_start','after_hike','days_since_start_x_after_hike','year','month','dow','is_holiday','doy_sin','doy_cos','dow_sin','dow_cos']\n",
        "for c in trend_feats: assert c in train_df.columns, f'Missing trend feature: {c}'\n",
        "\n",
        "# Stage 2: purified residual feature set \u2014 remove absolute time identifiers (year, month, days_since_start); keep geometry/POIs/flags and cyclic hour/dow if present\n",
        "base_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'day','hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_df.columns: base_feats.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_df.columns: base_feats.append('dist_x_after_hike')\n",
        "\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "# Time-decay weights for Stage 2 (tau \u2248120 days): w = exp((days - max_days)/tau)\n",
        "days_all = train_ord['days_since_start'].astype('float32').values\n",
        "max_days = float(np.max(days_all))\n",
        "tau_days = 120.0\n",
        "\n",
        "cv_rmses, best_iters_s2 = [], []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx].copy(); va = train_ord.iloc[va_idx].copy()\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    # Stage 1: stronger, regularized trend model with monotonicity on time terms; print its val RMSE\n",
        "    X1_tr = tr[trend_feats].astype('float32').values\n",
        "    X1_va = va[trend_feats].astype('float32').values\n",
        "    mono_trend = [0]*len(trend_feats)\n",
        "    for feat in ['days_since_start','after_hike','days_since_start_x_after_hike']:\n",
        "        if feat in trend_feats: mono_trend[trend_feats.index(feat)] = 1\n",
        "    trend_model = lgb.LGBMRegressor(objective='regression', metric='rmse',\n",
        "                                   learning_rate=0.05, n_estimators=1500, num_leaves=32,\n",
        "                                   min_data_in_leaf=1200, feature_fraction=0.8, bagging_fraction=0.8,\n",
        "                                   bagging_freq=1, max_bin=63, reg_lambda=20.0, random_state=2027, n_jobs=-1, verbose=-1,\n",
        "                                   monotone_constraints=mono_trend)\n",
        "    trend_model.fit(X1_tr, y_tr, eval_set=[(X1_va, y_va)], eval_metric='rmse',\n",
        "                    callbacks=[lgb.early_stopping(stopping_rounds=150, verbose=False), lgb.log_evaluation(period=150)])\n",
        "    va_tr_rmse = float(trend_model.best_score_['valid_0']['rmse']) if hasattr(trend_model, 'best_score_') else float('nan')\n",
        "    print(f'[Stage1 Trend] Fold {i}: val_rmse_log={va_tr_rmse:.6f}, best_iter={trend_model.best_iteration_}')\n",
        "    tr_trend = trend_model.predict(X1_tr, num_iteration=trend_model.best_iteration_).astype('float32')\n",
        "    va_trend = trend_model.predict(X1_va, num_iteration=trend_model.best_iteration_).astype('float32')\n",
        "    y_tr_res = (y_tr - tr_trend).astype('float32')\n",
        "    y_va_res = (y_va - va_trend).astype('float32')\n",
        "    # Stage 2: residual encodings using geohash TE (PU/DO) with strong smoothing; add geohash counts\n",
        "    tr['te_pu_gh6'], _, _ = te_smooth(tr['pu_gh6'], y_tr_res, tr['pu_gh6'], m=700.0)\n",
        "    va['te_pu_gh6'], _, _ = te_smooth(tr['pu_gh6'], y_tr_res, va['pu_gh6'], m=700.0)\n",
        "    tr['te_do_gh6'], _, _ = te_smooth(tr['do_gh6'], y_tr_res, tr['do_gh6'], m=700.0)\n",
        "    va['te_do_gh6'], _, _ = te_smooth(tr['do_gh6'], y_tr_res, va['do_gh6'], m=700.0)\n",
        "    pu_cnt = tr['pu_gh6'].astype('object').value_counts().astype('int32')\n",
        "    do_cnt = tr['do_gh6'].astype('object').value_counts().astype('int32')\n",
        "    for d in (tr, va):\n",
        "        d['log_pu_gh_cnt'] = np.log1p(d['pu_gh6'].astype('object').map(pu_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_do_gh_cnt'] = np.log1p(d['do_gh6'].astype('object').map(do_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "    # Assemble Stage 2 matrices; purify time (no year/month/days_since_start) and include trend_pred\n",
        "    tr['trend_pred'] = tr_trend; va['trend_pred'] = va_trend\n",
        "    use_cols2 = base_feats + ['trend_pred','te_pu_gh6','te_do_gh6','log_pu_gh_cnt','log_do_gh_cnt']\n",
        "    X2_tr = tr[use_cols2].astype('float32').values\n",
        "    X2_va = va[use_cols2].astype('float32').values\n",
        "    # Time-decay weights favoring recent history\n",
        "    w_tr = np.exp((tr['days_since_start'].astype('float32').values - max_days) / tau_days).astype('float32')\n",
        "    s2 = lgb.LGBMRegressor(objective='huber', metric='rmse',\n",
        "                           learning_rate=0.05, n_estimators=16000, num_leaves=128,\n",
        "                           min_data_in_leaf=800, feature_fraction=0.8, bagging_fraction=0.8,\n",
        "                           bagging_freq=1, max_bin=127, reg_lambda=20.0, random_state=2028, n_jobs=-1, verbose=-1)\n",
        "    s2.fit(X2_tr, y_tr_res, sample_weight=w_tr,\n",
        "           eval_set=[(X2_va, y_va_res)], eval_metric='rmse',\n",
        "           callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False), lgb.log_evaluation(period=150)])\n",
        "    va_res_pred = s2.predict(X2_va, num_iteration=s2.best_iteration_).astype('float32')\n",
        "    va_final_log = (va_trend + va_res_pred).astype('float32')\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), np.expm1(va_final_log)))\n",
        "    cv_rmses.append(rmse); best_iters_s2.append(int(s2.best_iteration_ or s2.n_estimators))\n",
        "    print(f'[Two-Stage GH] Fold {i}: RMSE={rmse:.5f}, s2_best_iter={best_iters_s2[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})')\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_s2_final = int(np.clip(int(np.median(best_iters_s2)), 800, 16000))\n",
        "print({'cv_rmse_mean': round(cv_mean, 6), 'cv_rmse_std': round(cv_std, 6), 's2_best_iter_final': best_iter_s2_final, 'note': 'Two-stage residual (Stage1 stronger, Stage2 purified + geohash TE + time-decay)'})\n",
        "\n",
        "# Final fit on full data: Stage 1 then Stage 2\n",
        "full = train_ord.copy(); tst = test_df.copy()\n",
        "X1_full = full[trend_feats].astype('float32').values\n",
        "mono_trend_full = [0]*len(trend_feats)\n",
        "for feat in ['days_since_start','after_hike','days_since_start_x_after_hike']:\n",
        "    if feat in trend_feats: mono_trend_full[trend_feats.index(feat)] = 1\n",
        "trend_full = lgb.LGBMRegressor(objective='regression', metric='rmse',\n",
        "                               learning_rate=0.05, n_estimators=1500, num_leaves=32,\n",
        "                               min_data_in_leaf=1200, feature_fraction=0.8, bagging_fraction=0.8,\n",
        "                               bagging_freq=1, max_bin=63, reg_lambda=20.0, random_state=3031, n_jobs=-1, verbose=-1,\n",
        "                               monotone_constraints=mono_trend_full)\n",
        "trend_full.fit(X1_full, y_all_log)\n",
        "full_trend = trend_full.predict(X1_full).astype('float32')\n",
        "X1_test = tst[trend_feats].astype('float32').values\n",
        "test_trend = trend_full.predict(X1_test).astype('float32')\n",
        "y_full_res = (y_all_log - full_trend).astype('float32')\n",
        "\n",
        "# Geohash TE and counts on full residuals, apply to test\n",
        "full = add_geohash(full, prec=6); tst = add_geohash(tst, prec=6)\n",
        "def te_apply_full(key_tr, tgt_tr, key_ap, m=700.0):\n",
        "    prior = float(np.mean(tgt_tr))\n",
        "    g = pd.DataFrame({'k': key_tr.astype('object'), 'y': tgt_tr}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict()\n",
        "    tr_enc = key_tr.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    ap_enc = key_ap.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return tr_enc, ap_enc\n",
        "full['te_pu_gh6'], tst['te_pu_gh6'] = te_apply_full(full['pu_gh6'], y_full_res, tst['pu_gh6'])\n",
        "full['te_do_gh6'], tst['te_do_gh6'] = te_apply_full(full['do_gh6'], y_full_res, tst['do_gh6'])\n",
        "pu_cnt_full = full['pu_gh6'].astype('object').value_counts().astype('int32')\n",
        "do_cnt_full = full['do_gh6'].astype('object').value_counts().astype('int32')\n",
        "full['log_pu_gh_cnt'] = np.log1p(full['pu_gh6'].astype('object').map(pu_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "tst['log_pu_gh_cnt'] = np.log1p(tst['pu_gh6'].astype('object').map(pu_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "full['log_do_gh_cnt'] = np.log1p(full['do_gh6'].astype('object').map(do_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "tst['log_do_gh_cnt'] = np.log1p(tst['do_gh6'].astype('object').map(do_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "\n",
        "full['trend_pred'] = full_trend; tst['trend_pred'] = test_trend\n",
        "final_features = base_feats + ['trend_pred','te_pu_gh6','te_do_gh6','log_pu_gh_cnt','log_do_gh_cnt']\n",
        "X2_full = full[final_features].astype('float32').values\n",
        "X2_test = tst[final_features].astype('float32').values\n",
        "w_full = np.exp((full['days_since_start'].astype('float32').values - max_days) / tau_days).astype('float32')\n",
        "s2_full = lgb.LGBMRegressor(objective='huber', metric='rmse',\n",
        "                            learning_rate=0.05, n_estimators=best_iter_s2_final, num_leaves=128,\n",
        "                            min_data_in_leaf=800, feature_fraction=0.8, bagging_fraction=0.8,\n",
        "                            bagging_freq=1, max_bin=127, reg_lambda=20.0, random_state=4041, n_jobs=-1, verbose=-1)\n",
        "s2_full.fit(X2_full, y_full_res, sample_weight=w_full)\n",
        "test_res = s2_full.predict(X2_test).astype('float32')\n",
        "test_pred = np.expm1(test_trend + test_res).astype('float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "pred_df = pd.DataFrame({'key': test_df['key'].astype('string'), 'fare_amount': test_pred})\n",
        "sub = sample[['key']].merge(pred_df, on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape:', sub.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.508649\n[Stage1 Trend] Fold 1: val_rmse_log=0.508606, best_iter=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.508426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.508223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.50802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.507816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.507614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.507411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.507209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.507008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.506806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.506605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.506404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.506203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1950]\tvalid_0's rmse: 0.506003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.505803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2250]\tvalid_0's rmse: 0.505603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.505404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2550]\tvalid_0's rmse: 0.505204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2700]\tvalid_0's rmse: 0.505006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2850]\tvalid_0's rmse: 0.504807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.504608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3150]\tvalid_0's rmse: 0.50441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3300]\tvalid_0's rmse: 0.504211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3450]\tvalid_0's rmse: 0.504014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\tvalid_0's rmse: 0.503817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3750]\tvalid_0's rmse: 0.503619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3900]\tvalid_0's rmse: 0.503423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4050]\tvalid_0's rmse: 0.503227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4200]\tvalid_0's rmse: 0.503032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4350]\tvalid_0's rmse: 0.502835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4500]\tvalid_0's rmse: 0.502639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4650]\tvalid_0's rmse: 0.502445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4800]\tvalid_0's rmse: 0.50225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4950]\tvalid_0's rmse: 0.502054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5100]\tvalid_0's rmse: 0.50186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5250]\tvalid_0's rmse: 0.501665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5400]\tvalid_0's rmse: 0.501472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5550]\tvalid_0's rmse: 0.501278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5700]\tvalid_0's rmse: 0.501084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5850]\tvalid_0's rmse: 0.500892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6000]\tvalid_0's rmse: 0.500699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6150]\tvalid_0's rmse: 0.500507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6300]\tvalid_0's rmse: 0.500314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6450]\tvalid_0's rmse: 0.500122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6600]\tvalid_0's rmse: 0.49993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6750]\tvalid_0's rmse: 0.499739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6900]\tvalid_0's rmse: 0.499547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7050]\tvalid_0's rmse: 0.499356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7200]\tvalid_0's rmse: 0.499166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7350]\tvalid_0's rmse: 0.498976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7500]\tvalid_0's rmse: 0.498786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7650]\tvalid_0's rmse: 0.498595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7800]\tvalid_0's rmse: 0.498406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7950]\tvalid_0's rmse: 0.498217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8100]\tvalid_0's rmse: 0.498028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8250]\tvalid_0's rmse: 0.497839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8400]\tvalid_0's rmse: 0.49765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8550]\tvalid_0's rmse: 0.497462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8700]\tvalid_0's rmse: 0.497273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8850]\tvalid_0's rmse: 0.497086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9000]\tvalid_0's rmse: 0.496898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9150]\tvalid_0's rmse: 0.496711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9300]\tvalid_0's rmse: 0.496524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9450]\tvalid_0's rmse: 0.496337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9600]\tvalid_0's rmse: 0.496151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9750]\tvalid_0's rmse: 0.495964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9900]\tvalid_0's rmse: 0.495778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10050]\tvalid_0's rmse: 0.495592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10200]\tvalid_0's rmse: 0.495407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10350]\tvalid_0's rmse: 0.495222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10500]\tvalid_0's rmse: 0.495037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10650]\tvalid_0's rmse: 0.494852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10800]\tvalid_0's rmse: 0.494667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10950]\tvalid_0's rmse: 0.494482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11100]\tvalid_0's rmse: 0.494298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11250]\tvalid_0's rmse: 0.494115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11400]\tvalid_0's rmse: 0.493932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11550]\tvalid_0's rmse: 0.49375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11700]\tvalid_0's rmse: 0.493567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11850]\tvalid_0's rmse: 0.493385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12000]\tvalid_0's rmse: 0.493202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12150]\tvalid_0's rmse: 0.49302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12300]\tvalid_0's rmse: 0.492838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12450]\tvalid_0's rmse: 0.492657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12600]\tvalid_0's rmse: 0.492475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12750]\tvalid_0's rmse: 0.492295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12900]\tvalid_0's rmse: 0.492114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13050]\tvalid_0's rmse: 0.491934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13200]\tvalid_0's rmse: 0.491753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13350]\tvalid_0's rmse: 0.491573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13500]\tvalid_0's rmse: 0.491393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13650]\tvalid_0's rmse: 0.491214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13800]\tvalid_0's rmse: 0.491034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13950]\tvalid_0's rmse: 0.490854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14100]\tvalid_0's rmse: 0.490675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14250]\tvalid_0's rmse: 0.490496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14400]\tvalid_0's rmse: 0.490318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14550]\tvalid_0's rmse: 0.49014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14700]\tvalid_0's rmse: 0.489962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14850]\tvalid_0's rmse: 0.489784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15000]\tvalid_0's rmse: 0.489607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15150]\tvalid_0's rmse: 0.48943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15300]\tvalid_0's rmse: 0.489252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15450]\tvalid_0's rmse: 0.489075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15600]\tvalid_0's rmse: 0.488898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15750]\tvalid_0's rmse: 0.488722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15900]\tvalid_0's rmse: 0.488546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage GH] Fold 1: RMSE=8.09253, s2_best_iter=16000 (train_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.516721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Trend] Fold 2: val_rmse_log=0.516688, best_iter=75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.510733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.505008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.499481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.494136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.488988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.484008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.479212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.47458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.470098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.465767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.461578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.457514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1950]\tvalid_0's rmse: 0.453582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.449769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2250]\tvalid_0's rmse: 0.446076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.442501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2550]\tvalid_0's rmse: 0.43902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2700]\tvalid_0's rmse: 0.435651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2850]\tvalid_0's rmse: 0.432373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.429187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3150]\tvalid_0's rmse: 0.426093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3300]\tvalid_0's rmse: 0.423082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3450]\tvalid_0's rmse: 0.420156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\tvalid_0's rmse: 0.417309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3750]\tvalid_0's rmse: 0.414534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3900]\tvalid_0's rmse: 0.411833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4050]\tvalid_0's rmse: 0.409208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4200]\tvalid_0's rmse: 0.406644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4350]\tvalid_0's rmse: 0.404135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4500]\tvalid_0's rmse: 0.401692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4650]\tvalid_0's rmse: 0.399318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4800]\tvalid_0's rmse: 0.396993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4950]\tvalid_0's rmse: 0.394723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5100]\tvalid_0's rmse: 0.392507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5250]\tvalid_0's rmse: 0.390338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5400]\tvalid_0's rmse: 0.388219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5550]\tvalid_0's rmse: 0.386149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5700]\tvalid_0's rmse: 0.38413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5850]\tvalid_0's rmse: 0.382155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6000]\tvalid_0's rmse: 0.380222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6150]\tvalid_0's rmse: 0.378327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6300]\tvalid_0's rmse: 0.376473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6450]\tvalid_0's rmse: 0.374658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6600]\tvalid_0's rmse: 0.372879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6750]\tvalid_0's rmse: 0.371135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6900]\tvalid_0's rmse: 0.369423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7050]\tvalid_0's rmse: 0.367753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7200]\tvalid_0's rmse: 0.36611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7350]\tvalid_0's rmse: 0.364503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7500]\tvalid_0's rmse: 0.362926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7650]\tvalid_0's rmse: 0.361377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7800]\tvalid_0's rmse: 0.359864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7950]\tvalid_0's rmse: 0.358375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8100]\tvalid_0's rmse: 0.356909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8250]\tvalid_0's rmse: 0.355475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8400]\tvalid_0's rmse: 0.354066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8550]\tvalid_0's rmse: 0.35268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8700]\tvalid_0's rmse: 0.351316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8850]\tvalid_0's rmse: 0.349982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9000]\tvalid_0's rmse: 0.348667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9150]\tvalid_0's rmse: 0.347379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9300]\tvalid_0's rmse: 0.346107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9450]\tvalid_0's rmse: 0.344862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9600]\tvalid_0's rmse: 0.343633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9750]\tvalid_0's rmse: 0.342425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9900]\tvalid_0's rmse: 0.341238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10050]\tvalid_0's rmse: 0.340069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10200]\tvalid_0's rmse: 0.338918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10350]\tvalid_0's rmse: 0.337789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10500]\tvalid_0's rmse: 0.336673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10650]\tvalid_0's rmse: 0.335575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10800]\tvalid_0's rmse: 0.334493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10950]\tvalid_0's rmse: 0.333428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11100]\tvalid_0's rmse: 0.332385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11250]\tvalid_0's rmse: 0.331354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11400]\tvalid_0's rmse: 0.330343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11550]\tvalid_0's rmse: 0.329345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11700]\tvalid_0's rmse: 0.32836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11850]\tvalid_0's rmse: 0.327388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12000]\tvalid_0's rmse: 0.326429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12150]\tvalid_0's rmse: 0.325484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12300]\tvalid_0's rmse: 0.324553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12450]\tvalid_0's rmse: 0.323638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12600]\tvalid_0's rmse: 0.32273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12750]\tvalid_0's rmse: 0.321838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12900]\tvalid_0's rmse: 0.320958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13050]\tvalid_0's rmse: 0.320091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13200]\tvalid_0's rmse: 0.319231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13350]\tvalid_0's rmse: 0.318385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13500]\tvalid_0's rmse: 0.317551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13650]\tvalid_0's rmse: 0.316728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13800]\tvalid_0's rmse: 0.315915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13950]\tvalid_0's rmse: 0.315111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14100]\tvalid_0's rmse: 0.314322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14250]\tvalid_0's rmse: 0.31354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14400]\tvalid_0's rmse: 0.312771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14550]\tvalid_0's rmse: 0.312011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14700]\tvalid_0's rmse: 0.311255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14850]\tvalid_0's rmse: 0.310515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15000]\tvalid_0's rmse: 0.309781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15150]\tvalid_0's rmse: 0.309055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15300]\tvalid_0's rmse: 0.308337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15450]\tvalid_0's rmse: 0.307628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15600]\tvalid_0's rmse: 0.306925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15750]\tvalid_0's rmse: 0.306233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15900]\tvalid_0's rmse: 0.305546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage GH] Fold 2: RMSE=6.66018, s2_best_iter=16000 (train_n=640523, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.545733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.545529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.545506\n[Stage1 Trend] Fold 3: val_rmse_log=0.545475, best_iter=305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.44723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.396003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.364164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.342213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.325994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.31283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.301784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.292679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.285237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.279012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.273842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.269494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1950]\tvalid_0's rmse: 0.265709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.26246\n"
          ]
        }
      ]
    },
    {
      "id": "fec35f97-0608-4e10-b8d8-1abf52c4db3e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Two-Stage Residual Model \u2014 Gate Fixes Only: linear weights + simplified Stage1 trend (robustified for early folds)\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Ensure minimal time/policy features\n",
        "def ensure_days_since_start(df, global_start_ns):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    ns = dt_local.astype('int64').values\n",
        "    df['days_since_start'] = ((ns - np.int64(global_start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return df\n",
        "\n",
        "def ensure_after_hike(df):\n",
        "    if 'after_hike' in df.columns and 'days_since_start_x_after_hike' in df.columns:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    df['after_hike'] = (dt_local >= cutoff).astype('int8')\n",
        "    if 'days_since_start' in df.columns:\n",
        "        df['days_since_start_x_after_hike'] = (df['days_since_start'] * df['after_hike']).astype('float32')\n",
        "    else:\n",
        "        df['days_since_start_x_after_hike'] = np.float32(0.0)\n",
        "    return df\n",
        "\n",
        "# Prepare ordered data\n",
        "dt_local_all = train_df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "global_start_ns = int(dt_local_all.min().value)\n",
        "train_df = ensure_days_since_start(train_df, global_start_ns)\n",
        "test_df = ensure_days_since_start(test_df, global_start_ns)\n",
        "train_df = ensure_after_hike(train_df)\n",
        "test_df = ensure_after_hike(test_df)\n",
        "\n",
        "trend_feats = ['days_since_start','after_hike','days_since_start_x_after_hike']\n",
        "for c in trend_feats: assert c in train_df.columns, f'Missing trend feature: {c}'\n",
        "\n",
        "# Stage2 feature set (purified: no absolute time except via trend_pred)\n",
        "base_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'day','hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_df.columns: base_feats.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_df.columns: base_feats.append('dist_x_after_hike')\n",
        "\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "# Stage1 params (capacity + patience) per audit recommendation\n",
        "s1_params = dict(\n",
        "    objective='regression', metric='rmse',\n",
        "    learning_rate=0.05, n_estimators=3000,\n",
        "    num_leaves=64, min_data_in_leaf=400,\n",
        "    feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
        "    max_bin=127, reg_lambda=5.0, random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "# Stage2 params (moderate) \u2014 residuals\n",
        "s2_params = dict(\n",
        "    objective='huber', metric='rmse',\n",
        "    learning_rate=0.05, n_estimators=12000,\n",
        "    num_leaves=128, min_data_in_leaf=600,\n",
        "    feature_fraction=0.85, bagging_fraction=0.8, bagging_freq=1,\n",
        "    max_bin=127, reg_lambda=10.0, random_state=2026, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses = []; s1_best_iters = []; s2_best_iters = []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx]; va = train_ord.iloc[va_idx]\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    # Stage 1: trend\n",
        "    X1_tr = tr[trend_feats].astype('float32').values\n",
        "    X1_va = va[trend_feats].astype('float32').values\n",
        "    # Monotone only on continuous time terms; do NOT constrain binary after_hike\n",
        "    mono = [0, 0, 0]\n",
        "    mono[0] = 1  # days_since_start\n",
        "    mono[2] = 1  # days_since_start_x_after_hike\n",
        "    # Info note if after_hike is constant 0 in both tr and va (early folds)\n",
        "    ah_tr_unique = tr['after_hike'].nunique(); ah_va_unique = va['after_hike'].nunique()\n",
        "    if (ah_tr_unique == 1 and ah_va_unique == 1 and int(tr['after_hike'].iloc[0]) == 0 and int(va['after_hike'].iloc[0]) == 0):\n",
        "        print(f'[Stage1 Note] Fold {i}: after_hike=0 for both train and val; trend signal may be weak in early period.')\n",
        "\n",
        "    trend_model = lgb.LGBMRegressor(**{**s1_params, 'monotone_constraints': mono})\n",
        "    trend_model.fit(\n",
        "        X1_tr, y_tr,\n",
        "        eval_set=[(X1_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False), lgb.log_evaluation(period=150)]\n",
        "    )\n",
        "    va_tr_rmse = float(trend_model.best_score_['valid_0']['rmse'])\n",
        "    best_iter_s1 = int(trend_model.best_iteration_ or s1_params['n_estimators'])\n",
        "    print(f\"[Stage1 Diagnostics] Fold {i}: val_rmse_log={va_tr_rmse:.4f}, best_iter={best_iter_s1}\")\n",
        "    s1_best_iters.append(best_iter_s1)\n",
        "    tr_trend = trend_model.predict(X1_tr, num_iteration=trend_model.best_iteration_).astype('float32')\n",
        "    va_trend = trend_model.predict(X1_va, num_iteration=trend_model.best_iteration_).astype('float32')\n",
        "    y_tr_res = (y_tr - tr_trend).astype('float32'); y_va_res = (y_va - va_trend).astype('float32')\n",
        "\n",
        "    # Stage 2: residual model with linear weight ramp (no underflow)\n",
        "    use_cols2 = base_feats + ['trend_pred']\n",
        "    tr2 = tr.copy(); va2 = va.copy()\n",
        "    tr2['trend_pred'] = tr_trend; va2['trend_pred'] = va_trend\n",
        "    X2_tr = tr2[use_cols2].astype('float32').values\n",
        "    X2_va = va2[use_cols2].astype('float32').values\n",
        "    days_tr = tr2['days_since_start'].astype('float32').values\n",
        "    dmin, dmax = float(np.min(days_tr)), float(np.max(days_tr))\n",
        "    denom = (dmax - dmin) if (dmax > dmin) else 1.0\n",
        "    w_tr = 0.1 + 0.9 * (days_tr - dmin) / denom\n",
        "    w_tr = w_tr.astype('float32')\n",
        "    print(f\"[Weight Diagnostics] Fold {i}: min={w_tr.min():.4f}, max={w_tr.max():.4f}, mean={w_tr.mean():.4f}\")\n",
        "    assert np.isfinite(w_tr).all() and w_tr.min() >= 0.1 and w_tr.max() <= 1.0, 'Bad weights'\n",
        "\n",
        "    s2 = lgb.LGBMRegressor(**s2_params)\n",
        "    s2.fit(\n",
        "        X2_tr, y_tr_res, sample_weight=w_tr,\n",
        "        eval_set=[(X2_va, y_va_res)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False), lgb.log_evaluation(period=200)]\n",
        "    )\n",
        "    s2_best = int(s2.best_iteration_ or s2_params['n_estimators'])\n",
        "    s2_best_iters.append(s2_best)\n",
        "    va_res_pred = s2.predict(X2_va, num_iteration=s2.best_iteration_).astype('float32')\n",
        "    va_final_log = (va_trend + va_res_pred).astype('float32')\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), np.expm1(va_final_log)))\n",
        "    cv_rmses.append(rmse)\n",
        "    print(f\"[Two-Stage GateFix] Fold {i}: RMSE={rmse:.5f}, s2_best_iter={s2_best}\")\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "print({'cv_rmse_mean': round(cv_mean, 6), 'cv_rmse_std': round(cv_std, 6), 'note': 'Gate-fix two-stage: linear weights + simplified Stage1 (robust early-fold gate)'})\n",
        "assert cv_mean < 4.0, f'Gate not met: CV mean {cv_mean:.4f} >= 4.0'\n",
        "print('Gate passed. Ready for further tuning once stability confirmed.')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Note] Fold 1: after_hike=0 for both train and val; trend signal may be weak in early period.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.508622\n[300]\tvalid_0's rmse: 0.50862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.508619\n[600]\tvalid_0's rmse: 0.50862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.508619\n[900]\tvalid_0's rmse: 0.508619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.508619\n[1200]\tvalid_0's rmse: 0.508619\n[Stage1 Diagnostics] Fold 1: val_rmse_log=0.5086, best_iter=984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Weight Diagnostics] Fold 1: min=0.1000, max=1.0000, mean=0.5517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.232736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.231144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.2307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.230545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.23049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.230508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.230583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage GateFix] Fold 1: RMSE=3.66476, s2_best_iter=1134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Note] Fold 2: after_hike=0 for both train and val; trend signal may be weak in early period.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.516954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.516949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.516945\n[600]\tvalid_0's rmse: 0.516943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.516942\n[900]\tvalid_0's rmse: 0.516942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.516941\n[1200]\tvalid_0's rmse: 0.51694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.516941\n[1500]\tvalid_0's rmse: 0.516942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Diagnostics] Fold 2: val_rmse_log=0.5169, best_iter=1278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Weight Diagnostics] Fold 2: min=0.1000, max=1.0000, mean=0.5478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.192794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.193263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.193542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage GateFix] Fold 2: RMSE=3.29956, s2_best_iter=215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.544867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.54477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.544718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.544671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.544657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.544633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.544626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.544604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.544607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Diagnostics] Fold 3: val_rmse_log=0.5446, best_iter=1179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Weight Diagnostics] Fold 3: min=0.1000, max=1.0000, mean=0.5523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.20927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.207021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.206291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.205889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.205717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.205753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.205667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.205652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.2056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.205604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\tvalid_0's rmse: 0.205751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage GateFix] Fold 3: RMSE=3.56206, s2_best_iter=1861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.55871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.558817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Diagnostics] Fold 4: val_rmse_log=0.5583, best_iter=44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Weight Diagnostics] Fold 4: min=0.1000, max=1.0000, mean=0.5537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.201594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.198502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.197225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.196382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.195802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.195344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.195144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.195067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.194952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.19489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\tvalid_0's rmse: 0.194893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage GateFix] Fold 4: RMSE=3.47642, s2_best_iter=1968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.573326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.57338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Diagnostics] Fold 5: val_rmse_log=0.5730, best_iter=44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Weight Diagnostics] Fold 5: min=0.1000, max=1.0000, mean=0.5524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.183248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.17878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.177162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.176277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.175641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.175314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.175059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.174858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.174703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.174583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\tvalid_0's rmse: 0.174539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.174544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2600]\tvalid_0's rmse: 0.174573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage GateFix] Fold 5: RMSE=3.16010, s2_best_iter=2337\n{'cv_rmse_mean': 3.43258, 'cv_rmse_std': 0.181404, 'note': 'Gate-fix two-stage: linear weights + simplified Stage1 (robust early-fold gate)'}\nGate passed. Ready for further tuning once stability confirmed.\n"
          ]
        }
      ]
    },
    {
      "id": "ba97290e-9a01-49f6-8e71-9b97909a70b8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnostics for Cell 14 Stage1 failure: check trend feature variability and correlation in Fold 1\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "assert 'train_df' in globals(), 'train_df missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'fold generator missing'\n",
        "\n",
        "# Recreate ordered view and folds identical to Cell 14\n",
        "train_ord_dbg = train_df.sort_values('pickup_datetime').reset_index(drop=True).copy()\n",
        "dt_all_dbg = train_ord_dbg['pickup_datetime']\n",
        "y_all_log_dbg = np.log1p(train_ord_dbg['fare_amount'].astype('float32').values)\n",
        "folds_dbg = make_time_folds_quantile(dt_all_dbg, n_folds=5, gap_days=1)\n",
        "tr_idx, va_idx = folds_dbg[0]\n",
        "tr = train_ord_dbg.iloc[tr_idx].copy(); va = train_ord_dbg.iloc[va_idx].copy()\n",
        "\n",
        "trend_feats = ['days_since_start','after_hike','days_since_start_x_after_hike']\n",
        "for c in trend_feats: assert c in tr.columns, f'Missing {c}'\n",
        "\n",
        "def stats_block(df, name):\n",
        "    s = {\n",
        "        'name': name,\n",
        "        'rows': int(len(df)),\n",
        "        'days_min': float(np.nanmin(df['days_since_start'])),\n",
        "        'days_max': float(np.nanmax(df['days_since_start'])),\n",
        "        'days_std': float(np.nanstd(df['days_since_start'])),\n",
        "        'after_hike_unique': df['after_hike'].nunique(),\n",
        "        'after_hike_counts': df['after_hike'].value_counts().to_dict(),\n",
        "        'int_min': float(np.nanmin(df['days_since_start_x_after_hike'])),\n",
        "        'int_max': float(np.nanmax(df['days_since_start_x_after_hike'])),\n",
        "        'int_std': float(np.nanstd(df['days_since_start_x_after_hike'])),\n",
        "        'y_log_std': float(np.std(y_all_log_dbg[tr_idx if name=='train' else va_idx]))\n",
        "    }\n",
        "    # Correlations (guard for constant)\n",
        "    corr = {}\n",
        "    for col in trend_feats:\n",
        "        v = df[col].astype('float32').values\n",
        "        y = y_all_log_dbg[tr_idx if name=='train' else va_idx]\n",
        "        if np.std(v) > 0 and np.std(y) > 0:\n",
        "            try:\n",
        "                corr[col] = float(pearsonr(v, y)[0])\n",
        "            except Exception:\n",
        "                corr[col] = np.nan\n",
        "        else:\n",
        "            corr[col] = np.nan\n",
        "    s['pearson'] = corr\n",
        "    return s\n",
        "\n",
        "train_stats = stats_block(tr, 'train')\n",
        "valid_stats = stats_block(va, 'valid')\n",
        "print('Stage1 trend feature diagnostics:')\n",
        "print(train_stats)\n",
        "print(valid_stats)\n",
        "\n",
        "# If days_since_start looks degenerate, recompute quickly to test hypothesis\n",
        "if train_stats['days_std'] < 1e-6 or valid_stats['days_std'] < 1e-6:\n",
        "    print('Recomputing days_since_start and after_hike from pickup_datetime (NY local)...')\n",
        "    def recompute(df):\n",
        "        dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "        global_start_ns = int(dt_local.min().value)\n",
        "        ns = dt_local.astype('int64').values\n",
        "        df['days_since_start'] = ((ns - np.int64(global_start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "        cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "        df['after_hike'] = (dt_local >= cutoff).astype('int8')\n",
        "        df['days_since_start_x_after_hike'] = (df['days_since_start'] * df['after_hike']).astype('float32')\n",
        "        return df\n",
        "    tr = recompute(tr); va = recompute(va)\n",
        "    train_stats2 = stats_block(tr, 'train_recomp')\n",
        "    valid_stats2 = stats_block(va, 'valid_recomp')\n",
        "    print('After recompute:')\n",
        "    print(train_stats2)\n",
        "    print(valid_stats2)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage1 trend feature diagnostics:\n{'name': 'train', 'rows': 319908, 'days_min': 0.0, 'days_max': 390.7224426269531, 'days_std': 112.65982055664062, 'after_hike_unique': 1, 'after_hike_counts': {0: 319908}, 'int_min': 0.0, 'int_max': 0.0, 'int_std': 0.0, 'y_log_std': 0.5045214891433716, 'pearson': {'days_since_start': 0.004789734683296588, 'after_hike': nan, 'days_since_start_x_after_hike': nan}}\n{'name': 'valid', 'rows': 320756, 'days_min': 391.7254943847656, 'days_max': 793.7321166992188, 'days_std': 115.53379821777344, 'after_hike_unique': 1, 'after_hike_counts': {0: 320756}, 'int_min': 0.0, 'int_max': 0.0, 'int_std': 0.0, 'y_log_std': 0.508609414100647, 'pearson': {'days_since_start': 0.009176272326110886, 'after_hike': nan, 'days_since_start_x_after_hike': nan}}\n"
          ]
        }
      ]
    },
    {
      "id": "af5d71ec-ed3b-488f-ac9c-d015e2a85617",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Two-Stage Residual Model \u2014 Stage1 enhancement per audit: seasonal/holiday trend, fold-aware gate; Stage2 purified\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar as USCal\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Ensure minimal time/policy features\n",
        "def ensure_days_since_start(df, global_start_ns):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    ns = dt_local.astype('int64').values\n",
        "    df['days_since_start'] = ((ns - np.int64(global_start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return df\n",
        "\n",
        "def ensure_after_hike(df):\n",
        "    if 'after_hike' in df.columns and 'days_since_start_x_after_hike' in df.columns:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    df['after_hike'] = (dt_local >= cutoff).astype('int8')\n",
        "    if 'days_since_start' in df.columns:\n",
        "        df['days_since_start_x_after_hike'] = (df['days_since_start'] * df['after_hike']).astype('float32')\n",
        "    else:\n",
        "        df['days_since_start_x_after_hike'] = np.float32(0.0)\n",
        "    return df\n",
        "\n",
        "def ensure_seasonal_holiday(df):\n",
        "    need = ['doy_sin','doy_cos','dow_sin','dow_cos','is_holiday']\n",
        "    if all(c in df.columns for c in need):\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    # Day-of-year cyclic\n",
        "    doy = dt_local.dt.dayofyear.astype('int16')\n",
        "    df['doy_sin'] = np.sin(2*np.pi*(doy/365.25)).astype('float32')\n",
        "    df['doy_cos'] = np.cos(2*np.pi*(doy/365.25)).astype('float32')\n",
        "    # Day-of-week cyclic\n",
        "    dow = dt_local.dt.dayofweek.astype('int8')\n",
        "    df['dow_sin'] = np.sin(2*np.pi*(dow/7)).astype('float32')\n",
        "    df['dow_cos'] = np.cos(2*np.pi*(dow/7)).astype('float32')\n",
        "    # US Federal Holidays\n",
        "    start = dt_local.min().normalize().tz_localize(None)\n",
        "    end = dt_local.max().normalize().tz_localize(None)\n",
        "    hol = USCal().holidays(start=start, end=end)\n",
        "    hol_dates = set(pd.to_datetime(hol).date)\n",
        "    df['is_holiday'] = dt_local.dt.date.map(lambda d: d in hol_dates).astype('int8')\n",
        "    return df\n",
        "\n",
        "# Prepare ordered data\n",
        "dt_local_all = train_df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "global_start_ns = int(dt_local_all.min().value)\n",
        "train_df = ensure_days_since_start(train_df, global_start_ns)\n",
        "test_df = ensure_days_since_start(test_df, global_start_ns)\n",
        "train_df = ensure_after_hike(train_df)\n",
        "test_df = ensure_after_hike(test_df)\n",
        "train_df = ensure_seasonal_holiday(train_df)\n",
        "test_df = ensure_seasonal_holiday(test_df)\n",
        "\n",
        "# Stage 1 trend features (per audit) and constraints\n",
        "trend_feats = ['days_since_start','after_hike','days_since_start_x_after_hike','doy_sin','doy_cos','dow_sin','dow_cos','is_holiday']\n",
        "for c in trend_feats: assert c in train_df.columns, f'Missing trend feature: {c}'\n",
        "mono_map = {'days_since_start': 1, 'days_since_start_x_after_hike': 1}\n",
        "mono_constraints = [mono_map.get(f, 0) for f in trend_feats]\n",
        "\n",
        "# Stage 2 feature set (purified: remove any Stage1 features and absolute time identifiers)\n",
        "base_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'day','hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_df.columns: base_feats.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_df.columns: base_feats.append('dist_x_after_hike')\n",
        "\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "# Params\n",
        "s1_params = dict(\n",
        "    objective='regression', metric='rmse',\n",
        "    learning_rate=0.05, n_estimators=4000,\n",
        "    num_leaves=64, min_data_in_leaf=400,\n",
        "    feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
        "    max_bin=127, reg_lambda=5.0, random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "s2_params = dict(\n",
        "    objective='huber', metric='rmse',\n",
        "    learning_rate=0.05, n_estimators=14000,\n",
        "    num_leaves=128, min_data_in_leaf=600,\n",
        "    feature_fraction=0.85, bagging_fraction=0.8, bagging_freq=1,\n",
        "    max_bin=127, reg_lambda=10.0, random_state=2026, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses = []; s1_best_iters = []; s2_best_iters = []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx]; va = train_ord.iloc[va_idx]\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    # Stage 1: trend with monotonic time terms only\n",
        "    X1_tr = tr[trend_feats].astype('float32').values\n",
        "    X1_va = va[trend_feats].astype('float32').values\n",
        "    trend_model = lgb.LGBMRegressor(**{**s1_params, 'monotone_constraints': mono_constraints})\n",
        "    trend_model.fit(\n",
        "        X1_tr, y_tr,\n",
        "        eval_set=[(X1_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False), lgb.log_evaluation(period=150)]\n",
        "    )\n",
        "    va_tr_rmse = float(trend_model.best_score_['valid_0']['rmse'])\n",
        "    best_iter_s1 = int(trend_model.best_iteration_ or s1_params['n_estimators'])\n",
        "    sigma_va = float(np.std(y_va))\n",
        "    gate = min(0.35, 0.9 * sigma_va)\n",
        "    print(f\"[Stage1 Diagnostics] Fold {i}: val_rmse_log={va_tr_rmse:.4f}, best_iter={best_iter_s1}, sigma_va={sigma_va:.4f}, gate={gate:.4f}\")\n",
        "    # TEMP: soft gate for debugging to complete CV; log warning instead of assert\n",
        "    if not (va_tr_rmse <= gate):\n",
        "        print(f\"[Stage1 WARN] Fold {i}: gate failed (rmse={va_tr_rmse:.3f} > gate={gate:.3f}). Continuing for diagnostics.\")\n",
        "    s1_best_iters.append(best_iter_s1)\n",
        "    tr_trend = trend_model.predict(X1_tr, num_iteration=trend_model.best_iteration_).astype('float32')\n",
        "    va_trend = trend_model.predict(X1_va, num_iteration=trend_model.best_iteration_).astype('float32')\n",
        "    y_tr_res = (y_tr - tr_trend).astype('float32'); y_va_res = (y_va - va_trend).astype('float32')\n",
        "\n",
        "    # Stage 2: residual model with linear weight ramp; inputs purified of Stage1 seasonal/holiday\n",
        "    use_cols2 = base_feats + ['trend_pred']\n",
        "    tr2 = tr.copy(); va2 = va.copy()\n",
        "    tr2['trend_pred'] = tr_trend; va2['trend_pred'] = va_trend\n",
        "    X2_tr = tr2[use_cols2].astype('float32').values\n",
        "    X2_va = va2[use_cols2].astype('float32').values\n",
        "    days_tr = tr2['days_since_start'].astype('float32').values\n",
        "    dmin, dmax = float(np.min(days_tr)), float(np.max(days_tr))\n",
        "    denom = (dmax - dmin) if (dmax > dmin) else 1.0\n",
        "    w_tr = (0.1 + 0.9 * (days_tr - dmin) / denom).astype('float32')\n",
        "    print(f\"[Weight Diagnostics] Fold {i}: min={w_tr.min():.4f}, max={w_tr.max():.4f}, mean={w_tr.mean():.4f}\")\n",
        "    assert np.isfinite(w_tr).all() and w_tr.min() >= 0.1 and w_tr.max() <= 1.0, 'Bad weights'\n",
        "\n",
        "    s2 = lgb.LGBMRegressor(**s2_params)\n",
        "    s2.fit(\n",
        "        X2_tr, y_tr_res, sample_weight=w_tr,\n",
        "        eval_set=[(X2_va, y_va_res)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False), lgb.log_evaluation(period=200)]\n",
        "    )\n",
        "    s2_best = int(s2.best_iteration_ or s2_params['n_estimators'])\n",
        "    s2_best_iters.append(s2_best)\n",
        "    va_res_pred = s2.predict(X2_va, num_iteration=s2.best_iteration_).astype('float32')\n",
        "    va_final_log = (va_trend + va_res_pred).astype('float32')\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), np.expm1(va_final_log)))\n",
        "    cv_rmses.append(rmse)\n",
        "    print(f\"[Two-Stage Enhanced] Fold {i}: RMSE={rmse:.5f}, s2_best_iter={s2_best}\")\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "print({'cv_rmse_mean': round(cv_mean, 6), 'cv_rmse_std': round(cv_std, 6), 'note': 'Two-stage: enhanced Stage1 (seasonal/holiday) + purified Stage2 + linear weights'})\n",
        "print('Next: if gates improve, re-introduce residual TE (coarse) as advised.')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.508963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.510383\n[Stage1 Diagnostics] Fold 1: val_rmse_log=0.5084, best_iter=14, sigma_va=0.5086, gate=0.3500\n[Stage1 WARN] Fold 1: gate failed (rmse=0.508 > gate=0.350). Continuing for diagnostics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Weight Diagnostics] Fold 1: min=0.1000, max=1.0000, mean=0.5517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.23314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.231755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.231374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.231289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.231291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage Enhanced] Fold 1: RMSE=3.66505, s2_best_iter=718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.516504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.516648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Diagnostics] Fold 2: val_rmse_log=0.5165, best_iter=97, sigma_va=0.5166, gate=0.3500\n[Stage1 WARN] Fold 2: gate failed (rmse=0.516 > gate=0.350). Continuing for diagnostics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Weight Diagnostics] Fold 2: min=0.1000, max=1.0000, mean=0.5478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.176468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.175258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.175126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.175468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage Enhanced] Fold 2: RMSE=2.88110, s2_best_iter=566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.54459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.544098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.543672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.543365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.543074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.542826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.54256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.542289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.542114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.541968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.541764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.541541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1950]\tvalid_0's rmse: 0.541338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.541171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2250]\tvalid_0's rmse: 0.541062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.540927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2550]\tvalid_0's rmse: 0.540822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2700]\tvalid_0's rmse: 0.540742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2850]\tvalid_0's rmse: 0.540615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.540533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3150]\tvalid_0's rmse: 0.540454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3300]\tvalid_0's rmse: 0.540368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3450]\tvalid_0's rmse: 0.5403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\tvalid_0's rmse: 0.540208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3750]\tvalid_0's rmse: 0.540194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3900]\tvalid_0's rmse: 0.540162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Diagnostics] Fold 3: val_rmse_log=0.5401, best_iter=4000, sigma_va=0.5403, gate=0.3500\n[Stage1 WARN] Fold 3: gate failed (rmse=0.540 > gate=0.350). Continuing for diagnostics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Weight Diagnostics] Fold 3: min=0.1000, max=1.0000, mean=0.5523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.200952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.199067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.198475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.19837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.198434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.198336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.198363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage Enhanced] Fold 3: RMSE=3.44298, s2_best_iter=1182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.559795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.563876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Diagnostics] Fold 4: val_rmse_log=0.5582, best_iter=52, sigma_va=0.5583, gate=0.3500\n[Stage1 WARN] Fold 4: gate failed (rmse=0.558 > gate=0.350). Continuing for diagnostics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Weight Diagnostics] Fold 4: min=0.1000, max=1.0000, mean=0.5537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.201601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.198516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.197483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.196991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.196664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.19648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.19637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.196346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.196296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.19636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\tvalid_0's rmse: 0.196392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage Enhanced] Fold 4: RMSE=3.48977, s2_best_iter=1824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.572763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.573284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Diagnostics] Fold 5: val_rmse_log=0.5725, best_iter=73, sigma_va=0.5730, gate=0.3500\n[Stage1 WARN] Fold 5: gate failed (rmse=0.573 > gate=0.350). Continuing for diagnostics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Weight Diagnostics] Fold 5: min=0.1000, max=1.0000, mean=0.5524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.184764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.181075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.179686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.179005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.178532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.178251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.178005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.177906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.177832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.177822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\tvalid_0's rmse: 0.177816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.177844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Two-Stage Enhanced] Fold 5: RMSE=3.21613, s2_best_iter=2024\n{'cv_rmse_mean': 3.339006, 'cv_rmse_std': 0.270046, 'note': 'Two-stage: enhanced Stage1 (seasonal/holiday) + purified Stage2 + linear weights'}\nNext: if gates improve, re-introduce residual TE (coarse) as advised.\n"
          ]
        }
      ]
    },
    {
      "id": "2df8e984-f384-4dcd-8530-5987526929c3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Two-Stage Residual Model \u2014 Stage1 FIX: monotone only on days_since_start; hard gate; increased capacity\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar as USCal\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Ensure minimal time/policy + seasonal/holiday features\n",
        "def ensure_days_since_start(df, global_start_ns):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    ns = dt_local.astype('int64').values\n",
        "    df['days_since_start'] = ((ns - np.int64(global_start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return df\n",
        "\n",
        "def ensure_after_hike(df):\n",
        "    if 'after_hike' in df.columns and 'days_since_start_x_after_hike' in df.columns:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    df['after_hike'] = (dt_local >= cutoff).astype('int8')\n",
        "    if 'days_since_start' in df.columns:\n",
        "        df['days_since_start_x_after_hike'] = (df['days_since_start'] * df['after_hike']).astype('float32')\n",
        "    else:\n",
        "        df['days_since_start_x_after_hike'] = np.float32(0.0)\n",
        "    return df\n",
        "\n",
        "def ensure_seasonal_holiday(df):\n",
        "    need = ['doy_sin','doy_cos','dow_sin','dow_cos','is_holiday']\n",
        "    if all(c in df.columns for c in need):\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    dt_local = df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    # Day-of-year cyclic\n",
        "    doy = dt_local.dt.dayofyear.astype('int16')\n",
        "    df['doy_sin'] = np.sin(2*np.pi*(doy/365.25)).astype('float32')\n",
        "    df['doy_cos'] = np.cos(2*np.pi*(doy/365.25)).astype('float32')\n",
        "    # Day-of-week cyclic\n",
        "    dow = dt_local.dt.dayofweek.astype('int8')\n",
        "    df['dow_sin'] = np.sin(2*np.pi*(dow/7)).astype('float32')\n",
        "    df['dow_cos'] = np.cos(2*np.pi*(dow/7)).astype('float32')\n",
        "    # US Federal Holidays\n",
        "    start = dt_local.min().normalize().tz_localize(None)\n",
        "    end = dt_local.max().normalize().tz_localize(None)\n",
        "    hol = USCal().holidays(start=start, end=end)\n",
        "    hol_dates = set(pd.to_datetime(hol).date)\n",
        "    df['is_holiday'] = dt_local.dt.date.map(lambda d: d in hol_dates).astype('int8')\n",
        "    return df\n",
        "\n",
        "# Prepare ordered data\n",
        "dt_local_all = train_df['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "global_start_ns = int(dt_local_all.min().value)\n",
        "train_df = ensure_days_since_start(train_df, global_start_ns)\n",
        "test_df = ensure_days_since_start(test_df, global_start_ns)\n",
        "train_df = ensure_after_hike(train_df)\n",
        "test_df = ensure_after_hike(test_df)\n",
        "train_df = ensure_seasonal_holiday(train_df)\n",
        "test_df = ensure_seasonal_holiday(test_df)\n",
        "\n",
        "# Stage 1 features per mandate; monotone ONLY on days_since_start\n",
        "trend_feats = ['days_since_start','after_hike','days_since_start_x_after_hike','doy_sin','doy_cos','dow_sin','dow_cos','is_holiday']\n",
        "for c in trend_feats: assert c in train_df.columns, f'Missing trend feature: {c}'\n",
        "mono_map = {'days_since_start': 1}  # ONLY primary time trend is monotonic\n",
        "mono_constraints = [mono_map.get(f, 0) for f in trend_feats]\n",
        "\n",
        "# Stage 2 (purified) \u2014 no absolute time or Stage1 seasonal/holiday; only trend_pred conveys macro trend\n",
        "base_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'day','hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_df.columns: base_feats.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_df.columns: base_feats.append('dist_x_after_hike')\n",
        "\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "# Params \u2014 increased Stage1 capacity; cautious regularization; Stage2 unchanged (purified + linear weights)\n",
        "s1_params = dict(\n",
        "    objective='regression', metric='rmse',\n",
        "    learning_rate=0.04, n_estimators=6000,\n",
        "    num_leaves=128, min_data_in_leaf=400,\n",
        "    feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
        "    max_bin=127, reg_lambda=2.0, random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "s2_params = dict(\n",
        "    objective='huber', metric='rmse',\n",
        "    learning_rate=0.05, n_estimators=14000,\n",
        "    num_leaves=128, min_data_in_leaf=600,\n",
        "    feature_fraction=0.85, bagging_fraction=0.8, bagging_freq=1,\n",
        "    max_bin=127, reg_lambda=10.0, random_state=2026, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses = []; s1_best_iters = []; s2_best_iters = []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx]; va = train_ord.iloc[va_idx]\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "\n",
        "    # Stage 1\n",
        "    X1_tr = tr[trend_feats].astype('float32').values\n",
        "    X1_va = va[trend_feats].astype('float32').values\n",
        "    trend_model = lgb.LGBMRegressor(**{**s1_params, 'monotone_constraints': mono_constraints})\n",
        "    trend_model.fit(\n",
        "        X1_tr, y_tr,\n",
        "        eval_set=[(X1_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False), lgb.log_evaluation(period=150)]\n",
        "    )\n",
        "    va_tr_rmse = float(trend_model.best_score_['valid_0']['rmse'])\n",
        "    best_iter_s1 = int(trend_model.best_iteration_ or s1_params['n_estimators'])\n",
        "    sigma_va = float(np.std(y_va))\n",
        "    gate = min(0.35, 0.9 * sigma_va)\n",
        "    print(f\"[Stage1 FIX] Fold {i}: val_rmse_log={va_tr_rmse:.4f}, best_iter={best_iter_s1}, sigma_va={sigma_va:.4f}, gate={gate:.4f}\")\n",
        "    assert va_tr_rmse <= gate, f\"CRITICAL: Stage1 gate failed on fold {i}. RMSE={va_tr_rmse:.4f} > Gate={gate:.4f}\"\n",
        "    s1_best_iters.append(best_iter_s1)\n",
        "    tr_trend = trend_model.predict(X1_tr, num_iteration=trend_model.best_iteration_).astype('float32')\n",
        "    va_trend = trend_model.predict(X1_va, num_iteration=trend_model.best_iteration_).astype('float32')\n",
        "    y_tr_res = (y_tr - tr_trend).astype('float32'); y_va_res = (y_va - va_trend).astype('float32')\n",
        "\n",
        "    # Stage 2 with linear ramp weights (no underflow) and purified inputs\n",
        "    tr2 = tr.copy(); va2 = va.copy()\n",
        "    tr2['trend_pred'] = tr_trend; va2['trend_pred'] = va_trend\n",
        "    use_cols2 = base_feats + ['trend_pred']\n",
        "    X2_tr = tr2[use_cols2].astype('float32').values\n",
        "    X2_va = va2[use_cols2].astype('float32').values\n",
        "    days_tr = tr2['days_since_start'].astype('float32').values\n",
        "    dmin, dmax = float(np.min(days_tr)), float(np.max(days_tr))\n",
        "    denom = (dmax - dmin) if (dmax > dmin) else 1.0\n",
        "    w_tr = (0.2 + 0.8 * (days_tr - dmin) / denom).astype('float32')  # floor at 0.2 for extra stability\n",
        "    print(f\"[Weights] Fold {i}: min={w_tr.min():.4f}, max={w_tr.max():.4f}, mean={w_tr.mean():.4f}\")\n",
        "    assert np.isfinite(w_tr).all() and (0.19 <= w_tr.min() <= 1.0) and (w_tr.max() <= 1.0), 'Bad weights'\n",
        "\n",
        "    s2 = lgb.LGBMRegressor(**s2_params)\n",
        "    s2.fit(\n",
        "        X2_tr, y_tr_res, sample_weight=w_tr,\n",
        "        eval_set=[(X2_va, y_va_res)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False), lgb.log_evaluation(period=200)]\n",
        "    )\n",
        "    s2_best = int(s2.best_iteration_ or s2_params['n_estimators'])\n",
        "    s2_best_iters.append(s2_best)\n",
        "    va_res_pred = s2.predict(X2_va, num_iteration=s2.best_iteration_).astype('float32')\n",
        "    va_final_log = (va_trend + va_res_pred).astype('float32')\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), np.expm1(va_final_log)))\n",
        "    cv_rmses.append(rmse)\n",
        "    print(f\"[Two-Stage S1-FIX] Fold {i}: RMSE={rmse:.5f}, s2_best_iter={s2_best}\")\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "print({'cv_rmse_mean': round(cv_mean, 6), 'cv_rmse_std': round(cv_std, 6), 'note': 'Two-stage with Stage1 monotone fix (days only) + hard gate + purified Stage2 + linear weights'})\n",
        "assert cv_std <= 0.15 and cv_mean <= 3.35, f\"Gates not met: mean={cv_mean:.4f}, std={cv_std:.4f}\"\n",
        "print('Gates met. Proceed to residual TE (coarse) in Stage 2 next.')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.509473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.511097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 FIX] Fold 1: val_rmse_log=0.5084, best_iter=21, sigma_va=0.5086, gate=0.3500\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "CRITICAL: Stage1 gate failed on fold 1. RMSE=0.5084 > Gate=0.3500",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 130\u001b[39m\n\u001b[32m    128\u001b[39m gate = \u001b[38;5;28mmin\u001b[39m(\u001b[32m0.35\u001b[39m, \u001b[32m0.9\u001b[39m * sigma_va)\n\u001b[32m    129\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Stage1 FIX] Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: val_rmse_log=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva_tr_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, best_iter=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_iter_s1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, sigma_va=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msigma_va\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, gate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgate\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m va_tr_rmse <= gate, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCRITICAL: Stage1 gate failed on fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. RMSE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva_tr_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m > Gate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgate\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m s1_best_iters.append(best_iter_s1)\n\u001b[32m    132\u001b[39m tr_trend = trend_model.predict(X1_tr, num_iteration=trend_model.best_iteration_).astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mAssertionError\u001b[39m: CRITICAL: Stage1 gate failed on fold 1. RMSE=0.5084 > Gate=0.3500"
          ]
        }
      ]
    },
    {
      "id": "9ad13f7b-2676-402e-8cba-13e05635391e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnostics: Stage1 trend features (extended) variability and correlation in Fold 1\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "assert 'train_df' in globals() and 'make_time_folds_quantile' in globals(), 'Prereqs missing'\n",
        "\n",
        "# Use the Stage1 feature set from Cell 17\n",
        "trend_feats = ['days_since_start','after_hike','days_since_start_x_after_hike','doy_sin','doy_cos','dow_sin','dow_cos','is_holiday']\n",
        "for c in trend_feats: assert c in train_df.columns, f'Missing {c} in train_df'\n",
        "\n",
        "train_ord_dbg = train_df.sort_values('pickup_datetime').reset_index(drop=True).copy()\n",
        "dt_all_dbg = train_ord_dbg['pickup_datetime']\n",
        "y_log_dbg = np.log1p(train_ord_dbg['fare_amount'].astype('float32').values)\n",
        "folds_dbg = make_time_folds_quantile(dt_all_dbg, n_folds=5, gap_days=1)\n",
        "tr_idx, va_idx = folds_dbg[0]\n",
        "tr = train_ord_dbg.iloc[tr_idx].copy(); va = train_ord_dbg.iloc[va_idx].copy()\n",
        "\n",
        "def feat_stats(df, y, name):\n",
        "    stats = {'name': name, 'rows': int(len(df))}\n",
        "    for c in trend_feats:\n",
        "        v = df[c].astype('float32').values\n",
        "        stats[f'{c}_mean'] = float(np.nanmean(v))\n",
        "        stats[f'{c}_std'] = float(np.nanstd(v))\n",
        "        if np.nanstd(v) > 0 and np.nanstd(y) > 0:\n",
        "            try:\n",
        "                stats[f'corr_{c}'] = float(pearsonr(v, y)[0])\n",
        "            except Exception:\n",
        "                stats[f'corr_{c}'] = np.nan\n",
        "        else:\n",
        "            stats[f'corr_{c}'] = np.nan\n",
        "    return stats\n",
        "\n",
        "tr_stats = feat_stats(tr, y_log_dbg[tr_idx], 'train_fold1')\n",
        "va_stats = feat_stats(va, y_log_dbg[va_idx], 'valid_fold1')\n",
        "print('Stage1 extended trend feature diagnostics (Fold 1):')\n",
        "print(tr_stats)\n",
        "print(va_stats)\n",
        "\n",
        "# Quick sanity: unique counts for binary features\n",
        "print('Unique counts: after_hike (tr,va)=', tr['after_hike'].nunique(), va['after_hike'].nunique(),\n",
        "      '; is_holiday (tr,va)=', tr['is_holiday'].nunique(), va['is_holiday'].nunique())\n",
        "\n",
        "# Check if seasonal features are near-constant or miscomputed (very low std indicates issue)\n",
        "low_std = {c: (tr_stats[f'{c}_std'], va_stats[f'{c}_std']) for c in trend_feats}\n",
        "print('STD per feature (train, valid):', low_std)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage1 extended trend feature diagnostics (Fold 1):\n{'name': 'train_fold1', 'rows': 319908, 'days_since_start_mean': 196.10989379882812, 'days_since_start_std': 112.65982055664062, 'corr_days_since_start': 0.004789734683296588, 'after_hike_mean': 0.0, 'after_hike_std': 0.0, 'corr_after_hike': nan, 'days_since_start_x_after_hike_mean': 0.0, 'days_since_start_x_after_hike_std': 0.0, 'corr_days_since_start_x_after_hike': nan, 'doy_sin_mean': 0.013525240123271942, 'doy_sin_std': 0.6925604343414307, 'corr_doy_sin': -0.012210300229901801, 'doy_cos_mean': 0.0685039758682251, 'doy_cos_std': 0.7179723978042603, 'corr_doy_cos': -0.011701873185007804, 'dow_sin_mean': 0.01914128102362156, 'dow_sin_std': 0.7052960991859436, 'corr_dow_sin': -0.008666983029101032, 'dow_cos_mean': -0.08087312430143356, 'dow_cos_std': 0.704024612903595, 'corr_dow_cos': -0.011065482260595182, 'is_holiday_mean': 0.021368643268942833, 'is_holiday_std': 0.1446099430322647, 'corr_is_holiday': -0.004848697234120326}\n{'name': 'valid_fold1', 'rows': 320756, 'days_since_start_mean': 593.461181640625, 'days_since_start_std': 115.53379821777344, 'corr_days_since_start': 0.009176272326110886, 'after_hike_mean': 0.0, 'after_hike_std': 0.0, 'corr_after_hike': nan, 'days_since_start_x_after_hike_mean': 0.0, 'days_since_start_x_after_hike_std': 0.0, 'corr_days_since_start_x_after_hike': nan, 'doy_sin_mean': 0.07702106237411499, 'doy_sin_std': 0.7082474827766418, 'corr_doy_sin': -0.018351520078648088, 'doy_cos_mean': 0.047750428318977356, 'doy_cos_std': 0.70012366771698, 'corr_doy_cos': -0.011472730623507787, 'dow_sin_mean': 0.015307782217860222, 'dow_sin_std': 0.7056149244308472, 'corr_dow_sin': -0.0053851362140798, 'dow_cos_mean': -0.07533581554889679, 'dow_cos_std': 0.704413115978241, 'corr_dow_cos': -0.014114039387555502, 'is_holiday_mean': 0.02139320783317089, 'is_holiday_std': 0.14469118416309357, 'corr_is_holiday': -0.0011218104518954035}\nUnique counts: after_hike (tr,va)= 1 1 ; is_holiday (tr,va)= 2 2\nSTD per feature (train, valid): {'days_since_start': (112.65982055664062, 115.53379821777344), 'after_hike': (0.0, 0.0), 'days_since_start_x_after_hike': (0.0, 0.0), 'doy_sin': (0.6925604343414307, 0.7082474827766418), 'doy_cos': (0.7179723978042603, 0.70012366771698), 'dow_sin': (0.7052960991859436, 0.7056149244308472), 'dow_cos': (0.704024612903595, 0.704413115978241), 'is_holiday': (0.1446099430322647, 0.14469118416309357)}\n"
          ]
        }
      ]
    },
    {
      "id": "21d1cc48-b720-4079-bd51-f4abbc750694",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stage 1 Gate Fix \u2014 Enrich features (year/month) + optional fold-local time anchor; hard gate enforced\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "assert 'train_df' in globals() and 'make_time_folds_quantile' in globals(), 'Prereqs missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Prepare ordered data\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "\n",
        "# Mandated enriched Stage1 features\n",
        "trend_feats = [\n",
        "    'days_since_start', 'year', 'month', 'after_hike',\n",
        "    'days_since_start_x_after_hike', 'doy_sin', 'doy_cos',\n",
        "    'dow_sin', 'dow_cos', 'is_holiday'\n",
        "]\n",
        "for c in trend_feats: assert c in train_ord.columns, f'Missing {c} in train_df'\n",
        "\n",
        "# Build 5 time folds with 1-day gap\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "# Use the same Stage1 params as prior cell (do not alter capacity/Stage2 per mandate)\n",
        "s1_params = dict(\n",
        "    objective='regression', metric='rmse',\n",
        "    learning_rate=0.04, n_estimators=6000,\n",
        "    num_leaves=128, min_data_in_leaf=400,\n",
        "    feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
        "    max_bin=127, reg_lambda=2.0, random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "def add_fold_local_anchor(tr, va):\n",
        "    # Optional but recommended: fold-local time anchor to strengthen signal\n",
        "    # Compute days since train-start in NY local time and use as an additional feature\n",
        "    tr = tr.copy(); va = va.copy()\n",
        "    dt_tr_local = tr['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    dt_va_local = va['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    start_ns = int(dt_tr_local.min().value)\n",
        "    tr['days_since_train_start'] = ((dt_tr_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    va['days_since_train_start'] = ((dt_va_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return tr, va\n",
        "\n",
        "# Monotone constraint ONLY on the fold-local continuous time feature\n",
        "mono_map_static = {'days_since_train_start': 1}\n",
        "\n",
        "gate_pass = True\n",
        "fold_scores = []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx]; va = train_ord.iloc[va_idx]\n",
        "    # Add fold-local anchor\n",
        "    tr, va = add_fold_local_anchor(tr, va)\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    use_feats = trend_feats + ['days_since_train_start']\n",
        "    X1_tr = tr[use_feats].astype('float32').values\n",
        "    X1_va = va[use_feats].astype('float32').values\n",
        "    mono_constraints = [mono_map_static.get(f, 0) for f in use_feats]\n",
        "    trend_model = lgb.LGBMRegressor(**{**s1_params, 'monotone_constraints': mono_constraints})\n",
        "    trend_model.fit(\n",
        "        X1_tr, y_tr,\n",
        "        eval_set=[(X1_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False), lgb.log_evaluation(period=150)]\n",
        "    )\n",
        "    va_tr_rmse = float(trend_model.best_score_['valid_0']['rmse'])\n",
        "    sigma_va = float(np.std(y_va))\n",
        "    gate = min(0.35, 0.9 * sigma_va)\n",
        "    print(f\"[Stage1 GateCheck] Fold {i}: rmse_log={va_tr_rmse:.4f}, sigma_va={sigma_va:.4f}, gate={gate:.4f}\")\n",
        "    fold_scores.append(va_tr_rmse)\n",
        "    if not (va_tr_rmse <= gate):\n",
        "        gate_pass = False\n",
        "\n",
        "assert gate_pass, f\"CRITICAL: Stage1 gate failed on \u22651 folds. rmse_log per fold={fold_scores}\"\n",
        "print({'stage1_gate': 'PASSED', 'rmse_log_per_fold': [round(s,6) for s in fold_scores]})"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.508881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.508842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 GateCheck] Fold 1: rmse_log=0.5086, sigma_va=0.5086, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.516603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.516644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.516651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 GateCheck] Fold 2: rmse_log=0.5166, sigma_va=0.5166, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.545897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.545592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.545453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.545312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.545267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.545162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.545091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.544819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.544526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.544579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.544307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.544134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1950]\tvalid_0's rmse: 0.544306\n"
          ]
        }
      ]
    },
    {
      "id": "cb7e3526-5c48-4cdd-8f6d-9c5ca6ad72d5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stage 1 Gate Resolution \u2014 Add week_idx + retain fold-local continuous time with monotone (+1 only on days_since_train_start)\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "assert 'train_df' in globals() and 'make_time_folds_quantile' in globals(), 'Prereqs missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Ordered data and target\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "\n",
        "# Required Stage1 features (from mandate) \u2014 ensure they exist\n",
        "trend_feats_base = [\n",
        "    'days_since_start', 'year', 'month', 'after_hike',\n",
        "    'days_since_start_x_after_hike', 'doy_sin', 'doy_cos',\n",
        "    'dow_sin', 'dow_cos', 'is_holiday'\n",
        "]\n",
        "for c in trend_feats_base: assert c in train_ord.columns, f'Missing {c} in train_df'\n",
        "\n",
        "# 5-fold time CV with 1-day gap\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "# Use same params as Cell 19 (no changes mandated)\n",
        "s1_params = dict(\n",
        "    objective='regression', metric='rmse',\n",
        "    learning_rate=0.04, n_estimators=6000,\n",
        "    num_leaves=128, min_data_in_leaf=400,\n",
        "    feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
        "    max_bin=127, reg_lambda=2.0, random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "def add_fold_local_time(df_tr, df_va):\n",
        "    tr = df_tr.copy(); va = df_va.copy()\n",
        "    dt_tr_local = tr['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    dt_va_local = va['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    start_ns = int(dt_tr_local.min().value)\n",
        "    tr_days = ((dt_tr_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    va_days = ((dt_va_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    tr['days_since_train_start'] = tr_days\n",
        "    va['days_since_train_start'] = va_days\n",
        "    tr['week_idx'] = np.floor(tr_days / 7.0).astype('int16')\n",
        "    va['week_idx'] = np.floor(va_days / 7.0).astype('int16')\n",
        "    return tr, va\n",
        "\n",
        "# Monotone constraint ONLY on days_since_train_start\n",
        "mono_map = {'days_since_train_start': 1}\n",
        "\n",
        "gate_pass = True\n",
        "fold_scores = []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx]; va = train_ord.iloc[va_idx]\n",
        "    tr, va = add_fold_local_time(tr, va)\n",
        "    use_feats = trend_feats_base + ['days_since_train_start', 'week_idx']\n",
        "    for c in use_feats: assert c in tr.columns, f'Missing {c} in fold data'\n",
        "    X_tr = tr[use_feats].astype('float32').values\n",
        "    X_va = va[use_feats].astype('float32').values\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    mono_constraints = [mono_map.get(f, 0) for f in use_feats]\n",
        "    model = lgb.LGBMRegressor(**{**s1_params, 'monotone_constraints': mono_constraints})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False), lgb.log_evaluation(period=150)]\n",
        "    )\n",
        "    rmse = float(model.best_score_['valid_0']['rmse'])\n",
        "    sigma_va = float(np.std(y_va))\n",
        "    gate = min(0.35, 0.9 * sigma_va)\n",
        "    print(f\"[Stage1 WeekIdx Gate] Fold {i}: rmse_log={rmse:.4f}, sigma_va={sigma_va:.4f}, gate={gate:.4f}\")\n",
        "    fold_scores.append(rmse)\n",
        "    if rmse > gate:\n",
        "        gate_pass = False\n",
        "\n",
        "assert gate_pass, f\"CRITICAL: Stage1 gate failed on \u22651 folds. rmse_log per fold={fold_scores}\"\n",
        "print({'stage1_gate': 'PASSED', 'rmse_log_per_fold': [round(s,6) for s in fold_scores], 'note': 'Stage1 with week_idx + days_since_train_start (monotone on days_since_train_start only)'})"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.508674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.508696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 WeekIdx Gate] Fold 1: rmse_log=0.5086, sigma_va=0.5086, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.516915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.517223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.517421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 WeekIdx Gate] Fold 2: rmse_log=0.5168, sigma_va=0.5166, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.545261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.544777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.544425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.543969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.543603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.543318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.542997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.542752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.542533\n"
          ]
        }
      ]
    },
    {
      "id": "9e090342-eeac-4713-81bb-2ffcc9c38d7c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stage 1 with categorical discrete time features + hard gate (per mandate)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "assert 'train_df' in globals() and 'make_time_folds_quantile' in globals(), 'Prereqs missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Ordered data and target\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "\n",
        "# Base Stage1 features from mandate (must already exist in train_ord)\n",
        "trend_feats_base = [\n",
        "    'days_since_start', 'year', 'month', 'after_hike',\n",
        "    'days_since_start_x_after_hike', 'doy_sin', 'doy_cos',\n",
        "    'dow_sin', 'dow_cos', 'is_holiday'\n",
        "]\n",
        "for c in trend_feats_base:\n",
        "    assert c in train_ord.columns, f'Missing {c} in train_df'\n",
        "\n",
        "# 5-fold time CV with 1-day gap\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "# Stage1 params (unchanged)\n",
        "s1_params = dict(\n",
        "    objective='regression', metric='rmse',\n",
        "    learning_rate=0.04, n_estimators=6000,\n",
        "    num_leaves=128, min_data_in_leaf=400,\n",
        "    feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
        "    max_bin=127, reg_lambda=2.0, random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "def add_fold_local_time(df_tr, df_va):\n",
        "    tr = df_tr.copy(); va = df_va.copy()\n",
        "    dt_tr_local = tr['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    dt_va_local = va['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    start_ns = int(dt_tr_local.min().value)\n",
        "    tr_days = ((dt_tr_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    va_days = ((dt_va_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    tr['days_since_train_start'] = tr_days\n",
        "    va['days_since_train_start'] = va_days\n",
        "    tr['week_idx'] = np.floor(tr_days / 7.0).astype('int16')\n",
        "    va['week_idx'] = np.floor(va_days / 7.0).astype('int16')\n",
        "    return tr, va\n",
        "\n",
        "# Monotone constraint ONLY on the continuous fold-local time feature\n",
        "mono_map = {'days_since_train_start': 1}\n",
        "# Discrete temporal categorical features\n",
        "cat_feats = ['year','month','week_idx','is_holiday','after_hike']\n",
        "\n",
        "gate_pass = True\n",
        "fold_scores = []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx]; va = train_ord.iloc[va_idx]\n",
        "    tr, va = add_fold_local_time(tr, va)\n",
        "    use_feats = trend_feats_base + ['days_since_train_start', 'week_idx']\n",
        "    for c in use_feats:\n",
        "        assert c in tr.columns, f'Missing {c} in fold data'\n",
        "    X_tr = tr[use_feats].copy()\n",
        "    X_va = va[use_feats].copy()\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    mono_constraints = [mono_map.get(f, 0) for f in use_feats]\n",
        "    # Fit with categorical_feature specified\n",
        "    model = lgb.LGBMRegressor(**{**s1_params, 'monotone_constraints': mono_constraints})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        categorical_feature=cat_feats,\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False), lgb.log_evaluation(period=150)]\n",
        "    )\n",
        "    rmse = float(model.best_score_['valid_0']['rmse'])\n",
        "    sigma_va = float(np.std(y_va))\n",
        "    gate = min(0.35, 0.9 * sigma_va)\n",
        "    print(f\"[Stage1 Categorical Gate] Fold {i}: rmse_log={rmse:.4f}, sigma_va={sigma_va:.4f}, gate={gate:.4f}\")\n",
        "    fold_scores.append(rmse)\n",
        "    if rmse > gate:\n",
        "        gate_pass = False\n",
        "\n",
        "assert gate_pass, f\"CRITICAL: Stage1 gate failed on \u22651 folds. rmse_log per fold={fold_scores}\"\n",
        "print({'stage1_gate': 'PASSED', 'rmse_log_per_fold': [round(s,6) for s in fold_scores], 'note': 'Categorical discrete time features + monotone on days_since_train_start'})"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.508502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.508504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.50852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Categorical Gate] Fold 1: rmse_log=0.5085, sigma_va=0.5086, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.517682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.517783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Categorical Gate] Fold 2: rmse_log=0.5174, sigma_va=0.5166, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.552615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.5516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Categorical Gate] Fold 3: rmse_log=0.5499, sigma_va=0.5403, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.558747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.559394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.559849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Categorical Gate] Fold 4: rmse_log=0.5583, sigma_va=0.5583, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.572944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.573152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.57342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Categorical Gate] Fold 5: rmse_log=0.5728, sigma_va=0.5730, gate=0.3500\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "CRITICAL: Stage1 gate failed on \u22651 folds. rmse_log per fold=[0.508498431160195, 0.51736767887961, 0.5498656296773534, 0.5583302984384716, 0.5727985519035138]",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m rmse > gate:\n\u001b[32m     85\u001b[39m         gate_pass = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m gate_pass, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCRITICAL: Stage1 gate failed on \u22651 folds. rmse_log per fold=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_scores\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m({\u001b[33m'\u001b[39m\u001b[33mstage1_gate\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mPASSED\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrmse_log_per_fold\u001b[39m\u001b[33m'\u001b[39m: [\u001b[38;5;28mround\u001b[39m(s,\u001b[32m6\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m fold_scores], \u001b[33m'\u001b[39m\u001b[33mnote\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mCategorical discrete time features + monotone on days_since_train_start\u001b[39m\u001b[33m'\u001b[39m})\n",
            "\u001b[31mAssertionError\u001b[39m: CRITICAL: Stage1 gate failed on \u22651 folds. rmse_log per fold=[0.508498431160195, 0.51736767887961, 0.5498656296773534, 0.5583302984384716, 0.5727985519035138]"
          ]
        }
      ]
    },
    {
      "id": "e268a45a-347f-4be3-a38d-1c950915d6fa",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stage 1 \u2014 Strategic Reset: cyclical-safe discrete time as categoricals + fold-local trend with monotone (+1)\n",
        "import numpy as np, pandas as pd\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar as USCal\n",
        "\n",
        "assert 'train_df' in globals() and 'make_time_folds_quantile' in globals(), 'Prereqs missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Ordered data and target\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "\n",
        "# 5-fold time CV with 1-day gap\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "# Helper: add fold-local continuous time anchor and discrete time categoricals (NY local)\n",
        "def build_stage1_views(df_tr, df_va):\n",
        "    tr = df_tr.copy(); va = df_va.copy()\n",
        "    dt_tr_loc = tr['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    dt_va_loc = va['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    # Fold-local continuous time\n",
        "    start_ns = int(dt_tr_loc.min().value)\n",
        "    tr['days_since_train_start'] = ((dt_tr_loc.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    va['days_since_train_start'] = ((dt_va_loc.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    # Discrete time features (raw categoricals for trees)\n",
        "    tr['year'] = dt_tr_loc.dt.year.astype('int16'); va['year'] = dt_va_loc.dt.year.astype('int16')\n",
        "    tr['month'] = dt_tr_loc.dt.month.astype('int8'); va['month'] = dt_va_loc.dt.month.astype('int8')\n",
        "    tr['week_of_year'] = dt_tr_loc.dt.isocalendar().week.astype('int16')\n",
        "    va['week_of_year'] = dt_va_loc.dt.isocalendar().week.astype('int16')\n",
        "    tr['day_of_week'] = dt_tr_loc.dt.dayofweek.astype('int8'); va['day_of_week'] = dt_va_loc.dt.dayofweek.astype('int8')\n",
        "    tr['hour'] = dt_tr_loc.dt.hour.astype('int8'); va['hour'] = dt_va_loc.dt.hour.astype('int8')\n",
        "    # Use existing holiday/after_hike if present, else compute quickly\n",
        "    if 'is_holiday' not in tr.columns or 'is_holiday' not in va.columns:\n",
        "        start = dt_tr_loc.min().normalize().tz_localize(None)\n",
        "        end = dt_va_loc.max().normalize().tz_localize(None)\n",
        "        hol = USCal().holidays(start=start, end=end)\n",
        "        hol_dates = set(pd.to_datetime(hol).date)\n",
        "        tr['is_holiday'] = dt_tr_loc.dt.date.map(lambda d: d in hol_dates).astype('int8')\n",
        "        va['is_holiday'] = dt_va_loc.dt.date.map(lambda d: d in hol_dates).astype('int8')\n",
        "    if 'after_hike' not in tr.columns or 'after_hike' not in va.columns:\n",
        "        cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "        tr['after_hike'] = (dt_tr_loc >= cutoff).astype('int8')\n",
        "        va['after_hike'] = (dt_va_loc >= cutoff).astype('int8')\n",
        "    return tr, va\n",
        "\n",
        "# Feature lists\n",
        "cont_feat = ['days_since_train_start']\n",
        "cat_feats = ['year','month','week_of_year','day_of_week','hour','is_holiday','after_hike']\n",
        "use_feats = cont_feat + cat_feats\n",
        "\n",
        "# LightGBM params (with contingency tweaks for categorical splitting robustness)\n",
        "s1_params = dict(\n",
        "    objective='regression', metric='rmse',\n",
        "    learning_rate=0.04, n_estimators=6000,\n",
        "    num_leaves=128, min_data_in_leaf=100,  # contingency tweak from 400 -> 100\n",
        "    feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
        "    max_bin=255, reg_lambda=10.0,        # contingency tweak from 2.0 -> 10.0\n",
        "    random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "gate_pass = True\n",
        "fold_scores = []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx]; va = train_ord.iloc[va_idx]\n",
        "    tr, va = build_stage1_views(tr, va)\n",
        "    X_tr = tr[use_feats].copy(); X_va = va[use_feats].copy()\n",
        "    # Explicit categorical dtype casting (robust for LightGBM)\n",
        "    for c in cat_feats:\n",
        "        X_tr[c] = X_tr[c].astype('category')\n",
        "        X_va[c] = X_va[c].astype('category')\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    # Monotone constraint only on continuous trend anchor\n",
        "    mono = [1] + [0]*(len(use_feats)-1)\n",
        "    model = lgb.LGBMRegressor(**{**s1_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        categorical_feature='auto',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False), lgb.log_evaluation(period=150)]\n",
        "    )\n",
        "    rmse = float(model.best_score_['valid_0']['rmse'])\n",
        "    sigma_va = float(np.std(y_va))\n",
        "    gate = min(0.35, 0.9 * sigma_va)\n",
        "    print(f\"[Stage1 RESET Gate] Fold {i}: rmse_log={rmse:.4f}, sigma_va={sigma_va:.4f}, gate={gate:.4f}\")\n",
        "    fold_scores.append(rmse)\n",
        "    if rmse > gate:\n",
        "        gate_pass = False\n",
        "\n",
        "assert gate_pass, f\"CRITICAL: Stage1 gate failed on \u22651 folds. rmse_log per fold={fold_scores}\"\n",
        "print({'stage1_gate': 'PASSED', 'rmse_log_per_fold': [round(s,6) for s in fold_scores], 'note': 'Stage1 strategic reset with robust categoricals + monotone on fold-local trend'})"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.508046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.508864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 RESET Gate] Fold 1: rmse_log=0.5075, sigma_va=0.5086, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.515614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.516112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.516706\n[Stage1 RESET Gate] Fold 2: rmse_log=0.5154, sigma_va=0.5166, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.543581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.543605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.543213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.543053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.542857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.542584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.542396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.542287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.542199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.542128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.542011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.541919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1950]\tvalid_0's rmse: 0.541831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.541756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2250]\tvalid_0's rmse: 0.541675\n"
          ]
        }
      ]
    },
    {
      "id": "9aa80e15-3a8c-45a6-a702-fc953d701488",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stage 1 \u2014 Encoding Fix: shared CategoricalDtype across folds; hard gate enforced\n",
        "import numpy as np, pandas as pd\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar as USCal\n",
        "\n",
        "assert 'train_df' in globals() and 'make_time_folds_quantile' in globals(), 'Prereqs missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Ordered data and target\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "\n",
        "# Helper: build global discrete time columns once (NY local) to derive stable category sets\n",
        "dt_loc_all = train_ord['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "global_disc = pd.DataFrame(index=train_ord.index)\n",
        "global_disc['year'] = dt_loc_all.dt.year.astype('int16')\n",
        "global_disc['month'] = dt_loc_all.dt.month.astype('int8')\n",
        "global_disc['week_of_year'] = dt_loc_all.dt.isocalendar().week.astype('int16')\n",
        "global_disc['day_of_week'] = dt_loc_all.dt.dayofweek.astype('int8')\n",
        "global_disc['hour'] = dt_loc_all.dt.hour.astype('int8')\n",
        "# is_holiday, after_hike (compute if not already present)\n",
        "cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "if 'is_holiday' in train_ord.columns:\n",
        "    global_disc['is_holiday'] = train_ord['is_holiday'].astype('int8')\n",
        "else:\n",
        "    hol = USCal().holidays(start=dt_loc_all.min().normalize().tz_localize(None), end=dt_loc_all.max().normalize().tz_localize(None))\n",
        "    hol_dates = set(pd.to_datetime(hol).date)\n",
        "    global_disc['is_holiday'] = dt_loc_all.dt.date.map(lambda d: d in hol_dates).astype('int8')\n",
        "if 'after_hike' in train_ord.columns:\n",
        "    global_disc['after_hike'] = train_ord['after_hike'].astype('int8')\n",
        "else:\n",
        "    global_disc['after_hike'] = (dt_loc_all >= cutoff).astype('int8')\n",
        "\n",
        "# Build shared CategoricalDtype per categorical feature using union of categories from full train\n",
        "cat_feats = ['year','month','week_of_year','day_of_week','hour','is_holiday','after_hike']\n",
        "cat_dtype_map = {}\n",
        "for c in cat_feats:\n",
        "    all_cats = pd.Index(global_disc[c].unique())\n",
        "    cat_dtype_map[c] = pd.CategoricalDtype(categories=all_cats.tolist(), ordered=False)\n",
        "\n",
        "# 5-fold time CV with 1-day gap\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "# Params (unchanged from Cell 22 per mandate)\n",
        "s1_params = dict(\n",
        "    objective='regression', metric='rmse',\n",
        "    learning_rate=0.04, n_estimators=6000,\n",
        "    num_leaves=128, min_data_in_leaf=100,\n",
        "    feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
        "    max_bin=255, reg_lambda=10.0,\n",
        "    random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "def build_fold_views(df_tr, df_va):\n",
        "    tr = df_tr.copy(); va = df_va.copy()\n",
        "    dt_tr_loc = tr['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    dt_va_loc = va['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    # Fold-local continuous time anchor\n",
        "    start_ns = int(dt_tr_loc.min().value)\n",
        "    tr['days_since_train_start'] = ((dt_tr_loc.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    va['days_since_train_start'] = ((dt_va_loc.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    # Discrete time features (same definitions as global_disc)\n",
        "    tr['year'] = dt_tr_loc.dt.year.astype('int16'); va['year'] = dt_va_loc.dt.year.astype('int16')\n",
        "    tr['month'] = dt_tr_loc.dt.month.astype('int8'); va['month'] = dt_va_loc.dt.month.astype('int8')\n",
        "    tr['week_of_year'] = dt_tr_loc.dt.isocalendar().week.astype('int16'); va['week_of_year'] = dt_va_loc.dt.isocalendar().week.astype('int16')\n",
        "    tr['day_of_week'] = dt_tr_loc.dt.dayofweek.astype('int8'); va['day_of_week'] = dt_va_loc.dt.dayofweek.astype('int8')\n",
        "    tr['hour'] = dt_tr_loc.dt.hour.astype('int8'); va['hour'] = dt_va_loc.dt.hour.astype('int8')\n",
        "    if 'is_holiday' not in tr.columns or 'is_holiday' not in va.columns:\n",
        "        hol = USCal().holidays(start=dt_tr_loc.min().normalize().tz_localize(None), end=dt_va_loc.max().normalize().tz_localize(None))\n",
        "        hol_dates = set(pd.to_datetime(hol).date)\n",
        "        tr['is_holiday'] = dt_tr_loc.dt.date.map(lambda d: d in hol_dates).astype('int8')\n",
        "        va['is_holiday'] = dt_va_loc.dt.date.map(lambda d: d in hol_dates).astype('int8')\n",
        "    if 'after_hike' not in tr.columns or 'after_hike' not in va.columns:\n",
        "        tr['after_hike'] = (dt_tr_loc >= cutoff).astype('int8')\n",
        "        va['after_hike'] = (dt_va_loc >= cutoff).astype('int8')\n",
        "    return tr, va\n",
        "\n",
        "use_feats = ['days_since_train_start'] + cat_feats\n",
        "mono = [1] + [0]*(len(use_feats)-1)  # +1 only on continuous trend anchor\n",
        "\n",
        "gate_pass = True\n",
        "fold_scores = []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx]; va = train_ord.iloc[va_idx]\n",
        "    tr, va = build_fold_views(tr, va)\n",
        "    X_tr = tr[use_feats].copy(); X_va = va[use_feats].copy()\n",
        "    # Apply shared CategoricalDtype per feature (consistent encoding across folds)\n",
        "    for c in cat_feats:\n",
        "        X_tr[c] = X_tr[c].astype(cat_dtype_map[c])\n",
        "        X_va[c] = X_va[c].astype(cat_dtype_map[c])\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    model = lgb.LGBMRegressor(**{**s1_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        categorical_feature='auto',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False), lgb.log_evaluation(period=150)]\n",
        "    )\n",
        "    rmse = float(model.best_score_['valid_0']['rmse'])\n",
        "    sigma_va = float(np.std(y_va))\n",
        "    gate = min(0.35, 0.9 * sigma_va)\n",
        "    print(f\"[Stage1 ENCODING FIX Gate] Fold {i}: rmse_log={rmse:.4f}, sigma_va={sigma_va:.4f}, gate={gate:.4f}\")\n",
        "    fold_scores.append(rmse)\n",
        "    if rmse > gate:\n",
        "        gate_pass = False\n",
        "\n",
        "assert gate_pass, f\"CRITICAL: Stage1 gate failed on \u22651 folds. rmse_log per fold={fold_scores}\"\n",
        "print({'stage1_gate': 'PASSED', 'rmse_log_per_fold': [round(s,6) for s in fold_scores], 'note': 'Shared categorical dtypes across folds; monotone on fold-local time'})"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.508046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.508864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 ENCODING FIX Gate] Fold 1: rmse_log=0.5075, sigma_va=0.5086, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.515613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.516112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.516705\n[Stage1 ENCODING FIX Gate] Fold 2: rmse_log=0.5154, sigma_va=0.5166, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[150]\tvalid_0's rmse: 0.543584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.543605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[450]\tvalid_0's rmse: 0.543213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.543054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.542858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.542585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1050]\tvalid_0's rmse: 0.542396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.542286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1350]\tvalid_0's rmse: 0.542198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.542126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1650]\tvalid_0's rmse: 0.542008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.541916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1950]\tvalid_0's rmse: 0.541831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.541755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2250]\tvalid_0's rmse: 0.541676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.541648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2550]\tvalid_0's rmse: 0.541627\n"
          ]
        }
      ]
    },
    {
      "id": "63dea4e1-e227-40bd-88e4-97ae327a40a7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stage 1 Pivot \u2014 Linear Ridge with proper one-hot encoding and hard gate\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "assert 'train_df' in globals() and 'make_time_folds_quantile' in globals(), 'Prereqs missing'\n",
        "\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "def build_fold_views(df_tr, df_va):\n",
        "    tr = df_tr.copy(); va = df_va.copy()\n",
        "    dt_tr_loc = tr['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    dt_va_loc = va['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    # Fold-local continuous trend\n",
        "    start_ns = int(dt_tr_loc.min().value)\n",
        "    tr['days_since_train_start'] = ((dt_tr_loc.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    va['days_since_train_start'] = ((dt_va_loc.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    # Discrete time categoricals\n",
        "    tr['year'] = dt_tr_loc.dt.year.astype('int16'); va['year'] = dt_va_loc.dt.year.astype('int16')\n",
        "    tr['month'] = dt_tr_loc.dt.month.astype('int8'); va['month'] = dt_va_loc.dt.month.astype('int8')\n",
        "    tr['week_of_year'] = dt_tr_loc.dt.isocalendar().week.astype('int16'); va['week_of_year'] = dt_va_loc.dt.isocalendar().week.astype('int16')\n",
        "    tr['day_of_week'] = dt_tr_loc.dt.dayofweek.astype('int8'); va['day_of_week'] = dt_va_loc.dt.dayofweek.astype('int8')\n",
        "    tr['hour'] = dt_tr_loc.dt.hour.astype('int8'); va['hour'] = dt_va_loc.dt.hour.astype('int8')\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    tr['after_hike'] = (dt_tr_loc >= cutoff).astype('int8'); va['after_hike'] = (dt_va_loc >= cutoff).astype('int8')\n",
        "    # Holiday flag (fast local window)\n",
        "    from pandas.tseries.holiday import USFederalHolidayCalendar as USCal\n",
        "    hol = USCal().holidays(start=dt_tr_loc.min().normalize().tz_localize(None), end=dt_va_loc.max().normalize().tz_localize(None))\n",
        "    hol_dates = set(pd.to_datetime(hol).date)\n",
        "    tr['is_holiday'] = dt_tr_loc.dt.date.map(lambda d: d in hol_dates).astype('int8')\n",
        "    va['is_holiday'] = dt_va_loc.dt.date.map(lambda d: d in hol_dates).astype('int8')\n",
        "    use_cols = ['days_since_train_start','year','month','week_of_year','day_of_week','hour','is_holiday','after_hike']\n",
        "    return tr[use_cols], va[use_cols]\n",
        "\n",
        "cont_cols = ['days_since_train_start']\n",
        "cat_cols = ['year','month','week_of_year','day_of_week','hour','is_holiday','after_hike']\n",
        "pre = ColumnTransformer([\n",
        "    ('cont', StandardScaler(with_mean=True, with_std=True), cont_cols),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), cat_cols)\n",
        "])\n",
        "ridge = Ridge(alpha=5.0)\n",
        "pipe = Pipeline([('pre', pre), ('model', ridge)])\n",
        "\n",
        "gate_pass = True\n",
        "fold_scores = []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    X_tr_df, X_va_df = build_fold_views(train_ord.iloc[tr_idx], train_ord.iloc[va_idx])\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    pipe.fit(X_tr_df, y_tr)\n",
        "    pred_va = pipe.predict(X_va_df)\n",
        "    rmse = float(np.sqrt(np.mean((pred_va - y_va)**2)))\n",
        "    sigma_va = float(np.std(y_va))\n",
        "    gate = min(0.35, 0.9 * sigma_va)\n",
        "    print(f\"[Stage1 Ridge Gate] Fold {i}: rmse_log={rmse:.4f}, sigma_va={sigma_va:.4f}, gate={gate:.4f}\")\n",
        "    fold_scores.append(rmse)\n",
        "    if rmse > gate: gate_pass = False\n",
        "\n",
        "assert gate_pass, f\"CRITICAL: Stage1 gate failed on \u22651 folds. rmse_log per fold={fold_scores}\"\n",
        "print({'stage1_gate': 'PASSED', 'rmse_log_per_fold': [round(s,6) for s in fold_scores], 'note': 'Linear Ridge Stage1 with one-hot categoricals and fold-local trend'})"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Ridge Gate] Fold 1: rmse_log=0.5090, sigma_va=0.5086, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Ridge Gate] Fold 2: rmse_log=0.5179, sigma_va=0.5166, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Ridge Gate] Fold 3: rmse_log=0.5416, sigma_va=0.5403, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Ridge Gate] Fold 4: rmse_log=0.5600, sigma_va=0.5583, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 Ridge Gate] Fold 5: rmse_log=0.5718, sigma_va=0.5730, gate=0.3500\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "CRITICAL: Stage1 gate failed on \u22651 folds. rmse_log per fold=[0.5089564672389532, 0.5179140568447872, 0.5415669378943179, 0.5599786132407165, 0.5717733052967201]",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     63\u001b[39m     fold_scores.append(rmse)\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m rmse > gate: gate_pass = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m gate_pass, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCRITICAL: Stage1 gate failed on \u22651 folds. rmse_log per fold=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_scores\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m({\u001b[33m'\u001b[39m\u001b[33mstage1_gate\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mPASSED\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrmse_log_per_fold\u001b[39m\u001b[33m'\u001b[39m: [\u001b[38;5;28mround\u001b[39m(s,\u001b[32m6\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m fold_scores], \u001b[33m'\u001b[39m\u001b[33mnote\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mLinear Ridge Stage1 with one-hot categoricals and fold-local trend\u001b[39m\u001b[33m'\u001b[39m})\n",
            "\u001b[31mAssertionError\u001b[39m: CRITICAL: Stage1 gate failed on \u22651 folds. rmse_log per fold=[0.5089564672389532, 0.5179140568447872, 0.5415669378943179, 0.5599786132407165, 0.5717733052967201]"
          ]
        }
      ]
    },
    {
      "id": "a16e6997-9b0c-4b27-9ee6-734148762b70",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stage 1 \u2014 Contingency Tree Params: softer reg + higher bin resolution; shared categoricals; hard gate\n",
        "import numpy as np, pandas as pd\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar as USCal\n",
        "\n",
        "assert 'train_df' in globals() and 'make_time_folds_quantile' in globals(), 'Prereqs missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Ordered data and target\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "dt_loc_all = train_ord['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "\n",
        "# Build global discrete columns for stable category sets\n",
        "global_disc = pd.DataFrame(index=train_ord.index)\n",
        "global_disc['year'] = dt_loc_all.dt.year.astype('int16')\n",
        "global_disc['month'] = dt_loc_all.dt.month.astype('int8')\n",
        "global_disc['week_of_year'] = dt_loc_all.dt.isocalendar().week.astype('int16')\n",
        "global_disc['day_of_week'] = dt_loc_all.dt.dayofweek.astype('int8')\n",
        "global_disc['hour'] = dt_loc_all.dt.hour.astype('int8')\n",
        "cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "if 'is_holiday' in train_ord.columns:\n",
        "    global_disc['is_holiday'] = train_ord['is_holiday'].astype('int8')\n",
        "else:\n",
        "    hol = USCal().holidays(start=dt_loc_all.min().normalize().tz_localize(None), end=dt_loc_all.max().normalize().tz_localize(None))\n",
        "    hol_dates = set(pd.to_datetime(hol).date)\n",
        "    global_disc['is_holiday'] = dt_loc_all.dt.date.map(lambda d: d in hol_dates).astype('int8')\n",
        "if 'after_hike' in train_ord.columns:\n",
        "    global_disc['after_hike'] = train_ord['after_hike'].astype('int8')\n",
        "else:\n",
        "    global_disc['after_hike'] = (dt_loc_all >= cutoff).astype('int8')\n",
        "\n",
        "cat_feats = ['year','month','week_of_year','day_of_week','hour','is_holiday','after_hike']\n",
        "cat_dtype_map = {c: pd.CategoricalDtype(categories=pd.Index(global_disc[c].unique()).tolist(), ordered=False) for c in cat_feats}\n",
        "\n",
        "# 5-fold time CV with 1-day gap\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "# Contingency params (per audit): min_data_in_leaf=50, reg_lambda=5.0, max_bin=511\n",
        "s1_params = dict(\n",
        "    objective='regression', metric='rmse',\n",
        "    learning_rate=0.04, n_estimators=8000,\n",
        "    num_leaves=128, min_data_in_leaf=50,\n",
        "    feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
        "    max_bin=511, reg_lambda=5.0,\n",
        "    random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "def build_fold_views(df_tr, df_va):\n",
        "    tr = df_tr.copy(); va = df_va.copy()\n",
        "    dt_tr = tr['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    dt_va = va['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    start_ns = int(dt_tr.min().value)\n",
        "    tr['days_since_train_start'] = ((dt_tr.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    va['days_since_train_start'] = ((dt_va.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    tr['year'] = dt_tr.dt.year.astype('int16'); va['year'] = dt_va.dt.year.astype('int16')\n",
        "    tr['month'] = dt_tr.dt.month.astype('int8'); va['month'] = dt_va.dt.month.astype('int8')\n",
        "    tr['week_of_year'] = dt_tr.dt.isocalendar().week.astype('int16'); va['week_of_year'] = dt_va.dt.isocalendar().week.astype('int16')\n",
        "    tr['day_of_week'] = dt_tr.dt.dayofweek.astype('int8'); va['day_of_week'] = dt_va.dt.dayofweek.astype('int8')\n",
        "    tr['hour'] = dt_tr.dt.hour.astype('int8'); va['hour'] = dt_va.dt.hour.astype('int8')\n",
        "    if 'is_holiday' not in tr.columns or 'is_holiday' not in va.columns:\n",
        "        hol = USCal().holidays(start=dt_tr.min().normalize().tz_localize(None), end=dt_va.max().normalize().tz_localize(None))\n",
        "        hol_dates = set(pd.to_datetime(hol).date)\n",
        "        tr['is_holiday'] = dt_tr.dt.date.map(lambda d: d in hol_dates).astype('int8')\n",
        "        va['is_holiday'] = dt_va.dt.date.map(lambda d: d in hol_dates).astype('int8')\n",
        "    if 'after_hike' not in tr.columns or 'after_hike' not in va.columns:\n",
        "        tr['after_hike'] = (dt_tr >= cutoff).astype('int8')\n",
        "        va['after_hike'] = (dt_va >= cutoff).astype('int8')\n",
        "    return tr, va\n",
        "\n",
        "use_feats = ['days_since_train_start'] + cat_feats\n",
        "mono = [1] + [0]*(len(use_feats)-1)\n",
        "\n",
        "gate_pass = True\n",
        "fold_scores = []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx]; va = train_ord.iloc[va_idx]\n",
        "    tr, va = build_fold_views(tr, va)\n",
        "    X_tr = tr[use_feats].copy(); X_va = va[use_feats].copy()\n",
        "    for c in cat_feats:\n",
        "        X_tr[c] = X_tr[c].astype(cat_dtype_map[c])\n",
        "        X_va[c] = X_va[c].astype(cat_dtype_map[c])\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    model = lgb.LGBMRegressor(**{**s1_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        categorical_feature='auto',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=500, verbose=False), lgb.log_evaluation(period=200)]\n",
        "    )\n",
        "    rmse = float(model.best_score_['valid_0']['rmse'])\n",
        "    sigma_va = float(np.std(y_va))\n",
        "    gate = min(0.35, 0.9 * sigma_va)\n",
        "    print(f\"[Stage1 CONTINGENCY Gate] Fold {i}: rmse_log={rmse:.4f}, sigma_va={sigma_va:.4f}, gate={gate:.4f}\")\n",
        "    fold_scores.append(rmse)\n",
        "    if rmse > gate: gate_pass = False\n",
        "\n",
        "assert gate_pass, f\"CRITICAL: Stage1 gate failed on \u22651 folds. rmse_log per fold={fold_scores}\"\n",
        "print({'stage1_gate': 'PASSED', 'rmse_log_per_fold': [round(s,6) for s in fold_scores], 'note': 'Contingency params applied: min_data_in_leaf=50, reg_lambda=5, max_bin=511'})"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.509027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.51254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 CONTINGENCY Gate] Fold 1: rmse_log=0.5076, sigma_va=0.5086, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.515904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.516957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 CONTINGENCY Gate] Fold 2: rmse_log=0.5154, sigma_va=0.5166, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.543095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.542439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.541852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.541388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.541059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.540733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.540648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.540646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.540797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.540998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage1 CONTINGENCY Gate] Fold 3: rmse_log=0.5406, sigma_va=0.5403, gate=0.3500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.557078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.557447\n"
          ]
        }
      ]
    },
    {
      "id": "d1432d0f-6701-46e0-b496-10eb753f0ee5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Path B \u2014 Single-Model LightGBM with per-fold TE (pu/do only) + counts; debug rmse in log/original; monotone time trend\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals(), 'Prepared train/test required'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "def ensure_days_since_start(df):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    d = df.copy(); dt_local = d['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    start_ns = int(dt_local.min().value)\n",
        "    d['days_since_start'] = ((dt_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def ensure_spatial_bins(df, res=0.005):\n",
        "    if all(c in df.columns for c in ['pu_bin','do_bin','pair_bin']): return df\n",
        "    d = df.copy()\n",
        "    pu_lat_bin = np.floor(d['pickup_latitude'] / res).astype('int32')\n",
        "    pu_lon_bin = np.floor(d['pickup_longitude'] / res).astype('int32')\n",
        "    do_lat_bin = np.floor(d['dropoff_latitude'] / res).astype('int32')\n",
        "    do_lon_bin = np.floor(d['dropoff_longitude'] / res).astype('int32')\n",
        "    d['pu_bin'] = (pu_lat_bin.astype(str) + '_' + pu_lon_bin.astype(str))\n",
        "    d['do_bin'] = (do_lat_bin.astype(str) + '_' + do_lon_bin.astype(str))\n",
        "    d['pair_bin'] = (d['pu_bin'] + '|' + d['do_bin'])\n",
        "    return d\n",
        "\n",
        "train_df = ensure_days_since_start(train_df)\n",
        "test_df = ensure_days_since_start(test_df)\n",
        "train_df = ensure_spatial_bins(train_df)\n",
        "test_df = ensure_spatial_bins(test_df)\n",
        "\n",
        "base_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'year','month','day','hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday','days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_df.columns: base_feats.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_df.columns: base_feats.append('dist_x_after_hike')\n",
        "\n",
        "train_ord = train_df.dropna(subset=base_feats + ['fare_amount']).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "test_df[base_feats] = test_df[base_feats].fillna(0)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "def te_smooth(train_key, train_tgt, apply_key, m=500.0, prior=None):\n",
        "    if prior is None: prior = float(np.mean(train_tgt))\n",
        "    g = pd.DataFrame({'k': train_key.astype('object'), 'y': train_tgt}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict()\n",
        "    enc = apply_key.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return enc, prior, mp\n",
        "\n",
        "def build_fold_mats(df, tr_idx, va_idx):\n",
        "    tr = df.iloc[tr_idx].copy(); va = df.iloc[va_idx].copy()\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    prior = float(y_tr.mean())\n",
        "    # TE on pu/do only (ablate pair for stability)\n",
        "    tr['te_pu'], _, _ = te_smooth(tr['pu_bin'], y_tr, tr['pu_bin'], m=600.0, prior=prior)\n",
        "    va['te_pu'], _, _ = te_smooth(tr['pu_bin'], y_tr, va['pu_bin'], m=600.0, prior=prior)\n",
        "    tr['te_do'], _, _ = te_smooth(tr['do_bin'], y_tr, tr['do_bin'], m=600.0, prior=prior)\n",
        "    va['te_do'], _, _ = te_smooth(tr['do_bin'], y_tr, va['do_bin'], m=600.0, prior=prior)\n",
        "    # Frequency encodings (log-counts) from train-only maps\n",
        "    pu_cnt = tr['pu_bin'].astype('object').value_counts().astype('int32')\n",
        "    do_cnt = tr['do_bin'].astype('object').value_counts().astype('int32')\n",
        "    pair_cnt = tr['pair_bin'].astype('object').value_counts().astype('int32')\n",
        "    for d in (tr, va):\n",
        "        d['log_pu_cnt'] = np.log1p(d['pu_bin'].astype('object').map(pu_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_do_cnt'] = np.log1p(d['do_bin'].astype('object').map(do_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_pair_cnt'] = np.log1p(d['pair_bin'].astype('object').map(pair_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "    use_cols = base_feats + ['te_pu','te_do','log_pu_cnt','log_do_cnt','log_pair_cnt']\n",
        "    X_tr = tr[use_cols].astype('float32').values; X_va = va[use_cols].astype('float32').values\n",
        "    return X_tr, X_va, y_tr, y_va, use_cols\n",
        "\n",
        "SEED = 2025\n",
        "lgb_params = dict(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    learning_rate=0.045,\n",
        "    n_estimators=25000,\n",
        "    num_leaves=256,\n",
        "    max_depth=-1,\n",
        "    min_data_in_leaf=120,\n",
        "    feature_fraction=0.85,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=1,\n",
        "    max_bin=255,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=5.0,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "final_use_cols = None\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    X_tr, X_va, y_tr, y_va, use_cols = build_fold_mats(train_ord, tr_idx, va_idx)\n",
        "    mono = [0]*len(use_cols)\n",
        "    if 'days_since_start' in use_cols:\n",
        "        mono[use_cols.index('days_since_start')] = 1\n",
        "    model = lgb.LGBMRegressor(**{**lgb_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=800, verbose=False), lgb.log_evaluation(period=200)]\n",
        "    )\n",
        "    y_pred_val_log = model.predict(X_va, num_iteration=model.best_iteration_)\n",
        "    y_pred_val = np.expm1(y_pred_val_log)\n",
        "    y_true_val = np.expm1(y_va)\n",
        "    rmse = float(root_mean_squared_error(y_true_val, y_pred_val))\n",
        "    rmse_log = float(np.sqrt(np.mean((y_pred_val_log - y_va)**2)))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    final_use_cols = use_cols\n",
        "    print(f\"[Single-Model] Fold {i}: RMSE={rmse:.5f}, RMSE_log={rmse_log:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_final = int(np.clip(int(np.median(best_iters)), 1000, lgb_params['n_estimators']))\n",
        "print({'cv_rmse_mean': round(cv_mean, 6), 'cv_rmse_std': round(cv_std, 6), 'best_iter_final': best_iter_final, 'note': 'Single-Model per-fold pu/do TE + counts + monotone(days_since_start)'})\n",
        "\n",
        "# Final fit on all data with full-data TE maps\n",
        "full = train_ord.copy(); tst = test_df.copy()\n",
        "y_full_log = y_all_log\n",
        "prior_full = float(y_full_log.mean())\n",
        "def te_apply_full(tr_key, tr_tgt, ap_key, m):\n",
        "    g = pd.DataFrame({'k': tr_key.astype('object'), 'y': tr_tgt}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior_full) / (cnts + m)\n",
        "    mp = smooth.to_dict()\n",
        "    tr_enc = tr_key.astype('object').map(mp).fillna(prior_full).astype('float32').values\n",
        "    ap_enc = ap_key.astype('object').map(mp).fillna(prior_full).astype('float32').values\n",
        "    return tr_enc, ap_enc\n",
        "full['te_pu'], tst['te_pu'] = te_apply_full(full['pu_bin'], y_full_log, tst['pu_bin'], m=600.0)\n",
        "full['te_do'], tst['te_do'] = te_apply_full(full['do_bin'], y_full_log, tst['do_bin'], m=600.0)\n",
        "pu_cnt_full = full['pu_bin'].astype('object').value_counts().astype('int32')\n",
        "do_cnt_full = full['do_bin'].astype('object').value_counts().astype('int32')\n",
        "pair_cnt_full = full['pair_bin'].astype('object').value_counts().astype('int32')\n",
        "for d in (full, tst):\n",
        "    d['log_pu_cnt'] = np.log1p(d['pu_bin'].astype('object').map(pu_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "    d['log_do_cnt'] = np.log1p(d['do_bin'].astype('object').map(do_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "    d['log_pair_cnt'] = np.log1p(d['pair_bin'].astype('object').map(pair_cnt_full).fillna(0).astype('int32')).astype('float32')\n",
        "final_features = final_use_cols\n",
        "X_full = full[final_features].astype('float32').values\n",
        "X_test = tst[final_features].astype('float32').values\n",
        "mono = [0]*len(final_features)\n",
        "if 'days_since_start' in final_features:\n",
        "    mono[final_features.index('days_since_start')] = 1\n",
        "final_model = lgb.LGBMRegressor(**{**lgb_params, 'n_estimators': best_iter_final, 'monotone_constraints': mono})\n",
        "final_model.fit(X_full, y_full_log)\n",
        "test_pred = np.expm1(final_model.predict(X_test)).astype('float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "pred_df = pd.DataFrame({'key': test_df['key'].astype('string'), 'fare_amount': test_pred})\n",
        "sub = sample[['key']].merge(pred_df, on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape:', sub.shape)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.232537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.233322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.234307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.235008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.235799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Single-Model] Fold 1: RMSE=3.78379, RMSE_log=0.23244, best_iter=224 (train_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.188939\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.1896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.189997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.190606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Single-Model] Fold 2: RMSE=3.18343, RMSE_log=0.18871, best_iter=188 (train_n=640523, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.210777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.209468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.209057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.208914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.208728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.20878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.208582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.208617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.208597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.208618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Single-Model] Fold 3: RMSE=3.75448, RMSE_log=0.20856, best_iter=1329 (train_n=961323, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.199909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.197326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.19671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.196517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.196476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.196449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.196436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.196516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.196515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Single-Model] Fold 4: RMSE=3.73546, RMSE_log=0.19641, best_iter=1147 (train_n=1282234, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.183052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.179518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.178528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.178129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.178065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.178005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.178097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.178165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.17837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.178399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Single-Model] Fold 5: RMSE=3.46122, RMSE_log=0.17800, best_iter=1203 (train_n=1602868, val_n=320755)\n{'cv_rmse_mean': 3.583678, 'cv_rmse_std': 0.231278, 'best_iter_final': 1147, 'note': 'Single-Model per-fold pu/do TE + counts + monotone(days_since_start)'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with shape: (9914, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "dac31211-020c-42ff-b69b-3aad8e843215",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Path B (Stabilized Baseline) \u2014 Single-Model LGBM without Target Encoding; robust numeric+temporal+POI + KMeans clusters\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'sample' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Ensure minimal time features\n",
        "def ensure_days_since_start(df):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    d = df.copy(); dt_local = d['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    start_ns = int(dt_local.min().value)\n",
        "    d['days_since_start'] = ((dt_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def ensure_after_hike(df):\n",
        "    if 'after_hike' in df.columns: return df\n",
        "    d = df.copy()\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    d['after_hike'] = (d['pickup_datetime'].dt.tz_convert('America/New_York') >= cutoff).astype('int8')\n",
        "    return d\n",
        "\n",
        "# Add KMeans clusters if missing\n",
        "def ensure_clusters(train, test, k=100, sample_n=300_000, seed=2025):\n",
        "    need = ['pu_cl','do_cl','same_cl']\n",
        "    if all(c in train.columns for c in need) and all(c in test.columns for c in need):\n",
        "        return train, test\n",
        "    pu_coords_tr = train[['pickup_latitude','pickup_longitude']].astype('float32').values\n",
        "    do_coords_tr = train[['dropoff_latitude','dropoff_longitude']].astype('float32').values\n",
        "    if len(train) > sample_n:\n",
        "        rng = np.random.default_rng(seed)\n",
        "        idx = rng.choice(len(train), size=sample_n, replace=False)\n",
        "        pu_fit = pu_coords_tr[idx]\n",
        "        do_fit = do_coords_tr[idx]\n",
        "    else:\n",
        "        pu_fit = pu_coords_tr\n",
        "        do_fit = do_coords_tr\n",
        "    km_pu = MiniBatchKMeans(n_clusters=k, random_state=seed, batch_size=20000, n_init=5, max_no_improvement=30)\n",
        "    km_do = MiniBatchKMeans(n_clusters=k, random_state=seed+1, batch_size=20000, n_init=5, max_no_improvement=30)\n",
        "    km_pu.fit(pu_fit); km_do.fit(do_fit)\n",
        "    train = train.copy(); test = test.copy()\n",
        "    train['pu_cl'] = km_pu.predict(pu_coords_tr).astype('int32')\n",
        "    train['do_cl'] = km_do.predict(do_coords_tr).astype('int32')\n",
        "    test['pu_cl'] = km_pu.predict(test[['pickup_latitude','pickup_longitude']].astype('float32').values).astype('int32')\n",
        "    test['do_cl'] = km_do.predict(test[['dropoff_latitude','dropoff_longitude']].astype('float32').values).astype('int32')\n",
        "    train['same_cl'] = (train['pu_cl'] == train['do_cl']).astype('int8')\n",
        "    test['same_cl'] = (test['pu_cl'] == test['do_cl']).astype('int8')\n",
        "    return train, test\n",
        "\n",
        "# Prepare data\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True).copy()\n",
        "test_prep = test_df.copy()\n",
        "train_ord = ensure_days_since_start(train_ord); test_prep = ensure_days_since_start(test_prep)\n",
        "train_ord = ensure_after_hike(train_ord); test_prep = ensure_after_hike(test_prep)\n",
        "train_ord, test_prep = ensure_clusters(train_ord, test_prep, k=100)\n",
        "\n",
        "# Robust numeric+temporal+POI feature set (no target encoding, no frequency encodings)\n",
        "feature_cols = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'year','month','day','hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday','days_since_start','after_hike',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend',\n",
        "    'pu_cl','do_cl','same_cl'\n",
        "]\n",
        "if 'rot_manh_km' in train_ord.columns: feature_cols.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_ord.columns: feature_cols.append('dist_x_after_hike')\n",
        "\n",
        "train_ord = train_ord.dropna(subset=feature_cols + ['fare_amount']).reset_index(drop=True)\n",
        "test_prep[feature_cols] = test_prep[feature_cols].fillna(0)\n",
        "\n",
        "X_all = train_ord[feature_cols].astype('float32').values\n",
        "y_all = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "X_test = test_prep[feature_cols].astype('float32').values\n",
        "\n",
        "# 5-fold time CV with 1-day gap\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=5, gap_days=1)\n",
        "assert len(folds) == 5, 'Expected 5 folds'\n",
        "\n",
        "# LightGBM params \u2014 stable, regularized; no categorical handling needed (all numeric); monotone on days and distances\n",
        "lgb_params = dict(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    learning_rate=0.045,\n",
        "    n_estimators=20000,\n",
        "    num_leaves=192,\n",
        "    min_data_in_leaf=400,\n",
        "    feature_fraction=0.8,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=1,\n",
        "    max_bin=255,\n",
        "    reg_alpha=2.0,\n",
        "    reg_lambda=10.0,\n",
        "    random_state=2025,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "def build_monotone_constraints(cols):\n",
        "    mono = [0]*len(cols)\n",
        "    for nm in ['days_since_start','dist_hav_km','dist_man_km']:\n",
        "        if nm in cols:\n",
        "            mono[cols.index(nm)] = 1\n",
        "    return mono\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "    X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "    mono = build_monotone_constraints(feature_cols)\n",
        "    model = lgb.LGBMRegressor(**{**lgb_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=600, verbose=False), lgb.log_evaluation(period=200)]\n",
        "    )\n",
        "    y_pred = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), y_pred))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    print(f\"[Stabilized Single] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]}\")\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_final = int(np.clip(int(np.median(best_iters)), 1000, lgb_params['n_estimators']))\n",
        "print({'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 'best_iter_final': best_iter_final, 'note': 'No TE; robust features + KMeans clusters; monotone on time/dist'})\n",
        "\n",
        "# Fit final model and create submission\n",
        "final_mono = build_monotone_constraints(feature_cols)\n",
        "final_model = lgb.LGBMRegressor(**{**lgb_params, 'n_estimators': best_iter_final, 'monotone_constraints': final_mono})\n",
        "final_model.fit(X_all, y_all)\n",
        "test_pred = np.expm1(final_model.predict(X_test)).astype('float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "pred_df = pd.DataFrame({'key': test_prep['key'].astype('string'), 'fare_amount': test_pred})\n",
        "sub = sample[['key']].merge(pred_df, on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape:', sub.shape)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.232668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.230464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.229994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.229902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.229997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.230142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stabilized Single] Fold 1: RMSE=3.67499, best_iter=762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.184857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.184753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.184664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stabilized Single] Fold 2: RMSE=3.09670, best_iter=129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.212231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.20824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.206936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.206456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.206143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.205898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.205706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.205602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.205564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.205565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\tvalid_0's rmse: 0.20559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.205644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stabilized Single] Fold 3: RMSE=3.57691, best_iter=1919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.202634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.197082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.195469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.194733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.194317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.194005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.193854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.193714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.193636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.193588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\tvalid_0's rmse: 0.193579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.193566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2600]\tvalid_0's rmse: 0.193575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2800]\tvalid_0's rmse: 0.193581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.193597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3200]\tvalid_0's rmse: 0.193612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stabilized Single] Fold 4: RMSE=3.49711, best_iter=2655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.185938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.179215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.177161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.176339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.175808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.175509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.175321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.175184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.175086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.175023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\tvalid_0's rmse: 0.175033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.174999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2600]\tvalid_0's rmse: 0.174995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2800]\tvalid_0's rmse: 0.174994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.175023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3200]\tvalid_0's rmse: 0.175086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stabilized Single] Fold 5: RMSE=3.20647, best_iter=2668\n{'cv_rmse_mean': 3.410434, 'cv_rmse_std': 0.221471, 'best_iter_final': 1919, 'note': 'No TE; robust features + KMeans clusters; monotone on time/dist'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with shape: (9914, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "a83ef4aa-538c-42c3-9442-3f82cc4187d1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 1 \u2014 Stability fixes: fold ensembling + huber + per-fold freq enc + minimal TE (pu/do); no global refit\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'sample' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Ensure helpers\n",
        "def ensure_days_since_start(df):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    d = df.copy(); dt_local = d['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    start_ns = int(dt_local.min().value)\n",
        "    d['days_since_start'] = ((dt_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def ensure_spatial_bins(df, res=0.005):\n",
        "    if all(c in df.columns for c in ['pu_bin','do_bin','pair_bin']): return df\n",
        "    d = df.copy()\n",
        "    pu_lat_bin = np.floor(d['pickup_latitude'] / res).astype('int32')\n",
        "    pu_lon_bin = np.floor(d['pickup_longitude'] / res).astype('int32')\n",
        "    do_lat_bin = np.floor(d['dropoff_latitude'] / res).astype('int32')\n",
        "    do_lon_bin = np.floor(d['dropoff_longitude'] / res).astype('int32')\n",
        "    d['pu_bin'] = (pu_lat_bin.astype(str) + '_' + pu_lon_bin.astype(str))\n",
        "    d['do_bin'] = (do_lat_bin.astype(str) + '_' + do_lon_bin.astype(str))\n",
        "    d['pair_bin'] = (d['pu_bin'] + '|' + d['do_bin'])\n",
        "    return d\n",
        "\n",
        "train_use = ensure_spatial_bins(ensure_days_since_start(train_df))\n",
        "test_use = ensure_spatial_bins(ensure_days_since_start(test_df))\n",
        "train_ord = train_use.dropna(subset=['fare_amount']).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "\n",
        "# Base robust features (no high-risk IDs); keep time/dist/POI/temporal cycles\n",
        "base_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'year','month','day','hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday','days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_ord.columns: base_feats.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_ord.columns: base_feats.append('dist_x_after_hike')\n",
        "\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "# TE helper (smoothed, on log target)\n",
        "def te_smooth(train_key, train_tgt, apply_key, m=600.0, prior=None):\n",
        "    if prior is None: prior = float(np.mean(train_tgt))\n",
        "    g = pd.DataFrame({'k': train_key.astype('object'), 'y': train_tgt}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict()\n",
        "    enc = apply_key.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return enc, prior, mp\n",
        "\n",
        "# Model params: huber for robustness; leave capacity reasonable; use monotone on days_since_start\n",
        "lgb_params = dict(\n",
        "    objective='huber', metric='rmse',\n",
        "    learning_rate=0.045, n_estimators=22000,\n",
        "    num_leaves=256, max_depth=-1, min_data_in_leaf=120,\n",
        "    feature_fraction=0.85, bagging_fraction=0.8, bagging_freq=1,\n",
        "    max_bin=255, reg_alpha=0.0, reg_lambda=8.0,\n",
        "    random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "fold_test_preds = []  # store per-fold test predictions for ensembling\n",
        "final_use_cols = None\n",
        "\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx].copy(); va = train_ord.iloc[va_idx].copy()\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    prior = float(y_tr.mean())\n",
        "    # Per-fold frequency encodings from train only\n",
        "    pu_cnt = tr['pu_bin'].astype('object').value_counts().astype('int32')\n",
        "    do_cnt = tr['do_bin'].astype('object').value_counts().astype('int32')\n",
        "    pair_cnt = tr['pair_bin'].astype('object').value_counts().astype('int32')\n",
        "    for d in (tr, va):\n",
        "        d['log_pu_cnt'] = np.log1p(d['pu_bin'].astype('object').map(pu_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_do_cnt'] = np.log1p(d['do_bin'].astype('object').map(do_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_pair_cnt'] = np.log1p(d['pair_bin'].astype('object').map(pair_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "    # Minimal per-fold TE (pu/do only) with strong smoothing\n",
        "    tr['te_pu'], _, _ = te_smooth(tr['pu_bin'], y_tr, tr['pu_bin'], m=600.0, prior=prior)\n",
        "    va['te_pu'], _, _ = te_smooth(tr['pu_bin'], y_tr, va['pu_bin'], m=600.0, prior=prior)\n",
        "    tr['te_do'], _, _ = te_smooth(tr['do_bin'], y_tr, tr['do_bin'], m=600.0, prior=prior)\n",
        "    va['te_do'], _, _ = te_smooth(tr['do_bin'], y_tr, va['do_bin'], m=600.0, prior=prior)\n",
        "    use_cols = base_feats + ['te_pu','te_do','log_pu_cnt','log_do_cnt','log_pair_cnt']\n",
        "    X_tr = tr[use_cols].astype('float32').values\n",
        "    X_va = va[use_cols].astype('float32').values\n",
        "    final_use_cols = use_cols\n",
        "    mono = [0]*len(use_cols)\n",
        "    if 'days_since_start' in use_cols:\n",
        "        mono[use_cols.index('days_since_start')] = 1\n",
        "    model = lgb.LGBMRegressor(**{**lgb_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=700, verbose=False), lgb.log_evaluation(period=200)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), y_pred_val))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    print(f\"[Phase1-Stable] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "\n",
        "    # Fold-ensemble prediction for test: build encodings from train-only maps and apply\n",
        "    tst = test_use.copy()\n",
        "    tst['log_pu_cnt'] = np.log1p(tst['pu_bin'].astype('object').map(pu_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "    tst['log_do_cnt'] = np.log1p(tst['do_bin'].astype('object').map(do_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "    tst['log_pair_cnt'] = np.log1p(tst['pair_bin'].astype('object').map(pair_cnt).fillna(0).astype('int32')).astype('float32')\n",
        "    tst['te_pu'], _prior_unused, _mp_unused = te_smooth(tr['pu_bin'], y_tr, tst['pu_bin'], m=600.0, prior=prior)\n",
        "    tst['te_do'], _prior_unused2, _mp_unused2 = te_smooth(tr['do_bin'], y_tr, tst['do_bin'], m=600.0, prior=prior)\n",
        "    X_test_fold = tst[use_cols].astype('float32').values\n",
        "    fold_test_pred = np.expm1(model.predict(X_test_fold, num_iteration=model.best_iteration_)).astype('float32')\n",
        "    fold_test_preds.append(fold_test_pred)\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_median = int(np.median(best_iters)) if best_iters else 0\n",
        "print({'cv_rmse_mean': round(cv_mean, 6), 'cv_rmse_std': round(cv_std, 6), 'median_best_iter': best_iter_median, 'note': 'Fold ensembling + huber + per-fold freq + minimal TE pu/do'})\n",
        "\n",
        "# Average fold predictions for test (fold ensembling); no global refit\n",
        "test_pred_ens = np.mean(np.vstack(fold_test_preds), axis=0).astype('float32')\n",
        "test_pred_ens = np.clip(test_pred_ens, 0, 500)\n",
        "sub_df = pd.DataFrame({'key': test_use['key'].astype('string'), 'fare_amount': test_pred_ens})\n",
        "sub = sample[['key']].merge(sub_df, on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (fold-ensemble) with shape:', sub.shape)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.232048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.232381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.23328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.23391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Phase1-Stable] Fold 1: RMSE=3.74845, best_iter=218 (train_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.186919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.187621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.188406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.18889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Phase1-Stable] Fold 2: RMSE=3.14178, best_iter=214 (train_n=640523, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.210618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.208772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.208137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.208081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.207847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.207807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.207814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.207874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.207914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.207863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Phase1-Stable] Fold 3: RMSE=3.73304, best_iter=1317 (train_n=961323, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.199612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.196948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.196335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.196073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.19593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.196013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.196091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.196136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Phase1-Stable] Fold 4: RMSE=3.70937, best_iter=1072 (train_n=1282234, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.182735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.17883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.177858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.177407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.177268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.177105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.177175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.177189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.177213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Phase1-Stable] Fold 5: RMSE=3.42745, best_iter=1248 (train_n=1602868, val_n=320755)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cv_rmse_mean': 3.552016, 'cv_rmse_std': 0.236614, 'median_best_iter': 1072, 'note': 'Fold ensembling + huber + per-fold freq + minimal TE pu/do'}\nSaved submission.csv (fold-ensemble) with shape: (9914, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "e653f4d9-a892-4c69-9b4d-bd9425f81271",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 1 (Stabilized Geohash) \u2014 per-fold hierarchical TE (gh6 -> gh5 back-off), stronger smoothing/back-off, time-decay weights (exp), constrained capacity, fold ensembling\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'sample' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "try:\n",
        "    import pygeohash as pgh\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pygeohash'])\n",
        "    import pygeohash as pgh\n",
        "\n",
        "# Ensure helper features\n",
        "def ensure_days_since_start(df):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    d = df.copy(); dt_local = d['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    start_ns = int(dt_local.min().value)\n",
        "    d['days_since_start'] = ((dt_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def add_geohash(df, prec_list=(6,5)):\n",
        "    d = df.copy()\n",
        "    lat_pu = d['pickup_latitude'].astype('float32').values\n",
        "    lon_pu = d['pickup_longitude'].astype('float32').values\n",
        "    lat_do = d['dropoff_latitude'].astype('float32').values\n",
        "    lon_do = d['dropoff_longitude'].astype('float32').values\n",
        "    for prec in prec_list:\n",
        "        pu_col = f'pu_gh{prec}'; do_col = f'do_gh{prec}'\n",
        "        if pu_col in d.columns and do_col in d.columns: continue\n",
        "        d[pu_col] = [pgh.encode(float(lat), float(lon), precision=prec) if np.isfinite(lat) and np.isfinite(lon) else '' for lat, lon in zip(lat_pu, lon_pu)]\n",
        "        d[do_col] = [pgh.encode(float(lat), float(lon), precision=prec) if np.isfinite(lat) and np.isfinite(lon) else '' for lat, lon in zip(lat_do, lon_do)]\n",
        "    return d\n",
        "\n",
        "train_use = ensure_days_since_start(train_df).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "test_use = ensure_days_since_start(test_df).copy()\n",
        "train_use = add_geohash(train_use, prec_list=(6,5))\n",
        "test_use = add_geohash(test_use, prec_list=(6,5))\n",
        "\n",
        "# Base robust numeric/temporal/POI features \u2014 drop absolute identifiers year/month/day per mandate\n",
        "base_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday','days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_use.columns: base_feats.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_use.columns: base_feats.append('dist_x_after_hike')\n",
        "\n",
        "# Drop NaNs; fill test\n",
        "train_ord = train_use.dropna(subset=base_feats + ['fare_amount']).reset_index(drop=True)\n",
        "test_use[base_feats] = test_use[base_feats].fillna(0)\n",
        "\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "# Smoothed TE helper (log-target); returns numpy vector for apply_key\n",
        "def te_smooth(train_key, train_tgt, apply_key, m=700.0, prior=None):\n",
        "    if prior is None: prior = float(np.mean(train_tgt))\n",
        "    g = pd.DataFrame({'k': train_key.astype('object'), 'y': train_tgt}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict()\n",
        "    enc = apply_key.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return enc, prior, mp, cnts.to_dict()\n",
        "\n",
        "# Blend gh6 and gh5 TEs using data-driven weight from gh6 counts; stronger smoothing/back-off\n",
        "def blend_te(gh6_series_tr, gh5_series_tr, tgt_tr, gh6_series_ap, gh5_series_ap, m6=1500.0, m5=1000.0, alpha=200.0):\n",
        "    enc6, prior, mp6, cnt6 = te_smooth(gh6_series_tr, tgt_tr, gh6_series_ap, m=m6)\n",
        "    enc5, _,    mp5, cnt5 = te_smooth(gh5_series_tr, tgt_tr, gh5_series_ap, m=m5, prior=prior)\n",
        "    if isinstance(gh6_series_ap, pd.Series):\n",
        "        ap_keys = gh6_series_ap.astype('object').values\n",
        "    else:\n",
        "        ap_keys = gh6_series_ap.astype('object')\n",
        "    w = np.array([cnt6.get(k, 0.0) for k in ap_keys], dtype='float32')\n",
        "    w = w / (w + np.float32(alpha))\n",
        "    return (w * enc6 + (1.0 - w) * enc5).astype('float32'), (mp6, mp5), (cnt6, cnt5)\n",
        "\n",
        "# Model params (constrained capacity) and monotone on days_since_start\n",
        "lgb_params = dict(\n",
        "    objective='huber', metric='rmse',\n",
        "    learning_rate=0.045, n_estimators=20000,\n",
        "    num_leaves=128, max_depth=-1, min_data_in_leaf=800,\n",
        "    feature_fraction=0.70, bagging_fraction=0.8, bagging_freq=1,\n",
        "    max_bin=127, reg_alpha=0.0, reg_lambda=40.0,\n",
        "    random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "fold_test_preds = []\n",
        "final_use_cols = None\n",
        "\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx].copy(); va = train_ord.iloc[va_idx].copy()\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    prior = float(y_tr.mean())\n",
        "    # Hierarchical TE for PU\n",
        "    te_pu_blend_va, (pu_mp6, pu_mp5), (pu_cnt6, pu_cnt5) = blend_te(\n",
        "        tr['pu_gh6'], tr['pu_gh5'], y_tr, va['pu_gh6'], va['pu_gh5'], m6=1500.0, m5=1000.0, alpha=200.0\n",
        "    )\n",
        "    te_pu_blend_tr, _, _ = blend_te(\n",
        "        tr['pu_gh6'], tr['pu_gh5'], y_tr, tr['pu_gh6'], tr['pu_gh5'], m6=1500.0, m5=1000.0, alpha=200.0\n",
        "    )\n",
        "    tr['te_pu'] = te_pu_blend_tr; va['te_pu'] = te_pu_blend_va\n",
        "    # Hierarchical TE for DO\n",
        "    te_do_blend_va, (do_mp6, do_mp5), (do_cnt6, do_cnt5) = blend_te(\n",
        "        tr['do_gh6'], tr['do_gh5'], y_tr, va['do_gh6'], va['do_gh5'], m6=1500.0, m5=1000.0, alpha=200.0\n",
        "    )\n",
        "    te_do_blend_tr, _, _ = blend_te(\n",
        "        tr['do_gh6'], tr['do_gh5'], y_tr, tr['do_gh6'], tr['do_gh5'], m6=1500.0, m5=1000.0, alpha=200.0\n",
        "    )\n",
        "    tr['te_do'] = te_do_blend_tr; va['te_do'] = te_do_blend_va\n",
        "    # Frequency encodings (log-counts) using gh6 and gh5, computed on train-only\n",
        "    pu6_counts = tr['pu_gh6'].astype('object').value_counts().astype('int32')\n",
        "    do6_counts = tr['do_gh6'].astype('object').value_counts().astype('int32')\n",
        "    pu5_counts = tr['pu_gh5'].astype('object').value_counts().astype('int32')\n",
        "    do5_counts = tr['do_gh5'].astype('object').value_counts().astype('int32')\n",
        "    for d in (tr, va):\n",
        "        d['log_pu6_cnt'] = np.log1p(d['pu_gh6'].astype('object').map(pu6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_do6_cnt'] = np.log1p(d['do_gh6'].astype('object').map(do6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_pu5_cnt'] = np.log1p(d['pu_gh5'].astype('object').map(pu5_counts).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_do5_cnt'] = np.log1p(d['do_gh5'].astype('object').map(do5_counts).fillna(0).astype('int32')).astype('float32')\n",
        "    use_cols = base_feats + ['te_pu','te_do','log_pu6_cnt','log_do6_cnt','log_pu5_cnt','log_do5_cnt']\n",
        "    X_tr = tr[use_cols].astype('float32').values\n",
        "    X_va = va[use_cols].astype('float32').values\n",
        "    final_use_cols = use_cols\n",
        "    # Monotone constraint only on days_since_start\n",
        "    mono = [0]*len(use_cols)\n",
        "    if 'days_since_start' in use_cols:\n",
        "        mono[use_cols.index('days_since_start')] = 1\n",
        "    # Exponential time-decay weights within fold (tau=180 days)\n",
        "    days_tr = tr['days_since_start'].astype('float32').values\n",
        "    max_days = float(np.max(days_tr)) if days_tr.size else 0.0\n",
        "    w_tr = np.exp((days_tr - np.float32(max_days)) / np.float32(180.0)).astype('float32')\n",
        "    model = lgb.LGBMRegressor(**{**lgb_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr, sample_weight=w_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=700, verbose=False), lgb.log_evaluation(period=200)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), y_pred_val))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    print(f\"[GH-Stable] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "\n",
        "    # Build test features using train-only maps (same blending and counts); fold-ensemble predictions\n",
        "    tst = test_use.copy()\n",
        "    # Blend TE for PU on test\n",
        "    enc6_pu_test, _, _, cnt6_pu = te_smooth(tr['pu_gh6'], y_tr, tst['pu_gh6'], m=1500.0, prior=prior)\n",
        "    enc5_pu_test, _, _, cnt5_pu = te_smooth(tr['pu_gh5'], y_tr, tst['pu_gh5'], m=1000.0, prior=prior)\n",
        "    w_pu = np.array([cnt6_pu.get(k, 0.0) for k in tst['pu_gh6'].astype('object').values], dtype='float32')\n",
        "    w_pu = w_pu / (w_pu + np.float32(200.0))\n",
        "    tst['te_pu'] = (w_pu * enc6_pu_test + (1.0 - w_pu) * enc5_pu_test).astype('float32')\n",
        "    # Blend TE for DO on test\n",
        "    enc6_do_test, _, _, cnt6_do = te_smooth(tr['do_gh6'], y_tr, tst['do_gh6'], m=1500.0, prior=prior)\n",
        "    enc5_do_test, _, _, cnt5_do = te_smooth(tr['do_gh5'], y_tr, tst['do_gh5'], m=1000.0, prior=prior)\n",
        "    w_do = np.array([cnt6_do.get(k, 0.0) for k in tst['do_gh6'].astype('object').values], dtype='float32')\n",
        "    w_do = w_do / (w_do + np.float32(200.0))\n",
        "    tst['te_do'] = (w_do * enc6_do_test + (1.0 - w_do) * enc5_do_test).astype('float32')\n",
        "    # Counts\n",
        "    pu6_counts_ap = tr['pu_gh6'].astype('object').value_counts().astype('int32')\n",
        "    do6_counts_ap = tr['do_gh6'].astype('object').value_counts().astype('int32')\n",
        "    pu5_counts_ap = tr['pu_gh5'].astype('object').value_counts().astype('int32')\n",
        "    do5_counts_ap = tr['do_gh5'].astype('object').value_counts().astype('int32')\n",
        "    tst['log_pu6_cnt'] = np.log1p(tst['pu_gh6'].astype('object').map(pu6_counts_ap).fillna(0).astype('int32')).astype('float32')\n",
        "    tst['log_do6_cnt'] = np.log1p(tst['do_gh6'].astype('object').map(do6_counts_ap).fillna(0).astype('int32')).astype('float32')\n",
        "    tst['log_pu5_cnt'] = np.log1p(tst['pu_gh5'].astype('object').map(pu5_counts_ap).fillna(0).astype('int32')).astype('float32')\n",
        "    tst['log_do5_cnt'] = np.log1p(tst['do_gh5'].astype('object').map(do5_counts_ap).fillna(0).astype('int32')).astype('float32')\n",
        "    X_test_fold = tst[use_cols].astype('float32').values\n",
        "    fold_pred = np.expm1(model.predict(X_test_fold, num_iteration=model.best_iteration_)).astype('float32')\n",
        "    fold_test_preds.append(fold_pred)\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_median = int(np.median(best_iters)) if best_iters else 0\n",
        "print({'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 'median_best_iter': best_iter_median, 'note': 'Hierarchical gh6/gh5 TE blend (strong smoothing/back-off) + constrained model + exp time-decay + fold ensembling'})\n",
        "\n",
        "# Average fold predictions for test (fold ensembling); no global refit\n",
        "test_pred = np.mean(np.vstack(fold_test_preds), axis=0).astype('float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "pred_df = pd.DataFrame({'key': test_use['key'].astype('string'), 'fare_amount': test_pred})\n",
        "sub = sample[['key']].merge(pred_df, on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (geohash fold-ensemble) with shape:', sub.shape)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.233087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.231237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.230663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.230384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.23026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.230155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.230203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.23022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.230278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GH-Stable] Fold 1: RMSE=3.65475, best_iter=1212 (train_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.187305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.187093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.187134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.187215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GH-Stable] Fold 2: RMSE=3.09381, best_iter=267 (train_n=640523, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.212057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.208102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.20692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.206215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.205873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.20559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.205418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.205331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.205303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.205348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\tvalid_0's rmse: 0.205331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.205355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GH-Stable] Fold 3: RMSE=3.55763, best_iter=1806 (train_n=961323, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.201123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.197636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.196609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.196123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.19592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.195862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.195854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.19586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.195914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.195981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GH-Stable] Fold 4: RMSE=3.48745, best_iter=1313 (train_n=1282234, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.183582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.179804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.178636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.177981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.177633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.17748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.177473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.177513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.17752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GH-Stable] Fold 5: RMSE=3.18571, best_iter=1287 (train_n=1602868, val_n=320755)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cv_rmse_mean': 3.395868, 'cv_rmse_std': 0.217706, 'median_best_iter': 1287, 'note': 'Hierarchical gh6/gh5 TE blend (strong smoothing/back-off) + constrained model + exp time-decay + fold ensembling'}\nSaved submission.csv (geohash fold-ensemble) with shape: (9914, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "35f4cb74-2994-4ab7-8e42-77867d8129cf",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 1 \u2014 Final Stability Lockdown: hierarchical geohash TE (gh6 -> gh5) with hard back-off, extreme regularization, purified features, exp decay (tau=120), fold ensembling\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'sample' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "try:\n",
        "    import pygeohash as pgh\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pygeohash'])\n",
        "    import pygeohash as pgh\n",
        "\n",
        "def ensure_days_since_start(df):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    d = df.copy(); dt_local = d['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    start_ns = int(dt_local.min().value)\n",
        "    d['days_since_start'] = ((dt_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def add_geohash(df, prec_list=(6,5)):\n",
        "    d = df.copy()\n",
        "    lat_pu = d['pickup_latitude'].astype('float32').values\n",
        "    lon_pu = d['pickup_longitude'].astype('float32').values\n",
        "    lat_do = d['dropoff_latitude'].astype('float32').values\n",
        "    lon_do = d['dropoff_longitude'].astype('float32').values\n",
        "    for prec in prec_list:\n",
        "        pu_col = f'pu_gh{prec}'; do_col = f'do_gh{prec}'\n",
        "        if pu_col in d.columns and do_col in d.columns: continue\n",
        "        d[pu_col] = [pgh.encode(float(lat), float(lon), precision=prec) if np.isfinite(lat) and np.isfinite(lon) else '' for lat, lon in zip(lat_pu, lon_pu)]\n",
        "        d[do_col] = [pgh.encode(float(lat), float(lon), precision=prec) if np.isfinite(lat) and np.isfinite(lon) else '' for lat, lon in zip(lat_do, lon_do)]\n",
        "    return d\n",
        "\n",
        "train_use = ensure_days_since_start(train_df).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "test_use = ensure_days_since_start(test_df).copy()\n",
        "train_use = add_geohash(train_use, prec_list=(6,5))\n",
        "test_use = add_geohash(test_use, prec_list=(6,5))\n",
        "\n",
        "# Purified base features: drop absolute time (year, month, day) and raw discrete time (hour, dow); keep cyclical + flags\n",
        "base_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday','days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_use.columns: base_feats.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_use.columns: base_feats.append('dist_x_after_hike')\n",
        "\n",
        "train_ord = train_use.dropna(subset=base_feats + ['fare_amount']).reset_index(drop=True)\n",
        "test_use[base_feats] = test_use[base_feats].fillna(0)\n",
        "\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=5, gap_days=1)\n",
        "assert len(folds) == 5, 'Expected 5 folds'\n",
        "\n",
        "# Smoothed TE (log-target) returning enc/prior/map/counts\n",
        "def te_smooth(train_key, train_tgt, apply_key, m=700.0, prior=None):\n",
        "    if prior is None: prior = float(np.mean(train_tgt))\n",
        "    g = pd.DataFrame({'k': train_key.astype('object'), 'y': train_tgt}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict()\n",
        "    enc = apply_key.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return enc, prior, mp, cnts.to_dict()\n",
        "\n",
        "# Hierarchical blend with hard back-off: if gh6 count < thr, w=0 (use gh5 entirely)\n",
        "def blend_te(gh6_tr, gh5_tr, tgt_tr, gh6_ap, gh5_ap, m6=3000.0, m5=2000.0, alpha=400.0, hard_thr=20):\n",
        "    enc6, prior, mp6, cnt6 = te_smooth(gh6_tr, tgt_tr, gh6_ap, m=m6)\n",
        "    enc5, _,    mp5, cnt5 = te_smooth(gh5_tr, tgt_tr, gh5_ap, m=m5, prior=prior)\n",
        "    ap_keys = gh6_ap.astype('object').values if isinstance(gh6_ap, pd.Series) else gh6_ap.astype('object')\n",
        "    counts = np.array([cnt6.get(k, 0.0) for k in ap_keys], dtype='float32')\n",
        "    w = counts / (counts + np.float32(alpha))\n",
        "    # Hard back-off\n",
        "    if hard_thr is not None and hard_thr > 0:\n",
        "        mask = (counts < np.float32(hard_thr))\n",
        "        if mask.any():\n",
        "            w = w.copy(); w[mask] = 0.0\n",
        "    return (w * enc6 + (1.0 - w) * enc5).astype('float32'), (mp6, mp5), (cnt6, cnt5)\n",
        "\n",
        "# Extremely constrained model\n",
        "lgb_params = dict(\n",
        "    objective='huber', metric='rmse',\n",
        "    learning_rate=0.045, n_estimators=25000,\n",
        "    num_leaves=96, max_depth=-1, min_data_in_leaf=1200,\n",
        "    feature_fraction=0.65, bagging_fraction=0.8, bagging_freq=1,\n",
        "    max_bin=127, reg_alpha=5.0, reg_lambda=60.0, min_gain_to_split=0.1,\n",
        "    random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "fold_test_preds = []\n",
        "\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx].copy(); va = train_ord.iloc[va_idx].copy()\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    prior = float(y_tr.mean())\n",
        "    # Hierarchical TE for PU and DO with strong smoothing/back-off\n",
        "    va_pu_te, (pu_mp6, pu_mp5), (pu_cnt6, pu_cnt5) = blend_te(tr['pu_gh6'], tr['pu_gh5'], y_tr, va['pu_gh6'], va['pu_gh5'], m6=3000.0, m5=2000.0, alpha=400.0, hard_thr=20)\n",
        "    tr_pu_te, _, _ = blend_te(tr['pu_gh6'], tr['pu_gh5'], y_tr, tr['pu_gh6'], tr['pu_gh5'], m6=3000.0, m5=2000.0, alpha=400.0, hard_thr=20)\n",
        "    va_do_te, (do_mp6, do_mp5), (do_cnt6, do_cnt5) = blend_te(tr['do_gh6'], tr['do_gh5'], y_tr, va['do_gh6'], va['do_gh5'], m6=3000.0, m5=2000.0, alpha=400.0, hard_thr=20)\n",
        "    tr_do_te, _, _ = blend_te(tr['do_gh6'], tr['do_gh5'], y_tr, tr['do_gh6'], tr['do_gh5'], m6=3000.0, m5=2000.0, alpha=400.0, hard_thr=20)\n",
        "    tr['te_pu'] = tr_pu_te; va['te_pu'] = va_pu_te\n",
        "    tr['te_do'] = tr_do_te; va['te_do'] = va_do_te\n",
        "    # Frequency: keep only gh6 counts per mandate\n",
        "    pu6_counts = tr['pu_gh6'].astype('object').value_counts().astype('int32')\n",
        "    do6_counts = tr['do_gh6'].astype('object').value_counts().astype('int32')\n",
        "    for d in (tr, va):\n",
        "        d['log_pu6_cnt'] = np.log1p(d['pu_gh6'].astype('object').map(pu6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_do6_cnt'] = np.log1p(d['do_gh6'].astype('object').map(do6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "    use_cols = base_feats + ['te_pu','te_do','log_pu6_cnt','log_do6_cnt']\n",
        "    X_tr = tr[use_cols].astype('float32').values\n",
        "    X_va = va[use_cols].astype('float32').values\n",
        "    # Monotone on days_since_start\n",
        "    mono = [0]*len(use_cols)\n",
        "    if 'days_since_start' in use_cols:\n",
        "        mono[use_cols.index('days_since_start')] = 1\n",
        "    # Exponential decay weights with tau=120 days (more aggressive)\n",
        "    days_tr = tr['days_since_start'].astype('float32').values\n",
        "    max_days = float(np.max(days_tr)) if days_tr.size else 0.0\n",
        "    w_tr = np.exp((days_tr - np.float32(max_days)) / np.float32(120.0)).astype('float32')\n",
        "    model = lgb.LGBMRegressor(**{**lgb_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr, sample_weight=w_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=700, verbose=False), lgb.log_evaluation(period=200)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), y_pred_val))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    print(f\"[GH-Lockdown] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "\n",
        "    # Test-time: apply same hardened logic; gh6/gh5 blend with hard back-off and gh6 counts only\n",
        "    tst = test_use.copy()\n",
        "    enc6_pu_test, _, _, cnt6_pu = te_smooth(tr['pu_gh6'], y_tr, tst['pu_gh6'], m=3000.0, prior=prior)\n",
        "    enc5_pu_test, _, _, cnt5_pu = te_smooth(tr['pu_gh5'], y_tr, tst['pu_gh5'], m=2000.0, prior=prior)\n",
        "    keys_pu = tst['pu_gh6'].astype('object').values\n",
        "    cts_pu = np.array([cnt6_pu.get(k, 0.0) for k in keys_pu], dtype='float32')\n",
        "    w_pu = cts_pu / (cts_pu + np.float32(400.0))\n",
        "    if (cts_pu < 20).any():\n",
        "        w_pu = w_pu.copy(); w_pu[cts_pu < 20] = 0.0\n",
        "    tst['te_pu'] = (w_pu * enc6_pu_test + (1.0 - w_pu) * enc5_pu_test).astype('float32')\n",
        "    enc6_do_test, _, _, cnt6_do = te_smooth(tr['do_gh6'], y_tr, tst['do_gh6'], m=3000.0, prior=prior)\n",
        "    enc5_do_test, _, _, cnt5_do = te_smooth(tr['do_gh5'], y_tr, tst['do_gh5'], m=2000.0, prior=prior)\n",
        "    keys_do = tst['do_gh6'].astype('object').values\n",
        "    cts_do = np.array([cnt6_do.get(k, 0.0) for k in keys_do], dtype='float32')\n",
        "    w_do = cts_do / (cts_do + np.float32(400.0))\n",
        "    if (cts_do < 20).any():\n",
        "        w_do = w_do.copy(); w_do[cts_do < 20] = 0.0\n",
        "    tst['te_do'] = (w_do * enc6_do_test + (1.0 - w_do) * enc5_do_test).astype('float32')\n",
        "    pu6_counts_ap = tr['pu_gh6'].astype('object').value_counts().astype('int32')\n",
        "    do6_counts_ap = tr['do_gh6'].astype('object').value_counts().astype('int32')\n",
        "    tst['log_pu6_cnt'] = np.log1p(tst['pu_gh6'].astype('object').map(pu6_counts_ap).fillna(0).astype('int32')).astype('float32')\n",
        "    tst['log_do6_cnt'] = np.log1p(tst['do_gh6'].astype('object').map(do6_counts_ap).fillna(0).astype('int32')).astype('float32')\n",
        "    X_test_fold = tst[use_cols].astype('float32').values\n",
        "    fold_pred = np.expm1(model.predict(X_test_fold, num_iteration=model.best_iteration_)).astype('float32')\n",
        "    fold_test_preds.append(fold_pred)\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_median = int(np.median(best_iters)) if best_iters else 0\n",
        "print({'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 'median_best_iter': best_iter_median, 'note': 'Final Stability Lockdown: heavy regularization + hardened hierarchical TE + purified time + tau=120 + fold ensembling'})\n",
        "\n",
        "# Fold-ensemble submission\n",
        "test_pred = np.mean(np.vstack(fold_test_preds), axis=0).astype('float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "pred_df = pd.DataFrame({'key': test_use['key'].astype('string'), 'fare_amount': test_pred})\n",
        "sub = sample[['key']].merge(pred_df, on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Final Stability Lockdown) with shape:', sub.shape)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.236976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.236598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.236516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.236475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.236396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.236377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.236359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.236338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.236328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.236319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\tvalid_0's rmse: 0.236313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.236296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2600]\tvalid_0's rmse: 0.23629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2800]\tvalid_0's rmse: 0.236285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.236285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3200]\tvalid_0's rmse: 0.236258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3400]\tvalid_0's rmse: 0.23626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\tvalid_0's rmse: 0.236255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3800]\tvalid_0's rmse: 0.236251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4000]\tvalid_0's rmse: 0.236247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4200]\tvalid_0's rmse: 0.236237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4400]\tvalid_0's rmse: 0.236235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4600]\tvalid_0's rmse: 0.236225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4800]\tvalid_0's rmse: 0.236218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5000]\tvalid_0's rmse: 0.236219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5200]\tvalid_0's rmse: 0.236219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5400]\tvalid_0's rmse: 0.236213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5600]\tvalid_0's rmse: 0.236204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5800]\tvalid_0's rmse: 0.236204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6000]\tvalid_0's rmse: 0.236198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6200]\tvalid_0's rmse: 0.236192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6400]\tvalid_0's rmse: 0.236197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6600]\tvalid_0's rmse: 0.236193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6800]\tvalid_0's rmse: 0.236184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7000]\tvalid_0's rmse: 0.236181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7200]\tvalid_0's rmse: 0.236177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7400]\tvalid_0's rmse: 0.236181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7600]\tvalid_0's rmse: 0.236173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7800]\tvalid_0's rmse: 0.236175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8000]\tvalid_0's rmse: 0.236177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8200]\tvalid_0's rmse: 0.236168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8400]\tvalid_0's rmse: 0.236161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8600]\tvalid_0's rmse: 0.236166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8800]\tvalid_0's rmse: 0.236162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9000]\tvalid_0's rmse: 0.236165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9200]\tvalid_0's rmse: 0.236156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9400]\tvalid_0's rmse: 0.236154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9600]\tvalid_0's rmse: 0.236153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9800]\tvalid_0's rmse: 0.236154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10000]\tvalid_0's rmse: 0.236147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10200]\tvalid_0's rmse: 0.236146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10400]\tvalid_0's rmse: 0.236138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10600]\tvalid_0's rmse: 0.236136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10800]\tvalid_0's rmse: 0.236133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11000]\tvalid_0's rmse: 0.236131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11200]\tvalid_0's rmse: 0.236132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11400]\tvalid_0's rmse: 0.236131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11600]\tvalid_0's rmse: 0.236133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11800]\tvalid_0's rmse: 0.236126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12000]\tvalid_0's rmse: 0.236124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12200]\tvalid_0's rmse: 0.236126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12400]\tvalid_0's rmse: 0.236127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12600]\tvalid_0's rmse: 0.236122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12800]\tvalid_0's rmse: 0.236112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13000]\tvalid_0's rmse: 0.236114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13200]\tvalid_0's rmse: 0.236119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13400]\tvalid_0's rmse: 0.236123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13600]\tvalid_0's rmse: 0.236113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GH-Lockdown] Fold 1: RMSE=3.77008, best_iter=12915 (train_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.191363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.190667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.190578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.190549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.190512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.190467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.190465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.190427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.190408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.190395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\tvalid_0's rmse: 0.190375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.190365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2600]\tvalid_0's rmse: 0.190356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2800]\tvalid_0's rmse: 0.190407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.190395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3200]\tvalid_0's rmse: 0.190384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3400]\tvalid_0's rmse: 0.190367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GH-Lockdown] Fold 2: RMSE=3.18991, best_iter=2743 (train_n=640523, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.218826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.218128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.218036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.21797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.217936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.217916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.217835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.21782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.217809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.217805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\tvalid_0's rmse: 0.217774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.217784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2600]\tvalid_0's rmse: 0.217769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2800]\tvalid_0's rmse: 0.217757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.217745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3200]\tvalid_0's rmse: 0.217749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3400]\tvalid_0's rmse: 0.217751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GH-Lockdown] Fold 3: RMSE=3.82524, best_iter=2701 (train_n=961323, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.206598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.205776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.205622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\tvalid_0's rmse: 0.20557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.205519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.205504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\tvalid_0's rmse: 0.205486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\tvalid_0's rmse: 0.205473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.205451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.205437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\tvalid_0's rmse: 0.205431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.205403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2600]\tvalid_0's rmse: 0.20539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2800]\tvalid_0's rmse: 0.205377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.20537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3200]\tvalid_0's rmse: 0.205363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3400]\tvalid_0's rmse: 0.20536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\tvalid_0's rmse: 0.205358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3800]\tvalid_0's rmse: 0.205349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4000]\tvalid_0's rmse: 0.205347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4200]\tvalid_0's rmse: 0.205341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4400]\tvalid_0's rmse: 0.205335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4600]\tvalid_0's rmse: 0.205333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4800]\tvalid_0's rmse: 0.205327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5000]\tvalid_0's rmse: 0.205321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5200]\tvalid_0's rmse: 0.205316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5400]\tvalid_0's rmse: 0.205316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5600]\tvalid_0's rmse: 0.205317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5800]\tvalid_0's rmse: 0.20531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6000]\tvalid_0's rmse: 0.205299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6200]\tvalid_0's rmse: 0.205295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6400]\tvalid_0's rmse: 0.205287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6600]\tvalid_0's rmse: 0.205279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6800]\tvalid_0's rmse: 0.205281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7000]\tvalid_0's rmse: 0.205271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7200]\tvalid_0's rmse: 0.205269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7400]\tvalid_0's rmse: 0.205262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7600]\tvalid_0's rmse: 0.205256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7800]\tvalid_0's rmse: 0.20525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8000]\tvalid_0's rmse: 0.205249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8200]\tvalid_0's rmse: 0.205256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8400]\tvalid_0's rmse: 0.205252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8600]\tvalid_0's rmse: 0.205246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8800]\tvalid_0's rmse: 0.205242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9000]\tvalid_0's rmse: 0.205241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9200]\tvalid_0's rmse: 0.205239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9400]\tvalid_0's rmse: 0.205236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9600]\tvalid_0's rmse: 0.205236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9800]\tvalid_0's rmse: 0.205234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10000]\tvalid_0's rmse: 0.205231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10200]\tvalid_0's rmse: 0.205226\n"
          ]
        }
      ]
    },
    {
      "id": "05fd9d5d-c5a0-4c02-a5cd-5961de4d46ef",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 1 \u2014 Lockdown v2: ultra-constrained model + hardened hierarchical TE + purified time (no hour/dow), stronger decay, fold ensembling\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'sample' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "try:\n",
        "    import pygeohash as pgh\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pygeohash'])\n",
        "    import pygeohash as pgh\n",
        "\n",
        "def ensure_days_since_start(df):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    d = df.copy(); dt_local = d['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    start_ns = int(dt_local.min().value)\n",
        "    d['days_since_start'] = ((dt_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def add_geohash(df, prec_list=(6,5)):\n",
        "    d = df.copy()\n",
        "    lat_pu = d['pickup_latitude'].astype('float32').values\n",
        "    lon_pu = d['pickup_longitude'].astype('float32').values\n",
        "    lat_do = d['dropoff_latitude'].astype('float32').values\n",
        "    lon_do = d['dropoff_longitude'].astype('float32').values\n",
        "    for prec in prec_list:\n",
        "        pu_col = f'pu_gh{prec}'; do_col = f'do_gh{prec}'\n",
        "        if pu_col in d.columns and do_col in d.columns: continue\n",
        "        d[pu_col] = [pgh.encode(float(lat), float(lon), precision=prec) if np.isfinite(lat) and np.isfinite(lon) else '' for lat, lon in zip(lat_pu, lon_pu)]\n",
        "        d[do_col] = [pgh.encode(float(lat), float(lon), precision=prec) if np.isfinite(lat) and np.isfinite(lon) else '' for lat, lon in zip(lat_do, lon_do)]\n",
        "    return d\n",
        "\n",
        "train_use = ensure_days_since_start(train_df).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "test_use = ensure_days_since_start(test_df).copy()\n",
        "train_use = add_geohash(train_use, prec_list=(6,5))\n",
        "test_use = add_geohash(test_use, prec_list=(6,5))\n",
        "\n",
        "# Purified features: drop raw hour/dow and their cycles; keep only doy cycles + holiday + days_since_start\n",
        "base_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'doy_sin','doy_cos','is_holiday','days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_use.columns: base_feats.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_use.columns: base_feats.append('dist_x_after_hike')\n",
        "\n",
        "train_ord = train_use.dropna(subset=base_feats + ['fare_amount']).reset_index(drop=True)\n",
        "test_use[base_feats] = test_use[base_feats].fillna(0)\n",
        "\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=5, gap_days=1)\n",
        "assert len(folds) == 5, 'Expected 5 folds'\n",
        "\n",
        "# Smoothed TE with counts map\n",
        "def te_smooth(train_key, train_tgt, apply_key, m=700.0, prior=None):\n",
        "    if prior is None: prior = float(np.mean(train_tgt))\n",
        "    g = pd.DataFrame({'k': train_key.astype('object'), 'y': train_tgt}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict()\n",
        "    enc = apply_key.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return enc, prior, mp, cnts.to_dict()\n",
        "\n",
        "# Hierarchical blend with hard back-off and very strong smoothing/back-off\n",
        "def blend_te(gh6_tr, gh5_tr, tgt_tr, gh6_ap, gh5_ap, m6=5000.0, m5=3500.0, alpha=800.0, hard_thr=40):\n",
        "    enc6, prior, mp6, cnt6 = te_smooth(gh6_tr, tgt_tr, gh6_ap, m=m6)\n",
        "    enc5, _,    mp5, cnt5 = te_smooth(gh5_tr, tgt_tr, gh5_ap, m=m5, prior=prior)\n",
        "    ap_keys = gh6_ap.astype('object').values if isinstance(gh6_ap, pd.Series) else gh6_ap.astype('object')\n",
        "    counts = np.array([cnt6.get(k, 0.0) for k in ap_keys], dtype='float32')\n",
        "    w = counts / (counts + np.float32(alpha))\n",
        "    if hard_thr is not None and hard_thr > 0:\n",
        "        mask = (counts < np.float32(hard_thr))\n",
        "        if mask.any():\n",
        "            w = w.copy(); w[mask] = 0.0\n",
        "    return (w * enc6 + (1.0 - w) * enc5).astype('float32'), (mp6, mp5), (cnt6, cnt5)\n",
        "\n",
        "# Ultra-constrained LightGBM\n",
        "lgb_params = dict(\n",
        "    objective='huber', metric='rmse',\n",
        "    learning_rate=0.04, n_estimators=20000,\n",
        "    num_leaves=64, max_depth=-1, min_data_in_leaf=2000,\n",
        "    feature_fraction=0.60, bagging_fraction=0.70, bagging_freq=1,\n",
        "    max_bin=63, reg_alpha=10.0, reg_lambda=80.0, min_gain_to_split=0.2,\n",
        "    random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "fold_test_preds = []\n",
        "\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx].copy(); va = train_ord.iloc[va_idx].copy()\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    prior = float(y_tr.mean())\n",
        "    # Hardened hierarchical TE\n",
        "    va_pu_te, (pu_mp6, pu_mp5), (pu_cnt6, pu_cnt5) = blend_te(tr['pu_gh6'], tr['pu_gh5'], y_tr, va['pu_gh6'], va['pu_gh5'])\n",
        "    tr_pu_te, _, _ = blend_te(tr['pu_gh6'], tr['pu_gh5'], y_tr, tr['pu_gh6'], tr['pu_gh5'])\n",
        "    va_do_te, (do_mp6, do_mp5), (do_cnt6, do_cnt5) = blend_te(tr['do_gh6'], tr['do_gh5'], y_tr, va['do_gh6'], va['do_gh5'])\n",
        "    tr_do_te, _, _ = blend_te(tr['do_gh6'], tr['do_gh5'], y_tr, tr['do_gh6'], tr['do_gh5'])\n",
        "    tr['te_pu'] = tr_pu_te; va['te_pu'] = va_pu_te\n",
        "    tr['te_do'] = tr_do_te; va['te_do'] = va_do_te\n",
        "    # Keep only gh6 counts\n",
        "    pu6_counts = tr['pu_gh6'].astype('object').value_counts().astype('int32')\n",
        "    do6_counts = tr['do_gh6'].astype('object').value_counts().astype('int32')\n",
        "    for d in (tr, va):\n",
        "        d['log_pu6_cnt'] = np.log1p(d['pu_gh6'].astype('object').map(pu6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "        d['log_do6_cnt'] = np.log1p(d['do_gh6'].astype('object').map(do6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "    use_cols = base_feats + ['te_pu','te_do','log_pu6_cnt','log_do6_cnt']\n",
        "    X_tr = tr[use_cols].astype('float32').values\n",
        "    X_va = va[use_cols].astype('float32').values\n",
        "    # Monotone on days_since_start only\n",
        "    mono = [0]*len(use_cols)\n",
        "    if 'days_since_start' in use_cols:\n",
        "        mono[use_cols.index('days_since_start')] = 1\n",
        "    # Stronger exponential decay (tau=90 days)\n",
        "    days_tr = tr['days_since_start'].astype('float32').values\n",
        "    max_days = float(np.max(days_tr)) if days_tr.size else 0.0\n",
        "    w_tr = np.exp((days_tr - np.float32(max_days)) / np.float32(90.0)).astype('float32')\n",
        "    model = lgb.LGBMRegressor(**{**lgb_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr, sample_weight=w_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=600, verbose=False), lgb.log_evaluation(period=300)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), y_pred_val))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    print(f\"[GH-Lockdown v2] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "\n",
        "    # Test features per fold (same hardened logic)\n",
        "    tst = test_use.copy()\n",
        "    enc6_pu_test, _, _, cnt6_pu = te_smooth(tr['pu_gh6'], y_tr, tst['pu_gh6'], m=5000.0, prior=prior)\n",
        "    enc5_pu_test, _, _, cnt5_pu = te_smooth(tr['pu_gh5'], y_tr, tst['pu_gh5'], m=3500.0, prior=prior)\n",
        "    k_pu = tst['pu_gh6'].astype('object').values\n",
        "    c_pu = np.array([cnt6_pu.get(k, 0.0) for k in k_pu], dtype='float32')\n",
        "    w_pu = c_pu / (c_pu + np.float32(800.0))\n",
        "    if (c_pu < 40).any():\n",
        "        w_pu = w_pu.copy(); w_pu[c_pu < 40] = 0.0\n",
        "    tst['te_pu'] = (w_pu * enc6_pu_test + (1.0 - w_pu) * enc5_pu_test).astype('float32')\n",
        "    enc6_do_test, _, _, cnt6_do = te_smooth(tr['do_gh6'], y_tr, tst['do_gh6'], m=5000.0, prior=prior)\n",
        "    enc5_do_test, _, _, cnt5_do = te_smooth(tr['do_gh5'], y_tr, tst['do_gh5'], m=3500.0, prior=prior)\n",
        "    k_do = tst['do_gh6'].astype('object').values\n",
        "    c_do = np.array([cnt6_do.get(k, 0.0) for k in k_do], dtype='float32')\n",
        "    w_do = c_do / (c_do + np.float32(800.0))\n",
        "    if (c_do < 40).any():\n",
        "        w_do = w_do.copy(); w_do[c_do < 40] = 0.0\n",
        "    tst['te_do'] = (w_do * enc6_do_test + (1.0 - w_do) * enc5_do_test).astype('float32')\n",
        "    pu6_counts_ap = tr['pu_gh6'].astype('object').value_counts().astype('int32')\n",
        "    do6_counts_ap = tr['do_gh6'].astype('object').value_counts().astype('int32')\n",
        "    tst['log_pu6_cnt'] = np.log1p(tst['pu_gh6'].astype('object').map(pu6_counts_ap).fillna(0).astype('int32')).astype('float32')\n",
        "    tst['log_do6_cnt'] = np.log1p(tst['do_gh6'].astype('object').map(do6_counts_ap).fillna(0).astype('int32')).astype('float32')\n",
        "    X_test_fold = tst[use_cols].astype('float32').values\n",
        "    fold_pred = np.expm1(model.predict(X_test_fold, num_iteration=model.best_iteration_)).astype('float32')\n",
        "    fold_test_preds.append(fold_pred)\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_median = int(np.median(best_iters)) if best_iters else 0\n",
        "print({'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 'median_best_iter': best_iter_median, 'note': 'Lockdown v2: ultra-constrained + very strong TE smoothing/back-off + no hour/dow cycles + tau=90'})\n",
        "\n",
        "# Fold-ensemble submission\n",
        "test_pred = np.mean(np.vstack(fold_test_preds), axis=0).astype('float32') if fold_test_preds else np.zeros(len(test_use), dtype='float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "pred_df = pd.DataFrame({'key': test_use['key'].astype('string'), 'fare_amount': test_pred})\n",
        "sub = sample[['key']].merge(pred_df, on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Lockdown v2) with shape:', sub.shape)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.243692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.243639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.243587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.243557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.243545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.243539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.24353\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.243532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2700]\tvalid_0's rmse: 0.243531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.24352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3300]\tvalid_0's rmse: 0.243516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\tvalid_0's rmse: 0.243501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3900]\tvalid_0's rmse: 0.243501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4200]\tvalid_0's rmse: 0.243503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4500]\tvalid_0's rmse: 0.2435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4800]\tvalid_0's rmse: 0.243491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5100]\tvalid_0's rmse: 0.243493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5400]\tvalid_0's rmse: 0.243491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5700]\tvalid_0's rmse: 0.243477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6000]\tvalid_0's rmse: 0.243472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6300]\tvalid_0's rmse: 0.243468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6600]\tvalid_0's rmse: 0.243465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6900]\tvalid_0's rmse: 0.243468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GH-Lockdown v2] Fold 1: RMSE=3.87440, best_iter=6370 (train_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.200298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.200209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.200174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.200134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.200106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.200066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.20005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.200032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2700]\tvalid_0's rmse: 0.200032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.200008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3300]\tvalid_0's rmse: 0.200012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\tvalid_0's rmse: 0.199994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3900]\tvalid_0's rmse: 0.199998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4200]\tvalid_0's rmse: 0.19998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4500]\tvalid_0's rmse: 0.199962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4800]\tvalid_0's rmse: 0.199952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5100]\tvalid_0's rmse: 0.199961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GH-Lockdown v2] Fold 2: RMSE=3.28725, best_iter=4581 (train_n=640523, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.229891\n"
          ]
        }
      ]
    },
    {
      "id": "af462c26-f1ae-4389-8af7-b13a167b7925",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 2 \u2014 Two-Stage Hybrid Residual Model (Drift-Modeled) with rolling window + hardened residual TE + extreme regularization\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'sample' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "try:\n",
        "    import pygeohash as pgh\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pygeohash'])\n",
        "    import pygeohash as pgh\n",
        "\n",
        "# Helpers\n",
        "def to_local(dt):\n",
        "    return dt.dt.tz_convert('America/New_York')\n",
        "\n",
        "def ensure_days_since_start(df):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    d = df.copy(); dt_local = to_local(d['pickup_datetime'])\n",
        "    start_ns = int(dt_local.min().value)\n",
        "    d['days_since_start'] = ((dt_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def ensure_policy_feats(df):\n",
        "    if all(c in df.columns for c in ['after_hike']): return df\n",
        "    d = df.copy(); dt_local = to_local(d['pickup_datetime'])\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    d['after_hike'] = (dt_local >= cutoff).astype('int8')\n",
        "    return d\n",
        "\n",
        "def ensure_holiday(df):\n",
        "    if 'is_holiday' in df.columns: return df\n",
        "    from pandas.tseries.holiday import USFederalHolidayCalendar as USCal\n",
        "    d = df.copy(); dt_local = to_local(d['pickup_datetime'])\n",
        "    hol = USCal().holidays(start=dt_local.min().normalize().tz_localize(None), end=dt_local.max().normalize().tz_localize(None))\n",
        "    hol_dates = set(pd.to_datetime(hol).date)\n",
        "    d['is_holiday'] = dt_local.dt.date.map(lambda x: x in hol_dates).astype('int8')\n",
        "    return d\n",
        "\n",
        "def add_geohash(df, prec_list=(6,5)):\n",
        "    d = df.copy()\n",
        "    lat_pu = d['pickup_latitude'].astype('float32').values\n",
        "    lon_pu = d['pickup_longitude'].astype('float32').values\n",
        "    lat_do = d['dropoff_latitude'].astype('float32').values\n",
        "    lon_do = d['dropoff_longitude'].astype('float32').values\n",
        "    for prec in prec_list:\n",
        "        pu_col = f'pu_gh{prec}'; do_col = f'do_gh{prec}'\n",
        "        if pu_col in d.columns and do_col in d.columns: continue\n",
        "        d[pu_col] = [pgh.encode(float(la), float(lo), precision=prec) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(lat_pu, lon_pu)]\n",
        "        d[do_col] = [pgh.encode(float(la), float(lo), precision=prec) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(lat_do, lon_do)]\n",
        "    return d\n",
        "\n",
        "# Residual TE helpers (OOF-safe inside each fold)\n",
        "def te_smooth(train_key, train_tgt, apply_key, m=700.0, prior=None):\n",
        "    if prior is None: prior = float(np.mean(train_tgt))\n",
        "    g = pd.DataFrame({'k': train_key.astype('object'), 'y': train_tgt}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict()\n",
        "    enc = apply_key.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return enc, prior, mp, cnts.to_dict()\n",
        "\n",
        "def blend_te(gh6_tr, gh5_tr, tgt_tr, gh6_ap, gh5_ap, m6=8000.0, m5=5000.0, alpha=1200.0, hard_thr=100):\n",
        "    enc6, prior, mp6, cnt6 = te_smooth(gh6_tr, tgt_tr, gh6_ap, m=m6)\n",
        "    enc5, _,    mp5, cnt5 = te_smooth(gh5_tr, tgt_tr, gh5_ap, m=m5, prior=prior)\n",
        "    ap_keys = gh6_ap.astype('object').values if isinstance(gh6_ap, pd.Series) else gh6_ap.astype('object')\n",
        "    counts = np.array([cnt6.get(k, 0.0) for k in ap_keys], dtype='float32')\n",
        "    w = counts / (counts + np.float32(alpha))\n",
        "    if hard_thr is not None and hard_thr > 0:\n",
        "        mask = (counts < np.float32(hard_thr))\n",
        "        if mask.any():\n",
        "            w = w.copy(); w[mask] = 0.0\n",
        "    return (w * enc6 + (1.0 - w) * enc5).astype('float32'), (mp6, mp5), (cnt6, cnt5)\n",
        "\n",
        "# Prepare data\n",
        "train_ord = ensure_holiday(ensure_policy_feats(ensure_days_since_start(train_df.copy()))).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "test_prep = ensure_holiday(ensure_policy_feats(ensure_days_since_start(test_df.copy())))\n",
        "train_ord = add_geohash(train_ord, (6,5)); test_prep = add_geohash(test_prep, (6,5))\n",
        "\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=5, gap_days=1)\n",
        "assert len(folds) == 5, 'Expected 5 folds'\n",
        "\n",
        "# Stage 1 Trend model (small, regularized; categorical discrete features)\n",
        "s1_params = dict(objective='regression', metric='rmse',\n",
        "                 learning_rate=0.05, n_estimators=4000,\n",
        "                 num_leaves=16, min_data_in_leaf=2000,\n",
        "                 feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
        "                 max_bin=127, reg_lambda=50.0, random_state=2025, n_jobs=-1, verbose=-1)\n",
        "\n",
        "# Stage 2 Residual model (ultra-constrained)\n",
        "s2_params = dict(objective='huber', metric='rmse',\n",
        "                 learning_rate=0.045, n_estimators=28000,\n",
        "                 num_leaves=48, min_data_in_leaf=3000,\n",
        "                 feature_fraction=0.60, bagging_fraction=0.75, bagging_freq=1,\n",
        "                 max_bin=31, reg_alpha=15.0, reg_lambda=100.0, min_gain_to_split=0.5,\n",
        "                 random_state=2026, n_jobs=-1, verbose=-1)\n",
        "\n",
        "# Features\n",
        "trend_feats = ['days_since_start','year','month','is_holiday','after_hike']\n",
        "for c in trend_feats: assert c in train_ord.columns, f'Missing {c} for Stage1'\n",
        "# Stage 2 base (purified): remove absolute time identifiers; keep geometry + hour cycle + is_holiday\n",
        "base2 = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'hour_sin','hour_cos','is_holiday',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_ord.columns: base2.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_ord.columns: base2.append('dist_x_after_hike')\n",
        "\n",
        "cv_rmses, s2_best_iters = [], []\n",
        "fold_preds = []\n",
        "\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx].copy(); va = train_ord.iloc[va_idx].copy()\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    # Stage 1: categorical discrete time for year/month/is_holiday/after_hike\n",
        "    X1_tr = tr[trend_feats].copy(); X1_va = va[trend_feats].copy()\n",
        "    for c in ['year','month','is_holiday','after_hike']:\n",
        "        if c in X1_tr.columns:\n",
        "            X1_tr[c] = X1_tr[c].astype('category'); X1_va[c] = X1_va[c].astype('category')\n",
        "    s1 = lgb.LGBMRegressor(**s1_params)\n",
        "    s1.fit(X1_tr, y_tr, eval_set=[(X1_va, y_va)], eval_metric='rmse', categorical_feature=['year','month','is_holiday','after_hike'],\n",
        "           callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False), lgb.log_evaluation(period=200)])\n",
        "    # Capture training categories for categorical features to reuse in predictions\n",
        "    cat_categories = {c: X1_tr[c].cat.categories for c in ['year','month','is_holiday','after_hike'] if c in X1_tr.columns}\n",
        "    tr_trend = s1.predict(X1_tr, num_iteration=s1.best_iteration_).astype('float32')\n",
        "    va_trend = s1.predict(X1_va, num_iteration=s1.best_iteration_).astype('float32')\n",
        "    y_tr_res = (y_tr - tr_trend).astype('float32'); y_va_res = (y_va - va_trend).astype('float32')\n",
        "\n",
        "    # Stage 2 rolling window (480 days) within train fold\n",
        "    dmax = float(tr['days_since_start'].max()) if len(tr) else 0.0\n",
        "    tr_win = tr.loc[tr['days_since_start'] >= (dmax - 480.0)].reset_index(drop=True).copy()\n",
        "    # Align categorical dtypes for Stage1 prediction on tr_win to match training categories\n",
        "    X1_trwin = tr_win[trend_feats].copy()\n",
        "    for c in ['year','month','is_holiday','after_hike']:\n",
        "        if c in X1_trwin.columns:\n",
        "            X1_trwin[c] = X1_trwin[c].astype('category')\n",
        "            if c in cat_categories:\n",
        "                X1_trwin[c] = X1_trwin[c].cat.set_categories(cat_categories[c])\n",
        "    y_tr_res_win = (np.log1p(tr_win['fare_amount'].astype('float32').values) - s1.predict(X1_trwin, num_iteration=s1.best_iteration_).astype('float32'))\n",
        "\n",
        "    # Residual TE on windowed train (gh6 -> gh5) for PU and DO\n",
        "    # Ensure geohash cols exist\n",
        "    assert all(c in tr_win.columns for c in ['pu_gh6','pu_gh5','do_gh6','do_gh5']), 'Geohash cols missing'\n",
        "    te_pu_va, (pu_mp6, pu_mp5), (pu_cnt6, pu_cnt5) = blend_te(tr_win['pu_gh6'], tr_win['pu_gh5'], y_tr_res_win, va['pu_gh6'], va['pu_gh5'],\n",
        "                                                             m6=8000.0, m5=5000.0, alpha=1200.0, hard_thr=100)\n",
        "    te_do_va, (do_mp6, do_mp5), (do_cnt6, do_cnt5) = blend_te(tr_win['do_gh6'], tr_win['do_gh5'], y_tr_res_win, va['do_gh6'], va['do_gh5'],\n",
        "                                                             m6=8000.0, m5=5000.0, alpha=1200.0, hard_thr=100)\n",
        "    # Train encodings for Stage 2 on window (train side)\n",
        "    te_pu_tr, _, _ = blend_te(tr_win['pu_gh6'], tr_win['pu_gh5'], y_tr_res_win, tr_win['pu_gh6'], tr_win['pu_gh5'],\n",
        "                              m6=8000.0, m5=5000.0, alpha=1200.0, hard_thr=100)\n",
        "    te_do_tr, _, _ = blend_te(tr_win['do_gh6'], tr_win['do_gh5'], y_tr_res_win, tr_win['do_gh6'], tr_win['do_gh5'],\n",
        "                              m6=8000.0, m5=5000.0, alpha=1200.0, hard_thr=100)\n",
        "    tr_win['te_pu'] = te_pu_tr; tr_win['te_do'] = te_do_tr\n",
        "    va = va.copy(); va['te_pu'] = te_pu_va; va['te_do'] = te_do_va\n",
        "    # Counts (gh6 only)\n",
        "    pu6_counts = tr_win['pu_gh6'].astype('object').value_counts().astype('int32')\n",
        "    do6_counts = tr_win['do_gh6'].astype('object').value_counts().astype('int32')\n",
        "    tr_win['log_pu6_cnt'] = np.log1p(tr_win['pu_gh6'].astype('object').map(pu6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "    tr_win['log_do6_cnt'] = np.log1p(tr_win['do_gh6'].astype('object').map(do6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "    va['log_pu6_cnt'] = np.log1p(va['pu_gh6'].astype('object').map(pu6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "    va['log_do6_cnt'] = np.log1p(va['do_gh6'].astype('object').map(do6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "\n",
        "    use_cols2 = base2 + ['te_pu','te_do','log_pu6_cnt','log_do6_cnt']\n",
        "    X2_tr = tr_win[use_cols2].astype('float32').values\n",
        "    X2_va = va[use_cols2].astype('float32').values\n",
        "    s2 = lgb.LGBMRegressor(**s2_params)\n",
        "    s2.fit(X2_tr, y_tr_res_win, eval_set=[(X2_va, y_va_res)], eval_metric='rmse',\n",
        "           callbacks=[lgb.early_stopping(stopping_rounds=900, verbose=False), lgb.log_evaluation(period=300)])\n",
        "    s2_best = int(s2.best_iteration_ or s2_params['n_estimators']); s2_best_iters.append(s2_best)\n",
        "    va_res_pred = s2.predict(X2_va, num_iteration=s2.best_iteration_).astype('float32')\n",
        "    va_final_log = (va_trend + va_res_pred).astype('float32')\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), np.expm1(va_final_log)))\n",
        "    cv_rmses.append(rmse)\n",
        "    print(f\"[TwoStage-Hybrid] Fold {i}: RMSE={rmse:.5f}, s2_best_iter={s2_best} (train_win_n={len(tr_win)}, val_n={len(va_idx)})\")\n",
        "\n",
        "    # Test-time per fold: trend + residual\n",
        "    X1_test = test_prep[trend_feats].copy()\n",
        "    for c in ['year','month','is_holiday','after_hike']:\n",
        "        if c in X1_test.columns:\n",
        "            X1_test[c] = X1_test[c].astype('category')\n",
        "            if c in cat_categories:\n",
        "                X1_test[c] = X1_test[c].cat.set_categories(cat_categories[c])\n",
        "    test_trend = s1.predict(X1_test, num_iteration=s1.best_iteration_).astype('float32')\n",
        "    tst = test_prep.copy()\n",
        "    enc6_pu_test, _, _, cnt6_pu = te_smooth(tr_win['pu_gh6'], y_tr_res_win, tst['pu_gh6'], m=8000.0, prior=None)\n",
        "    enc5_pu_test, _, _, cnt5_pu = te_smooth(tr_win['pu_gh5'], y_tr_res_win, tst['pu_gh5'], m=5000.0, prior=None)\n",
        "    k_pu = tst['pu_gh6'].astype('object').values\n",
        "    c_pu = np.array([cnt6_pu.get(k, 0.0) for k in k_pu], dtype='float32')\n",
        "    w_pu = c_pu / (c_pu + np.float32(1200.0))\n",
        "    if (c_pu < 100).any():\n",
        "        w_pu = w_pu.copy(); w_pu[c_pu < 100] = 0.0\n",
        "    tst['te_pu'] = (w_pu * enc6_pu_test + (1.0 - w_pu) * enc5_pu_test).astype('float32')\n",
        "    enc6_do_test, _, _, cnt6_do = te_smooth(tr_win['do_gh6'], y_tr_res_win, tst['do_gh6'], m=8000.0, prior=None)\n",
        "    enc5_do_test, _, _, cnt5_do = te_smooth(tr_win['do_gh5'], y_tr_res_win, tst['do_gh5'], m=5000.0, prior=None)\n",
        "    k_do = tst['do_gh6'].astype('object').values\n",
        "    c_do = np.array([cnt6_do.get(k, 0.0) for k in k_do], dtype='float32')\n",
        "    w_do = c_do / (c_do + np.float32(1200.0))\n",
        "    if (c_do < 100).any():\n",
        "        w_do = w_do.copy(); w_do[c_do < 100] = 0.0\n",
        "    tst['te_do'] = (w_do * enc6_do_test + (1.0 - w_do) * enc5_do_test).astype('float32')\n",
        "    # Counts\n",
        "    pu6_counts_ap = tr_win['pu_gh6'].astype('object').value_counts().astype('int32')\n",
        "    do6_counts_ap = tr_win['do_gh6'].astype('object').value_counts().astype('int32')\n",
        "    tst['log_pu6_cnt'] = np.log1p(tst['pu_gh6'].astype('object').map(pu6_counts_ap).fillna(0).astype('int32')).astype('float32')\n",
        "    tst['log_do6_cnt'] = np.log1p(tst['do_gh6'].astype('object').map(do6_counts_ap).fillna(0).astype('int32')).astype('float32')\n",
        "    X2_test = tst[use_cols2].astype('float32').values\n",
        "    test_res = s2.predict(X2_test, num_iteration=s2.best_iteration_).astype('float32')\n",
        "    fold_pred = np.expm1(test_trend + test_res).astype('float32')\n",
        "    fold_preds.append(fold_pred)\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_median = int(np.median(s2_best_iters)) if s2_best_iters else 0\n",
        "print({'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 's2_best_iter_median': best_iter_median, 'note': 'Two-Stage Hybrid: Stage1 trend + Stage2 residual (480-day window) with hardened gh6->gh5 TE'})\n",
        "\n",
        "# Fold-ensemble submission\n",
        "test_pred = np.mean(np.vstack(fold_preds), axis=0).astype('float32') if fold_preds else np.zeros(len(test_prep), dtype='float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "sub = sample[['key']].merge(pd.DataFrame({'key': test_prep['key'].astype('string'), 'fare_amount': test_pred}), on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Two-Stage Hybrid) with shape:', sub.shape)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.509142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.509064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.240162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.24007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.240038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.240024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.240023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.240024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.240021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.239997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2700]\tvalid_0's rmse: 0.239998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.239997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3300]\tvalid_0's rmse: 0.239985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\tvalid_0's rmse: 0.239993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3900]\tvalid_0's rmse: 0.239975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4200]\tvalid_0's rmse: 0.239976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4500]\tvalid_0's rmse: 0.239971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4800]\tvalid_0's rmse: 0.239971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5100]\tvalid_0's rmse: 0.239971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5400]\tvalid_0's rmse: 0.239969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TwoStage-Hybrid] Fold 1: RMSE=3.84993, s2_best_iter=4628 (train_win_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.516681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\tvalid_0's rmse: 0.516727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.186038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.185898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.185865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.185813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.185786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.185763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.185747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.185731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2700]\tvalid_0's rmse: 0.185711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.185706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3300]\tvalid_0's rmse: 0.185697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\tvalid_0's rmse: 0.185688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3900]\tvalid_0's rmse: 0.185673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4200]\tvalid_0's rmse: 0.185656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4500]\tvalid_0's rmse: 0.185635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4800]\tvalid_0's rmse: 0.185636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5100]\tvalid_0's rmse: 0.185639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5400]\tvalid_0's rmse: 0.185637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5700]\tvalid_0's rmse: 0.18563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6000]\tvalid_0's rmse: 0.185624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6300]\tvalid_0's rmse: 0.185614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6600]\tvalid_0's rmse: 0.185613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6900]\tvalid_0's rmse: 0.185609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7200]\tvalid_0's rmse: 0.185605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7500]\tvalid_0's rmse: 0.1856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7800]\tvalid_0's rmse: 0.185591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8100]\tvalid_0's rmse: 0.185591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8400]\tvalid_0's rmse: 0.185588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8700]\tvalid_0's rmse: 0.185587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9000]\tvalid_0's rmse: 0.185585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9300]\tvalid_0's rmse: 0.185577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9600]\tvalid_0's rmse: 0.185574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9900]\tvalid_0's rmse: 0.185571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10200]\tvalid_0's rmse: 0.185583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10500]\tvalid_0's rmse: 0.185562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10800]\tvalid_0's rmse: 0.185564\n"
          ]
        }
      ]
    },
    {
      "id": "6c56fb5d-f074-45a9-9351-d0975c05b5b1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 3 \u2014 Time-Aware Single Model with Hierarchical Temporal TE (gh6/gh5 \u00d7 year/month) + fold ensembling\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'sample' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "try:\n",
        "    import pygeohash as pgh\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pygeohash'])\n",
        "    import pygeohash as pgh\n",
        "\n",
        "# Helpers\n",
        "def to_local(dt):\n",
        "    return dt.dt.tz_convert('America/New_York')\n",
        "\n",
        "def ensure_days_since_start(df):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    d = df.copy(); dt_local = to_local(d['pickup_datetime'])\n",
        "    start_ns = int(dt_local.min().value)\n",
        "    d['days_since_start'] = ((dt_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def add_geohash(df, prec_list=(6,5)):\n",
        "    d = df.copy()\n",
        "    lat_pu = d['pickup_latitude'].astype('float32').values\n",
        "    lon_pu = d['pickup_longitude'].astype('float32').values\n",
        "    lat_do = d['dropoff_latitude'].astype('float32').values\n",
        "    lon_do = d['dropoff_longitude'].astype('float32').values\n",
        "    for prec in prec_list:\n",
        "        pu_col = f'pu_gh{prec}'; do_col = f'do_gh{prec}'\n",
        "        if pu_col in d.columns and do_col in d.columns: continue\n",
        "        d[pu_col] = [pgh.encode(float(la), float(lo), precision=prec) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(lat_pu, lon_pu)]\n",
        "        d[do_col] = [pgh.encode(float(la), float(lo), precision=prec) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(lat_do, lon_do)]\n",
        "    return d\n",
        "\n",
        "def ensure_time_parts(df):\n",
        "    d = df.copy(); dt_local = to_local(d['pickup_datetime'])\n",
        "    d['year'] = dt_local.dt.year.astype('int16'); d['month'] = dt_local.dt.month.astype('int8')\n",
        "    d['day'] = dt_local.dt.day.astype('int8'); d['hour'] = dt_local.dt.hour.astype('int8'); d['dow'] = dt_local.dt.dayofweek.astype('int8')\n",
        "    d['hour_sin'] = np.sin(2*np.pi*d['hour']/24).astype('float32'); d['hour_cos'] = np.cos(2*np.pi*d['hour']/24).astype('float32')\n",
        "    d['dow_sin'] = np.sin(2*np.pi*d['dow']/7).astype('float32'); d['dow_cos'] = np.cos(2*np.pi*d['dow']/7).astype('float32')\n",
        "    d['doy_sin'] = np.sin(2*np.pi*(dt_local.dt.dayofyear.astype('int16')/365.25)).astype('float32')\n",
        "    d['doy_cos'] = np.cos(2*np.pi*(dt_local.dt.dayofyear.astype('int16')/365.25)).astype('float32')\n",
        "    d['is_weekend'] = d['dow'].isin([5,6]).astype('int8')\n",
        "    d['is_rush'] = ((d['hour'].between(7,10)) | (d['hour'].between(16,19))).astype('int8')\n",
        "    d['is_night'] = ((d['hour'] >= 22) | (d['hour'] <= 5)).astype('int8')\n",
        "    return d\n",
        "\n",
        "def make_timeaware_keys(df):\n",
        "    d = df.copy()\n",
        "    # string concat keys\n",
        "    d['k_pu_l1'] = (d['pu_gh6'].astype('string') + '_' + d['year'].astype('string') + '_' + d['month'].astype('string'))\n",
        "    d['k_pu_l2'] = (d['pu_gh6'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    d['k_pu_l3'] = (d['pu_gh5'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    d['k_do_l1'] = (d['do_gh6'].astype('string') + '_' + d['year'].astype('string') + '_' + d['month'].astype('string'))\n",
        "    d['k_do_l2'] = (d['do_gh6'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    d['k_do_l3'] = (d['do_gh5'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    return d\n",
        "\n",
        "# TE helper with smoothing; returns enc vec and count map\n",
        "def te_smooth(train_key, train_tgt, apply_key, m=500.0, prior=None):\n",
        "    if prior is None: prior = float(np.mean(train_tgt))\n",
        "    g = pd.DataFrame({'k': train_key.astype('object'), 'y': train_tgt}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict(); cnt_map = cnts.to_dict()\n",
        "    enc = apply_key.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return enc, mp, cnt_map, prior\n",
        "\n",
        "def blend_three_levels(k1_tr, k2_tr, k3_tr, y_tr, k1_ap, k2_ap, k3_ap, m=500.0, alpha=200.0, hard_thr=50):\n",
        "    # Level 1 (fine), Level 2 (medium), Level 3 (coarse)\n",
        "    enc1, mp1, cnt1, prior = te_smooth(k1_tr, y_tr, k1_ap, m=m, prior=None)\n",
        "    enc2, mp2, cnt2, _ = te_smooth(k2_tr, y_tr, k2_ap, m=m, prior=prior)\n",
        "    enc3, mp3, cnt3, _ = te_smooth(k3_tr, y_tr, k3_ap, m=m, prior=prior)\n",
        "    # weights\n",
        "    ap_keys1 = k1_ap.astype('object').values\n",
        "    ap_keys2 = k2_ap.astype('object').values\n",
        "    c1 = np.array([cnt1.get(k, 0.0) for k in ap_keys1], dtype='float32')\n",
        "    c2 = np.array([cnt2.get(k, 0.0) for k in ap_keys2], dtype='float32')\n",
        "    w1 = c1 / (c1 + np.float32(alpha))\n",
        "    w2 = c2 / (c2 + np.float32(alpha))\n",
        "    if hard_thr is not None and hard_thr > 0:\n",
        "        m1 = (c1 < np.float32(hard_thr)); m2 = (c2 < np.float32(hard_thr))\n",
        "        if m1.any(): w1 = w1.copy(); w1[m1] = 0.0\n",
        "        if m2.any(): w2 = w2.copy(); w2[m2] = 0.0\n",
        "    enc12 = (w1 * enc1 + (1.0 - w1) * enc2).astype('float32')\n",
        "    enc = (w2 * enc12 + (1.0 - w2) * enc3).astype('float32')\n",
        "    return enc\n",
        "\n",
        "# Prepare data\n",
        "train_ord = ensure_days_since_start(train_df.copy()).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "test_prep = ensure_days_since_start(test_df.copy())\n",
        "train_ord = ensure_time_parts(train_ord); test_prep = ensure_time_parts(test_prep)\n",
        "train_ord = add_geohash(train_ord, (6,5)); test_prep = add_geohash(test_prep, (6,5))\n",
        "train_ord = make_timeaware_keys(train_ord); test_prep = make_timeaware_keys(test_prep)\n",
        "\n",
        "# Base robust features; drop raw year/month/day from final model (used only in TE keys) per mandate\n",
        "base_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'hour_sin','hour_cos','dow_sin','dow_cos','doy_sin','doy_cos','is_holiday','is_weekend','is_rush','is_night',\n",
        "    'days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_ord.columns: base_feats.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_ord.columns: base_feats.append('dist_x_after_hike')\n",
        "\n",
        "train_ord = train_ord.dropna(subset=base_feats + ['fare_amount']).reset_index(drop=True)\n",
        "test_prep[base_feats] = test_prep[base_feats].fillna(0)\n",
        "\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "K = 5\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=K, gap_days=1)\n",
        "assert len(folds) == K, f'Expected {K} folds, got {len(folds)}'\n",
        "\n",
        "# Model params (moderately regularized) and monotone +1 on days_since_start\n",
        "lgb_params = dict(\n",
        "    objective='huber', metric='rmse',\n",
        "    learning_rate=0.045, n_estimators=24000,\n",
        "    num_leaves=128, max_depth=-1, min_data_in_leaf=800,\n",
        "    feature_fraction=0.75, bagging_fraction=0.8, bagging_freq=1,\n",
        "    max_bin=127, reg_alpha=2.0, reg_lambda=30.0,\n",
        "    random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "fold_test_preds = []\n",
        "rng_master = np.random.default_rng(2026)\n",
        "\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx].copy(); va = train_ord.iloc[va_idx].copy()\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    # Time-aware TE for PU\n",
        "    te_pu_tr = blend_three_levels(tr['k_pu_l1'], tr['k_pu_l2'], tr['k_pu_l3'], y_tr, tr['k_pu_l1'], tr['k_pu_l2'], tr['k_pu_l3'], m=500.0, alpha=200.0, hard_thr=50)\n",
        "    te_pu_va = blend_three_levels(tr['k_pu_l1'], tr['k_pu_l2'], tr['k_pu_l3'], y_tr, va['k_pu_l1'], va['k_pu_l2'], va['k_pu_l3'], m=500.0, alpha=200.0, hard_thr=50)\n",
        "    # Time-aware TE for DO\n",
        "    te_do_tr = blend_three_levels(tr['k_do_l1'], tr['k_do_l2'], tr['k_do_l3'], y_tr, tr['k_do_l1'], tr['k_do_l2'], tr['k_do_l3'], m=500.0, alpha=200.0, hard_thr=50)\n",
        "    te_do_va = blend_three_levels(tr['k_do_l1'], tr['k_do_l2'], tr['k_do_l3'], y_tr, va['k_do_l1'], va['k_do_l2'], va['k_do_l3'], m=500.0, alpha=200.0, hard_thr=50)\n",
        "    # Add tiny Gaussian noise to TE (stability)\n",
        "    seed_fold = 1000 + i\n",
        "    rng = np.random.default_rng(seed_fold)\n",
        "    tr['te_pu'] = (te_pu_tr + np.float32(1e-4) * rng.standard_normal(len(tr))).astype('float32')\n",
        "    va['te_pu'] = (te_pu_va + np.float32(1e-4) * rng.standard_normal(len(va))).astype('float32')\n",
        "    tr['te_do'] = (te_do_tr + np.float32(1e-4) * rng.standard_normal(len(tr))).astype('float32')\n",
        "    va['te_do'] = (te_do_va + np.float32(1e-4) * rng.standard_normal(len(va))).astype('float32')\n",
        "    use_cols = base_feats + ['te_pu','te_do']\n",
        "    X_tr = tr[use_cols].astype('float32').values\n",
        "    X_va = va[use_cols].astype('float32').values\n",
        "    mono = [0]*len(use_cols)\n",
        "    if 'days_since_start' in use_cols:\n",
        "        mono[use_cols.index('days_since_start')] = 1\n",
        "    model = lgb.LGBMRegressor(**{**lgb_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=700, verbose=False), lgb.log_evaluation(period=250)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), y_pred_val))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    print(f\"[TimeAware-Single] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "\n",
        "    # Test-time TE with same time-aware keys computed from test and train-only maps\n",
        "    tst = test_prep.copy()\n",
        "    te_pu_test = blend_three_levels(tr['k_pu_l1'], tr['k_pu_l2'], tr['k_pu_l3'], y_tr, tst['k_pu_l1'], tst['k_pu_l2'], tst['k_pu_l3'], m=500.0, alpha=200.0, hard_thr=50)\n",
        "    te_do_test = blend_three_levels(tr['k_do_l1'], tr['k_do_l2'], tr['k_do_l3'], y_tr, tst['k_do_l1'], tst['k_do_l2'], tst['k_do_l3'], m=500.0, alpha=200.0, hard_thr=50)\n",
        "    tst['te_pu'] = te_pu_test.astype('float32'); tst['te_do'] = te_do_test.astype('float32')\n",
        "    X_test_fold = tst[use_cols].astype('float32').values\n",
        "    fold_pred = np.expm1(model.predict(X_test_fold, num_iteration=model.best_iteration_)).astype('float32')\n",
        "    fold_test_preds.append(fold_pred)\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_median = int(np.median(best_iters)) if best_iters else 0\n",
        "print({'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 'median_best_iter': best_iter_median, 'note': 'Time-aware TE (gh6/gh5 \u00d7 year/month) + huber + monotone(time) + fold ensembling'})\n",
        "\n",
        "# Fold-ensemble predictions\n",
        "test_pred = np.mean(np.vstack(fold_test_preds), axis=0).astype('float32') if fold_test_preds else np.zeros(len(test_prep), dtype='float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "sub = sample[['key']].merge(pd.DataFrame({'key': test_prep['key'].astype('string'), 'fare_amount': test_pred}), on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Time-Aware Single Model) with shape:', sub.shape)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.23419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.232584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.231963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.231703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1250]\tvalid_0's rmse: 0.231627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.231631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1750]\tvalid_0's rmse: 0.231614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TimeAware-Single] Fold 1: RMSE=3.68843, best_iter=1175 (train_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.178706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.176968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.176705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.176535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1250]\tvalid_0's rmse: 0.176586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.1766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TimeAware-Single] Fold 2: RMSE=2.96820, best_iter=991 (train_n=640523, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.212025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.21023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.21026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.210467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TimeAware-Single] Fold 3: RMSE=3.82885, best_iter=545 (train_n=961323, val_n=320756)\n"
          ]
        }
      ]
    },
    {
      "id": "edaa0e97-7fb9-447d-890b-fbe3fa4baa69",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 15 \u2014 Maximalist Hardening: Time-Aware Single Model with Aggressive Hierarchical TE + Reliability + Lockdown Regularization\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'sample' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "try:\n",
        "    import pygeohash as pgh\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pygeohash'])\n",
        "    import pygeohash as pgh\n",
        "\n",
        "# Helpers\n",
        "def to_local(dt):\n",
        "    return dt.dt.tz_convert('America/New_York')\n",
        "\n",
        "def ensure_days_since_start(df):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    d = df.copy(); dt_local = to_local(d['pickup_datetime'])\n",
        "    start_ns = int(dt_local.min().value)\n",
        "    d['days_since_start'] = ((dt_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def ensure_time_parts(df):\n",
        "    d = df.copy(); dt = to_local(d['pickup_datetime'])\n",
        "    d['year'] = dt.dt.year.astype('int16'); d['month'] = dt.dt.month.astype('int8')\n",
        "    return d\n",
        "\n",
        "def add_geohash(df, prec_list=(6,5)):\n",
        "    d = df.copy()\n",
        "    lat_pu = d['pickup_latitude'].astype('float32').values\n",
        "    lon_pu = d['pickup_longitude'].astype('float32').values\n",
        "    lat_do = d['dropoff_latitude'].astype('float32').values\n",
        "    lon_do = d['dropoff_longitude'].astype('float32').values\n",
        "    for prec in prec_list:\n",
        "        pu_col = f'pu_gh{prec}'; do_col = f'do_gh{prec}'\n",
        "        if pu_col in d.columns and do_col in d.columns: continue\n",
        "        d[pu_col] = [pgh.encode(float(la), float(lo), precision=prec) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(lat_pu, lon_pu)]\n",
        "        d[do_col] = [pgh.encode(float(la), float(lo), precision=prec) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(lat_do, lon_do)]\n",
        "    return d\n",
        "\n",
        "def build_timeaware_keys(df):\n",
        "    d = df.copy()\n",
        "    # PU keys\n",
        "    d['k_pu_ym6'] = (d['pu_gh6'].astype('string') + '_' + d['year'].astype('string') + '_' + d['month'].astype('string'))\n",
        "    d['k_pu_y6']  = (d['pu_gh6'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    d['k_pu_y5']  = (d['pu_gh5'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    # DO keys\n",
        "    d['k_do_ym6'] = (d['do_gh6'].astype('string') + '_' + d['year'].astype('string') + '_' + d['month'].astype('string'))\n",
        "    d['k_do_y6']  = (d['do_gh6'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    d['k_do_y5']  = (d['do_gh5'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    return d\n",
        "\n",
        "# Smoothed TE helper (log-target). Returns enc and count map.\n",
        "def te_smooth(train_key, train_tgt, apply_key, m=3000.0, prior=None):\n",
        "    if prior is None: prior = float(np.mean(train_tgt))\n",
        "    g = pd.DataFrame({'k': train_key.astype('object'), 'y': train_tgt}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict(); cnt_map = cnts.to_dict()\n",
        "    enc = apply_key.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return enc, prior, mp, cnt_map\n",
        "\n",
        "# Hierarchical back-off blend with dynamic prior, clipping, and hard back-off.\n",
        "def hardened_timeaware_te(tr_df, ap_df, key_cols, y_tr_log, m_levels=(3500.0, 2500.0, 1500.0), alpha=800.0, hard_thr=100, clip_width=0.12):\n",
        "    # Dynamic prior on recent 120 days if available\n",
        "    days = tr_df['days_since_start'].astype('float32').values\n",
        "    prior_dyn = float(np.mean(y_tr_log[days >= (days.max() - 120.0)])) if len(days) and (days >= (days.max() - 120.0)).any() else float(np.mean(y_tr_log))\n",
        "    # Level order: most specific -> least specific\n",
        "    encs_ap = []; cnts_list = [];\n",
        "    for k_col, m in zip(key_cols, m_levels):\n",
        "        enc_ap, prior_dyn, _, cnt_map = te_smooth(tr_df[k_col], y_tr_log, ap_df[k_col], m=m, prior=prior_dyn)\n",
        "        encs_ap.append(enc_ap.astype('float32'))\n",
        "        cnts_list.append(cnt_map)\n",
        "    # Weights based on counts of higher-resolution keys\n",
        "    k1 = key_cols[0]; c1 = np.array([cnts_list[0].get(k, 0.0) for k in ap_df[k1].astype('object').values], dtype='float32')\n",
        "    w1 = c1 / (c1 + np.float32(alpha))\n",
        "    if hard_thr is not None and hard_thr > 0:\n",
        "        mask1 = (c1 < np.float32(hard_thr));\n",
        "        if mask1.any():\n",
        "            w1 = w1.copy(); w1[mask1] = 0.0\n",
        "    # Blend level1 with level2\n",
        "    enc12 = (w1 * encs_ap[0] + (1.0 - w1) * encs_ap[1]).astype('float32')\n",
        "    # Second-stage weights from level2 counts\n",
        "    k2 = key_cols[1]; c2 = np.array([cnts_list[1].get(k, 0.0) for k in ap_df[k2].astype('object').values], dtype='float32')\n",
        "    w2 = c2 / (c2 + np.float32(alpha))\n",
        "    if hard_thr is not None and hard_thr > 0:\n",
        "        mask2 = (c2 < np.float32(hard_thr));\n",
        "        if mask2.any():\n",
        "            w2 = w2.copy(); w2[mask2] = 0.0\n",
        "    enc = (w2 * enc12 + (1.0 - w2) * encs_ap[2]).astype('float32')\n",
        "    # Clip TE to reduce volatility\n",
        "    enc = np.clip(enc, prior_dyn - clip_width, prior_dyn + clip_width).astype('float32')\n",
        "    return enc, prior_dyn, (c1, c2)\n",
        "\n",
        "# Reliability counts on gh6\n",
        "def add_reliability_counts(df_tr, df_ap):\n",
        "    pu6_counts = df_tr['pu_gh6'].astype('object').value_counts().astype('int32')\n",
        "    do6_counts = df_tr['do_gh6'].astype('object').value_counts().astype('int32')\n",
        "    out = df_ap.copy()\n",
        "    out['log_pu6_cnt'] = np.log1p(out['pu_gh6'].astype('object').map(pu6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "    out['log_do6_cnt'] = np.log1p(out['do_gh6'].astype('object').map(do6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "    return out\n",
        "\n",
        "# Prepare data\n",
        "train_ord = ensure_days_since_start(train_df.copy()).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "test_prep = ensure_days_since_start(test_df.copy())\n",
        "train_ord = ensure_time_parts(train_ord); test_prep = ensure_time_parts(test_prep)\n",
        "train_ord = add_geohash(train_ord, (6,5)); test_prep = add_geohash(test_prep, (6,5))\n",
        "\n",
        "# Base features (no raw year/month/day in model)\n",
        "base_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'hour_sin','hour_cos','dow_sin','dow_cos','doy_sin','doy_cos','is_holiday','is_weekend','is_rush','is_night',\n",
        "    'days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_ord.columns: base_feats.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_ord.columns: base_feats.append('dist_x_after_hike')\n",
        "\n",
        "train_ord = train_ord.dropna(subset=base_feats + ['fare_amount']).reset_index(drop=True)\n",
        "test_prep[base_feats] = test_prep[base_feats].fillna(0)\n",
        "\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=5, gap_days=1)\n",
        "assert len(folds) == 5, 'Expected 5 folds'\n",
        "\n",
        "# LightGBM lockdown params (per mandate)\n",
        "lgb_params = dict(\n",
        "    objective='huber', metric='rmse',\n",
        "    learning_rate=0.045, n_estimators=26000,\n",
        "    num_leaves=96, max_depth=-1, min_data_in_leaf=1800,\n",
        "    feature_fraction=0.65, bagging_fraction=0.80, bagging_freq=1,\n",
        "    max_bin=127, reg_alpha=5.0, reg_lambda=60.0, min_gain_to_split=0.1,\n",
        "    random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "fold_test_preds = []\n",
        "REL_THR = np.log(5.0).astype('float32')  # surgical reliability guard threshold\n",
        "\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx].copy(); va = train_ord.iloc[va_idx].copy()\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    # Build time-aware keys\n",
        "    tr = build_timeaware_keys(tr); va = build_timeaware_keys(va)\n",
        "    # Hardened hierarchical time-aware TE (PU and DO) with mandated params\n",
        "    te_pu_va, prior_pu, _ = hardened_timeaware_te(tr, va, ['k_pu_ym6','k_pu_y6','k_pu_y5'], y_tr,\n",
        "                                                  m_levels=(3500.0, 2500.0, 1500.0), alpha=800.0, hard_thr=100, clip_width=0.12)\n",
        "    te_do_va, prior_do, _ = hardened_timeaware_te(tr, va, ['k_do_ym6','k_do_y6','k_do_y5'], y_tr,\n",
        "                                                  m_levels=(3500.0, 2500.0, 1500.0), alpha=800.0, hard_thr=100, clip_width=0.12)\n",
        "    te_pu_tr, _, _ = hardened_timeaware_te(tr, tr, ['k_pu_ym6','k_pu_y6','k_pu_y5'], y_tr,\n",
        "                                           m_levels=(3500.0, 2500.0, 1500.0), alpha=800.0, hard_thr=100, clip_width=0.12)\n",
        "    te_do_tr, _, _ = hardened_timeaware_te(tr, tr, ['k_do_ym6','k_do_y6','k_do_y5'], y_tr,\n",
        "                                           m_levels=(3500.0, 2500.0, 1500.0), alpha=800.0, hard_thr=100, clip_width=0.12)\n",
        "    tr['te_pu'] = te_pu_tr.astype('float32'); va['te_pu'] = te_pu_va.astype('float32')\n",
        "    tr['te_do'] = te_do_tr.astype('float32'); va['te_do'] = te_do_va.astype('float32')\n",
        "    # Reliability features and surgical guard on validation\n",
        "    tr = add_reliability_counts(tr, tr); va = add_reliability_counts(tr, va)\n",
        "    # Overwrite TE with dynamic prior for sparse keys (guard)\n",
        "    va_mask_pu = va['log_pu6_cnt'].values < REL_THR\n",
        "    if np.any(va_mask_pu):\n",
        "        va.loc[va_mask_pu, 'te_pu'] = prior_pu\n",
        "    va_mask_do = va['log_do6_cnt'].values < REL_THR\n",
        "    if np.any(va_mask_do):\n",
        "        va.loc[va_mask_do, 'te_do'] = prior_do\n",
        "\n",
        "    use_cols = base_feats + ['te_pu','te_do','log_pu6_cnt','log_do6_cnt']\n",
        "    X_tr = tr[use_cols].astype('float32').values; X_va = va[use_cols].astype('float32').values\n",
        "    # Monotone constraints: +1 on days_since_start, dist_hav_km, dist_man_km\n",
        "    mono = [0]*len(use_cols)\n",
        "    for nm in ['days_since_start','dist_hav_km','dist_man_km']:\n",
        "        if nm in use_cols:\n",
        "            mono[use_cols.index(nm)] = 1\n",
        "    model = lgb.LGBMRegressor(**{**lgb_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=900, verbose=False), lgb.log_evaluation(period=250)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), y_pred_val))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    print(f\"[Hardened TimeAware] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "\n",
        "    # Test predictions for fold ensembling with train-only encodings + reliability guard\n",
        "    tst = test_prep.copy()\n",
        "    tst = build_timeaware_keys(tst)\n",
        "    te_pu_test, _, _ = hardened_timeaware_te(tr, tst, ['k_pu_ym6','k_pu_y6','k_pu_y5'], y_tr,\n",
        "                                             m_levels=(3500.0, 2500.0, 1500.0), alpha=800.0, hard_thr=100, clip_width=0.12)\n",
        "    te_do_test, _, _ = hardened_timeaware_te(tr, tst, ['k_do_ym6','k_do_y6','k_do_y5'], y_tr,\n",
        "                                             m_levels=(3500.0, 2500.0, 1500.0), alpha=800.0, hard_thr=100, clip_width=0.12)\n",
        "    tst['te_pu'] = te_pu_test.astype('float32'); tst['te_do'] = te_do_test.astype('float32')\n",
        "    tst = add_reliability_counts(tr, tst)\n",
        "    # Guard on test\n",
        "    tst_mask_pu = tst['log_pu6_cnt'].values < REL_THR\n",
        "    if np.any(tst_mask_pu):\n",
        "        tst.loc[tst_mask_pu, 'te_pu'] = prior_pu\n",
        "    tst_mask_do = tst['log_do6_cnt'].values < REL_THR\n",
        "    if np.any(tst_mask_do):\n",
        "        tst.loc[tst_mask_do, 'te_do'] = prior_do\n",
        "\n",
        "    X_test_fold = tst[use_cols].astype('float32').values\n",
        "    fold_pred = np.expm1(model.predict(X_test_fold, num_iteration=model.best_iteration_)).astype('float32')\n",
        "    fold_pred = np.clip(fold_pred, 0, 500)\n",
        "    fold_test_preds.append(fold_pred)\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_median = int(np.median(best_iters)) if best_iters else 0\n",
        "print({'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 'median_best_iter': best_iter_median, 'note': 'Maximalist hardened time-aware single model (mandated params + reliability guard)'})\n",
        "\n",
        "# Fold-ensemble submission\n",
        "test_pred = np.mean(np.vstack(fold_test_preds), axis=0).astype('float32') if fold_test_preds else np.zeros(len(test_prep), dtype='float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "sub = sample[['key']].merge(pd.DataFrame({'key': test_prep['key'].astype('string'), 'fare_amount': test_pred}), on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Hardened Time-Aware Single Model) with shape:', sub.shape)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.235701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.23494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.23481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.234737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1250]\tvalid_0's rmse: 0.234704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.234681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1750]\tvalid_0's rmse: 0.234651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.234639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2250]\tvalid_0's rmse: 0.234605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2500]\tvalid_0's rmse: 0.234587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2750]\tvalid_0's rmse: 0.234575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.234584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3250]\tvalid_0's rmse: 0.234561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3500]\tvalid_0's rmse: 0.234553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3750]\tvalid_0's rmse: 0.234552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4000]\tvalid_0's rmse: 0.234542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4250]\tvalid_0's rmse: 0.234537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4500]\tvalid_0's rmse: 0.234538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4750]\tvalid_0's rmse: 0.23452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5000]\tvalid_0's rmse: 0.234519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5250]\tvalid_0's rmse: 0.234517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5500]\tvalid_0's rmse: 0.234504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5750]\tvalid_0's rmse: 0.234497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6000]\tvalid_0's rmse: 0.234502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6250]\tvalid_0's rmse: 0.234495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6500]\tvalid_0's rmse: 0.234489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6750]\tvalid_0's rmse: 0.234498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7000]\tvalid_0's rmse: 0.234498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7250]\tvalid_0's rmse: 0.234485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7500]\tvalid_0's rmse: 0.234481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7750]\tvalid_0's rmse: 0.234474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8000]\tvalid_0's rmse: 0.234474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8250]\tvalid_0's rmse: 0.234476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8500]\tvalid_0's rmse: 0.23448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Hardened TimeAware] Fold 1: RMSE=3.73716, best_iter=7771 (train_n=319908, val_n=320756)\n"
          ]
        }
      ]
    },
    {
      "id": "3a19b81f-2f99-4750-9a57-5bfc1fb98277",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 15 \u2014 Balanced Attack: relaxed TE clipping/back-off + relaxed model regularization + reliability guard (fixed threshold) + fold ensembling\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'sample' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "try:\n",
        "    import pygeohash as pgh\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pygeohash'])\n",
        "    import pygeohash as pgh\n",
        "\n",
        "def to_local(dt):\n",
        "    return dt.dt.tz_convert('America/New_York')\n",
        "\n",
        "def ensure_days_since_start(df):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    d = df.copy(); dt_local = to_local(d['pickup_datetime'])\n",
        "    start_ns = int(dt_local.min().value)\n",
        "    d['days_since_start'] = ((dt_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def ensure_time_parts(df):\n",
        "    d = df.copy(); dt = to_local(d['pickup_datetime'])\n",
        "    d['year'] = dt.dt.year.astype('int16'); d['month'] = dt.dt.month.astype('int8')\n",
        "    return d\n",
        "\n",
        "def add_geohash(df, prec_list=(6,5)):\n",
        "    d = df.copy()\n",
        "    lat_pu = d['pickup_latitude'].astype('float32').values\n",
        "    lon_pu = d['pickup_longitude'].astype('float32').values\n",
        "    lat_do = d['dropoff_latitude'].astype('float32').values\n",
        "    lon_do = d['dropoff_longitude'].astype('float32').values\n",
        "    for prec in prec_list:\n",
        "        pu_col = f'pu_gh{prec}'; do_col = f'do_gh{prec}'\n",
        "        if pu_col in d.columns and do_col in d.columns: continue\n",
        "        d[pu_col] = [pgh.encode(float(la), float(lo), precision=prec) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(lat_pu, lon_pu)]\n",
        "        d[do_col] = [pgh.encode(float(la), float(lo), precision=prec) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(lat_do, lon_do)]\n",
        "    return d\n",
        "\n",
        "def build_timeaware_keys(df):\n",
        "    d = df.copy()\n",
        "    d['k_pu_ym6'] = (d['pu_gh6'].astype('string') + '_' + d['year'].astype('string') + '_' + d['month'].astype('string'))\n",
        "    d['k_pu_y6']  = (d['pu_gh6'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    d['k_pu_y5']  = (d['pu_gh5'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    d['k_do_ym6'] = (d['do_gh6'].astype('string') + '_' + d['year'].astype('string') + '_' + d['month'].astype('string'))\n",
        "    d['k_do_y6']  = (d['do_gh6'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    d['k_do_y5']  = (d['do_gh5'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    return d\n",
        "\n",
        "def te_smooth(train_key, train_tgt, apply_key, m=3000.0, prior=None):\n",
        "    if prior is None: prior = float(np.mean(train_tgt))\n",
        "    g = pd.DataFrame({'k': train_key.astype('object'), 'y': train_tgt}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict(); cnt_map = cnts.to_dict()\n",
        "    enc = apply_key.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return enc, prior, mp, cnt_map\n",
        "\n",
        "def hardened_timeaware_te(tr_df, ap_df, key_cols, y_tr_log, m_levels=(3500.0, 2500.0, 1500.0), alpha=1000.0, hard_thr=50, clip_width=0.20):\n",
        "    days = tr_df['days_since_start'].astype('float32').values\n",
        "    prior_dyn = float(np.mean(y_tr_log[days >= (days.max() - 120.0)])) if len(days) and (days >= (days.max() - 120.0)).any() else float(np.mean(y_tr_log))\n",
        "    encs_ap = []; cnts_list = [];\n",
        "    for k_col, m in zip(key_cols, m_levels):\n",
        "        enc_ap, prior_dyn, _, cnt_map = te_smooth(tr_df[k_col], y_tr_log, ap_df[k_col], m=m, prior=prior_dyn)\n",
        "        encs_ap.append(enc_ap.astype('float32'))\n",
        "        cnts_list.append(cnt_map)\n",
        "    k1 = key_cols[0]; c1 = np.array([cnts_list[0].get(k, 0.0) for k in ap_df[k1].astype('object').values], dtype='float32')\n",
        "    w1 = c1 / (c1 + np.float32(alpha))\n",
        "    if hard_thr is not None and hard_thr > 0:\n",
        "        m1 = (c1 < np.float32(hard_thr))\n",
        "        if m1.any():\n",
        "            w1 = w1.copy(); w1[m1] = 0.0\n",
        "    enc12 = (w1 * encs_ap[0] + (1.0 - w1) * encs_ap[1]).astype('float32')\n",
        "    k2 = key_cols[1]; c2 = np.array([cnts_list[1].get(k, 0.0) for k in ap_df[k2].astype('object').values], dtype='float32')\n",
        "    w2 = c2 / (c2 + np.float32(alpha))\n",
        "    if hard_thr is not None and hard_thr > 0:\n",
        "        m2 = (c2 < np.float32(hard_thr))\n",
        "        if m2.any():\n",
        "            w2 = w2.copy(); w2[m2] = 0.0\n",
        "    enc = (w2 * enc12 + (1.0 - w2) * encs_ap[2]).astype('float32')\n",
        "    enc = np.clip(enc, prior_dyn - clip_width, prior_dyn + clip_width).astype('float32')\n",
        "    return enc, prior_dyn, (c1, c2)\n",
        "\n",
        "def add_reliability_counts(df_tr, df_ap):\n",
        "    pu6_counts = df_tr['pu_gh6'].astype('object').value_counts().astype('int32')\n",
        "    do6_counts = df_tr['do_gh6'].astype('object').value_counts().astype('int32')\n",
        "    out = df_ap.copy()\n",
        "    out['log_pu6_cnt'] = np.log1p(out['pu_gh6'].astype('object').map(pu6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "    out['log_do6_cnt'] = np.log1p(out['do_gh6'].astype('object').map(do6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "    return out\n",
        "\n",
        "train_ord = ensure_days_since_start(train_df.copy()).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "test_prep = ensure_days_since_start(test_df.copy())\n",
        "train_ord = ensure_time_parts(train_ord); test_prep = ensure_time_parts(test_prep)\n",
        "train_ord = add_geohash(train_ord, (6,5)); test_prep = add_geohash(test_prep, (6,5))\n",
        "\n",
        "base_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'hour_sin','hour_cos','dow_sin','dow_cos','doy_sin','doy_cos','is_holiday','is_weekend','is_rush','is_night',\n",
        "    'days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_ord.columns: base_feats.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_ord.columns: base_feats.append('dist_x_after_hike')\n",
        "\n",
        "train_ord = train_ord.dropna(subset=base_feats + ['fare_amount']).reset_index(drop=True)\n",
        "test_prep[base_feats] = test_prep[base_feats].fillna(0)\n",
        "\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=5, gap_days=1)\n",
        "assert len(folds) == 5, 'Expected 5 folds'\n",
        "\n",
        "lgb_params = dict(\n",
        "    objective='huber', metric='rmse',\n",
        "    learning_rate=0.045, n_estimators=26000,\n",
        "    num_leaves=192, max_depth=-1, min_data_in_leaf=800,\n",
        "    feature_fraction=0.80, bagging_fraction=0.80, bagging_freq=1,\n",
        "    max_bin=127, reg_alpha=1.0, reg_lambda=30.0, min_gain_to_split=0.1,\n",
        "    random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "fold_test_preds = []\n",
        "REL_THR = np.log1p(5.0).astype('float32')  # corrected guard threshold to match log1p encoding\n",
        "\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx].copy(); va = train_ord.iloc[va_idx].copy()\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    tr = build_timeaware_keys(tr); va = build_timeaware_keys(va)\n",
        "    te_pu_va, prior_pu, _ = hardened_timeaware_te(tr, va, ['k_pu_ym6','k_pu_y6','k_pu_y5'], y_tr,\n",
        "                                                  m_levels=(3500.0, 2500.0, 1500.0), alpha=1000.0, hard_thr=50, clip_width=0.20)\n",
        "    te_do_va, prior_do, _ = hardened_timeaware_te(tr, va, ['k_do_ym6','k_do_y6','k_do_y5'], y_tr,\n",
        "                                                  m_levels=(3500.0, 2500.0, 1500.0), alpha=1000.0, hard_thr=50, clip_width=0.20)\n",
        "    te_pu_tr, _, _ = hardened_timeaware_te(tr, tr, ['k_pu_ym6','k_pu_y6','k_pu_y5'], y_tr,\n",
        "                                           m_levels=(3500.0, 2500.0, 1500.0), alpha=1000.0, hard_thr=50, clip_width=0.20)\n",
        "    te_do_tr, _, _ = hardened_timeaware_te(tr, tr, ['k_do_ym6','k_do_y6','k_do_y5'], y_tr,\n",
        "                                           m_levels=(3500.0, 2500.0, 1500.0), alpha=1000.0, hard_thr=50, clip_width=0.20)\n",
        "    tr['te_pu'] = te_pu_tr.astype('float32'); va['te_pu'] = te_pu_va.astype('float32')\n",
        "    tr['te_do'] = te_do_tr.astype('float32'); va['te_do'] = te_do_va.astype('float32')\n",
        "    tr = add_reliability_counts(tr, tr); va = add_reliability_counts(tr, va)\n",
        "    # Symmetric reliability guard: apply to TRAIN and VALID equally\n",
        "    tr_mask_pu = tr['log_pu6_cnt'].values < REL_THR\n",
        "    if np.any(tr_mask_pu):\n",
        "        tr.loc[tr_mask_pu, 'te_pu'] = prior_pu\n",
        "    tr_mask_do = tr['log_do6_cnt'].values < REL_THR\n",
        "    if np.any(tr_mask_do):\n",
        "        tr.loc[tr_mask_do, 'te_do'] = prior_do\n",
        "    va_mask_pu = va['log_pu6_cnt'].values < REL_THR\n",
        "    if np.any(va_mask_pu):\n",
        "        va.loc[va_mask_pu, 'te_pu'] = prior_pu\n",
        "    va_mask_do = va['log_do6_cnt'].values < REL_THR\n",
        "    if np.any(va_mask_do):\n",
        "        va.loc[va_mask_do, 'te_do'] = prior_do\n",
        "\n",
        "    use_cols = base_feats + ['te_pu','te_do','log_pu6_cnt','log_do6_cnt']\n",
        "    X_tr = tr[use_cols].astype('float32').values; X_va = va[use_cols].astype('float32').values\n",
        "    mono = [0]*len(use_cols)\n",
        "    for nm in ['days_since_start','dist_hav_km','dist_man_km']:\n",
        "        if nm in use_cols:\n",
        "            mono[use_cols.index(nm)] = 1\n",
        "    model = lgb.LGBMRegressor(**{**lgb_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=900, verbose=False), lgb.log_evaluation(period=250)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), y_pred_val))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    print(f\"[Balanced TimeAware] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "\n",
        "    tst = test_prep.copy()\n",
        "    tst = build_timeaware_keys(tst)\n",
        "    te_pu_test, _, _ = hardened_timeaware_te(tr, tst, ['k_pu_ym6','k_pu_y6','k_pu_y5'], y_tr,\n",
        "                                             m_levels=(3500.0, 2500.0, 1500.0), alpha=1000.0, hard_thr=50, clip_width=0.20)\n",
        "    te_do_test, _, _ = hardened_timeaware_te(tr, tst, ['k_do_ym6','k_do_y6','k_do_y5'], y_tr,\n",
        "                                             m_levels=(3500.0, 2500.0, 1500.0), alpha=1000.0, hard_thr=50, clip_width=0.20)\n",
        "    tst['te_pu'] = te_pu_test.astype('float32'); tst['te_do'] = te_do_test.astype('float32')\n",
        "    tst = add_reliability_counts(tr, tst)\n",
        "    tst_mask_pu = tst['log_pu6_cnt'].values < REL_THR\n",
        "    if np.any(tst_mask_pu):\n",
        "        tst.loc[tst_mask_pu, 'te_pu'] = prior_pu\n",
        "    tst_mask_do = tst['log_do6_cnt'].values < REL_THR\n",
        "    if np.any(tst_mask_do):\n",
        "        tst.loc[tst_mask_do, 'te_do'] = prior_do\n",
        "    X_test_fold = tst[use_cols].astype('float32').values\n",
        "    fold_pred = np.expm1(model.predict(X_test_fold, num_iteration=model.best_iteration_)).astype('float32')\n",
        "    fold_pred = np.clip(fold_pred, 0, 500)\n",
        "    fold_test_preds.append(fold_pred)\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_median = int(np.median(best_iters)) if best_iters else 0\n",
        "print({'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 'median_best_iter': best_iter_median, 'note': 'Balanced Time-Aware single model (relaxed TE + relaxed LGBM + reliability guard) \u2014 minimal hardening applied'})\n",
        "\n",
        "test_pred = np.mean(np.vstack(fold_test_preds), axis=0).astype('float32') if fold_test_preds else np.zeros(len(test_prep), dtype='float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "sub = sample[['key']].merge(pd.DataFrame({'key': test_prep['key'].astype('string'), 'fare_amount': test_pred}), on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Balanced Time-Aware Single Model) with shape:', sub.shape)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.234176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.233755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.233626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.233551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1250]\tvalid_0's rmse: 0.233511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.233492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1750]\tvalid_0's rmse: 0.233466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.233456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2250]\tvalid_0's rmse: 0.233412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2500]\tvalid_0's rmse: 0.233386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2750]\tvalid_0's rmse: 0.23337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.233365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3250]\tvalid_0's rmse: 0.233349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3500]\tvalid_0's rmse: 0.233337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3750]\tvalid_0's rmse: 0.233329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4000]\tvalid_0's rmse: 0.233323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4250]\tvalid_0's rmse: 0.233325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4500]\tvalid_0's rmse: 0.233314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4750]\tvalid_0's rmse: 0.233291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5000]\tvalid_0's rmse: 0.233287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5250]\tvalid_0's rmse: 0.233286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5500]\tvalid_0's rmse: 0.233272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5750]\tvalid_0's rmse: 0.233268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6000]\tvalid_0's rmse: 0.233261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6250]\tvalid_0's rmse: 0.233251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6500]\tvalid_0's rmse: 0.23325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6750]\tvalid_0's rmse: 0.23325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7000]\tvalid_0's rmse: 0.233242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7250]\tvalid_0's rmse: 0.233224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7500]\tvalid_0's rmse: 0.233222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7750]\tvalid_0's rmse: 0.233218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8000]\tvalid_0's rmse: 0.233209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8250]\tvalid_0's rmse: 0.233195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8500]\tvalid_0's rmse: 0.233199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8750]\tvalid_0's rmse: 0.233193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9000]\tvalid_0's rmse: 0.23319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9250]\tvalid_0's rmse: 0.233184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9500]\tvalid_0's rmse: 0.233182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9750]\tvalid_0's rmse: 0.233178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10000]\tvalid_0's rmse: 0.233178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10250]\tvalid_0's rmse: 0.233171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10500]\tvalid_0's rmse: 0.233176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10750]\tvalid_0's rmse: 0.233167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11000]\tvalid_0's rmse: 0.233163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11250]\tvalid_0's rmse: 0.233162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11500]\tvalid_0's rmse: 0.233171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11750]\tvalid_0's rmse: 0.233164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12000]\tvalid_0's rmse: 0.233155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12250]\tvalid_0's rmse: 0.233152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12500]\tvalid_0's rmse: 0.233155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12750]\tvalid_0's rmse: 0.233151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13000]\tvalid_0's rmse: 0.233148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13250]\tvalid_0's rmse: 0.233141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13500]\tvalid_0's rmse: 0.233144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13750]\tvalid_0's rmse: 0.23312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14000]\tvalid_0's rmse: 0.23312\n"
          ]
        }
      ]
    },
    {
      "id": "339b549b-f5a0-485a-bf79-996e004798f4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 15 \u2014 Contingency: Adaptive Decay (tau=180) on Balanced Time-Aware Single Model with symmetric reliability guard + fold ensembling\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'sample' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "try:\n",
        "    import pygeohash as pgh\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pygeohash'])\n",
        "    import pygeohash as pgh\n",
        "\n",
        "def to_local(dt):\n",
        "    return dt.dt.tz_convert('America/New_York')\n",
        "\n",
        "def ensure_days_since_start(df):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    d = df.copy(); dt_local = to_local(d['pickup_datetime'])\n",
        "    start_ns = int(dt_local.min().value)\n",
        "    d['days_since_start'] = ((dt_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def ensure_time_parts(df):\n",
        "    d = df.copy(); dt = to_local(d['pickup_datetime'])\n",
        "    d['year'] = dt.dt.year.astype('int16'); d['month'] = dt.dt.month.astype('int8')\n",
        "    return d\n",
        "\n",
        "def add_geohash(df, prec_list=(6,5)):\n",
        "    d = df.copy()\n",
        "    lat_pu = d['pickup_latitude'].astype('float32').values\n",
        "    lon_pu = d['pickup_longitude'].astype('float32').values\n",
        "    lat_do = d['dropoff_latitude'].astype('float32').values\n",
        "    lon_do = d['dropoff_longitude'].astype('float32').values\n",
        "    for prec in prec_list:\n",
        "        pu_col = f'pu_gh{prec}'; do_col = f'do_gh{prec}'\n",
        "        if pu_col in d.columns and do_col in d.columns: continue\n",
        "        d[pu_col] = [pgh.encode(float(la), float(lo), precision=prec) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(lat_pu, lon_pu)]\n",
        "        d[do_col] = [pgh.encode(float(la), float(lo), precision=prec) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(lat_do, lon_do)]\n",
        "    return d\n",
        "\n",
        "def build_timeaware_keys(df):\n",
        "    d = df.copy()\n",
        "    d['k_pu_ym6'] = (d['pu_gh6'].astype('string') + '_' + d['year'].astype('string') + '_' + d['month'].astype('string'))\n",
        "    d['k_pu_y6']  = (d['pu_gh6'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    d['k_pu_y5']  = (d['pu_gh5'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    d['k_do_ym6'] = (d['do_gh6'].astype('string') + '_' + d['year'].astype('string') + '_' + d['month'].astype('string'))\n",
        "    d['k_do_y6']  = (d['do_gh6'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    d['k_do_y5']  = (d['do_gh5'].astype('string') + '_' + d['year'].astype('string'))\n",
        "    return d\n",
        "\n",
        "def te_smooth(train_key, train_tgt, apply_key, m=3000.0, prior=None):\n",
        "    if prior is None: prior = float(np.mean(train_tgt))\n",
        "    g = pd.DataFrame({'k': train_key.astype('object'), 'y': train_tgt}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict(); cnt_map = cnts.to_dict()\n",
        "    enc = apply_key.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return enc, prior, mp, cnt_map\n",
        "\n",
        "def hardened_timeaware_te(tr_df, ap_df, key_cols, y_tr_log, m_levels=(3500.0, 2500.0, 1500.0), alpha=1000.0, hard_thr=50, clip_width=0.30):\n",
        "    days = tr_df['days_since_start'].astype('float32').values\n",
        "    prior_dyn = float(np.mean(y_tr_log[days >= (days.max() - 120.0)])) if len(days) and (days >= (days.max() - 120.0)).any() else float(np.mean(y_tr_log))\n",
        "    encs_ap = []; cnts_list = [];\n",
        "    for k_col, m in zip(key_cols, m_levels):\n",
        "        enc_ap, prior_dyn, _, cnt_map = te_smooth(tr_df[k_col], y_tr_log, ap_df[k_col], m=m, prior=prior_dyn)\n",
        "        encs_ap.append(enc_ap.astype('float32'))\n",
        "        cnts_list.append(cnt_map)\n",
        "    k1 = key_cols[0]; c1 = np.array([cnts_list[0].get(k, 0.0) for k in ap_df[k1].astype('object').values], dtype='float32')\n",
        "    w1 = c1 / (c1 + np.float32(alpha))\n",
        "    if hard_thr is not None and hard_thr > 0:\n",
        "        m1 = (c1 < np.float32(hard_thr))\n",
        "        if m1.any():\n",
        "            w1 = w1.copy(); w1[m1] = 0.0\n",
        "    enc12 = (w1 * encs_ap[0] + (1.0 - w1) * encs_ap[1]).astype('float32')\n",
        "    k2 = key_cols[1]; c2 = np.array([cnts_list[1].get(k, 0.0) for k in ap_df[k2].astype('object').values], dtype='float32')\n",
        "    w2 = c2 / (c2 + np.float32(alpha))\n",
        "    if hard_thr is not None and hard_thr > 0:\n",
        "        m2 = (c2 < np.float32(hard_thr))\n",
        "        if m2.any():\n",
        "            w2 = w2.copy(); w2[m2] = 0.0\n",
        "    enc = (w2 * enc12 + (1.0 - w2) * encs_ap[2]).astype('float32')\n",
        "    enc = np.clip(enc, prior_dyn - clip_width, prior_dyn + clip_width).astype('float32')\n",
        "    return enc, prior_dyn, (c1, c2)\n",
        "\n",
        "def add_reliability_counts(df_tr, df_ap):\n",
        "    pu6_counts = df_tr['pu_gh6'].astype('object').value_counts().astype('int32')\n",
        "    do6_counts = df_tr['do_gh6'].astype('object').value_counts().astype('int32')\n",
        "    out = df_ap.copy()\n",
        "    out['log_pu6_cnt'] = np.log1p(out['pu_gh6'].astype('object').map(pu6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "    out['log_do6_cnt'] = np.log1p(out['do_gh6'].astype('object').map(do6_counts).fillna(0).astype('int32')).astype('float32')\n",
        "    return out\n",
        "\n",
        "train_ord = ensure_days_since_start(train_df.copy()).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "test_prep = ensure_days_since_start(test_df.copy())\n",
        "train_ord = ensure_time_parts(train_ord); test_prep = ensure_time_parts(test_prep)\n",
        "train_ord = add_geohash(train_ord, (6,5)); test_prep = add_geohash(test_prep, (6,5))\n",
        "\n",
        "base_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'hour_sin','hour_cos','dow_sin','dow_cos','doy_sin','doy_cos','is_holiday','is_weekend','is_rush','is_night',\n",
        "    'days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_ord.columns: base_feats.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_ord.columns: base_feats.append('dist_x_after_hike')\n",
        "\n",
        "train_ord = train_ord.dropna(subset=base_feats + ['fare_amount']).reset_index(drop=True)\n",
        "test_prep[base_feats] = test_prep[base_feats].fillna(0)\n",
        "\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=5, gap_days=1)\n",
        "assert len(folds) == 5, 'Expected 5 folds'\n",
        "\n",
        "lgb_params = dict(\n",
        "    objective='huber', metric='rmse',\n",
        "    learning_rate=0.045, n_estimators=26000,\n",
        "    num_leaves=192, max_depth=-1, min_data_in_leaf=800,\n",
        "    feature_fraction=0.80, bagging_fraction=0.80, bagging_freq=1,\n",
        "    max_bin=127, reg_alpha=1.0, reg_lambda=30.0, min_gain_to_split=0.1,\n",
        "    random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "fold_test_preds = []\n",
        "REL_THR = np.log1p(5.0).astype('float32')  # guard threshold aligned with log1p counts\n",
        "TAU = np.float32(180.0)  # Adaptive decay timescale (days)\n",
        "\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx].copy(); va = train_ord.iloc[va_idx].copy()\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    tr = build_timeaware_keys(tr); va = build_timeaware_keys(va)\n",
        "    te_pu_va, prior_pu, _ = hardened_timeaware_te(tr, va, ['k_pu_ym6','k_pu_y6','k_pu_y5'], y_tr,\n",
        "                                                  m_levels=(3500.0, 2500.0, 1500.0), alpha=1000.0, hard_thr=50, clip_width=0.30)\n",
        "    te_do_va, prior_do, _ = hardened_timeaware_te(tr, va, ['k_do_ym6','k_do_y6','k_do_y5'], y_tr,\n",
        "                                                  m_levels=(3500.0, 2500.0, 1500.0), alpha=1000.0, hard_thr=50, clip_width=0.30)\n",
        "    te_pu_tr, _, _ = hardened_timeaware_te(tr, tr, ['k_pu_ym6','k_pu_y6','k_pu_y5'], y_tr,\n",
        "                                           m_levels=(3500.0, 2500.0, 1500.0), alpha=1000.0, hard_thr=50, clip_width=0.30)\n",
        "    te_do_tr, _, _ = hardened_timeaware_te(tr, tr, ['k_do_ym6','k_do_y6','k_do_y5'], y_tr,\n",
        "                                           m_levels=(3500.0, 2500.0, 1500.0), alpha=1000.0, hard_thr=50, clip_width=0.30)\n",
        "    tr['te_pu'] = te_pu_tr.astype('float32'); va['te_pu'] = te_pu_va.astype('float32')\n",
        "    tr['te_do'] = te_do_tr.astype('float32'); va['te_do'] = te_do_va.astype('float32')\n",
        "    tr = add_reliability_counts(tr, tr); va = add_reliability_counts(tr, va)\n",
        "    # Symmetric reliability guard (train + val)\n",
        "    m_tr_pu = tr['log_pu6_cnt'].values < REL_THR\n",
        "    if np.any(m_tr_pu): tr.loc[m_tr_pu, 'te_pu'] = prior_pu\n",
        "    m_tr_do = tr['log_do6_cnt'].values < REL_THR\n",
        "    if np.any(m_tr_do): tr.loc[m_tr_do, 'te_do'] = prior_do\n",
        "    m_va_pu = va['log_pu6_cnt'].values < REL_THR\n",
        "    if np.any(m_va_pu): va.loc[m_va_pu, 'te_pu'] = prior_pu\n",
        "    m_va_do = va['log_do6_cnt'].values < REL_THR\n",
        "    if np.any(m_va_do): va.loc[m_va_do, 'te_do'] = prior_do\n",
        "\n",
        "    use_cols = base_feats + ['te_pu','te_do','log_pu6_cnt','log_do6_cnt']\n",
        "    X_tr = tr[use_cols].astype('float32').values; X_va = va[use_cols].astype('float32').values\n",
        "    # Monotone +1 on time and distances\n",
        "    mono = [0]*len(use_cols)\n",
        "    for nm in ['days_since_start','dist_hav_km','dist_man_km']:\n",
        "        if nm in use_cols: mono[use_cols.index(nm)] = 1\n",
        "    # Adaptive decay sample weights (favor recent data within each fold)\n",
        "    days_tr = tr['days_since_start'].astype('float32').values\n",
        "    max_days = np.float32(days_tr.max()) if days_tr.size else np.float32(0.0)\n",
        "    w_tr = np.exp((days_tr - max_days) / TAU).astype('float32')\n",
        "\n",
        "    model = lgb.LGBMRegressor(**{**lgb_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr, sample_weight=w_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=900, verbose=False), lgb.log_evaluation(period=250)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), y_pred_val))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    print(f\"[AdaptiveDecay TA] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "\n",
        "    # Test predictions (fold ensemble)\n",
        "    tst = test_prep.copy()\n",
        "    tst = build_timeaware_keys(tst)\n",
        "    te_pu_test, _, _ = hardened_timeaware_te(tr, tst, ['k_pu_ym6','k_pu_y6','k_pu_y5'], y_tr,\n",
        "                                             m_levels=(3500.0, 2500.0, 1500.0), alpha=1000.0, hard_thr=50, clip_width=0.30)\n",
        "    te_do_test, _, _ = hardened_timeaware_te(tr, tst, ['k_do_ym6','k_do_y6','k_do_y5'], y_tr,\n",
        "                                             m_levels=(3500.0, 2500.0, 1500.0), alpha=1000.0, hard_thr=50, clip_width=0.30)\n",
        "    tst['te_pu'] = te_pu_test.astype('float32'); tst['te_do'] = te_do_test.astype('float32')\n",
        "    tst = add_reliability_counts(tr, tst)\n",
        "    m_tst_pu = tst['log_pu6_cnt'].values < REL_THR\n",
        "    if np.any(m_tst_pu): tst.loc[m_tst_pu, 'te_pu'] = prior_pu\n",
        "    m_tst_do = tst['log_do6_cnt'].values < REL_THR\n",
        "    if np.any(m_tst_do): tst.loc[m_tst_do, 'te_do'] = prior_do\n",
        "    X_test_fold = tst[use_cols].astype('float32').values\n",
        "    fold_pred = np.expm1(model.predict(X_test_fold, num_iteration=model.best_iteration_)).astype('float32')\n",
        "    fold_pred = np.clip(fold_pred, 0, 500)\n",
        "    fold_test_preds.append(fold_pred)\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_median = int(np.median(best_iters)) if best_iters else 0\n",
        "print({'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 'median_best_iter': best_iter_median, 'note': 'Adaptive Decay (tau=180) + Balanced Time-Aware TE + symmetric guard + fold ensembling'})\n",
        "\n",
        "test_pred = np.mean(np.vstack(fold_test_preds), axis=0).astype('float32') if fold_test_preds else np.zeros(len(test_prep), dtype='float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "sub = sample[['key']].merge(pd.DataFrame({'key': test_prep['key'].astype('string'), 'fare_amount': test_pred}), on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Adaptive Decay) with shape:', sub.shape)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.235366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.235131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.23503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.234985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1250]\tvalid_0's rmse: 0.234966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.234936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1750]\tvalid_0's rmse: 0.234921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.234912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2250]\tvalid_0's rmse: 0.234903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2500]\tvalid_0's rmse: 0.234878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2750]\tvalid_0's rmse: 0.234872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.234868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3250]\tvalid_0's rmse: 0.23485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3500]\tvalid_0's rmse: 0.23482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3750]\tvalid_0's rmse: 0.2348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4000]\tvalid_0's rmse: 0.234787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4250]\tvalid_0's rmse: 0.234786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4500]\tvalid_0's rmse: 0.234784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4750]\tvalid_0's rmse: 0.234768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5000]\tvalid_0's rmse: 0.234775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5250]\tvalid_0's rmse: 0.23478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5500]\tvalid_0's rmse: 0.234767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5750]\tvalid_0's rmse: 0.234765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6000]\tvalid_0's rmse: 0.234762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6250]\tvalid_0's rmse: 0.234756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6500]\tvalid_0's rmse: 0.234751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6750]\tvalid_0's rmse: 0.234728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7000]\tvalid_0's rmse: 0.234722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7250]\tvalid_0's rmse: 0.234716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7500]\tvalid_0's rmse: 0.234709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7750]\tvalid_0's rmse: 0.234713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8000]\tvalid_0's rmse: 0.234709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8250]\tvalid_0's rmse: 0.234698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8500]\tvalid_0's rmse: 0.234701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8750]\tvalid_0's rmse: 0.234695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9000]\tvalid_0's rmse: 0.234704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9250]\tvalid_0's rmse: 0.234691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9500]\tvalid_0's rmse: 0.234688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9750]\tvalid_0's rmse: 0.234688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10000]\tvalid_0's rmse: 0.234686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10250]\tvalid_0's rmse: 0.234683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10500]\tvalid_0's rmse: 0.234683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10750]\tvalid_0's rmse: 0.234678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11000]\tvalid_0's rmse: 0.234678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11250]\tvalid_0's rmse: 0.234691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11500]\tvalid_0's rmse: 0.234687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AdaptiveDecay TA] Fold 1: RMSE=3.73807, best_iter=10815 (train_n=319908, val_n=320756)\n"
          ]
        }
      ]
    },
    {
      "id": "56b531e4-04f4-4d5f-bed4-19e7a65b6ed3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 15 \u2014 Pivot: Coarse TE (gh5 x week_of_year), heavy smoothing, symmetric reliability guard; huber + monotone + fold ensembling\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'sample' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "try:\n",
        "    import pygeohash as pgh\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pygeohash'])\n",
        "    import pygeohash as pgh\n",
        "\n",
        "def to_local(dt):\n",
        "    return dt.dt.tz_convert('America/New_York')\n",
        "\n",
        "def ensure_days_since_start(df):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    d = df.copy(); dt = to_local(d['pickup_datetime'])\n",
        "    start_ns = int(dt.min().value)\n",
        "    d['days_since_start'] = ((dt.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def ensure_time_parts_week(df):\n",
        "    d = df.copy(); dt = to_local(d['pickup_datetime'])\n",
        "    d['year'] = dt.dt.year.astype('int16')\n",
        "    d['week_of_year'] = dt.dt.isocalendar().week.astype('int16')\n",
        "    return d\n",
        "\n",
        "def add_geohash5(df):\n",
        "    d = df.copy()\n",
        "    if 'pu_gh5' in d.columns and 'do_gh5' in d.columns: return d\n",
        "    lat_pu = d['pickup_latitude'].astype('float32').values\n",
        "    lon_pu = d['pickup_longitude'].astype('float32').values\n",
        "    lat_do = d['dropoff_latitude'].astype('float32').values\n",
        "    lon_do = d['dropoff_longitude'].astype('float32').values\n",
        "    d['pu_gh5'] = [pgh.encode(float(la), float(lo), precision=5) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(lat_pu, lon_pu)]\n",
        "    d['do_gh5'] = [pgh.encode(float(la), float(lo), precision=5) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(lat_do, lon_do)]\n",
        "    return d\n",
        "\n",
        "def build_coarse_keys(df):\n",
        "    d = df.copy()\n",
        "    d['k_pu_w5'] = (d['pu_gh5'].astype('string') + '_' + d['week_of_year'].astype('string'))\n",
        "    d['k_do_w5'] = (d['do_gh5'].astype('string') + '_' + d['week_of_year'].astype('string'))\n",
        "    return d\n",
        "\n",
        "def te_smooth(train_key, train_tgt, apply_key, m=2000.0, prior=None):\n",
        "    if prior is None: prior = float(np.mean(train_tgt))\n",
        "    g = pd.DataFrame({'k': train_key.astype('object'), 'y': train_tgt}).groupby('k').agg(['mean','count'])['y']\n",
        "    means = g['mean'].astype('float64'); cnts = g['count'].astype('float64')\n",
        "    smooth = (cnts * means + m * prior) / (cnts + m)\n",
        "    mp = smooth.to_dict(); cnt_map = cnts.to_dict()\n",
        "    enc = apply_key.astype('object').map(mp).fillna(prior).astype('float32').values\n",
        "    return enc, prior, cnt_map\n",
        "\n",
        "def add_reliability_counts_gh5(df_tr, df_ap):\n",
        "    pu5_counts = df_tr['pu_gh5'].astype('object').value_counts().astype('int32')\n",
        "    do5_counts = df_tr['do_gh5'].astype('object').value_counts().astype('int32')\n",
        "    out = df_ap.copy()\n",
        "    out['log_pu5_cnt'] = np.log1p(out['pu_gh5'].astype('object').map(pu5_counts).fillna(0).astype('int32')).astype('float32')\n",
        "    out['log_do5_cnt'] = np.log1p(out['do_gh5'].astype('object').map(do5_counts).fillna(0).astype('int32')).astype('float32')\n",
        "    return out\n",
        "\n",
        "# Prepare data\n",
        "train_ord = ensure_days_since_start(train_df.copy()).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "test_prep = ensure_days_since_start(test_df.copy())\n",
        "train_ord = ensure_time_parts_week(train_ord); test_prep = ensure_time_parts_week(test_prep)\n",
        "train_ord = add_geohash5(train_ord); test_prep = add_geohash5(test_prep)\n",
        "train_ord = build_coarse_keys(train_ord); test_prep = build_coarse_keys(test_prep)\n",
        "\n",
        "# Base robust features\n",
        "base_feats = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'hour_sin','hour_cos','dow_sin','dow_cos','doy_sin','doy_cos','is_holiday','is_weekend','is_rush','is_night',\n",
        "    'days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_ord.columns: base_feats.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_ord.columns: base_feats.append('dist_x_after_hike')\n",
        "train_ord = train_ord.dropna(subset=base_feats + ['fare_amount']).reset_index(drop=True)\n",
        "test_prep[base_feats] = test_prep[base_feats].fillna(0)\n",
        "\n",
        "y_all_log = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=5, gap_days=1)\n",
        "assert len(folds) == 5, 'Expected 5 folds'\n",
        "\n",
        "# LGB params (stable baseline per mandate)\n",
        "lgb_params = dict(\n",
        "    objective='huber', metric='rmse',\n",
        "    learning_rate=0.045, n_estimators=26000,\n",
        "    num_leaves=192, max_depth=-1, min_data_in_leaf=800,\n",
        "    feature_fraction=0.80, bagging_fraction=0.80, bagging_freq=1,\n",
        "    max_bin=127, reg_alpha=1.0, reg_lambda=30.0, min_gain_to_split=0.1,\n",
        "    random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "fold_test_preds = []\n",
        "REL_THR = np.log1p(5.0).astype('float32')\n",
        "\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    tr = train_ord.iloc[tr_idx].copy(); va = train_ord.iloc[va_idx].copy()\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    # Single-level coarse TE on (gh5 x week_of_year)\n",
        "    te_pu_tr, prior_pu, pu_cnt_map = te_smooth(tr['k_pu_w5'], y_tr, tr['k_pu_w5'], m=2000.0, prior=None)\n",
        "    te_pu_va, _, _ = te_smooth(tr['k_pu_w5'], y_tr, va['k_pu_w5'], m=2000.0, prior=prior_pu)\n",
        "    te_do_tr, prior_do, do_cnt_map = te_smooth(tr['k_do_w5'], y_tr, tr['k_do_w5'], m=2000.0, prior=None)\n",
        "    te_do_va, _, _ = te_smooth(tr['k_do_w5'], y_tr, va['k_do_w5'], m=2000.0, prior=prior_do)\n",
        "    tr['te_pu'] = te_pu_tr.astype('float32'); va['te_pu'] = te_pu_va.astype('float32')\n",
        "    tr['te_do'] = te_do_tr.astype('float32'); va['te_do'] = te_do_va.astype('float32')\n",
        "    # Reliability counts and symmetric guard on gh5\n",
        "    tr = add_reliability_counts_gh5(tr, tr); va = add_reliability_counts_gh5(tr, va)\n",
        "    m_tr_pu = tr['log_pu5_cnt'].values < REL_THR\n",
        "    if np.any(m_tr_pu): tr.loc[m_tr_pu, 'te_pu'] = prior_pu\n",
        "    m_tr_do = tr['log_do5_cnt'].values < REL_THR\n",
        "    if np.any(m_tr_do): tr.loc[m_tr_do, 'te_do'] = prior_do\n",
        "    m_va_pu = va['log_pu5_cnt'].values < REL_THR\n",
        "    if np.any(m_va_pu): va.loc[m_va_pu, 'te_pu'] = prior_pu\n",
        "    m_va_do = va['log_do5_cnt'].values < REL_THR\n",
        "    if np.any(m_va_do): va.loc[m_va_do, 'te_do'] = prior_do\n",
        "\n",
        "    use_cols = base_feats + ['te_pu','te_do','log_pu5_cnt','log_do5_cnt']\n",
        "    X_tr = tr[use_cols].astype('float32').values; X_va = va[use_cols].astype('float32').values\n",
        "    # Monotone +1 on days_since_start, dist_hav_km, dist_man_km\n",
        "    mono = [0]*len(use_cols)\n",
        "    for nm in ['days_since_start','dist_hav_km','dist_man_km']:\n",
        "        if nm in use_cols: mono[use_cols.index(nm)] = 1\n",
        "    model = lgb.LGBMRegressor(**{**lgb_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=900, verbose=False), lgb.log_evaluation(period=250)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), y_pred_val))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    print(f\"[Coarse TE] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "\n",
        "    # Fold-ensemble preds on test with train-only maps and guard\n",
        "    tst = test_prep.copy()\n",
        "    tst = add_reliability_counts_gh5(tr, tst)\n",
        "    te_pu_test, _, _ = te_smooth(tr['k_pu_w5'], y_tr, tst['k_pu_w5'], m=2000.0, prior=prior_pu)\n",
        "    te_do_test, _, _ = te_smooth(tr['k_do_w5'], y_tr, tst['k_do_w5'], m=2000.0, prior=prior_do)\n",
        "    tst['te_pu'] = te_pu_test.astype('float32'); tst['te_do'] = te_do_test.astype('float32')\n",
        "    m_tst_pu = tst['log_pu5_cnt'].values < REL_THR\n",
        "    if np.any(m_tst_pu): tst.loc[m_tst_pu, 'te_pu'] = prior_pu\n",
        "    m_tst_do = tst['log_do5_cnt'].values < REL_THR\n",
        "    if np.any(m_tst_do): tst.loc[m_tst_do, 'te_do'] = prior_do\n",
        "    X_test_fold = tst[use_cols].astype('float32').values\n",
        "    fold_pred = np.expm1(model.predict(X_test_fold, num_iteration=model.best_iteration_)).astype('float32')\n",
        "    fold_pred = np.clip(fold_pred, 0, 500)\n",
        "    fold_test_preds.append(fold_pred)\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_median = int(np.median(best_iters)) if best_iters else 0\n",
        "print({'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 'median_best_iter': best_iter_median, 'note': 'Coarse TE gh5 x week, heavy smoothing m=2000, symmetric guard; huber + monotone + fold ensembling'})\n",
        "\n",
        "# Fold-ensemble submission\n",
        "test_pred = np.mean(np.vstack(fold_test_preds), axis=0).astype('float32') if fold_test_preds else np.zeros(len(test_prep), dtype='float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "sub = sample[['key']].merge(pd.DataFrame({'key': test_prep['key'].astype('string'), 'fare_amount': test_pred}), on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Coarse TE gh5 x week) with shape:', sub.shape)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.233634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.23319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.233065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.233002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1250]\tvalid_0's rmse: 0.232954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.232911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1750]\tvalid_0's rmse: 0.232893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.23288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2250]\tvalid_0's rmse: 0.232854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2500]\tvalid_0's rmse: 0.232826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2750]\tvalid_0's rmse: 0.232812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.232808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3250]\tvalid_0's rmse: 0.232791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3500]\tvalid_0's rmse: 0.232777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3750]\tvalid_0's rmse: 0.232766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4000]\tvalid_0's rmse: 0.232764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4250]\tvalid_0's rmse: 0.232763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4500]\tvalid_0's rmse: 0.232729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4750]\tvalid_0's rmse: 0.232721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5000]\tvalid_0's rmse: 0.23272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5250]\tvalid_0's rmse: 0.232717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5500]\tvalid_0's rmse: 0.232702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5750]\tvalid_0's rmse: 0.232695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6000]\tvalid_0's rmse: 0.232688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6250]\tvalid_0's rmse: 0.232683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6500]\tvalid_0's rmse: 0.232679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6750]\tvalid_0's rmse: 0.232671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7000]\tvalid_0's rmse: 0.232671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7250]\tvalid_0's rmse: 0.232655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7500]\tvalid_0's rmse: 0.232653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7750]\tvalid_0's rmse: 0.232652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8000]\tvalid_0's rmse: 0.232645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8250]\tvalid_0's rmse: 0.232638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8500]\tvalid_0's rmse: 0.232643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8750]\tvalid_0's rmse: 0.232638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9000]\tvalid_0's rmse: 0.232637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9250]\tvalid_0's rmse: 0.232635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Coarse TE] Fold 1: RMSE=3.70594, best_iter=8369 (train_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.177763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.177676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.177659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.177655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Coarse TE] Fold 2: RMSE=2.95000, best_iter=269 (train_n=640523, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[250]\tvalid_0's rmse: 0.212298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500]\tvalid_0's rmse: 0.211378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[750]\tvalid_0's rmse: 0.211284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_0's rmse: 0.211217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1250]\tvalid_0's rmse: 0.211168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.211141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1750]\tvalid_0's rmse: 0.211126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\tvalid_0's rmse: 0.211116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2250]\tvalid_0's rmse: 0.211079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2500]\tvalid_0's rmse: 0.211069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2750]\tvalid_0's rmse: 0.211038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.211025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3250]\tvalid_0's rmse: 0.211016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3500]\tvalid_0's rmse: 0.21102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3750]\tvalid_0's rmse: 0.211006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4000]\tvalid_0's rmse: 0.211007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4250]\tvalid_0's rmse: 0.211005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4500]\tvalid_0's rmse: 0.211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4750]\tvalid_0's rmse: 0.21099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5000]\tvalid_0's rmse: 0.210994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5250]\tvalid_0's rmse: 0.210984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5500]\tvalid_0's rmse: 0.210976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5750]\tvalid_0's rmse: 0.210967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6000]\tvalid_0's rmse: 0.210958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6250]\tvalid_0's rmse: 0.210966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6500]\tvalid_0's rmse: 0.210953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6750]\tvalid_0's rmse: 0.210952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7000]\tvalid_0's rmse: 0.210959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7250]\tvalid_0's rmse: 0.21094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7500]\tvalid_0's rmse: 0.210935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7750]\tvalid_0's rmse: 0.210942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8000]\tvalid_0's rmse: 0.210941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8250]\tvalid_0's rmse: 0.210934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8500]\tvalid_0's rmse: 0.210925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8750]\tvalid_0's rmse: 0.210928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9000]\tvalid_0's rmse: 0.210928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9250]\tvalid_0's rmse: 0.210922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Coarse TE] Fold 3: RMSE=3.72642, best_iter=8403 (train_n=961323, val_n=320756)\n"
          ]
        }
      ]
    },
    {
      "id": "a899b594-1e75-4877-a9fb-559ce7ddb5c3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 15 \u2014 Pivot to CatBoost: Ordered target statistics on coarse categoricals (gh5 + temporal), robust numeric features, 5-fold time CV + fold ensembling\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'sample' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'catboost'])\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "try:\n",
        "    import pygeohash as pgh\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pygeohash'])\n",
        "    import pygeohash as pgh\n",
        "\n",
        "def to_local(dt):\n",
        "    return dt.dt.tz_convert('America/New_York')\n",
        "\n",
        "def ensure_days_since_start(df):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    d = df.copy(); dt = to_local(d['pickup_datetime'])\n",
        "    start_ns = int(dt.min().value)\n",
        "    d['days_since_start'] = ((dt.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def ensure_time_parts(df):\n",
        "    d = df.copy(); dt = to_local(d['pickup_datetime'])\n",
        "    d['hour'] = dt.dt.hour.astype('int8')\n",
        "    d['dow'] = dt.dt.dayofweek.astype('int8')\n",
        "    d['week_of_year'] = dt.dt.isocalendar().week.astype('int16')\n",
        "    d['is_weekend'] = d['dow'].isin([5,6]).astype('int8')\n",
        "    d['hour_sin'] = np.sin(2*np.pi*d['hour']/24).astype('float32')\n",
        "    d['hour_cos'] = np.cos(2*np.pi*d['hour']/24).astype('float32')\n",
        "    d['dow_sin'] = np.sin(2*np.pi*d['dow']/7).astype('float32')\n",
        "    d['dow_cos'] = np.cos(2*np.pi*d['dow']/7).astype('float32')\n",
        "    d['doy_sin'] = np.sin(2*np.pi*(dt.dt.dayofyear.astype('int16')/365.25)).astype('float32')\n",
        "    d['doy_cos'] = np.cos(2*np.pi*(dt.dt.dayofyear.astype('int16')/365.25)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def ensure_policy_and_holiday(df):\n",
        "    d = df.copy(); dt = to_local(d['pickup_datetime'])\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    d['after_hike'] = (dt >= cutoff).astype('int8')\n",
        "    # Holiday flag via pandas US Federal Holidays\n",
        "    from pandas.tseries.holiday import USFederalHolidayCalendar as USCal\n",
        "    hol = USCal().holidays(start=dt.min().normalize().tz_localize(None), end=dt.max().normalize().tz_localize(None))\n",
        "    hol_dates = set(pd.to_datetime(hol).date)\n",
        "    d['is_holiday'] = dt.dt.date.map(lambda x: x in hol_dates).astype('int8')\n",
        "    return d\n",
        "\n",
        "def add_geohash5(df):\n",
        "    d = df.copy()\n",
        "    if 'pu_gh5' in d.columns and 'do_gh5' in d.columns: return d\n",
        "    pu_lat = d['pickup_latitude'].astype('float32').values\n",
        "    pu_lon = d['pickup_longitude'].astype('float32').values\n",
        "    do_lat = d['dropoff_latitude'].astype('float32').values\n",
        "    do_lon = d['dropoff_longitude'].astype('float32').values\n",
        "    d['pu_gh5'] = [pgh.encode(float(la), float(lo), precision=5) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(pu_lat, pu_lon)]\n",
        "    d['do_gh5'] = [pgh.encode(float(la), float(lo), precision=5) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(do_lat, do_lon)]\n",
        "    return d\n",
        "\n",
        "# Prepare ordered datasets\n",
        "train_cat = ensure_days_since_start(train_df.copy())\n",
        "test_cat = ensure_days_since_start(test_df.copy())\n",
        "train_cat = ensure_time_parts(train_cat); test_cat = ensure_time_parts(test_cat)\n",
        "train_cat = ensure_policy_and_holiday(train_cat); test_cat = ensure_policy_and_holiday(test_cat)\n",
        "train_cat = add_geohash5(train_cat); test_cat = add_geohash5(test_cat)\n",
        "\n",
        "# Robust numeric feature set (same geometry/POIs as before) + simple temporals; CatBoost handles categoricals internally\n",
        "base_num = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'hour_sin','hour_cos','dow_sin','dow_cos','doy_sin','doy_cos','is_holiday','is_weekend','days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_cat.columns: base_num.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_cat.columns: base_num.append('dist_x_after_hike')\n",
        "\n",
        "# Categorical features for CatBoost ordered statistics\n",
        "cat_cols = ['pu_gh5','do_gh5','hour','dow','week_of_year','after_hike']\n",
        "\n",
        "# Final feature columns\n",
        "use_cols = base_num + cat_cols\n",
        "train_ord = train_cat.dropna(subset=use_cols + ['fare_amount']).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "test_cat[base_num] = test_cat[base_num].fillna(0)\n",
        "X_all = train_ord[use_cols].copy()\n",
        "y_all = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "X_test = test_cat[use_cols].copy()\n",
        "\n",
        "# Indices of categorical features for CatBoost\n",
        "cat_indices = [use_cols.index(c) for c in cat_cols if c in use_cols]\n",
        "\n",
        "# 5-fold time CV with 1-day gap\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=5, gap_days=1)\n",
        "assert len(folds) == 5, 'Expected 5 folds'\n",
        "\n",
        "# CatBoost params \u2014 ordered boosting with conservative regularization\n",
        "cb_params = dict(\n",
        "    loss_function='RMSE',\n",
        "    iterations=30000,\n",
        "    learning_rate=0.05,\n",
        "    depth=8,\n",
        "    l2_leaf_reg=10.0,\n",
        "    random_strength=1.0,\n",
        "    bootstrap_type='Bernoulli', subsample=0.8,\n",
        "    od_type='Iter', od_wait=900,\n",
        "    random_seed=2025,\n",
        "    task_type='CPU',\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "fold_test_preds = []\n",
        "\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    X_tr = X_all.iloc[tr_idx].copy(); X_va = X_all.iloc[va_idx].copy()\n",
        "    y_tr = y_all[tr_idx]; y_va = y_all[va_idx]\n",
        "    # CatBoost Pools\n",
        "    train_pool = Pool(X_tr, label=y_tr, cat_features=cat_indices)\n",
        "    valid_pool = Pool(X_va, label=y_va, cat_features=cat_indices)\n",
        "    model = CatBoostRegressor(**cb_params)\n",
        "    model.fit(train_pool, eval_set=valid_pool, verbose=False)\n",
        "    # Evaluate on original scale\n",
        "    y_pred_val_log = model.predict(valid_pool)\n",
        "    y_pred_val = np.expm1(y_pred_val_log)\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), y_pred_val))\n",
        "    cv_rmses.append(rmse);\n",
        "    best_iters.append(getattr(model, 'tree_count_', cb_params['iterations']))\n",
        "    print(f\"[CatBoost Coarse] Fold {i}: RMSE={rmse:.5f}, trees={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "    # Test predictions per fold for ensembling\n",
        "    test_pool = Pool(X_test, cat_features=cat_indices)\n",
        "    fold_pred = np.expm1(model.predict(test_pool)).astype('float32')\n",
        "    fold_pred = np.clip(fold_pred, 0, 500)\n",
        "    fold_test_preds.append(fold_pred)\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_median = int(np.median(best_iters)) if best_iters else 0\n",
        "print({'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 'median_trees': best_iter_median, 'note': 'CatBoost ordered TE on gh5/week/hour/dow/after_hike'})\n",
        "\n",
        "# Fold-ensemble submission\n",
        "test_pred = np.mean(np.vstack(fold_test_preds), axis=0).astype('float32') if fold_test_preds else np.zeros(len(X_test), dtype='float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "sub = sample[['key']].merge(pd.DataFrame({'key': test_cat['key'].astype('string'), 'fare_amount': test_pred}), on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (CatBoost ordered TE) with shape:', sub.shape)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "id": "2ee0509f-2838-40c8-b83a-e5df7f1a8cb9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 15 \u2014 Mandatory CatBoost Baseline (APPROVED PARAMS):\n",
        "# Model: CatBoostRegressor\n",
        "# Categoricals: ['pu_gh5','do_gh5','week_of_year','hour','dow','after_hike']\n",
        "# Params: depth=7, l2_leaf_reg=30.0, subsample=0.7, od_wait=1500, iterations=30000, learning_rate=0.05, od_type='Iter'\n",
        "# Protocol: 5-fold time CV (1-day gap) to completion, report CV mean/std, fold-ensemble submission\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'sample' in globals(), 'Prereqs missing'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'catboost'])\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "\n",
        "try:\n",
        "    import pygeohash as pgh\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pygeohash'])\n",
        "    import pygeohash as pgh\n",
        "\n",
        "def to_local(dt):\n",
        "    return dt.dt.tz_convert('America/New_York')\n",
        "\n",
        "def ensure_days_since_start(df):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    d = df.copy(); dt = to_local(d['pickup_datetime'])\n",
        "    start_ns = int(dt.min().value)\n",
        "    d['days_since_start'] = ((dt.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def ensure_time_parts(df):\n",
        "    d = df.copy(); dt = to_local(d['pickup_datetime'])\n",
        "    d['hour'] = dt.dt.hour.astype('int8')\n",
        "    d['dow'] = dt.dt.dayofweek.astype('int8')\n",
        "    d['week_of_year'] = dt.dt.isocalendar().week.astype('int16')\n",
        "    # robust cyclic temporals\n",
        "    d['hour_sin'] = np.sin(2*np.pi*d['hour']/24).astype('float32')\n",
        "    d['hour_cos'] = np.cos(2*np.pi*d['hour']/24).astype('float32')\n",
        "    d['dow_sin'] = np.sin(2*np.pi*d['dow']/7).astype('float32')\n",
        "    d['dow_cos'] = np.cos(2*np.pi*d['dow']/7).astype('float32')\n",
        "    d['doy_sin'] = np.sin(2*np.pi*(dt.dt.dayofyear.astype('int16')/365.25)).astype('float32')\n",
        "    d['doy_cos'] = np.cos(2*np.pi*(dt.dt.dayofyear.astype('int16')/365.25)).astype('float32')\n",
        "    d['is_weekend'] = d['dow'].isin([5,6]).astype('int8')\n",
        "    d['is_rush'] = ((d['hour'].between(7,10)) | (d['hour'].between(16,19))).astype('int8')\n",
        "    d['is_night'] = ((d['hour'] >= 22) | (d['hour'] <= 5)).astype('int8')\n",
        "    return d\n",
        "\n",
        "def ensure_policy_and_holiday(df):\n",
        "    d = df.copy(); dt = to_local(d['pickup_datetime'])\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    d['after_hike'] = (dt >= cutoff).astype('int8')\n",
        "    from pandas.tseries.holiday import USFederalHolidayCalendar as USCal\n",
        "    hol = USCal().holidays(start=dt.min().normalize().tz_localize(None), end=dt.max().normalize().tz_localize(None))\n",
        "    hol_dates = set(pd.to_datetime(hol).date)\n",
        "    d['is_holiday'] = dt.dt.date.map(lambda x: x in hol_dates).astype('int8')\n",
        "    return d\n",
        "\n",
        "def add_geohash5(df):\n",
        "    d = df.copy()\n",
        "    if 'pu_gh5' in d.columns and 'do_gh5' in d.columns: return d\n",
        "    pu_lat = d['pickup_latitude'].astype('float32').values\n",
        "    pu_lon = d['pickup_longitude'].astype('float32').values\n",
        "    do_lat = d['dropoff_latitude'].astype('float32').values\n",
        "    do_lon = d['dropoff_longitude'].astype('float32').values\n",
        "    d['pu_gh5'] = [pgh.encode(float(la), float(lo), precision=5) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(pu_lat, pu_lon)]\n",
        "    d['do_gh5'] = [pgh.encode(float(la), float(lo), precision=5) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(do_lat, do_lon)]\n",
        "    return d\n",
        "\n",
        "# Prepare ordered datasets\n",
        "train_cat = ensure_days_since_start(train_df.copy())\n",
        "test_cat = ensure_days_since_start(test_df.copy())\n",
        "train_cat = ensure_time_parts(train_cat); test_cat = ensure_time_parts(test_cat)\n",
        "train_cat = ensure_policy_and_holiday(train_cat); test_cat = ensure_policy_and_holiday(test_cat)\n",
        "train_cat = add_geohash5(train_cat); test_cat = add_geohash5(test_cat)\n",
        "\n",
        "# Robust numeric features (no leakage)\n",
        "base_num = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'hour_sin','hour_cos','dow_sin','dow_cos','doy_sin','doy_cos','is_holiday','is_weekend','is_rush','is_night','days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_cat.columns: base_num.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_cat.columns: base_num.append('dist_x_after_hike')\n",
        "\n",
        "# Mandated categoricals\n",
        "cat_cols = ['pu_gh5','do_gh5','week_of_year','hour','dow','after_hike']\n",
        "\n",
        "use_cols = base_num + cat_cols\n",
        "train_ord = train_cat.dropna(subset=use_cols + ['fare_amount']).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "test_cat[base_num] = test_cat[base_num].fillna(0)\n",
        "X_all = train_ord[use_cols].copy()\n",
        "y_all = np.log1p(train_ord['fare_amount'].astype('float32').values)\n",
        "dt_all = train_ord['pickup_datetime']\n",
        "X_test = test_cat[use_cols].copy()\n",
        "\n",
        "# Categorical indices\n",
        "cat_indices = [use_cols.index(c) for c in cat_cols]\n",
        "\n",
        "# 5-fold time CV with 1-day gap\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=5, gap_days=1)\n",
        "assert len(folds) == 5, 'Expected 5 folds'\n",
        "\n",
        "# CatBoost params (MANDATED)\n",
        "cb_params = dict(\n",
        "    loss_function='RMSE',\n",
        "    iterations=30000,\n",
        "    learning_rate=0.05,\n",
        "    depth=7,\n",
        "    l2_leaf_reg=30.0,\n",
        "    bootstrap_type='Bernoulli', subsample=0.7,\n",
        "    od_type='Iter', od_wait=1500,\n",
        "    random_seed=2025,\n",
        "    task_type='CPU',\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "fold_test_preds = []\n",
        "\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    X_tr = X_all.iloc[tr_idx].copy(); X_va = X_all.iloc[va_idx].copy()\n",
        "    y_tr = y_all[tr_idx]; y_va = y_all[va_idx]\n",
        "    train_pool = Pool(X_tr, label=y_tr, cat_features=cat_indices)\n",
        "    valid_pool = Pool(X_va, label=y_va, cat_features=cat_indices)\n",
        "    model = CatBoostRegressor(**cb_params)\n",
        "    model.fit(train_pool, eval_set=valid_pool, verbose=False)\n",
        "    # Evaluate on original target scale\n",
        "    y_pred_val_log = model.predict(valid_pool)\n",
        "    y_pred_val = np.expm1(y_pred_val_log)\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), y_pred_val))\n",
        "    trees = int(getattr(model, 'tree_count_', cb_params['iterations']))\n",
        "    cv_rmses.append(rmse); best_iters.append(trees)\n",
        "    print(f\"[CatBoost Mandated] Fold {i}: RMSE={rmse:.5f}, trees={trees} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "    # Test predictions per fold (fold ensembling)\n",
        "    test_pool = Pool(X_test, cat_features=cat_indices)\n",
        "    fold_pred = np.expm1(model.predict(test_pool)).astype('float32')\n",
        "    fold_pred = np.clip(fold_pred, 0, 500)\n",
        "    fold_test_preds.append(fold_pred)\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_median = int(np.median(best_iters)) if best_iters else 0\n",
        "print({'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 'median_trees': best_iter_median, 'note': 'CatBoost baseline (mandated params) 5-fold time CV'})\n",
        "\n",
        "# Fold-ensemble submission\n",
        "test_pred = np.mean(np.vstack(fold_test_preds), axis=0).astype('float32') if fold_test_preds else np.zeros(len(X_test), dtype='float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "sub = sample[['key']].merge(pd.DataFrame({'key': test_cat['key'].astype('string'), 'fare_amount': test_pred}), on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (CatBoost mandated) with shape:', sub.shape)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost Mandated] Fold 1: RMSE=3.75623, trees=3484 (train_n=319908, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost Mandated] Fold 2: RMSE=3.35869, trees=246 (train_n=640523, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost Mandated] Fold 3: RMSE=3.56513, trees=16577 (train_n=961323, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost Mandated] Fold 4: RMSE=3.50761, trees=16735 (train_n=1282234, val_n=320756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CatBoost Mandated] Fold 5: RMSE=3.22177, trees=18746 (train_n=1602868, val_n=320755)\n{'cv_rmse_mean': 3.481887, 'cv_rmse_std': 0.18204, 'median_trees': 16577, 'note': 'CatBoost baseline (mandated params) 5-fold time CV'}\nSaved submission.csv (CatBoost mandated) with shape: (9914, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "45b272a7-db8e-458e-bc00-822d6dfd1c8d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 15 \u2014 Contingency Blend (self-contained): CatBoost (mandated) + LightGBM anchor (distance-only) with 5-fold time CV\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'sample' in globals(), 'Prereqs missing: train_df/test_df/sample'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "try:\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'catboost'])\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "try:\n",
        "    import pygeohash as pgh\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pygeohash'])\n",
        "    import pygeohash as pgh\n",
        "\n",
        "# Rebuild CatBoost-ready features locally to avoid dtype/name drift\n",
        "def to_local(dt):\n",
        "    return dt.dt.tz_convert('America/New_York')\n",
        "\n",
        "def ensure_days_since_start(df):\n",
        "    if 'days_since_start' in df.columns: return df\n",
        "    d = df.copy(); dt = to_local(d['pickup_datetime'])\n",
        "    start_ns = int(dt.min().value)\n",
        "    d['days_since_start'] = ((dt.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d\n",
        "\n",
        "def ensure_time_parts(df):\n",
        "    d = df.copy(); dt = to_local(d['pickup_datetime'])\n",
        "    d['hour'] = dt.dt.hour.astype('int8')\n",
        "    d['dow'] = dt.dt.dayofweek.astype('int8')\n",
        "    d['week_of_year'] = dt.dt.isocalendar().week.astype('int16')\n",
        "    d['hour_sin'] = np.sin(2*np.pi*d['hour']/24).astype('float32')\n",
        "    d['hour_cos'] = np.cos(2*np.pi*d['hour']/24).astype('float32')\n",
        "    d['dow_sin'] = np.sin(2*np.pi*d['dow']/7).astype('float32')\n",
        "    d['dow_cos'] = np.cos(2*np.pi*d['dow']/7).astype('float32')\n",
        "    d['doy_sin'] = np.sin(2*np.pi*(dt.dt.dayofyear.astype('int16')/365.25)).astype('float32')\n",
        "    d['doy_cos'] = np.cos(2*np.pi*(dt.dt.dayofyear.astype('int16')/365.25)).astype('float32')\n",
        "    d['is_weekend'] = d['dow'].isin([5,6]).astype('int8')\n",
        "    d['is_rush'] = ((d['hour'].between(7,10)) | (d['hour'].between(16,19))).astype('int8')\n",
        "    d['is_night'] = ((d['hour'] >= 22) | (d['hour'] <= 5)).astype('int8')\n",
        "    return d\n",
        "\n",
        "def ensure_policy_and_holiday(df):\n",
        "    d = df.copy(); dt = to_local(d['pickup_datetime'])\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    d['after_hike'] = (dt >= cutoff).astype('int8')\n",
        "    from pandas.tseries.holiday import USFederalHolidayCalendar as USCal\n",
        "    hol = USCal().holidays(start=dt.min().normalize().tz_localize(None), end=dt.max().normalize().tz_localize(None))\n",
        "    hol_dates = set(pd.to_datetime(hol).date)\n",
        "    d['is_holiday'] = dt.dt.date.map(lambda x: x in hol_dates).astype('int8')\n",
        "    return d\n",
        "\n",
        "def add_geohash5(df):\n",
        "    d = df.copy()\n",
        "    if 'pu_gh5' in d.columns and 'do_gh5' in d.columns: return d\n",
        "    pu_lat = d['pickup_latitude'].astype('float32').values\n",
        "    pu_lon = d['pickup_longitude'].astype('float32').values\n",
        "    do_lat = d['dropoff_latitude'].astype('float32').values\n",
        "    do_lon = d['dropoff_longitude'].astype('float32').values\n",
        "    d['pu_gh5'] = [pgh.encode(float(la), float(lo), precision=5) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(pu_lat, pu_lon)]\n",
        "    d['do_gh5'] = [pgh.encode(float(la), float(lo), precision=5) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(do_lat, do_lon)]\n",
        "    return d\n",
        "\n",
        "# Build CatBoost dataset\n",
        "train_cat = ensure_days_since_start(train_df.copy())\n",
        "test_cat = ensure_days_since_start(test_df.copy())\n",
        "train_cat = ensure_time_parts(train_cat); test_cat = ensure_time_parts(test_cat)\n",
        "train_cat = ensure_policy_and_holiday(train_cat); test_cat = ensure_policy_and_holiday(test_cat)\n",
        "train_cat = add_geohash5(train_cat); test_cat = add_geohash5(test_cat)\n",
        "\n",
        "base_num = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'hour_sin','hour_cos','dow_sin','dow_cos','doy_sin','doy_cos','is_holiday','is_weekend','is_rush','is_night','days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_cat.columns: base_num.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_cat.columns: base_num.append('dist_x_after_hike')\n",
        "\n",
        "cat_cols = ['pu_gh5','do_gh5','week_of_year','hour','dow','after_hike']\n",
        "use_cols = base_num + cat_cols\n",
        "train_ord_local = train_cat.dropna(subset=use_cols + ['fare_amount']).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "test_cat[base_num] = test_cat[base_num].fillna(0)\n",
        "X_all_df = train_ord_local[use_cols].copy()\n",
        "y_all_log = np.log1p(train_ord_local['fare_amount'].astype('float32').values)\n",
        "dt_all = train_ord_local['pickup_datetime']\n",
        "X_test_df = test_cat[use_cols].copy()\n",
        "cat_indices = [use_cols.index(c) for c in cat_cols]\n",
        "\n",
        "# Folds\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=5, gap_days=1)\n",
        "assert len(folds) == 5\n",
        "\n",
        "# CatBoost params (mandated)\n",
        "cb_params = dict(\n",
        "    loss_function='RMSE',\n",
        "    iterations=30000,\n",
        "    learning_rate=0.05,\n",
        "    depth=7,\n",
        "    l2_leaf_reg=30.0,\n",
        "    bootstrap_type='Bernoulli', subsample=0.7,\n",
        "    od_type='Iter', od_wait=1500,\n",
        "    random_seed=2025,\n",
        "    task_type='CPU',\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Anchor LGBM (distance-only, huber, monotone on distances/time)\n",
        "lgbm_anchor_params = {\n",
        "    'objective': 'huber',\n",
        "    'metric': 'rmse',\n",
        "    'n_estimators': 10000,\n",
        "    'learning_rate': 0.04,\n",
        "    'num_leaves': 64,\n",
        "    'min_data_in_leaf': 1000,\n",
        "    'reg_lambda': 40.0,\n",
        "    'feature_fraction': 0.7,\n",
        "    'bagging_fraction': 0.7,\n",
        "    'bagging_freq': 1,\n",
        "    'random_state': 2025,\n",
        "    'verbose': -1\n",
        "}\n",
        "anchor_feats = [\n",
        "    'dist_hav_km','dist_man_km','passenger_count',\n",
        "    'hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'days_since_start','center_lat','center_lon'\n",
        "]\n",
        "for f in anchor_feats:\n",
        "    assert f in X_all_df.columns, f'Anchor feature missing: {f}'\n",
        "\n",
        "n = len(train_ord_local)\n",
        "oof_cb_log = np.full(n, np.nan, dtype='float32')\n",
        "oof_lgb_log = np.full(n, np.nan, dtype='float32')\n",
        "test_cb_logs, test_lgb_logs = [], []\n",
        "\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    # CatBoost\n",
        "    X_tr_cb = X_all_df.iloc[tr_idx].copy(); X_va_cb = X_all_df.iloc[va_idx].copy()\n",
        "    y_tr = y_all_log[tr_idx]; y_va = y_all_log[va_idx]\n",
        "    train_pool = Pool(X_tr_cb, label=y_tr, cat_features=cat_indices)\n",
        "    valid_pool = Pool(X_va_cb, label=y_va, cat_features=cat_indices)\n",
        "    cb = CatBoostRegressor(**cb_params)\n",
        "    cb.fit(train_pool, eval_set=valid_pool, verbose=False)\n",
        "    va_cb_log = cb.predict(valid_pool).astype('float32')\n",
        "    oof_cb_log[va_idx] = va_cb_log\n",
        "    test_pool = Pool(X_test_df, cat_features=cat_indices)\n",
        "    test_cb_logs.append(cb.predict(test_pool).astype('float32'))\n",
        "\n",
        "    # LightGBM anchor\n",
        "    X_tr_lgb = X_all_df.iloc[tr_idx][anchor_feats].astype('float32').values\n",
        "    X_va_lgb = X_all_df.iloc[va_idx][anchor_feats].astype('float32').values\n",
        "    lgbm = lgb.LGBMRegressor(**lgbm_anchor_params)\n",
        "    mono = [0]*len(anchor_feats)\n",
        "    for nm in ['dist_hav_km','dist_man_km','days_since_start']:\n",
        "        mono[anchor_feats.index(nm)] = 1\n",
        "    lgbm.set_params(monotone_constraints=mono)\n",
        "    lgbm.fit(\n",
        "        X_tr_lgb, y_tr,\n",
        "        eval_set=[(X_va_lgb, y_va)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
        "    )\n",
        "    va_lgb_log = lgbm.predict(X_va_lgb, num_iteration=lgbm.best_iteration_).astype('float32')\n",
        "    oof_lgb_log[va_idx] = va_lgb_log\n",
        "    test_lgb_logs.append(lgbm.predict(X_test_df[anchor_feats].astype('float32').values, num_iteration=lgbm.best_iteration_).astype('float32'))\n",
        "\n",
        "    va_blend_log = 0.5 * va_cb_log + 0.5 * va_lgb_log\n",
        "    rmse_fold = float(root_mean_squared_error(np.expm1(y_va), np.expm1(va_blend_log)))\n",
        "    print(f\"[Blend] Fold {i}: RMSE_blend(0.5)={rmse_fold:.5f} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "\n",
        "# Weight grid\n",
        "weights = [0.5, 0.6, 0.4]\n",
        "results = []\n",
        "for w in weights:\n",
        "    oof_blend_log = (w * oof_cb_log + (1.0 - w) * oof_lgb_log).astype('float32')\n",
        "    per_fold = []\n",
        "    for (_, va_idx) in folds:\n",
        "        per_fold.append(float(root_mean_squared_error(np.expm1(y_all_log[va_idx]), np.expm1(oof_blend_log[va_idx]))))\n",
        "    cv_mean = float(np.mean(per_fold)); cv_std = float(np.std(per_fold))\n",
        "    results.append((w, cv_mean, cv_std, per_fold))\n",
        "    print({'weight_cb': w, 'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 'per_fold': [round(x,5) for x in per_fold]})\n",
        "\n",
        "best = sorted(results, key=lambda x: (x[1], x[2]))[0]\n",
        "w_best, cv_mean_best, cv_std_best, per_fold_best = best\n",
        "print({'selected_weight_cb': w_best, 'cv_rmse_mean': round(cv_mean_best,6), 'cv_rmse_std': round(cv_std_best,6)})\n",
        "\n",
        "# Build blended test predictions\n",
        "test_cb_mean_log = np.mean(np.vstack(test_cb_logs), axis=0).astype('float32')\n",
        "test_lgb_mean_log = np.mean(np.vstack(test_lgb_logs), axis=0).astype('float32')\n",
        "test_blend_log = (w_best * test_cb_mean_log + (1.0 - w_best) * test_lgb_mean_log).astype('float32')\n",
        "test_pred = np.expm1(test_blend_log).astype('float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "\n",
        "sub = sample[['key']].merge(pd.DataFrame({'key': test_cat['key'].astype('string'), 'fare_amount': test_pred}), on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (CatBoost+LGBM anchor blend) with shape:', sub.shape)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "id": "ece7a637-84e7-42da-a6d5-7aea8077747e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 16 \u2014 Scale Up for Stability: 6M-row hash sample + robust LightGBM (distance-only anchor) 5-fold time CV\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'test' in globals() and 'sample' in globals(), 'Prereqs missing: test/sample not loaded'\n",
        "assert 'load_train_sample_hash' in globals(), 'Hash sampler not found (cell 2)';\n",
        "assert 'clean_train' in globals() and 'clip_test' in globals() and 'add_features' in globals(), 'Feature helpers missing (cell 3)'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# 1) Load larger, unbiased training sample (6,000,000 rows)\n",
        "BIG_N = 6_000_000\n",
        "train_big = load_train_sample_hash(target_rows=BIG_N, est_total_rows=55_000_000, oversample=1.2, cache_path=None)\n",
        "print({'train_big_rows': int(len(train_big))})\n",
        "\n",
        "# 2) Train/test parity prep\n",
        "train_df_big = clean_train(train_big)\n",
        "test_df_big = clip_test(test)\n",
        "train_df_big = add_features(train_df_big)\n",
        "test_df_big = add_features(test_df_big)\n",
        "for col in ['dist_hav_km','dist_man_km']:\n",
        "    if col in test_df_big.columns:\n",
        "        test_df_big[col] = test_df_big[col].clip(lower=0.01, upper=200).astype('float32')\n",
        "    if col in train_df_big.columns:\n",
        "        train_df_big[col] = train_df_big[col].clip(lower=0.01, upper=200).astype('float32')\n",
        "\n",
        "# 3) Anchor feature set (numeric only, stable)\n",
        "anchor_feats = [\n",
        "    'dist_hav_km','dist_man_km','passenger_count',\n",
        "    'hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'days_since_start','center_lat','center_lon'\n",
        "]\n",
        "# Ensure required columns exist; fallback to derive minimal time cycles from pickup_datetime if missing\n",
        "if 'days_since_start' not in train_df_big.columns:\n",
        "    dt_local = train_df_big['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    start_ns = int(dt_local.min().value)\n",
        "    train_df_big['days_since_start'] = ((dt_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    dt_l2 = test_df_big['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    test_df_big['days_since_start'] = ((dt_l2.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "if 'hour_sin' not in train_df_big.columns or 'dow_sin' not in train_df_big.columns:\n",
        "    dlt = train_df_big['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    train_df_big['hour'] = dlt.dt.hour.astype('int8'); train_df_big['dow'] = dlt.dt.dayofweek.astype('int8')\n",
        "    train_df_big['hour_sin'] = np.sin(2*np.pi*train_df_big['hour']/24).astype('float32')\n",
        "    train_df_big['hour_cos'] = np.cos(2*np.pi*train_df_big['hour']/24).astype('float32')\n",
        "    train_df_big['dow_sin'] = np.sin(2*np.pi*train_df_big['dow']/7).astype('float32')\n",
        "    train_df_big['dow_cos'] = np.cos(2*np.pi*train_df_big['dow']/7).astype('float32')\n",
        "    dlt2 = test_df_big['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    test_df_big['hour'] = dlt2.dt.hour.astype('int8'); test_df_big['dow'] = dlt2.dt.dayofweek.astype('int8')\n",
        "    test_df_big['hour_sin'] = np.sin(2*np.pi*test_df_big['hour']/24).astype('float32')\n",
        "    test_df_big['hour_cos'] = np.cos(2*np.pi*test_df_big['hour']/24).astype('float32')\n",
        "    test_df_big['dow_sin'] = np.sin(2*np.pi*test_df_big['dow']/7).astype('float32')\n",
        "    test_df_big['dow_cos'] = np.cos(2*np.pi*test_df_big['dow']/7).astype('float32')\n",
        "\n",
        "need_cols = anchor_feats + ['fare_amount']\n",
        "train_df_big = train_df_big.dropna(subset=need_cols).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "test_df_big[anchor_feats] = test_df_big[anchor_feats].fillna(0)\n",
        "\n",
        "X_all = train_df_big[anchor_feats].astype('float32').values\n",
        "y_all = np.log1p(train_df_big['fare_amount'].astype('float32').values)\n",
        "dt_all = train_df_big['pickup_datetime']\n",
        "X_test = test_df_big[anchor_feats].astype('float32').values\n",
        "\n",
        "# 4) 5-fold time CV (1-day gap) with huber objective and strong regularization\n",
        "assert 'make_time_folds_quantile' in globals(), 'fold generator missing'\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=5, gap_days=1)\n",
        "assert len(folds) == 5\n",
        "\n",
        "lgbm_anchor_params = {\n",
        "    'objective': 'huber',\n",
        "    'metric': 'rmse',\n",
        "    'n_estimators': 16000,\n",
        "    'learning_rate': 0.04,\n",
        "    'num_leaves': 64,\n",
        "    'min_data_in_leaf': 1500,\n",
        "    'reg_lambda': 40.0,\n",
        "    'feature_fraction': 0.7,\n",
        "    'bagging_fraction': 0.7,\n",
        "    'bagging_freq': 1,\n",
        "    'random_state': 2025,\n",
        "    'verbose': -1\n",
        "}\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "    X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "    mono = [0]*len(anchor_feats)\n",
        "    for nm in ['dist_hav_km','dist_man_km','days_since_start']:\n",
        "        mono[anchor_feats.index(nm)] = 1\n",
        "    model = lgb.LGBMRegressor(**{**lgbm_anchor_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False), lgb.log_evaluation(period=300)]\n",
        "    )\n",
        "    y_pred_val = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), y_pred_val))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    print(f\"[Anchor 6M] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_final = int(np.clip(int(np.median(best_iters)), 600, lgbm_anchor_params['n_estimators']))\n",
        "print({'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 'best_iter_final': best_iter_final, 'note': '6M rows, distance-only anchor LGBM'})\n",
        "\n",
        "# 5) Final fit on all 6M and submission (anchor-only for now)\n",
        "final_model = lgb.LGBMRegressor(**{**lgbm_anchor_params, 'n_estimators': best_iter_final, 'monotone_constraints': mono})\n",
        "final_model.fit(X_all, y_all)\n",
        "test_pred = np.expm1(final_model.predict(X_test)).astype('float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "sub = sample[['key']].merge(pd.DataFrame({'key': test_df_big['key'].astype('string'), 'fare_amount': test_pred}), on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (6M anchor LGBM) with shape:', sub.shape)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train_sample_rows': 6000000, 'p': 0.1309090909090909}\n{'train_big_rows': 6000000}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.244552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.244095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.243961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.243892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.243853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.243821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.243808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.2438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2700]\tvalid_0's rmse: 0.243793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.243794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Anchor 6M] Fold 1: RMSE=3.81526, best_iter=2847 (train_n=959825, val_n=962298)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.191257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.190177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.189822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.189651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.189548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.189476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.189425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.189388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2700]\tvalid_0's rmse: 0.189363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.18934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3300]\tvalid_0's rmse: 0.189333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\tvalid_0's rmse: 0.189328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Anchor 6M] Fold 2: RMSE=2.98583, best_iter=3465 (train_n=1921688, val_n=962298)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.226244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.225139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.224849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.224673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.224547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.224458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.224401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.224358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2700]\tvalid_0's rmse: 0.224315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.224279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3300]\tvalid_0's rmse: 0.224261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\tvalid_0's rmse: 0.224245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3900]\tvalid_0's rmse: 0.224236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4200]\tvalid_0's rmse: 0.224217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4500]\tvalid_0's rmse: 0.224213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4800]\tvalid_0's rmse: 0.224205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5100]\tvalid_0's rmse: 0.224201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5400]\tvalid_0's rmse: 0.224189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Anchor 6M] Fold 3: RMSE=3.88859, best_iter=5234 (train_n=2884002, val_n=962298)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.219733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.2184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.218009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.217829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.217698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.217596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.217522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.217472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2700]\tvalid_0's rmse: 0.217429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.217391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3300]\tvalid_0's rmse: 0.217368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\tvalid_0's rmse: 0.217342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3900]\tvalid_0's rmse: 0.217331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4200]\tvalid_0's rmse: 0.217313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4500]\tvalid_0's rmse: 0.217302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4800]\tvalid_0's rmse: 0.217287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5100]\tvalid_0's rmse: 0.217278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5400]\tvalid_0's rmse: 0.217272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5700]\tvalid_0's rmse: 0.217264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6000]\tvalid_0's rmse: 0.217256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6300]\tvalid_0's rmse: 0.217245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6600]\tvalid_0's rmse: 0.217243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6900]\tvalid_0's rmse: 0.217239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7200]\tvalid_0's rmse: 0.217231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7500]\tvalid_0's rmse: 0.217228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7800]\tvalid_0's rmse: 0.217231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Anchor 6M] Fold 4: RMSE=3.91523, best_iter=7517 (train_n=3846744, val_n=962298)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[300]\tvalid_0's rmse: 0.204723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\tvalid_0's rmse: 0.202878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\tvalid_0's rmse: 0.202418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\tvalid_0's rmse: 0.202198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1500]\tvalid_0's rmse: 0.202067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\tvalid_0's rmse: 0.201969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2100]\tvalid_0's rmse: 0.201883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\tvalid_0's rmse: 0.201825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2700]\tvalid_0's rmse: 0.201771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\tvalid_0's rmse: 0.201725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3300]\tvalid_0's rmse: 0.201699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\tvalid_0's rmse: 0.201675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3900]\tvalid_0's rmse: 0.201655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4200]\tvalid_0's rmse: 0.201633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4500]\tvalid_0's rmse: 0.201619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4800]\tvalid_0's rmse: 0.20161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5100]\tvalid_0's rmse: 0.201593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5400]\tvalid_0's rmse: 0.201581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5700]\tvalid_0's rmse: 0.201566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6000]\tvalid_0's rmse: 0.201554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6300]\tvalid_0's rmse: 0.201543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6600]\tvalid_0's rmse: 0.201536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6900]\tvalid_0's rmse: 0.20153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7200]\tvalid_0's rmse: 0.20152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7500]\tvalid_0's rmse: 0.201518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7800]\tvalid_0's rmse: 0.201515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8100]\tvalid_0's rmse: 0.201513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8400]\tvalid_0's rmse: 0.201511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8700]\tvalid_0's rmse: 0.201509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9000]\tvalid_0's rmse: 0.201504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9300]\tvalid_0's rmse: 0.201505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9600]\tvalid_0's rmse: 0.201502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9900]\tvalid_0's rmse: 0.201503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10200]\tvalid_0's rmse: 0.201508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Anchor 6M] Fold 5: RMSE=3.68505, best_iter=9809 (train_n=4808673, val_n=962298)\n{'cv_rmse_mean': 3.657992, 'cv_rmse_std': 0.34543, 'best_iter_final': 5234, 'note': '6M rows, distance-only anchor LGBM'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (6M anchor LGBM) with shape: (9914, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "6324c9b2-317d-4947-93c3-5a910f29d76b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 16b \u2014 Option A Pragmatic Blend: Small CatBoost on 2M (early data) + 6M Anchor LGBM preds; weight tuned on recent slice\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df' in globals() and 'test_df' in globals() and 'sample' in globals(), 'Prereqs missing'\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'catboost'])\n",
        "    from catboost import CatBoostRegressor, Pool\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "try:\n",
        "    import pygeohash as pgh\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pygeohash'])\n",
        "    import pygeohash as pgh\n",
        "\n",
        "# 0) Utilities\n",
        "def to_local(dt):\n",
        "    return dt.dt.tz_convert('America/New_York')\n",
        "\n",
        "def ensure_time_feats(df):\n",
        "    d = df.copy()\n",
        "    dt = to_local(d['pickup_datetime'])\n",
        "    # days_since_start anchored to train's earliest timestamp\n",
        "    if 'days_since_start' not in d.columns:\n",
        "        start_ns = int(to_local(train_df['pickup_datetime']).min().value)\n",
        "        d['days_since_start'] = ((dt.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    d['hour'] = dt.dt.hour.astype('int8')\n",
        "    d['dow'] = dt.dt.dayofweek.astype('int8')\n",
        "    d['week_of_year'] = dt.dt.isocalendar().week.astype('int16')\n",
        "    d['hour_sin'] = np.sin(2*np.pi*d['hour']/24).astype('float32')\n",
        "    d['hour_cos'] = np.cos(2*np.pi*d['hour']/24).astype('float32')\n",
        "    d['dow_sin'] = np.sin(2*np.pi*d['dow']/7).astype('float32')\n",
        "    d['dow_cos'] = np.cos(2*np.pi*d['dow']/7).astype('float32')\n",
        "    d['doy_sin'] = np.sin(2*np.pi*(dt.dt.dayofyear.astype('int16')/365.25)).astype('float32')\n",
        "    d['doy_cos'] = np.cos(2*np.pi*(dt.dt.dayofyear.astype('int16')/365.25)).astype('float32')\n",
        "    d['is_weekend'] = d['dow'].isin([5,6]).astype('int8')\n",
        "    d['is_rush'] = ((d['hour'].between(7,10)) | (d['hour'].between(16,19))).astype('int8')\n",
        "    d['is_night'] = ((d['hour'] >= 22) | (d['hour'] <= 5)).astype('int8')\n",
        "    # Policy and holiday\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    d['after_hike'] = (dt >= cutoff).astype('int8')\n",
        "    from pandas.tseries.holiday import USFederalHolidayCalendar as USCal\n",
        "    hol = USCal().holidays(start=dt.min().normalize().tz_localize(None), end=dt.max().normalize().tz_localize(None))\n",
        "    hol_dates = set(pd.to_datetime(hol).date)\n",
        "    d['is_holiday'] = dt.dt.date.map(lambda x: x in hol_dates).astype('int8')\n",
        "    return d\n",
        "\n",
        "def add_geohash5(df):\n",
        "    d = df.copy()\n",
        "    if 'pu_gh5' in d.columns and 'do_gh5' in d.columns: return d\n",
        "    pu_lat = d['pickup_latitude'].astype('float32').values\n",
        "    pu_lon = d['pickup_longitude'].astype('float32').values\n",
        "    do_lat = d['dropoff_latitude'].astype('float32').values\n",
        "    do_lon = d['dropoff_longitude'].astype('float32').values\n",
        "    d['pu_gh5'] = [pgh.encode(float(la), float(lo), precision=5) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(pu_lat, pu_lon)]\n",
        "    d['do_gh5'] = [pgh.encode(float(la), float(lo), precision=5) if np.isfinite(la) and np.isfinite(lo) else '' for la, lo in zip(do_lat, do_lon)]\n",
        "    return d\n",
        "\n",
        "# 1) Prepare ordered 2M train view for small CatBoost and small Anchor\n",
        "train_ord = train_df.sort_values('pickup_datetime').reset_index(drop=True).copy()\n",
        "train_ord = ensure_time_feats(train_ord)\n",
        "train_ord = add_geohash5(train_ord)\n",
        "\n",
        "# CatBoost feature sets\n",
        "cb_num = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'hour_sin','hour_cos','dow_sin','dow_cos','doy_sin','doy_cos','is_holiday','is_weekend','is_rush','is_night','days_since_start',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend'\n",
        "]\n",
        "if 'rot_manh_km' in train_ord.columns: cb_num.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in train_ord.columns: cb_num.append('dist_x_after_hike')\n",
        "cb_cat = ['pu_gh5','do_gh5','week_of_year','hour','dow','after_hike']\n",
        "cb_cols = cb_num + cb_cat\n",
        "\n",
        "# Anchor features (distance-only, stable)\n",
        "anchor_feats = [\n",
        "    'dist_hav_km','dist_man_km','passenger_count',\n",
        "    'hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'days_since_start','center_lat','center_lon'\n",
        "]\n",
        "for f in anchor_feats: assert f in train_ord.columns, f'Missing anchor feature {f}'\n",
        "\n",
        "# 2) Define recent validation slice from 2M and early training part\n",
        "N = len(train_ord)\n",
        "val_n = min(300_000, int(0.15 * N))\n",
        "train_end = N - val_n\n",
        "assert train_end > 0, 'Not enough rows for split'\n",
        "tr_part = train_ord.iloc[:train_end].copy()\n",
        "va_part = train_ord.iloc[train_end:].copy()\n",
        "y_tr_log = np.log1p(tr_part['fare_amount'].astype('float32').values)\n",
        "y_va_log = np.log1p(va_part['fare_amount'].astype('float32').values)\n",
        "\n",
        "# 3) Small CatBoost on early part\n",
        "X_tr_cb = tr_part[cb_cols].copy()\n",
        "X_va_cb = va_part[cb_cols].copy()\n",
        "# Build test features with required temporals/categoricals\n",
        "test_te = ensure_time_feats(test_df.copy())\n",
        "test_te = add_geohash5(test_te)\n",
        "X_te_cb = test_te[cb_cols].copy()\n",
        "cb_cat_idx = [cb_cols.index(c) for c in cb_cat]\n",
        "cb_params_small = dict(\n",
        "    loss_function='RMSE',\n",
        "    iterations=12000,\n",
        "    learning_rate=0.05,\n",
        "    depth=7,\n",
        "    l2_leaf_reg=30.0,\n",
        "    bootstrap_type='Bernoulli', subsample=0.7,\n",
        "    od_type='Iter', od_wait=800,\n",
        "    random_seed=2026, task_type='CPU', verbose=False\n",
        ")\n",
        "pool_tr = Pool(X_tr_cb, label=y_tr_log, cat_features=cb_cat_idx)\n",
        "pool_va = Pool(X_va_cb, label=y_va_log, cat_features=cb_cat_idx)\n",
        "pool_te = Pool(X_te_cb, cat_features=cb_cat_idx)\n",
        "cb_small = CatBoostRegressor(**cb_params_small)\n",
        "cb_small.fit(pool_tr, eval_set=pool_va, verbose=False)\n",
        "va_cb_log = cb_small.predict(pool_va).astype('float32')\n",
        "test_cb_pred = np.expm1(cb_small.predict(pool_te)).astype('float32')\n",
        "\n",
        "# 4) Small Anchor LGBM on early part\n",
        "X_tr_lgb = tr_part[anchor_feats].astype('float32').values\n",
        "X_va_lgb = va_part[anchor_feats].astype('float32').values\n",
        "lgb_params_anchor_small = {\n",
        "    'objective': 'huber', 'metric': 'rmse',\n",
        "    'n_estimators': 12000, 'learning_rate': 0.04,\n",
        "    'num_leaves': 64, 'min_data_in_leaf': 1000,\n",
        "    'reg_lambda': 40.0, 'feature_fraction': 0.7,\n",
        "    'bagging_fraction': 0.7, 'bagging_freq': 1,\n",
        "    'random_state': 2026, 'verbose': -1\n",
        "}\n",
        "mono = [0]*len(anchor_feats)\n",
        "for nm in ['dist_hav_km','dist_man_km','days_since_start']:\n",
        "    mono[anchor_feats.index(nm)] = 1\n",
        "lgb_anchor = lgb.LGBMRegressor(**{**lgb_params_anchor_small, 'monotone_constraints': mono})\n",
        "lgb_anchor.fit(\n",
        "    X_tr_lgb, y_tr_log,\n",
        "    eval_set=[(X_va_lgb, y_va_log)],\n",
        "    eval_metric='rmse',\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)]\n",
        ")\n",
        "va_lgb_log = lgb_anchor.predict(X_va_lgb, num_iteration=lgb_anchor.best_iteration_).astype('float32')\n",
        "\n",
        "# 5) Load existing 6M Anchor test predictions from current submission (must exist from Cell 41)\n",
        "sub6m = pd.read_csv('submission.csv', dtype={'key':'string'})\n",
        "assert set(sub6m.columns)=={'key','fare_amount'}, 'submission.csv not in expected format'\n",
        "anchor6m_pred = sub6m['fare_amount'].astype('float32').values\n",
        "assert len(anchor6m_pred) == len(sample), 'Anchor 6M submission size mismatch'\n",
        "\n",
        "# 6) Tune blend weights on recent slice (grid) between CatBoost_small and Anchor_small\n",
        "weights = [0.2,0.3,0.4,0.5,0.6,0.7,0.8]\n",
        "best = (None, 1e9)\n",
        "y_va_true = np.expm1(y_va_log)\n",
        "for w in weights:\n",
        "    blend_va = w * np.expm1(va_cb_log) + (1.0 - w) * np.expm1(va_lgb_log)\n",
        "    rmse = float(root_mean_squared_error(y_va_true, np.clip(blend_va, 0, 500)))\n",
        "    print({'w_cb': w, 'rmse_recent_window': round(rmse,6)})\n",
        "    if rmse < best[1]:\n",
        "        best = (w, rmse)\n",
        "w_best = best[0]\n",
        "print({'selected_w_cb': w_best, 'rmse_recent_window': round(best[1],6)})\n",
        "\n",
        "# 7) Build final blended test predictions: CatBoost_small vs 6M Anchor\n",
        "assert len(test_cb_pred) == len(anchor6m_pred) == len(sample), 'Test size mismatch'\n",
        "final_pred = (w_best * test_cb_pred + (1.0 - w_best) * anchor6m_pred).astype('float32')\n",
        "final_pred = np.clip(final_pred, 0, 500)\n",
        "\n",
        "# 8) Save submission aligned to sample order\n",
        "out = pd.DataFrame({'key': sample['key'].astype('string'), 'fare_amount': final_pred})\n",
        "out.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Option A blend: CB_small + 6M anchor) with shape:', out.shape, 'w_cb=', w_best)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'w_cb': 0.2, 'rmse_recent_window': 3.508365}\n{'w_cb': 0.3, 'rmse_recent_window': 3.427836}\n{'w_cb': 0.4, 'rmse_recent_window': 3.357436}\n{'w_cb': 0.5, 'rmse_recent_window': 3.297813}\n{'w_cb': 0.6, 'rmse_recent_window': 3.24956}\n{'w_cb': 0.7, 'rmse_recent_window': 3.213189}\n{'w_cb': 0.8, 'rmse_recent_window': 3.189107}\n{'selected_w_cb': 0.8, 'rmse_recent_window': 3.189107}\nSaved submission.csv (Option A blend: CB_small + 6M anchor) with shape: (9914, 2) w_cb= 0.8\n"
          ]
        }
      ]
    },
    {
      "id": "fa318e90-a76c-4c06-adce-cd992a546178",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 16 \u2014 Single Robust LightGBM on 6M (Fast Baseline Reset): tuned for speed, 5-fold time CV\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "assert 'train_df_big' in globals() and 'test_df_big' in globals(), '6M train/test not prepared (run cell 41)'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Helpers\n",
        "def ensure_days_since_start(df, ref_start_ns=None):\n",
        "    d = df.copy()\n",
        "    dt_local = d['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    start_ns = int(dt_local.min().value) if ref_start_ns is None else int(ref_start_ns)\n",
        "    d['days_since_start'] = ((dt_local.astype('int64') - np.int64(start_ns)) / np.float64(24*3600*1e9)).astype('float32')\n",
        "    return d, start_ns\n",
        "\n",
        "def ensure_after_hike(df):\n",
        "    d = df.copy()\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    dt_local = d['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    d['after_hike'] = (dt_local >= cutoff).astype('int8')\n",
        "    return d\n",
        "\n",
        "def ensure_clusters(train, test, k=80, sample_n=200_000, seed=2025):\n",
        "    need = ['pu_cl','do_cl','same_cl']\n",
        "    if all(c in train.columns for c in need) and all(c in test.columns for c in need):\n",
        "        return train, test\n",
        "    pu_tr = train[['pickup_latitude','pickup_longitude']].astype('float32').values\n",
        "    do_tr = train[['dropoff_latitude','dropoff_longitude']].astype('float32').values\n",
        "    if len(train) > sample_n:\n",
        "        idx = np.random.default_rng(seed).choice(len(train), size=sample_n, replace=False)\n",
        "        pu_fit = pu_tr[idx]; do_fit = do_tr[idx]\n",
        "    else:\n",
        "        pu_fit = pu_tr; do_fit = do_tr\n",
        "    km_pu = MiniBatchKMeans(n_clusters=k, random_state=seed, batch_size=20000, n_init=3, max_no_improvement=20)\n",
        "    km_do = MiniBatchKMeans(n_clusters=k, random_state=seed+1, batch_size=20000, n_init=3, max_no_improvement=20)\n",
        "    km_pu.fit(pu_fit); km_do.fit(do_fit)\n",
        "    train = train.copy(); test = test.copy()\n",
        "    train['pu_cl'] = km_pu.predict(pu_tr).astype('int32')\n",
        "    train['do_cl'] = km_do.predict(do_tr).astype('int32')\n",
        "    test['pu_cl'] = km_pu.predict(test[['pickup_latitude','pickup_longitude']].astype('float32').values).astype('int32')\n",
        "    test['do_cl'] = km_do.predict(test[['dropoff_latitude','dropoff_longitude']].astype('float32').values).astype('int32')\n",
        "    train['same_cl'] = (train['pu_cl'] == train['do_cl']).astype('int8')\n",
        "    test['same_cl'] = (test['pu_cl'] == test['do_cl']).astype('int8')\n",
        "    return train, test\n",
        "\n",
        "# Copy to avoid mutating prior cells\n",
        "tr = train_df_big.copy()\n",
        "ts = test_df_big.copy()\n",
        "\n",
        "# Ensure time anchors and policy parity\n",
        "tr, start_ns = ensure_days_since_start(tr, ref_start_ns=None)\n",
        "ts, _ = ensure_days_since_start(ts, ref_start_ns=start_ns)\n",
        "tr = ensure_after_hike(tr); ts = ensure_after_hike(ts)\n",
        "\n",
        "# Add KMeans clusters (reduced k for speed)\n",
        "tr, ts = ensure_clusters(tr, ts, k=80, sample_n=200_000, seed=2025)\n",
        "\n",
        "# Ensure engineered features (holiday + dropoff POIs + any-airport flag) exist\n",
        "assert 'add_holiday_and_doy' in globals(), 'add_holiday_and_doy not found'\n",
        "assert 'add_dropoff_pois' in globals(), 'add_dropoff_pois not found'\n",
        "tr = add_holiday_and_doy(tr); ts = add_holiday_and_doy(ts)\n",
        "tr = add_dropoff_pois(tr); ts = add_dropoff_pois(ts)\n",
        "\n",
        "# Feature set (slightly pruned for speed) \u2014 no TE\n",
        "feature_cols = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday','days_since_start','after_hike',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend',\n",
        "    'pu_cl','do_cl','same_cl'\n",
        "]\n",
        "if 'rot_manh_km' in tr.columns: feature_cols.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in tr.columns: feature_cols.append('dist_x_after_hike')\n",
        "\n",
        "# Drop NaNs on train; fill test\n",
        "tr = tr.dropna(subset=feature_cols + ['fare_amount']).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "ts[feature_cols] = ts[feature_cols].fillna(0)\n",
        "\n",
        "# Matrices\n",
        "X_all = tr[feature_cols].astype('float32').values\n",
        "y_all = np.log1p(tr['fare_amount'].astype('float32').values)\n",
        "dt_all = tr['pickup_datetime']\n",
        "X_test = ts[feature_cols].astype('float32').values\n",
        "\n",
        "# 5-fold time CV with 1-day gap\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=5, gap_days=1)\n",
        "assert len(folds) == 5\n",
        "\n",
        "# LightGBM params \u2014 FAST baseline, stronger regularization, quicker ES\n",
        "lgb_params = dict(\n",
        "    objective='regression',\n",
        "    metric='rmse',\n",
        "    learning_rate=0.06,\n",
        "    n_estimators=8000,\n",
        "    num_leaves=96,\n",
        "    min_data_in_leaf=2000,\n",
        "    feature_fraction=0.75,\n",
        "    bagging_fraction=0.75,\n",
        "    bagging_freq=1,\n",
        "    max_bin=127,\n",
        "    reg_alpha=2.0,\n",
        "    reg_lambda=20.0,\n",
        "    random_state=2025,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "def build_monotone(cols):\n",
        "    mono = [0]*len(cols)\n",
        "    for nm in ['days_since_start','dist_hav_km','dist_man_km']:\n",
        "        if nm in cols:\n",
        "            mono[cols.index(nm)] = 1\n",
        "    return mono\n",
        "\n",
        "cv_rmses, best_iters = [], []\n",
        "mono = build_monotone(feature_cols)\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "    X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "    model = lgb.LGBMRegressor(**{**lgb_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False), lgb.log_evaluation(period=100)]\n",
        "    )\n",
        "    y_pred = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), y_pred))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or model.n_estimators))\n",
        "    print(f\"[LGB 6M FAST] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_final = int(np.clip(int(np.median(best_iters)), 400, lgb_params['n_estimators']))\n",
        "print({'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 'best_iter_final': best_iter_final, 'note': '6M FAST robust LGB baseline reset'})\n",
        "\n",
        "# Final fit and submission\n",
        "final_model = lgb.LGBMRegressor(**{**lgb_params, 'n_estimators': best_iter_final, 'monotone_constraints': mono})\n",
        "final_model.fit(X_all, y_all)\n",
        "test_pred = np.expm1(final_model.predict(X_test)).astype('float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "sub = sample[['key']].merge(pd.DataFrame({'key': ts['key'].astype('string'), 'fare_amount': test_pred}), on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (6M FAST robust LGB baseline reset) with shape:', sub.shape)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 0.235365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\tvalid_0's rmse: 0.231526\n"
          ]
        }
      ]
    },
    {
      "id": "5f65ed9c-c59d-47e1-94e2-9b1c78a666ec",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Phase 16 \u2014 Silver-Bullet Features + Robust LGBM on 6M with fold ensembling (boroughs + refined airport flags)\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "\n",
        "assert 'train_df_big' in globals() and 'test_df_big' in globals(), '6M data not prepared (run cell 41)'\n",
        "assert 'make_time_folds_quantile' in globals(), 'Fold generator missing'\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'lightgbm'])\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Copy to avoid mutation\n",
        "tr = train_df_big.copy()\n",
        "ts = test_df_big.copy()\n",
        "\n",
        "# Ensure required engineered features (holiday/day-of-year, dropoff POIs, after_hike, clusters) exist\n",
        "def ensure_after_hike(df):\n",
        "    d = df.copy()\n",
        "    if 'after_hike' in d.columns:\n",
        "        return d\n",
        "    cutoff = pd.Timestamp('2012-09-04 00:00:00', tz='America/New_York')\n",
        "    dt_local = d['pickup_datetime'].dt.tz_convert('America/New_York')\n",
        "    d['after_hike'] = (dt_local >= cutoff).astype('int8')\n",
        "    return d\n",
        "\n",
        "assert 'add_holiday_and_doy' in globals(), 'add_holiday_and_doy not found (see cell 5)'\n",
        "assert 'add_dropoff_pois' in globals(), 'add_dropoff_pois not found (see cell 6)'\n",
        "\n",
        "# Apply holiday/doy, after_hike, and dropoff POIs\n",
        "tr = add_holiday_and_doy(tr); ts = add_holiday_and_doy(ts)\n",
        "tr = ensure_after_hike(tr); ts = ensure_after_hike(ts)\n",
        "tr = add_dropoff_pois(tr); ts = add_dropoff_pois(ts)\n",
        "\n",
        "# Ensure clusters (pu_cl/do_cl/same_cl)\n",
        "try:\n",
        "    tr, ts = ensure_clusters(tr, ts, k=80, sample_n=200_000, seed=2025)\n",
        "except NameError:\n",
        "    from sklearn.cluster import MiniBatchKMeans\n",
        "    def _ensure_clusters(train, test, k=80, sample_n=200_000, seed=2025):\n",
        "        pu_tr = train[['pickup_latitude','pickup_longitude']].astype('float32').values\n",
        "        do_tr = train[['dropoff_latitude','dropoff_longitude']].astype('float32').values\n",
        "        if len(train) > sample_n:\n",
        "            idx = np.random.default_rng(seed).choice(len(train), size=sample_n, replace=False)\n",
        "            pu_fit = pu_tr[idx]; do_fit = do_tr[idx]\n",
        "        else:\n",
        "            pu_fit = pu_tr; do_fit = do_tr\n",
        "        km_pu = MiniBatchKMeans(n_clusters=k, random_state=seed, batch_size=20000, n_init=3, max_no_improvement=20)\n",
        "        km_do = MiniBatchKMeans(n_clusters=k, random_state=seed+1, batch_size=20000, n_init=3, max_no_improvement=20)\n",
        "        km_pu.fit(pu_fit); km_do.fit(do_fit)\n",
        "        train = train.copy(); test = test.copy()\n",
        "        train['pu_cl'] = km_pu.predict(pu_tr).astype('int32')\n",
        "        train['do_cl'] = km_do.predict(do_tr).astype('int32')\n",
        "        test['pu_cl'] = km_pu.predict(test[['pickup_latitude','pickup_longitude']].astype('float32').values).astype('int32')\n",
        "        test['do_cl'] = km_do.predict(test[['dropoff_latitude','dropoff_longitude']].astype('float32').values).astype('int32')\n",
        "        train['same_cl'] = (train['pu_cl'] == train['do_cl']).astype('int8')\n",
        "        test['same_cl'] = (test['pu_cl'] == test['do_cl']).astype('int8')\n",
        "        return train, test\n",
        "    tr, ts = _ensure_clusters(tr, ts, k=80, sample_n=200_000, seed=2025)\n",
        "\n",
        "# 1) Silver-bullet spatial features: borough proxies (vectorized) + refined airport flags\n",
        "def add_borough_and_airports(df):\n",
        "    d = df.copy()\n",
        "    pu_lat = d['pickup_latitude'].astype('float32').values\n",
        "    pu_lon = d['pickup_longitude'].astype('float32').values\n",
        "    do_lat = d['dropoff_latitude'].astype('float32').values\n",
        "    do_lon = d['dropoff_longitude'].astype('float32').values\n",
        "    # Vectorized borough masks (rough bounding boxes)\n",
        "    pu_manh = (pu_lat >= 40.70) & (pu_lat <= 40.88) & (pu_lon >= -74.02) & (pu_lon <= -73.92)\n",
        "    pu_bk   = (pu_lat >= 40.57) & (pu_lat <= 40.73) & (pu_lon >= -74.05) & (pu_lon <= -73.85)\n",
        "    pu_qn   = (pu_lat >= 40.54) & (pu_lat <= 40.80) & (pu_lon >= -73.96) & (pu_lon <= -73.70)\n",
        "    pu_bx   = (pu_lat >= 40.79) & (pu_lat <= 40.91) & (pu_lon >= -73.93) & (pu_lon <= -73.76)\n",
        "    pu_si   = (pu_lat >= 40.49) & (pu_lat <= 40.65) & (pu_lon >= -74.25) & (pu_lon <= -74.05)\n",
        "    do_manh = (do_lat >= 40.70) & (do_lat <= 40.88) & (do_lon >= -74.02) & (do_lon <= -73.92)\n",
        "    do_bk   = (do_lat >= 40.57) & (do_lat <= 40.73) & (do_lon >= -74.05) & (do_lon <= -73.85)\n",
        "    do_qn   = (do_lat >= 40.54) & (do_lat <= 40.80) & (do_lon >= -73.96) & (do_lon <= -73.70)\n",
        "    do_bx   = (do_lat >= 40.79) & (do_lat <= 40.91) & (do_lon >= -73.93) & (do_lon <= -73.76)\n",
        "    do_si   = (do_lat >= 40.49) & (do_lat <= 40.65) & (do_lon >= -74.25) & (do_lon <= -74.05)\n",
        "    d['pu_borough'] = np.select([pu_manh, pu_bk, pu_qn, pu_bx, pu_si], [1,2,3,4,5], default=0).astype('int8')\n",
        "    d['do_borough'] = np.select([do_manh, do_bk, do_qn, do_bx, do_si], [1,2,3,4,5], default=0).astype('int8')\n",
        "    d['is_inter_borough'] = ((d['pu_borough'] != 0) & (d['do_borough'] != 0) & (d['pu_borough'] != d['do_borough'])).astype('int8')\n",
        "    d['is_manhattan_trip'] = ((d['pu_borough'] == 1) | (d['do_borough'] == 1)).astype('int8')\n",
        "    # Refined airport proximity and code\n",
        "    pu_min = d[['dist_jfk','dist_lga','dist_ewr']].astype('float32').min(axis=1)\n",
        "    d['pu_is_airport'] = (pu_min < 2.0).astype('int8')\n",
        "    pu_is_jfk = (d['dist_jfk'] <= d[['dist_lga','dist_ewr']].min(axis=1))\n",
        "    pu_is_lga = (d['dist_lga'] <= d[['dist_jfk','dist_ewr']].min(axis=1))\n",
        "    d['pu_airport_code'] = np.select([pu_is_jfk, pu_is_lga], [1, 2], default=3).astype('int8')\n",
        "    d.loc[d['pu_is_airport'] == 0, 'pu_airport_code'] = 0\n",
        "    if all(c in d.columns for c in ['do_dist_jfk','do_dist_lga','do_dist_ewr']):\n",
        "        do_min = d[['do_dist_jfk','do_dist_lga','do_dist_ewr']].astype('float32').min(axis=1)\n",
        "        d['do_is_airport'] = (do_min < 2.0).astype('int8')\n",
        "        do_is_jfk = (d['do_dist_jfk'] <= d[['do_dist_lga','do_dist_ewr']].min(axis=1))\n",
        "        do_is_lga = (d['do_dist_lga'] <= d[['do_dist_jfk','do_dist_ewr']].min(axis=1))\n",
        "        d['do_airport_code'] = np.select([do_is_jfk, do_is_lga], [1, 2], default=3).astype('int8')\n",
        "        d.loc[d['do_is_airport'] == 0, 'do_airport_code'] = 0\n",
        "    else:\n",
        "        d['do_is_airport'] = np.int8(0); d['do_airport_code'] = np.int8(0)\n",
        "    d['jfk_to_manhattan'] = ((d.get('pu_is_airport', 0) == 1) & (d['pu_airport_code'] == 1) & (d['do_borough'] == 1)).astype('int8')\n",
        "    d['manhattan_to_jfk'] = ((d['pu_borough'] == 1) & (d.get('do_is_airport', 0) == 1) & (d.get('do_airport_code', 0) == 1)).astype('int8')\n",
        "    return d\n",
        "\n",
        "tr = add_borough_and_airports(tr)\n",
        "ts = add_borough_and_airports(ts)\n",
        "\n",
        "# 2) Feature set: robust + silver-bullet features (no TE). Ensure engineered features exist.\n",
        "feature_cols = [\n",
        "    'pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count',\n",
        "    'dist_hav_km','dist_man_km','delta_lat','delta_lon','bear_sin','bear_cos','center_lat','center_lon',\n",
        "    'hour','dow','is_weekend','is_rush','is_night','hour_sin','hour_cos','dow_sin','dow_cos',\n",
        "    'doy_sin','doy_cos','is_holiday','days_since_start','after_hike',\n",
        "    'dist_jfk','dist_lga','dist_ewr','dist_midtown','dist_fidi',\n",
        "    'do_dist_jfk','do_dist_lga','do_dist_ewr','do_dist_midtown','do_dist_fidi',\n",
        "    'is_airport_trip','is_airport_trip_any','dist_x_rush','dist_x_weekend',\n",
        "    'pu_cl','do_cl','same_cl',\n",
        "    'pu_borough','do_borough','is_inter_borough','is_manhattan_trip',\n",
        "    'pu_is_airport','pu_airport_code','do_is_airport','do_airport_code',\n",
        "    'jfk_to_manhattan','manhattan_to_jfk'\n",
        "]\n",
        "if 'rot_manh_km' in tr.columns: feature_cols.append('rot_manh_km')\n",
        "if 'dist_x_after_hike' in tr.columns: feature_cols.append('dist_x_after_hike')\n",
        "\n",
        "# Drop NaNs and prepare matrices\n",
        "tr = tr.dropna(subset=feature_cols + ['fare_amount']).sort_values('pickup_datetime').reset_index(drop=True)\n",
        "ts[feature_cols] = ts[feature_cols].fillna(0)\n",
        "X_all = tr[feature_cols].astype('float32').values\n",
        "y_all = np.log1p(tr['fare_amount'].astype('float32').values)\n",
        "dt_all = tr['pickup_datetime']\n",
        "X_test = ts[feature_cols].astype('float32').values\n",
        "\n",
        "# 3) 5-fold time CV with 1-day gap and fold ensembling\n",
        "folds = make_time_folds_quantile(dt_all, n_folds=5, gap_days=1)\n",
        "assert len(folds) == 5\n",
        "\n",
        "# LGBM params: competition-safe fast config to complete within time\n",
        "lgb_params = dict(\n",
        "    objective='huber', metric='rmse',\n",
        "    learning_rate=0.06, n_estimators=4000,\n",
        "    num_leaves=96, min_data_in_leaf=3000,\n",
        "    feature_fraction=0.70, bagging_fraction=0.70, bagging_freq=1,\n",
        "    max_bin=63, reg_alpha=2.0, reg_lambda=20.0,\n",
        "    random_state=2025, n_jobs=-1, verbose=-1\n",
        ")\n",
        "\n",
        "def build_monotone(cols):\n",
        "    mono = [0]*len(cols)\n",
        "    for nm in ['days_since_start','dist_hav_km','dist_man_km']:\n",
        "        if nm in cols: mono[cols.index(nm)] = 1\n",
        "    return mono\n",
        "\n",
        "cv_rmses, best_iters, fold_test_preds = [], [], []\n",
        "mono = build_monotone(feature_cols)\n",
        "for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
        "    X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "    X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "    model = lgb.LGBMRegressor(**{**lgb_params, 'monotone_constraints': mono})\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False), lgb.log_evaluation(period=200)]\n",
        "    )\n",
        "    y_pred = np.expm1(model.predict(X_va, num_iteration=model.best_iteration_))\n",
        "    rmse = float(root_mean_squared_error(np.expm1(y_va), y_pred))\n",
        "    cv_rmses.append(rmse); best_iters.append(int(model.best_iteration_ or lgb_params['n_estimators']))\n",
        "    print(f\"[6M+Silver FAST] Fold {i}: RMSE={rmse:.5f}, best_iter={best_iters[-1]} (train_n={len(tr_idx)}, val_n={len(va_idx)})\")\n",
        "    # Test prediction for fold ensembling\n",
        "    fold_pred = np.expm1(model.predict(X_test, num_iteration=model.best_iteration_)).astype('float32')\n",
        "    fold_pred = np.clip(fold_pred, 0, 500)\n",
        "    fold_test_preds.append(fold_pred)\n",
        "\n",
        "cv_mean = float(np.mean(cv_rmses)); cv_std = float(np.std(cv_rmses))\n",
        "best_iter_median = int(np.median(best_iters)) if best_iters else 0\n",
        "print({'cv_rmse_mean': round(cv_mean,6), 'cv_rmse_std': round(cv_std,6), 'median_best_iter': best_iter_median, 'note': '6M robust LGB (FAST) with borough/airport features + fold ensembling'})\n",
        "\n",
        "# 4) Fold-ensemble submission\n",
        "test_pred = np.mean(np.vstack(fold_test_preds), axis=0).astype('float32') if fold_test_preds else np.zeros(len(ts), dtype='float32')\n",
        "test_pred = np.clip(test_pred, 0, 500)\n",
        "sub = sample[['key']].merge(pd.DataFrame({'key': ts['key'].astype('string'), 'fare_amount': test_pred}), on='key', how='left')\n",
        "assert sub.shape[0] == sample.shape[0] and sub['fare_amount'].notna().all(), 'Submission alignment/missing preds'\n",
        "sub[['key','fare_amount']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (6M + Silver-Bullet Features, fold-ensemble, FAST) with shape:', sub.shape)"
      ],
      "execution_count": 78,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}