{
  "cells": [
    {
      "id": "64f1deac-37cb-46c6-9312-1708fc4b70f1",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan\n",
        "\n",
        "Goals:\n",
        "- Verify GPU and environment\n",
        "- Inspect provided artifacts and define train/test splits\n",
        "- Establish fast baseline (tabular: tracking + baseline helmets); defer video modeling unless needed\n",
        "- Build robust CV mirroring test (game/time/fold discipline), avoid leakage\n",
        "- Train quick baseline (XGBoost GPU if possible), produce OOF and test preds\n",
        "- Iterate with feature engineering and model ensembling\n",
        "\n",
        "Milestones (request expert review at each):\n",
        "1) Plan + environment check\n",
        "2) Data audit/EDA + fold strategy\n",
        "3) Baseline features + baseline model\n",
        "4) Error analysis + FE v1\n",
        "5) Model tuning / blend\n",
        "6) Finalize submission\n",
        "\n",
        "Metric: MCC on test. Submission: submission.csv.\n",
        "\n",
        "Assumption here: Prepared artifacts already include extracted features from tracking and helmets; we start tabular. We will log progress, cache OOF/preds, and keep deterministic seeds."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "e5808200-7255-4ce3-aabd-48629c81f435",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, time, json, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def run(cmd):\n",
        "    print('>>', ' '.join(cmd), flush=True)\n",
        "    return subprocess.run(cmd, check=False, text=True, capture_output=True).stdout\n",
        "\n",
        "start = time.time()\n",
        "print('Env check...')\n",
        "print(run(['bash','-lc','nvidia-smi || true']))\n",
        "\n",
        "print('Python:', sys.version)\n",
        "print('CWD:', os.getcwd())\n",
        "\n",
        "files = sorted(os.listdir('.'))\n",
        "print('Files:', files)\n",
        "\n",
        "def info(df, name):\n",
        "    print(f'[{name}] shape={df.shape}')\n",
        "    print('cols:', list(df.columns)[:20], ('... total %d cols' % len(df.columns) if len(df.columns)>20 else ''))\n",
        "\n",
        "train_labels = pd.read_csv('train_labels.csv')\n",
        "train_track = pd.read_csv('train_player_tracking.csv')\n",
        "train_helm = pd.read_csv('train_baseline_helmets.csv')\n",
        "train_vmeta = pd.read_csv('train_video_metadata.csv')\n",
        "test_track = pd.read_csv('test_player_tracking.csv')\n",
        "test_helm = pd.read_csv('test_baseline_helmets.csv')\n",
        "test_vmeta = pd.read_csv('test_video_metadata.csv')\n",
        "\n",
        "info(train_labels, 'train_labels')\n",
        "info(train_track, 'train_player_tracking')\n",
        "info(train_helm, 'train_baseline_helmets')\n",
        "info(train_vmeta, 'train_video_metadata')\n",
        "info(test_track, 'test_player_tracking')\n",
        "info(test_helm, 'test_baseline_helmets')\n",
        "info(test_vmeta, 'test_video_metadata')\n",
        "\n",
        "print('Label distribution:')\n",
        "lbl_col = None\n",
        "for c in train_labels.columns:\n",
        "    if c.lower() in ('contact','is_contact','contact_label','label'):\n",
        "        lbl_col = c; break\n",
        "print('label_col:', lbl_col)\n",
        "if lbl_col is not None:\n",
        "    print(train_labels[lbl_col].value_counts(dropna=False))\n",
        "else:\n",
        "    print('No obvious label col found; will inspect later head:')\n",
        "    print(train_labels.head(3))\n",
        "\n",
        "print('Sample submission head:')\n",
        "try:\n",
        "    ss = pd.read_csv('sample_submission.csv')\n",
        "    print(ss.head())\n",
        "except Exception as e:\n",
        "    print('no sample_submission.csv:', e)\n",
        "\n",
        "elapsed = time.time()-start\n",
        "print(f'Env+EDA done in {elapsed:.2f}s', flush=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Env check...\n>> bash -lc nvidia-smi || true\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep 25 23:47:32 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     182MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\nPython: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\nCWD: /var/lib/simon/agent_run_states/nfl-player-contact-detection-20250925-233951\nFiles: ['.00_eda_and_planning_kernel_state.json', '00_eda_and_planning.ipynb', 'agent_metadata', 'description.md', 'docker_run.log', 'requirements.txt', 'sample_submission.csv', 'submission.csv', 'task.txt', 'test', 'test_baseline_helmets.csv', 'test_player_tracking.csv', 'test_video_metadata.csv', 'train', 'train_baseline_helmets.csv', 'train_labels.csv', 'train_player_tracking.csv', 'train_video_metadata.csv']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train_labels] shape=(4258375, 7)\ncols: ['contact_id', 'game_play', 'datetime', 'step', 'nfl_player_id_1', 'nfl_player_id_2', 'contact'] \n[train_player_tracking] shape=(1225299, 17)\ncols: ['game_play', 'game_key', 'play_id', 'nfl_player_id', 'datetime', 'step', 'team', 'position', 'jersey_number', 'x_position', 'y_position', 'speed', 'distance', 'direction', 'orientation', 'acceleration', 'sa'] \n[train_baseline_helmets] shape=(3412208, 12)\ncols: ['game_play', 'game_key', 'play_id', 'view', 'video', 'frame', 'nfl_player_id', 'player_label', 'left', 'width', 'top', 'height'] \n[train_video_metadata] shape=(432, 7)\ncols: ['game_play', 'game_key', 'play_id', 'view', 'start_time', 'end_time', 'snap_time'] \n[test_player_tracking] shape=(127754, 17)\ncols: ['game_play', 'game_key', 'play_id', 'nfl_player_id', 'datetime', 'step', 'team', 'position', 'jersey_number', 'x_position', 'y_position', 'speed', 'distance', 'direction', 'orientation', 'acceleration', 'sa'] \n[test_baseline_helmets] shape=(371408, 12)\ncols: ['game_play', 'game_key', 'play_id', 'view', 'video', 'frame', 'nfl_player_id', 'player_label', 'left', 'width', 'top', 'height'] \n[test_video_metadata] shape=(48, 7)\ncols: ['game_play', 'game_key', 'play_id', 'view', 'start_time', 'end_time', 'snap_time'] \nLabel distribution:\nlabel_col: contact\ncontact\n0    4200484\n1      57891\nName: count, dtype: int64\nSample submission head:\n                   contact_id  contact\n0  58187_001341_0_47795_52650        0\n1  58187_001341_0_47795_47804        0\n2  58187_001341_0_47795_52863        0\n3  58187_001341_0_47795_52574        0\n4  58187_001341_0_47795_52483        0\nEnv+EDA done in 6.20s\n"
          ]
        }
      ]
    },
    {
      "id": "7408833d-1f14-4514-82e1-b3632095740f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import re, itertools, math, time\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "print('Setting up CV and keys...')\n",
        "t0 = time.time()\n",
        "\n",
        "# Parse contact_id -> (game_play, step, p1, p2)\n",
        "def parse_contact_id(cid: str):\n",
        "    # format: {game_play}_{step}_{p1}_{p2}\n",
        "    # game_play itself has an underscore: e.g., 58187_001341\n",
        "    parts = cid.split('_')\n",
        "    # Expect 5 parts: [g1, g2, step, p1, p2]\n",
        "    if len(parts) != 5:\n",
        "        raise ValueError(f'Unexpected contact_id format: {cid}')\n",
        "    game_play = parts[0] + '_' + parts[1]\n",
        "    step = int(parts[2])\n",
        "    p1 = parts[3]; p2 = parts[4]\n",
        "    # canonicalize pair (handle 'G' ground) keep as strings\n",
        "    if p1 == 'G' or p2 == 'G':\n",
        "        p1c, p2c = ('G', p2) if p1 == 'G' else ('G', p1)\n",
        "    else:\n",
        "        a, b = int(p1), int(p2)\n",
        "        p1c, p2c = (str(a), str(b)) if a <= b else (str(b), str(a))\n",
        "    return game_play, step, p1c, p2c\n",
        "\n",
        "# Quick sanity on sample_submission format\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "g, s, a, b = parse_contact_id(ss.loc[0, 'contact_id'])\n",
        "print('Parsed sample row:', g, s, a, b)\n",
        "\n",
        "# Build GroupKFold on train_labels grouped by game_play\n",
        "unique_gp = train_labels[['game_play']].drop_duplicates().reset_index(drop=True)\n",
        "groups = unique_gp['game_play'].values\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "fold_map = {}\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(unique_gp, groups=groups)):\n",
        "    for idx in va_idx:\n",
        "        fold_map[unique_gp.loc[idx, 'game_play']] = fold\n",
        "folds_df = pd.DataFrame({'game_play': list(fold_map.keys()), 'fold': list(fold_map.values())})\n",
        "folds_df.to_csv('folds_game_play.csv', index=False)\n",
        "print('Folds saved:', folds_df['fold'].value_counts().sort_index().to_dict())\n",
        "\n",
        "# Ensure key dtypes align and canonicalize player pair in training labels\n",
        "train_labels['pid1'] = train_labels['nfl_player_id_1'].astype(str)\n",
        "train_labels['pid2'] = train_labels['nfl_player_id_2'].astype(str)\n",
        "def canon_pair(p1, p2):\n",
        "    if p1 == 'G' or p2 == 'G':\n",
        "        return ('G', p2) if p1 == 'G' else ('G', p1)\n",
        "    a, b = int(p1), int(p2)\n",
        "    return (str(a), str(b)) if a <= b else (str(b), str(a))\n",
        "cp = [canon_pair(p1, p2) for p1, p2 in zip(train_labels['pid1'], train_labels['pid2'])]\n",
        "train_labels['p1'] = [x[0] for x in cp]\n",
        "train_labels['p2'] = [x[1] for x in cp]\n",
        "\n",
        "# Attach fold to labels\n",
        "train_labels = train_labels.merge(folds_df, on='game_play', how='left')\n",
        "assert train_labels['fold'].notna().all(), 'Missing fold assignment'\n",
        "print('Labels+folds shape:', train_labels.shape)\n",
        "\n",
        "# Basic index for tracking per step (reduced columns for speed)\n",
        "trk_cols = ['game_play','step','nfl_player_id','team','position','x_position','y_position','speed','acceleration','direction','orientation']\n",
        "train_track_idx = train_track[trk_cols].copy()\n",
        "test_track_idx = test_track[trk_cols].copy()\n",
        "for df in (train_track_idx, test_track_idx):\n",
        "    df['nfl_player_id'] = df['nfl_player_id'].astype(int)\n",
        "\n",
        "print('Prepared tracking indices:', train_track_idx.shape, test_track_idx.shape)\n",
        "print(f'Setup done in {time.time()-t0:.2f}s', flush=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up CV and keys...\nParsed sample row: 58187_001341 0 47795 52650\nFolds saved: {0: 44, 1: 43, 2: 43, 3: 43, 4: 43}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels+folds shape: (4258375, 12)\nPrepared tracking indices: (1225299, 11) (127754, 11)\nSetup done in 4.52s\n"
          ]
        }
      ]
    },
    {
      "id": "0267e94a-9fc9-4381-b983-55b750bdb872",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math, time\n",
        "from itertools import combinations\n",
        "\n",
        "print('Building candidate pairs and minimal features (r=3.0 yd)...')\n",
        "t0 = time.time()\n",
        "\n",
        "def cosd(a):\n",
        "    return math.cos(math.radians(a)) if pd.notna(a) else 0.0\n",
        "def sind(a):\n",
        "    return math.sin(math.radians(a)) if pd.notna(a) else 0.0\n",
        "def heading_diff(a, b):\n",
        "    if pd.isna(a) or pd.isna(b):\n",
        "        return np.nan\n",
        "    d = (a - b + 180) % 360 - 180\n",
        "    return abs(d)\n",
        "\n",
        "def build_pairs_for_group(gdf, r=3.0):\n",
        "    rows = []\n",
        "    arr = gdf[['nfl_player_id','team','position','x_position','y_position','speed','acceleration','direction']].values\n",
        "    n = arr.shape[0]\n",
        "    for i, j in combinations(range(n), 2):\n",
        "        pid_i, team_i, pos_i, xi, yi, si, ai, diri = arr[i]\n",
        "        pid_j, team_j, pos_j, xj, yj, sj, aj, dirj = arr[j]\n",
        "        dx = xj - xi; dy = yj - yi\n",
        "        dist = math.hypot(dx, dy)\n",
        "        if dist > r:\n",
        "            continue\n",
        "        # canonicalize pair ids as strings\n",
        "        a = int(pid_i); b = int(pid_j)\n",
        "        p1, p2 = (str(a), str(b)) if a <= b else (str(b), str(a))\n",
        "        # velocities from speed+direction (tracking dir: degrees, 0 = east per NFL; use cos/sin)\n",
        "        vxi = si * cosd(diri); vyi = si * sind(diri)\n",
        "        vxj = sj * cosd(dirj); vyj = sj * sind(dirj)\n",
        "        rvx = vxj - vxi; rvy = vyj - vyi\n",
        "        if dist > 0:\n",
        "            ux = dx / dist; uy = dy / dist\n",
        "            closing = rvx * ux + rvy * uy\n",
        "        else:\n",
        "            closing = 0.0\n",
        "        hd = heading_diff(diri, dirj)\n",
        "        rows.append((p1, p2, dist, dx, dy, si, sj, ai, aj, closing, abs(closing), hd, int(team_i == team_j), str(team_i), str(team_j), str(pos_i), str(pos_j)))\n",
        "    if not rows:\n",
        "        return pd.DataFrame(columns=['p1','p2','distance','rel_dx','rel_dy','speed1','speed2','accel1','accel2','closing','abs_closing','abs_d_heading','same_team','team1','team2','pos1','pos2'])\n",
        "    df = pd.DataFrame(rows, columns=['p1','p2','distance','rel_dx','rel_dy','speed1','speed2','accel1','accel2','closing','abs_closing','abs_d_heading','same_team','team1','team2','pos1','pos2'])\n",
        "    return df\n",
        "\n",
        "def build_feature_table(track_df, r=3.0):\n",
        "    feats = []\n",
        "    cnt = 0\n",
        "    last_log = time.time()\n",
        "    for (gp, step), gdf in track_df.groupby(['game_play','step'], sort=False):\n",
        "        f = build_pairs_for_group(gdf, r=r)\n",
        "        if not f.empty:\n",
        "            f.insert(0, 'step', step)\n",
        "            f.insert(0, 'game_play', gp)\n",
        "            feats.append(f)\n",
        "        cnt += 1\n",
        "        if cnt % 500 == 0:\n",
        "            now = time.time()\n",
        "            print(f' processed {cnt} groups in {now - last_log:.1f}s; total elapsed {now - t0:.1f}s', flush=True)\n",
        "            last_log = now\n",
        "    if feats:\n",
        "        return pd.concat(feats, ignore_index=True)\n",
        "    return pd.DataFrame(columns=['game_play','step','p1','p2','distance','rel_dx','rel_dy','speed1','speed2','accel1','accel2','closing','abs_closing','abs_d_heading','same_team','team1','team2','pos1','pos2'])\n",
        "\n",
        "# Build train features (radius 3.0 yds)\n",
        "train_feats = build_feature_table(train_track_idx, r=3.0)\n",
        "print('Train feats shape:', train_feats.shape)\n",
        "train_feats.to_parquet('train_pairs_v1.parquet', index=False)\n",
        "\n",
        "# Merge labels to get target\n",
        "key_cols = ['game_play','step','p1','p2']\n",
        "lab_cols = key_cols + ['contact']\n",
        "train_supervised = train_feats.merge(train_labels[lab_cols], on=key_cols, how='left')\n",
        "missing = train_supervised['contact'].isna().mean()\n",
        "print(f'Label NaN rate after merge: {missing:.3f}')\n",
        "train_supervised = train_supervised.dropna(subset=['contact'])\n",
        "train_supervised['contact'] = train_supervised['contact'].astype(int)\n",
        "print('Supervised rows:', train_supervised.shape)\n",
        "train_supervised.to_parquet('train_supervised_v1.parquet', index=False)\n",
        "\n",
        "# Build test features\n",
        "test_feats = build_feature_table(test_track_idx, r=3.0)\n",
        "print('Test feats shape:', test_feats.shape)\n",
        "test_feats.to_parquet('test_pairs_v1.parquet', index=False)\n",
        "\n",
        "print(f'All done in {time.time()-t0:.1f}s', flush=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building candidate pairs and minimal features (r=3.0 yd)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 500 groups in 0.9s; total elapsed 0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1000 groups in 0.6s; total elapsed 1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1500 groups in 0.7s; total elapsed 2.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2000 groups in 0.6s; total elapsed 2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2500 groups in 0.6s; total elapsed 3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3000 groups in 0.6s; total elapsed 3.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3500 groups in 0.8s; total elapsed 4.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4000 groups in 0.6s; total elapsed 5.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4500 groups in 0.6s; total elapsed 5.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5000 groups in 0.6s; total elapsed 6.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5500 groups in 0.6s; total elapsed 6.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 6000 groups in 0.9s; total elapsed 7.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 6500 groups in 0.6s; total elapsed 8.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 7000 groups in 0.6s; total elapsed 8.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 7500 groups in 0.6s; total elapsed 9.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 8000 groups in 0.6s; total elapsed 10.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 8500 groups in 0.6s; total elapsed 10.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 9000 groups in 0.8s; total elapsed 11.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 9500 groups in 0.6s; total elapsed 11.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 10000 groups in 0.6s; total elapsed 12.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 10500 groups in 0.6s; total elapsed 13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 11000 groups in 0.5s; total elapsed 13.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 11500 groups in 0.5s; total elapsed 14.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 12000 groups in 0.9s; total elapsed 15.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 12500 groups in 0.6s; total elapsed 15.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 13000 groups in 0.6s; total elapsed 16.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 13500 groups in 0.5s; total elapsed 16.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 14000 groups in 0.6s; total elapsed 17.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 14500 groups in 0.6s; total elapsed 17.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 15000 groups in 0.6s; total elapsed 18.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 15500 groups in 0.5s; total elapsed 19.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 16000 groups in 0.6s; total elapsed 19.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 16500 groups in 1.1s; total elapsed 20.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 17000 groups in 0.5s; total elapsed 21.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 17500 groups in 0.6s; total elapsed 21.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 18000 groups in 0.5s; total elapsed 22.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 18500 groups in 0.6s; total elapsed 22.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 19000 groups in 0.6s; total elapsed 23.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 19500 groups in 0.5s; total elapsed 24.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 20000 groups in 0.6s; total elapsed 24.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 20500 groups in 0.6s; total elapsed 25.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 21000 groups in 0.6s; total elapsed 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 21500 groups in 1.2s; total elapsed 26.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 22000 groups in 0.6s; total elapsed 27.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 22500 groups in 0.6s; total elapsed 28.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 23000 groups in 0.6s; total elapsed 28.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 23500 groups in 0.6s; total elapsed 29.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 24000 groups in 0.6s; total elapsed 29.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 24500 groups in 0.6s; total elapsed 30.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 25000 groups in 0.6s; total elapsed 30.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 25500 groups in 0.6s; total elapsed 31.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 26000 groups in 0.6s; total elapsed 32.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 26500 groups in 0.6s; total elapsed 32.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 27000 groups in 0.6s; total elapsed 33.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 27500 groups in 0.6s; total elapsed 33.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 28000 groups in 1.1s; total elapsed 34.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 28500 groups in 0.6s; total elapsed 35.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 29000 groups in 0.6s; total elapsed 35.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 29500 groups in 0.6s; total elapsed 36.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 30000 groups in 0.6s; total elapsed 37.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 30500 groups in 0.6s; total elapsed 37.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 31000 groups in 0.6s; total elapsed 38.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 31500 groups in 0.6s; total elapsed 38.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 32000 groups in 0.5s; total elapsed 39.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 32500 groups in 0.5s; total elapsed 39.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 33000 groups in 0.5s; total elapsed 40.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 33500 groups in 0.6s; total elapsed 40.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 34000 groups in 0.6s; total elapsed 41.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 34500 groups in 0.6s; total elapsed 42.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 35000 groups in 0.5s; total elapsed 42.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 35500 groups in 0.6s; total elapsed 43.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 36000 groups in 1.4s; total elapsed 44.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 36500 groups in 0.6s; total elapsed 45.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 37000 groups in 0.6s; total elapsed 45.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 37500 groups in 0.6s; total elapsed 46.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 38000 groups in 0.6s; total elapsed 46.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 38500 groups in 0.6s; total elapsed 47.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 39000 groups in 0.6s; total elapsed 48.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 39500 groups in 0.5s; total elapsed 48.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 40000 groups in 0.6s; total elapsed 49.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 40500 groups in 0.6s; total elapsed 49.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 41000 groups in 0.5s; total elapsed 50.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 41500 groups in 0.6s; total elapsed 50.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 42000 groups in 0.5s; total elapsed 51.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 42500 groups in 0.6s; total elapsed 51.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 43000 groups in 0.6s; total elapsed 52.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 43500 groups in 0.6s; total elapsed 53.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 44000 groups in 0.5s; total elapsed 53.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 44500 groups in 0.6s; total elapsed 54.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 45000 groups in 0.5s; total elapsed 54.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 45500 groups in 1.6s; total elapsed 56.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 46000 groups in 0.6s; total elapsed 56.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 46500 groups in 0.6s; total elapsed 57.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 47000 groups in 0.6s; total elapsed 57.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 47500 groups in 0.6s; total elapsed 58.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 48000 groups in 0.6s; total elapsed 59.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 48500 groups in 0.6s; total elapsed 59.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 49000 groups in 0.6s; total elapsed 60.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 49500 groups in 0.5s; total elapsed 60.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 50000 groups in 0.5s; total elapsed 61.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 50500 groups in 0.6s; total elapsed 61.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 51000 groups in 0.6s; total elapsed 62.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 51500 groups in 0.6s; total elapsed 63.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 52000 groups in 0.6s; total elapsed 63.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 52500 groups in 0.6s; total elapsed 64.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 53000 groups in 0.5s; total elapsed 64.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 53500 groups in 0.6s; total elapsed 65.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 54000 groups in 0.6s; total elapsed 65.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 54500 groups in 0.6s; total elapsed 66.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 55000 groups in 0.5s; total elapsed 66.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 55500 groups in 0.6s; total elapsed 67.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train feats shape: (1641668, 19)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label NaN rate after merge: 0.746\nSupervised rows: (416574, 20)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 500 groups in 0.6s; total elapsed 76.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1000 groups in 0.6s; total elapsed 77.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1500 groups in 0.6s; total elapsed 77.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2000 groups in 0.6s; total elapsed 78.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2500 groups in 0.6s; total elapsed 78.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3000 groups in 0.5s; total elapsed 79.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3500 groups in 0.6s; total elapsed 79.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4000 groups in 0.6s; total elapsed 80.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4500 groups in 0.6s; total elapsed 81.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5000 groups in 0.5s; total elapsed 81.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5500 groups in 0.6s; total elapsed 82.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test feats shape: (191559, 19)\nAll done in 82.9s\n"
          ]
        }
      ]
    },
    {
      "id": "12d95b83-0718-483b-a7a8-70ce1e32862b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time, math, subprocess, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Ensure xgboost is available; print version\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version:', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def mcc_score(y_true, y_prob, thr):\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    return matthews_corrcoef(y_true, y_pred)\n",
        "\n",
        "print('Loading supervised train (W5+pos-exp+helm) and test features...')\n",
        "train_sup = pd.read_parquet('train_supervised_w5_helm.parquet')\n",
        "test_feats = pd.read_parquet('test_pairs_w5_helm.parquet')\n",
        "folds_df = pd.read_csv('folds_game_play.csv')\n",
        "print('train_sup:', train_sup.shape, 'test_feats:', test_feats.shape)\n",
        "\n",
        "# Attach folds\n",
        "train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "assert train_sup['fold'].notna().all()\n",
        "\n",
        "# Fill NaNs for helmet features: no-helmet => large distance, 0 views\n",
        "for df in (train_sup, test_feats):\n",
        "    if 'px_dist_norm_min' in df.columns:\n",
        "        df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns:\n",
        "        df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "\n",
        "# Feature set: base + temporal windows (past-5) + counts + trend + helmet\n",
        "feat_cols = [\n",
        "    'distance','rel_dx','rel_dy','speed1','speed2','accel1','accel2','closing','abs_closing','abs_d_heading','same_team',\n",
        "    'dist_min_p5','dist_mean_p5','dist_max_p5','dist_std_p5',\n",
        "    'abs_close_min_p5','abs_close_mean_p5','abs_close_max_p5','abs_close_std_p5',\n",
        "    'cnt_dist_lt15_p5','cnt_dist_lt20_p5','cnt_dist_lt25_p5',\n",
        "    'dist_delta_p5',\n",
        "    'px_dist_norm_min','views_both_present'\n",
        "]\n",
        "missing_feats = [c for c in feat_cols if c not in train_sup.columns]\n",
        "if missing_feats:\n",
        "    raise RuntimeError(f'Missing features: {missing_feats}')\n",
        "\n",
        "X = train_sup[feat_cols].astype(float).values\n",
        "y = train_sup['contact'].astype(int).values\n",
        "groups = train_sup['game_play'].values\n",
        "\n",
        "print('Pos rate:', y.mean())\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "oof = np.zeros(len(train_sup), dtype=float)\n",
        "models = []  # list of (booster, best_iteration)\n",
        "start = time.time()\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups)):\n",
        "    t0 = time.time()\n",
        "    X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
        "    X_va, y_va = X[va_idx], y[va_idx]\n",
        "    # class imbalance handling\n",
        "    neg = (y_tr == 0).sum(); pos = (y_tr == 1).sum()\n",
        "    spw = max(1.0, neg / max(1, pos))\n",
        "    print(f'Fold {fold}: train {len(tr_idx)} (pos {pos}), valid {len(va_idx)} (pos {(y_va==1).sum()}), scale_pos_weight={spw:.1f}', flush=True)\n",
        "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
        "    dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "    params = {\n",
        "        'tree_method': 'hist',\n",
        "        'device': 'cuda',\n",
        "        'max_depth': 8,\n",
        "        'eta': 0.05,\n",
        "        'subsample': 0.9,\n",
        "        'colsample_bytree': 0.7,\n",
        "        'min_child_weight': 8,\n",
        "        'lambda': 1.5,\n",
        "        'alpha': 0.0,\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'logloss',\n",
        "        'scale_pos_weight': float(spw),\n",
        "        'seed': 42\n",
        "    }\n",
        "    evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
        "    booster = xgb.train(\n",
        "        params=params,\n",
        "        dtrain=dtrain,\n",
        "        num_boost_round=3000,\n",
        "        evals=evals,\n",
        "        early_stopping_rounds=100,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "    # best iteration\n",
        "    if hasattr(booster, 'best_iteration') and booster.best_iteration is not None:\n",
        "        best_it = int(booster.best_iteration)\n",
        "    else:\n",
        "        best_it = int(booster.num_boosted_rounds()) - 1\n",
        "    p = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "    oof[va_idx] = p\n",
        "    models.append((booster, best_it))\n",
        "    print(f' Fold {fold} done in {time.time()-t0:.1f}s; best_iteration={best_it}', flush=True)\n",
        "\n",
        "print('OOF threshold sweep for MCC...')\n",
        "best_thr, best_mcc = 0.5, -1.0\n",
        "for thr in np.linspace(0.01, 0.99, 99):\n",
        "    m = mcc_score(y, oof, thr)\n",
        "    if m > best_mcc:\n",
        "        best_mcc, best_thr = m, thr\n",
        "print(f'Best OOF MCC={best_mcc:.5f} at thr={best_thr:.3f}')\n",
        "\n",
        "# Predict test with same features\n",
        "Xt = test_feats[feat_cols].astype(float).values\n",
        "dtest = xgb.DMatrix(Xt)\n",
        "pt = np.zeros(len(test_feats), dtype=float)\n",
        "for i, (booster, best_it) in enumerate(models):\n",
        "    t0 = time.time()\n",
        "    pt += booster.predict(dtest, iteration_range=(0, best_it + 1))\n",
        "    print(f' Inference model {i} took {time.time()-t0:.1f}s')\n",
        "pt /= len(models)\n",
        "\n",
        "# Optional simple temporal smoothing (2-of-3 via rolling max over probs by (game_play,p1,p2))\n",
        "pred_tmp = test_feats[['game_play','step','p1','p2']].copy()\n",
        "pred_tmp['prob'] = pt\n",
        "pred_tmp = pred_tmp.sort_values(['game_play','p1','p2','step'])\n",
        "grp = pred_tmp.groupby(['game_play','p1','p2'], sort=False)\n",
        "pred_tmp['prob_smooth'] = grp['prob'].transform(lambda s: s.rolling(3, center=True, min_periods=1).max())\n",
        "pt_smooth = pred_tmp['prob_smooth'].values\n",
        "\n",
        "# Build contact_id for test pairs\n",
        "cid = test_feats['game_play'].astype(str) + '_' + test_feats['step'].astype(str) + '_' + test_feats['p1'].astype(str) + '_' + test_feats['p2'].astype(str)\n",
        "pred_df = pd.DataFrame({'contact_id': cid, 'contact_prob': pt_smooth})\n",
        "\n",
        "# Map to sample_submission; fill missing with 0.0 prob\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.merge(pred_df, on='contact_id', how='left')\n",
        "sub['contact_prob'] = sub['contact_prob'].fillna(0.0)\n",
        "sub['contact'] = (sub['contact_prob'] >= best_thr).astype(int)\n",
        "sub[['contact_id','contact']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv')\n",
        "print('Head:\\n', sub.head())\n",
        "print('Done. Total time:', f'{time.time()-start:.1f}s', flush=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version: 2.1.4\nLoading supervised train (W5+pos-exp+helm) and test features...\ntrain_sup: (416574, 34) test_feats: (191559, 33)\nPos rate: 0.10227714643736767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: train 333273 (pos 33885), valid 83301 (pos 8721), scale_pos_weight=8.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 0 done in 14.0s; best_iteration=2107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train 333268 (pos 34206), valid 83306 (pos 8400), scale_pos_weight=8.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 1 done in 15.2s; best_iteration=2320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: train 333273 (pos 34570), valid 83301 (pos 8036), scale_pos_weight=8.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 2 done in 13.7s; best_iteration=2125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: train 333286 (pos 33688), valid 83288 (pos 8918), scale_pos_weight=8.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 3 done in 13.8s; best_iteration=2088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: train 333196 (pos 34075), valid 83378 (pos 8531), scale_pos_weight=8.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 4 done in 12.4s; best_iteration=1913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF threshold sweep for MCC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best OOF MCC=0.69162 at thr=0.550\n Inference model 0 took 0.1s\n Inference model 1 took 0.1s\n Inference model 2 took 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 3 took 0.0s\n Inference model 4 took 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv\nHead:\n                    contact_id  contact  contact_prob\n0  58187_001341_0_47795_52650        0           0.0\n1  58187_001341_0_47795_47804        0           0.0\n2  58187_001341_0_47795_52863        0           0.0\n3  58187_001341_0_47795_52574        0           0.0\n4  58187_001341_0_47795_52483        0           0.0\nDone. Total time: 77.2s\n"
          ]
        }
      ]
    },
    {
      "id": "05c2f857-2d36-43fb-8e7a-5eb7811943b1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Temporal window features (past-only W=5) + positive expansion (\u00b11) and rebuild supervised/train/test tables\n",
        "import pandas as pd, numpy as np, time\n",
        "\n",
        "t0 = time.time()\n",
        "print('Loading base pair features...')\n",
        "train_pairs = pd.read_parquet('train_pairs_v1.parquet')\n",
        "test_pairs = pd.read_parquet('test_pairs_v1.parquet')\n",
        "print('Loaded train_pairs:', train_pairs.shape, 'test_pairs:', test_pairs.shape)\n",
        "\n",
        "def add_window_feats(df: pd.DataFrame, W: int = 5):\n",
        "    df = df.sort_values(['game_play','p1','p2','step']).copy()\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    # rolling on distance\n",
        "    df['dist_min_p5'] = grp['distance'].rolling(W, min_periods=1).min().reset_index(level=[0,1,2], drop=True)\n",
        "    df['dist_mean_p5'] = grp['distance'].rolling(W, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n",
        "    df['dist_max_p5'] = grp['distance'].rolling(W, min_periods=1).max().reset_index(level=[0,1,2], drop=True)\n",
        "    df['dist_std_p5'] = grp['distance'].rolling(W, min_periods=1).std().reset_index(level=[0,1,2], drop=True)\n",
        "    # abs_closing rolling\n",
        "    df['abs_close_min_p5'] = grp['abs_closing'].rolling(W, min_periods=1).min().reset_index(level=[0,1,2], drop=True)\n",
        "    df['abs_close_mean_p5'] = grp['abs_closing'].rolling(W, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n",
        "    df['abs_close_max_p5'] = grp['abs_closing'].rolling(W, min_periods=1).max().reset_index(level=[0,1,2], drop=True)\n",
        "    df['abs_close_std_p5'] = grp['abs_closing'].rolling(W, min_periods=1).std().reset_index(level=[0,1,2], drop=True)\n",
        "    # counts distance under thresholds\n",
        "    for thr, name in [(1.5,'lt15'), (2.0,'lt20'), (2.5,'lt25')]:\n",
        "        key = f'cnt_dist_{name}_p5'\n",
        "        df[key] = grp['distance'].apply(lambda s: s.lt(thr).rolling(W, min_periods=1).sum()).reset_index(level=[0,1,2], drop=True)\n",
        "    # trend over 5: distance[t] - distance[t-5]\n",
        "    df['dist_delta_p5'] = df['distance'] - grp['distance'].shift(W)\n",
        "    return df\n",
        "\n",
        "print('Adding window features to train...')\n",
        "train_w = add_window_feats(train_pairs, W=5)\n",
        "print('Adding window features to test...')\n",
        "test_w = add_window_feats(test_pairs, W=5)\n",
        "\n",
        "train_w.to_parquet('train_pairs_w5.parquet', index=False)\n",
        "test_w.to_parquet('test_pairs_w5.parquet', index=False)\n",
        "print('Saved windowed pairs parquet.')\n",
        "\n",
        "# Rebuild supervised set using INNER JOIN to label domain; then apply positive expansion (\u00b11) within existing rows\n",
        "key_cols = ['game_play','step','p1','p2']\n",
        "lab_cols = key_cols + ['contact']\n",
        "labels_min = train_labels[lab_cols].copy()\n",
        "sup = labels_min.merge(train_w, on=key_cols, how='inner')\n",
        "print('Supervised(inner) shape:', sup.shape, 'pos rate:', sup['contact'].mean())\n",
        "\n",
        "# Positive expansion: set contact=1 at step\u00b11 for rows present in sup\n",
        "pos = sup[sup['contact'] == 1][['game_play','p1','p2','step']].copy()\n",
        "pos_m1 = pos.copy(); pos_m1['step'] = pos_m1['step'] - 1\n",
        "pos_p1 = pos.copy(); pos_p1['step'] = pos_p1['step'] + 1\n",
        "pos_exp = pd.concat([pos_m1, pos_p1], axis=0, ignore_index=True).drop_duplicates()\n",
        "pos_exp['flag_pos_exp'] = 1\n",
        "sup = sup.merge(pos_exp, on=['game_play','p1','p2','step'], how='left')\n",
        "sup.loc[sup['flag_pos_exp'] == 1, 'contact'] = 1\n",
        "sup = sup.drop(columns=['flag_pos_exp'])\n",
        "print('After positive expansion, pos rate:', sup['contact'].mean())\n",
        "\n",
        "sup.to_parquet('train_supervised_w5.parquet', index=False)\n",
        "print('Saved train_supervised_w5.parquet. Total time {:.1f}s'.format(time.time()-t0), flush=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base pair features...\nLoaded train_pairs: (1641668, 19) test_pairs: (191559, 19)\nAdding window features to train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding window features to test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved windowed pairs parquet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supervised(inner) shape: (416574, 32) pos rate: 0.10227714643736767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After positive expansion, pos rate: 0.11705963406261552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved train_supervised_w5.parquet. Total time 43.2s\n"
          ]
        }
      ]
    },
    {
      "id": "18d530de-7859-48a7-b16d-0923cacbed64",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Helmet proximity features v1: map frames->steps via snap_frame, aggregate per (game_play, step, view, player), merge to pairs, compute min normalized pixel distance across views\n",
        "import pandas as pd, numpy as np, time\n",
        "from math import sqrt\n",
        "\n",
        "t0 = time.time()\n",
        "FPS = 59.94\n",
        "print('Loading helmet and metadata CSVs...')\n",
        "train_helm = pd.read_csv('train_baseline_helmets.csv')\n",
        "test_helm = pd.read_csv('test_baseline_helmets.csv')\n",
        "train_vmeta = pd.read_csv('train_video_metadata.csv')\n",
        "test_vmeta = pd.read_csv('test_video_metadata.csv')\n",
        "print('Helm train/test:', train_helm.shape, test_helm.shape)\n",
        "\n",
        "def prep_meta(vmeta: pd.DataFrame):\n",
        "    vm = vmeta.copy()\n",
        "    # parse times to seconds (assume string s with seconds float or hh:mm:ss.sss); pandas to_datetime then total_seconds\n",
        "    for c in ['start_time','snap_time']:\n",
        "        if np.issubdtype(vm[c].dtype, np.number):\n",
        "            continue\n",
        "        ts = pd.to_datetime(vm[c], errors='coerce')\n",
        "        # If already numeric-like strings, coerce to numeric\n",
        "        if ts.notna().any():\n",
        "            vm[c] = (ts - ts.dt.floor('D')).dt.total_seconds().astype(float)\n",
        "        else:\n",
        "            vm[c] = pd.to_numeric(vm[c], errors='coerce')\n",
        "    vm['snap_frame'] = ((vm['snap_time'] - vm['start_time']) * FPS).round().astype('Int64')\n",
        "    return vm[['game_play','view','snap_frame']].drop_duplicates()\n",
        "\n",
        "meta_tr = prep_meta(train_vmeta)\n",
        "meta_te = prep_meta(test_vmeta)\n",
        "\n",
        "def dedup_and_step(helm: pd.DataFrame, meta: pd.DataFrame):\n",
        "    df = helm[['game_play','view','frame','nfl_player_id','left','top','width','height']].copy()\n",
        "    df = df.dropna(subset=['nfl_player_id'])\n",
        "    df['nfl_player_id'] = df['nfl_player_id'].astype(int).astype(str)\n",
        "    df['area'] = df['width'] * df['height']\n",
        "    df['cx'] = df['left'] + 0.5 * df['width']\n",
        "    df['cy'] = df['top'] + 0.5 * df['height']\n",
        "    # dedup per (gp,view,frame,player) by largest area\n",
        "    df = df.sort_values(['game_play','view','frame','nfl_player_id','area'], ascending=[True,True,True,True,False])\n",
        "    df = df.drop_duplicates(['game_play','view','frame','nfl_player_id'], keep='first')\n",
        "    # map to step using snap_frame\n",
        "    df = df.merge(meta, on=['game_play','view'], how='left')\n",
        "    # step = round((frame - snap_frame)/6)\n",
        "    df['step'] = ((df['frame'] - df['snap_frame']).astype('float') / 6.0).round().astype('Int64')\n",
        "    df = df.dropna(subset=['step'])\n",
        "    df['step'] = df['step'].astype(int)\n",
        "    # expand to target_step in {step-1, step, step+1}\n",
        "    dm1 = df.copy(); dm1['target_step'] = dm1['step'] - 1\n",
        "    d0 = df.copy(); d0['target_step'] = d0['step']\n",
        "    dp1 = df.copy(); dp1['target_step'] = dp1['step'] + 1\n",
        "    d = pd.concat([dm1, d0, dp1], ignore_index=True)\n",
        "    # aggregate per (gp,view,target_step,player)\n",
        "    agg = d.groupby(['game_play','view','target_step','nfl_player_id'], sort=False).agg(\n",
        "        cx_mean=('cx','mean'),\n",
        "        cy_mean=('cy','mean'),\n",
        "        h_mean=('height','mean'),\n",
        "        cnt=('cx','size')\n",
        "    ).reset_index().rename(columns={'target_step':'step'})\n",
        "    return agg\n",
        "\n",
        "print('Preparing per-step helmet aggregates...')\n",
        "h_tr = dedup_and_step(train_helm, meta_tr)\n",
        "h_te = dedup_and_step(test_helm, meta_te)\n",
        "print('Agg helmets train/test:', h_tr.shape, h_te.shape)\n",
        "\n",
        "def merge_helmet_to_pairs(pairs_path: str, h_agg: pd.DataFrame, out_path: str):\n",
        "    pairs = pd.read_parquet(pairs_path)\n",
        "    ha = h_agg[['game_play','step','view','nfl_player_id','cx_mean','cy_mean','h_mean']].copy()\n",
        "    # self-join per (gp,step,view) to compute per-view pair distances\n",
        "    a = ha.rename(columns={'nfl_player_id':'p1','cx_mean':'cx1','cy_mean':'cy1','h_mean':'h1'})\n",
        "    b = ha.rename(columns={'nfl_player_id':'p2','cx_mean':'cx2','cy_mean':'cy2','h_mean':'h2'})\n",
        "    merged = a.merge(b, on=['game_play','step','view'], how='inner')\n",
        "    # keep ordered pairs p1 < p2 (string compare but ids are numeric strings)\n",
        "    mask = merged['p1'] < merged['p2']\n",
        "    merged = merged[mask]\n",
        "    merged['px_dist'] = np.sqrt((merged['cx1'] - merged['cx2'])**2 + (merged['cy1'] - merged['cy2'])**2)\n",
        "    merged['px_dist_norm'] = merged['px_dist'] / np.sqrt(np.maximum(1e-6, merged['h1'] * merged['h2']))\n",
        "    agg = merged.groupby(['game_play','step','p1','p2'], as_index=False).agg(\n",
        "        px_dist_norm_min=('px_dist_norm','min'),\n",
        "        views_both_present=('px_dist_norm', lambda s: int(s.notna().sum()))\n",
        "    )\n",
        "    out = pairs.merge(agg, on=['game_play','step','p1','p2'], how='left')\n",
        "    out.to_parquet(out_path, index=False)\n",
        "    print('Saved', out_path, 'shape', out.shape, 'with helmet cols')\n",
        "\n",
        "# Build and save merged pairs with helmet features\n",
        "print('Merging helmet features into train pairs...')\n",
        "merge_helmet_to_pairs('train_pairs_w5.parquet', h_tr, 'train_pairs_w5_helm.parquet')\n",
        "print('Merging helmet features into test pairs...')\n",
        "merge_helmet_to_pairs('test_pairs_w5.parquet', h_te, 'test_pairs_w5_helm.parquet')\n",
        "\n",
        "# Rebuild supervised with helmet features via inner-join to labels\n",
        "pairs_tr_helm = pd.read_parquet('train_pairs_w5_helm.parquet')\n",
        "key_cols = ['game_play','step','p1','p2']\n",
        "sup = train_labels[key_cols + ['contact']].merge(pairs_tr_helm, on=key_cols, how='inner')\n",
        "sup.to_parquet('train_supervised_w5_helm.parquet', index=False)\n",
        "print('Saved train_supervised_w5_helm.parquet', sup.shape)\n",
        "print('Helmet feature build done in {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading helmet and metadata CSVs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helm train/test: (3412208, 12) (371408, 12)\nPreparing per-step helmet aggregates...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agg helmets train/test: (620840, 8) (67667, 8)\nMerging helmet features into train pairs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved train_pairs_w5_helm.parquet shape (1641668, 33) with helmet cols\nMerging helmet features into test pairs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved test_pairs_w5_helm.parquet shape (191559, 33) with helmet cols\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved train_supervised_w5_helm.parquet (416574, 34)\nHelmet feature build done in 215.9s\n"
          ]
        }
      ]
    },
    {
      "id": "d4a22498-00ab-45e1-99ca-742b3619b068",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Add TTC/delta/relative/helmet-dynamics features to pairs, then rebuild supervised via INNER JOIN and apply \u00b11 positive expansion\n",
        "import pandas as pd, numpy as np, time, math\n",
        "\n",
        "t0 = time.time()\n",
        "print('Loading pairs with W5 and helmet features...')\n",
        "tr = pd.read_parquet('train_pairs_w5_helm.parquet')\n",
        "te = pd.read_parquet('test_pairs_w5_helm.parquet')\n",
        "print('train pairs:', tr.shape, 'test pairs:', te.shape)\n",
        "\n",
        "def add_dyn_feats(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.sort_values(['game_play','p1','p2','step']).copy()\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    # Impute helmet base before dynamics\n",
        "    if 'px_dist_norm_min' in df.columns:\n",
        "        df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns:\n",
        "        df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "\n",
        "    # 1) TTC (approach-aware)\n",
        "    # approaching when closing < 0 (players moving toward each other along line of centers)\n",
        "    df['approaching_flag'] = (df['closing'] < 0).astype(int)\n",
        "    denom = (-df['closing']).clip(lower=1e-3)\n",
        "    ttc_raw = df['distance'] / denom\n",
        "    ttc_raw = ttc_raw.where(df['approaching_flag'] == 1, 10.0)\n",
        "    df['ttc_raw'] = ttc_raw.astype(float)\n",
        "    df['ttc_clip'] = df['ttc_raw'].clip(0, 5)\n",
        "    df['ttc_log'] = np.log1p(df['ttc_clip'])\n",
        "    df['inv_ttc'] = 1.0 / (1.0 + df['ttc_clip'])\n",
        "\n",
        "    # 2) Deltas (fillna 0)\n",
        "    for col in ['distance','closing','abs_closing','speed1','speed2','accel1','accel2']:\n",
        "        # shift 1/2/5 where applicable\n",
        "        if col in ['distance']:\n",
        "            df['d_dist_1'] = df['distance'] - grp['distance'].shift(1)\n",
        "            df['d_dist_2'] = df['distance'] - grp['distance'].shift(2)\n",
        "            df['d_dist_5'] = df['distance'] - grp['distance'].shift(5)\n",
        "        elif col == 'closing':\n",
        "            df['d_close_1'] = df['closing'] - grp['closing'].shift(1)\n",
        "        elif col == 'abs_closing':\n",
        "            df['d_absclose_1'] = df['abs_closing'] - grp['abs_closing'].shift(1)\n",
        "        elif col == 'speed1':\n",
        "            df['d_speed1_1'] = df['speed1'] - grp['speed1'].shift(1)\n",
        "        elif col == 'speed2':\n",
        "            df['d_speed2_1'] = df['speed2'] - grp['speed2'].shift(1)\n",
        "        elif col == 'accel1':\n",
        "            df['d_accel1_1'] = df['accel1'] - grp['accel1'].shift(1)\n",
        "        elif col == 'accel2':\n",
        "            df['d_accel2_1'] = df['accel2'] - grp['accel2'].shift(1)\n",
        "    # small smoothers\n",
        "    df['rm3_d_dist_1'] = grp['d_dist_1'].transform(lambda s: s.rolling(3, min_periods=1).mean())\n",
        "    df['rm3_d_close_1'] = grp['d_close_1'].transform(lambda s: s.rolling(3, min_periods=1).mean())\n",
        "    # fill deltas NaN with 0\n",
        "    for c in ['d_dist_1','d_dist_2','d_dist_5','d_close_1','d_absclose_1','d_speed1_1','d_speed2_1','d_accel1_1','d_accel2_1','rm3_d_dist_1','rm3_d_close_1']:\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].fillna(0.0)\n",
        "\n",
        "    # 3) Relative motion\n",
        "    df['rel_speed'] = (df['speed2'] - df['speed1']).astype(float)\n",
        "    df['abs_rel_speed'] = df['rel_speed'].abs()\n",
        "    df['rel_accel'] = (df['accel2'] - df['accel1']).astype(float)\n",
        "    df['abs_rel_accel'] = df['rel_accel'].abs()\n",
        "    # Jerk per player\n",
        "    df['jerk1'] = grp['accel1'].diff().fillna(0.0)\n",
        "    df['jerk2'] = grp['accel2'].diff().fillna(0.0)\n",
        "\n",
        "    # 4) Helmet dynamics (cheap)\n",
        "    if 'px_dist_norm_min' in df.columns:\n",
        "        df['d_px_norm_1'] = df['px_dist_norm_min'] - grp['px_dist_norm_min'].shift(1)\n",
        "        df['d_px_norm_1'] = df['d_px_norm_1'].fillna(0.0)\n",
        "        df['cnt_px_lt006_p3'] = grp['px_dist_norm_min'].transform(lambda s: s.lt(0.06).rolling(3, min_periods=1).sum()).astype(float)\n",
        "        df['cnt_px_lt008_p3'] = grp['px_dist_norm_min'].transform(lambda s: s.lt(0.08).rolling(3, min_periods=1).sum()).astype(float)\n",
        "    else:\n",
        "        df['d_px_norm_1'] = 0.0\n",
        "        df['cnt_px_lt006_p3'] = 0.0\n",
        "        df['cnt_px_lt008_p3'] = 0.0\n",
        "\n",
        "    return df\n",
        "\n",
        "print('Adding dynamic features to train...')\n",
        "tr_dyn = add_dyn_feats(tr)\n",
        "print('Adding dynamic features to test...')\n",
        "te_dyn = add_dyn_feats(te)\n",
        "\n",
        "tr_dyn.to_parquet('train_pairs_w5_helm_dyn.parquet', index=False)\n",
        "te_dyn.to_parquet('test_pairs_w5_helm_dyn.parquet', index=False)\n",
        "print('Saved dyn pairs: train', tr_dyn.shape, 'test', te_dyn.shape)\n",
        "\n",
        "# Rebuild supervised via INNER JOIN and apply \u00b11 positive expansion within supervised only\n",
        "key_cols = ['game_play','step','p1','p2']\n",
        "lab_cols = key_cols + ['contact']\n",
        "labels_min = train_labels[lab_cols].copy()\n",
        "sup = labels_min.merge(tr_dyn, on=key_cols, how='inner')\n",
        "print('Supervised (inner) before expansion:', sup.shape, 'pos rate:', sup['contact'].mean())\n",
        "\n",
        "# Positive expansion \u00b11: flip existing rows at t-1 and t+1 to contact=1 when present\n",
        "pos = sup.loc[sup['contact'] == 1, ['game_play','p1','p2','step']]\n",
        "pos_m1 = pos.copy(); pos_m1['step'] = pos_m1['step'] - 1\n",
        "pos_p1 = pos.copy(); pos_p1['step'] = pos_p1['step'] + 1\n",
        "pos_exp = pd.concat([pos_m1, pos_p1], ignore_index=True).drop_duplicates()\n",
        "pos_exp['flag_pos_exp'] = 1\n",
        "sup = sup.merge(pos_exp, on=['game_play','p1','p2','step'], how='left')\n",
        "sup.loc[sup['flag_pos_exp'] == 1, 'contact'] = 1\n",
        "sup.drop(columns=['flag_pos_exp'], inplace=True)\n",
        "print('After positive expansion: pos rate:', sup['contact'].mean())\n",
        "\n",
        "sup.to_parquet('train_supervised_w5_helm_dyn.parquet', index=False)\n",
        "print('Saved train_supervised_w5_helm_dyn.parquet', sup.shape, 'Elapsed {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pairs with W5 and helmet features...\ntrain pairs: (1641668, 33) test pairs: (191559, 33)\nAdding dynamic features to train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding dynamic features to test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved dyn pairs: train (1641668, 58) test (191559, 58)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supervised (inner) before expansion: (416574, 59) pos rate: 0.10227714643736767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After positive expansion: pos rate: 0.11705963406261552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved train_supervised_w5_helm_dyn.parquet (416574, 59) Elapsed 22.4s\n"
          ]
        }
      ]
    },
    {
      "id": "71377738-b476-46a6-9f53-f267eb18ec35",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train XGBoost on dyn features, smooth OOF, dual thresholds (same vs opp), predict test\n",
        "import time, math, subprocess, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version:', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def mcc_from_counts(tp, tn, fp, fn):\n",
        "    num = tp * tn - fp * fn\n",
        "    den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
        "    den = np.where(den == 0, 1.0, den)\n",
        "    return num / den\n",
        "\n",
        "def fast_dual_threshold_mcc(y_true, prob, same_flag, grid_points=151):\n",
        "    # Build cohort arrays\n",
        "    res = {}\n",
        "    for cohort in (0, 1):\n",
        "        mask = (same_flag == cohort)\n",
        "        y_c = y_true[mask].astype(int)\n",
        "        p_c = prob[mask].astype(float)\n",
        "        n = len(y_c)\n",
        "        if n == 0:\n",
        "            res[cohort] = {\n",
        "                'k_grid': np.array([0], dtype=int),\n",
        "                'tp': np.array([0], dtype=float),\n",
        "                'fp': np.array([0], dtype=float),\n",
        "                'tn': np.array([0], dtype=float),\n",
        "                'fn': np.array([0], dtype=float),\n",
        "                'thr_vals': np.array([1.0], dtype=float)\n",
        "            }\n",
        "            continue\n",
        "        order = np.argsort(-p_c)  # descending by prob\n",
        "        y_sorted = y_c[order]\n",
        "        p_sorted = p_c[order]\n",
        "        cum_pos = np.concatenate([[0], np.cumsum(y_sorted)])  # length n+1\n",
        "        # Grid of k = number predicted positives (top-k rule)\n",
        "        k_grid = np.unique(np.linspace(0, n, num=min(grid_points, n + 1), dtype=int))\n",
        "        tp = cum_pos[k_grid]\n",
        "        fp = k_grid - tp\n",
        "        P = y_sorted.sum()\n",
        "        N = n - P\n",
        "        fn = P - tp\n",
        "        tn = N - fp\n",
        "        thr_vals = np.where(k_grid == 0, 1.0 + 1e-6, p_sorted[np.maximum(0, k_grid - 1)])\n",
        "        res[cohort] = {'k_grid': k_grid, 'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn, 'thr_vals': thr_vals}\n",
        "\n",
        "    # Combine cohorts: iterate small grids and compute MCC from summed counts\n",
        "    tp0, fp0, tn0, fn0, thr0 = res[0]['tp'], res[0]['fp'], res[0]['tn'], res[0]['fn'], res[0]['thr_vals']\n",
        "    tp1, fp1, tn1, fn1, thr1 = res[1]['tp'], res[1]['fp'], res[1]['tn'], res[1]['fn'], res[1]['thr_vals']\n",
        "    best = (-1.0, 0.5, 0.5)\n",
        "    for i in range(len(thr0)):\n",
        "        tp_i = tp0[i]; fp_i = fp0[i]; tn_i = tn0[i]; fn_i = fn0[i]\n",
        "        tp_sum = tp_i + tp1\n",
        "        fp_sum = fp_i + fp1\n",
        "        tn_sum = tn_i + tn1\n",
        "        fn_sum = fn_i + fn1\n",
        "        m_arr = mcc_from_counts(tp_sum, tn_sum, fp_sum, fn_sum)\n",
        "        j = int(np.argmax(m_arr))\n",
        "        m = float(m_arr[j])\n",
        "        if m > best[0]:\n",
        "            best = (m, float(thr0[i]), float(thr1[j]))\n",
        "    return best  # (best_mcc, thr_opp(=cohort0), thr_same(=cohort1))\n",
        "\n",
        "print('Loading supervised dyn train and dyn test features...')\n",
        "train_sup = pd.read_parquet('train_supervised_w5_helm_dyn.parquet')\n",
        "test_feats = pd.read_parquet('test_pairs_w5_helm_dyn.parquet')\n",
        "folds_df = pd.read_csv('folds_game_play.csv')\n",
        "print('train_sup:', train_sup.shape, 'test_feats:', test_feats.shape)\n",
        "\n",
        "# Attach folds\n",
        "train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "assert train_sup['fold'].notna().all()\n",
        "\n",
        "# Ensure helmet imputations (already done earlier, but safe)\n",
        "for df in (train_sup, test_feats):\n",
        "    if 'px_dist_norm_min' in df.columns:\n",
        "        df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns:\n",
        "        df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "\n",
        "# Build feature columns: use numeric columns excluding keys/label\n",
        "drop_cols = {'contact','game_play','step','p1','p2','team1','team2','pos1','pos2','fold'}\n",
        "feat_cols = [c for c in train_sup.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_sup[c])]\n",
        "print('Using', len(feat_cols), 'features')\n",
        "\n",
        "X_all = train_sup[feat_cols].astype(float).values\n",
        "y_all = train_sup['contact'].astype(int).values\n",
        "groups = train_sup['game_play'].values\n",
        "same_flag_all = train_sup['same_team'].astype(int).values if 'same_team' in train_sup.columns else np.zeros(len(train_sup), dtype=int)\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "oof = np.full(len(train_sup), np.nan, dtype=float)\n",
        "models = []\n",
        "start = time.time()\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "    t0 = time.time()\n",
        "    X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "    X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "    neg = (y_tr == 0).sum(); pos = (y_tr == 1).sum()\n",
        "    spw = max(1.0, neg / max(1, pos))\n",
        "    print(f'Fold {fold}: train {len(tr_idx)} (pos {pos}), valid {len(va_idx)} (pos {(y_va==1).sum()}), spw={spw:.2f}', flush=True)\n",
        "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
        "    dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "    params = {\n",
        "        'tree_method': 'hist',\n",
        "        'device': 'cuda',\n",
        "        'max_depth': 7,\n",
        "        'eta': 0.05,\n",
        "        'subsample': 0.9,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'min_child_weight': 10,\n",
        "        'lambda': 1.5,\n",
        "        'alpha': 0.1,\n",
        "        'gamma': 0.1,\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'logloss',\n",
        "        'scale_pos_weight': float(spw),\n",
        "        'seed': 42 + fold\n",
        "    }\n",
        "    evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
        "    booster = xgb.train(\n",
        "        params=params,\n",
        "        dtrain=dtrain,\n",
        "        num_boost_round=4000,\n",
        "        evals=evals,\n",
        "        early_stopping_rounds=200,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "    best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "    oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "    models.append((booster, best_it))\n",
        "    print(f' Fold {fold} done in {time.time()-t0:.1f}s; best_iteration={best_it}', flush=True)\n",
        "\n",
        "# Smooth OOF per (gp,p1,p2) with centered rolling-max window=3\n",
        "oof_df = train_sup[['game_play','p1','p2','step','views_both_present']].copy()\n",
        "oof_df['oof'] = oof\n",
        "oof_df = oof_df.sort_values(['game_play','p1','p2','step'])\n",
        "grp = oof_df.groupby(['game_play','p1','p2'], sort=False)\n",
        "oof_df['oof_smooth'] = grp['oof'].transform(lambda s: s.rolling(3, center=True, min_periods=1).max())\n",
        "oof_smooth = oof_df['oof_smooth'].values\n",
        "\n",
        "# Align labels and flags to the sorted oof_df row order\n",
        "idx_ord = oof_df.index.to_numpy()\n",
        "y_sorted = train_sup['contact'].astype(int).to_numpy()[idx_ord]\n",
        "if 'same_team' in train_sup.columns:\n",
        "    same_flag_sorted = train_sup['same_team'].fillna(0).astype(int).to_numpy()[idx_ord]\n",
        "else:\n",
        "    same_flag_sorted = np.zeros(len(oof_df), dtype=int)\n",
        "vb_sorted = (oof_df['views_both_present'].to_numpy() > 0).astype(int)\n",
        "\n",
        "# Two cohorts by views_both_present, each with dual thresholds (same vs opp)\n",
        "thr_dict = {}  # (vb)->(thr_opp, thr_same)\n",
        "for vb in (0, 1):\n",
        "    mask = (vb_sorted == vb)\n",
        "    if mask.sum() == 0:\n",
        "        thr_dict[vb] = (0.77, 0.77)  # default\n",
        "        continue\n",
        "    best_mcc_sub, thr_opp_sub, thr_same_sub = fast_dual_threshold_mcc(y_sorted[mask], oof_smooth[mask], same_flag_sorted[mask], grid_points=151)\n",
        "    if not np.isfinite(best_mcc_sub) or best_mcc_sub < 0:\n",
        "        thrs = np.linspace(0.01, 0.99, 99)\n",
        "        m_list = []\n",
        "        for t in thrs:\n",
        "            pred = (oof_smooth[mask] >= t).astype(int)\n",
        "            m_list.append(matthews_corrcoef(y_sorted[mask], pred))\n",
        "        j = int(np.argmax(m_list))\n",
        "        thr_opp_sub = thr_same_sub = float(thrs[j])\n",
        "    thr_dict[vb] = (float(thr_opp_sub), float(thr_same_sub))\n",
        "print('Thresholds by views flag:', thr_dict)\n",
        "\n",
        "# Evaluate combined OOF MCC with 4 thresholds\n",
        "thr_arr = np.empty(len(oof_df), dtype=float)\n",
        "for vb in (0, 1):\n",
        "    mask = (vb_sorted == vb)\n",
        "    t_opp, t_same = thr_dict[vb]\n",
        "    thr_arr[mask] = np.where(same_flag_sorted[mask] == 1, t_same, t_opp)\n",
        "pred_oof = (oof_smooth >= thr_arr).astype(int)\n",
        "oof_mcc_all = matthews_corrcoef(y_sorted, pred_oof)\n",
        "print(f'OOF MCC with 4 thresholds: {oof_mcc_all:.5f}')\n",
        "\n",
        "# Inference on test and smoothing\n",
        "Xt = test_feats[feat_cols].astype(float).values\n",
        "dtest = xgb.DMatrix(Xt)\n",
        "pt = np.zeros(len(test_feats), dtype=float)\n",
        "for i, (booster, best_it) in enumerate(models):\n",
        "    t0 = time.time()\n",
        "    pt += booster.predict(dtest, iteration_range=(0, best_it + 1))\n",
        "    print(f' Inference model {i} took {time.time()-t0:.1f}s', flush=True)\n",
        "pt /= max(1, len(models))\n",
        "pred_tmp = test_feats[['game_play','step','p1','p2','views_both_present']].copy()\n",
        "pred_tmp['prob'] = pt\n",
        "pred_tmp = pred_tmp.sort_values(['game_play','p1','p2','step'])\n",
        "grp_t = pred_tmp.groupby(['game_play','p1','p2'], sort=False)\n",
        "pred_tmp['prob_smooth'] = grp_t['prob'].transform(lambda s: s.rolling(3, center=True, min_periods=1).max())\n",
        "\n",
        "# Apply 4 thresholds by same_team and views_both_present on test\n",
        "same_flag_test = test_feats['same_team'].astype(int).values if 'same_team' in test_feats.columns else np.zeros(len(test_feats), dtype=int)\n",
        "vb_test = (pred_tmp['views_both_present'].to_numpy() > 0).astype(int)\n",
        "thr_arr_test = np.empty(len(pred_tmp), dtype=float)\n",
        "for vb in (0, 1):\n",
        "    mask = (vb_test == vb)\n",
        "    t_opp, t_same = thr_dict[vb]\n",
        "    thr_arr_test[mask] = np.where(same_flag_test[mask] == 1, t_same, t_opp)\n",
        "pred_bin = (pred_tmp['prob_smooth'].values >= thr_arr_test).astype(int)\n",
        "\n",
        "# Build submission safely (avoid column clash)\n",
        "cid = (test_feats['game_play'].astype(str) + '_' + test_feats['step'].astype(str) + '_' +\n",
        "       test_feats['p1'].astype(str) + '_' + test_feats['p2'].astype(str))\n",
        "pred_df = pd.DataFrame({'contact_id': cid, 'pred_contact': pred_bin})\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.copy()\n",
        "sub['contact'] = sub['contact_id'].map(pred_df.set_index('contact_id')['pred_contact']).fillna(0).astype(int)\n",
        "sub[['contact_id','contact']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv')\n",
        "print('Done. Total time:', f'{time.time()-start:.1f}s', flush=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version: 2.1.4\nLoading supervised dyn train and dyn test features...\ntrain_sup: (416574, 59) test_feats: (191559, 58)\nUsing 50 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: train 333273 (pos 38783), valid 83301 (pos 9981), spw=7.59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 0 done in 22.7s; best_iteration=2724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train 333268 (pos 39102), valid 83306 (pos 9662), spw=7.52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 1 done in 24.9s; best_iteration=3085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: train 333273 (pos 39536), valid 83301 (pos 9228), spw=7.43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 2 done in 22.7s; best_iteration=2732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: train 333286 (pos 38584), valid 83288 (pos 10180), spw=7.64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 3 done in 23.1s; best_iteration=2751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: train 333196 (pos 39051), valid 83378 (pos 9713), spw=7.53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 4 done in 23.5s; best_iteration=2720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_94/1136908956.py:18: RuntimeWarning: invalid value encountered in sqrt\n  den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thresholds by views flag: {0: (0.36875900626182556, 0.40483367443084717), 1: (0.77, 0.77)}\nOOF MCC with 4 thresholds: 0.70980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 0 took 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 1 took 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 2 took 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 3 took 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 4 took 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv\nDone. Total time: 126.2s\n"
          ]
        }
      ]
    },
    {
      "id": "8e463c59-5a0c-4275-b4d8-1a2d9f6fb19b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Rebuild full pipeline with candidate radius r=3.5 and save *_r35 artifacts (pairs -> W5 -> helmets -> dyn -> supervised + \u00b11 expansion)\n",
        "import pandas as pd, numpy as np, time, math\n",
        "from itertools import combinations\n",
        "\n",
        "t0 = time.time()\n",
        "print('Rebuilding pipeline with r=3.5 ...')\n",
        "\n",
        "# 1) Build pairs at r=3.5 using existing tracking indices and build_feature_table/build_pairs_for_group from earlier cells\n",
        "def build_pairs_for_group_r(gdf, r=3.5):\n",
        "    rows = []\n",
        "    arr = gdf[['nfl_player_id','team','position','x_position','y_position','speed','acceleration','direction']].values\n",
        "    n = arr.shape[0]\n",
        "    for i, j in combinations(range(n), 2):\n",
        "        pid_i, team_i, pos_i, xi, yi, si, ai, diri = arr[i]\n",
        "        pid_j, team_j, pos_j, xj, yj, sj, aj, dirj = arr[j]\n",
        "        dx = xj - xi; dy = yj - yi\n",
        "        dist = math.hypot(dx, dy)\n",
        "        if dist > r:\n",
        "            continue\n",
        "        a = int(pid_i); b = int(pid_j)\n",
        "        p1, p2 = (str(a), str(b)) if a <= b else (str(b), str(a))\n",
        "        vxi = si * math.cos(math.radians(diri)) if not pd.isna(diri) else 0.0\n",
        "        vyi = si * math.sin(math.radians(diri)) if not pd.isna(diri) else 0.0\n",
        "        vxj = sj * math.cos(math.radians(dirj)) if not pd.isna(dirj) else 0.0\n",
        "        vyj = sj * math.sin(math.radians(dirj)) if not pd.isna(dirj) else 0.0\n",
        "        rvx = vxj - vxi; rvy = vyj - vyi\n",
        "        if dist > 0:\n",
        "            ux = dx / dist; uy = dy / dist\n",
        "            closing = rvx * ux + rvy * uy\n",
        "        else:\n",
        "            closing = 0.0\n",
        "        # heading difference abs\n",
        "        if pd.isna(diri) or pd.isna(dirj):\n",
        "            hd = np.nan\n",
        "        else:\n",
        "            d = (diri - dirj + 180) % 360 - 180\n",
        "            hd = abs(d)\n",
        "        rows.append((p1, p2, dist, dx, dy, si, sj, ai, aj, closing, abs(closing), hd, int(team_i == team_j), str(team_i), str(team_j), str(pos_i), str(pos_j)))\n",
        "    if not rows:\n",
        "        return pd.DataFrame(columns=['p1','p2','distance','rel_dx','rel_dy','speed1','speed2','accel1','accel2','closing','abs_closing','abs_d_heading','same_team','team1','team2','pos1','pos2'])\n",
        "    return pd.DataFrame(rows, columns=['p1','p2','distance','rel_dx','rel_dy','speed1','speed2','accel1','accel2','closing','abs_closing','abs_d_heading','same_team','team1','team2','pos1','pos2'])\n",
        "\n",
        "def build_feature_table_r(track_df, r=3.5):\n",
        "    feats = []\n",
        "    cnt = 0\n",
        "    last = time.time()\n",
        "    for (gp, step), gdf in track_df.groupby(['game_play','step'], sort=False):\n",
        "        f = build_pairs_for_group_r(gdf, r=r)\n",
        "        if not f.empty:\n",
        "            f.insert(0, 'step', step)\n",
        "            f.insert(0, 'game_play', gp)\n",
        "            feats.append(f)\n",
        "        cnt += 1\n",
        "        if cnt % 500 == 0:\n",
        "            now = time.time()\n",
        "            print(f' processed {cnt} steps; +{now-last:.1f}s; total {now-t0:.1f}s', flush=True)\n",
        "            last = now\n",
        "    if feats:\n",
        "        return pd.concat(feats, ignore_index=True)\n",
        "    return pd.DataFrame(columns=['game_play','step','p1','p2','distance','rel_dx','rel_dy','speed1','speed2','accel1','accel2','closing','abs_closing','abs_d_heading','same_team','team1','team2','pos1','pos2'])\n",
        "\n",
        "print('Building train pairs r=3.5 ...')\n",
        "train_pairs_r35 = build_feature_table_r(train_track_idx, r=3.5)\n",
        "print('train_pairs_r35:', train_pairs_r35.shape)\n",
        "train_pairs_r35.to_parquet('train_pairs_r35.parquet', index=False)\n",
        "print('Building test pairs r=3.5 ...')\n",
        "test_pairs_r35 = build_feature_table_r(test_track_idx, r=3.5)\n",
        "print('test_pairs_r35:', test_pairs_r35.shape)\n",
        "test_pairs_r35.to_parquet('test_pairs_r35.parquet', index=False)\n",
        "\n",
        "# 2) Add W5 past-only features using existing add_window_feats from earlier cells\n",
        "def add_window_feats_local(df: pd.DataFrame, W: int = 5):\n",
        "    df = df.sort_values(['game_play','p1','p2','step']).copy()\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    df['dist_min_p5'] = grp['distance'].rolling(W, min_periods=1).min().reset_index(level=[0,1,2], drop=True)\n",
        "    df['dist_mean_p5'] = grp['distance'].rolling(W, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n",
        "    df['dist_max_p5'] = grp['distance'].rolling(W, min_periods=1).max().reset_index(level=[0,1,2], drop=True)\n",
        "    df['dist_std_p5'] = grp['distance'].rolling(W, min_periods=1).std().reset_index(level=[0,1,2], drop=True)\n",
        "    df['abs_close_min_p5'] = grp['abs_closing'].rolling(W, min_periods=1).min().reset_index(level=[0,1,2], drop=True)\n",
        "    df['abs_close_mean_p5'] = grp['abs_closing'].rolling(W, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n",
        "    df['abs_close_max_p5'] = grp['abs_closing'].rolling(W, min_periods=1).max().reset_index(level=[0,1,2], drop=True)\n",
        "    df['abs_close_std_p5'] = grp['abs_closing'].rolling(W, min_periods=1).std().reset_index(level=[0,1,2], drop=True)\n",
        "    for thr, name in [(1.5,'lt15'), (2.0,'lt20'), (2.5,'lt25')]:\n",
        "        key = f'cnt_dist_{name}_p5'\n",
        "        df[key] = grp['distance'].apply(lambda s: s.lt(thr).rolling(W, min_periods=1).sum()).reset_index(level=[0,1,2], drop=True)\n",
        "    df['dist_delta_p5'] = df['distance'] - grp['distance'].shift(W)\n",
        "    return df\n",
        "\n",
        "print('Adding W5 features (train/test)...')\n",
        "train_w_r35 = add_window_feats_local(train_pairs_r35, W=5)\n",
        "test_w_r35 = add_window_feats_local(test_pairs_r35, W=5)\n",
        "train_w_r35.to_parquet('train_pairs_w5_r35.parquet', index=False)\n",
        "test_w_r35.to_parquet('test_pairs_w5_r35.parquet', index=False)\n",
        "\n",
        "# 3) Helmet merge: recompute aggregates (dedup + step mapping) and merge into pairs\n",
        "FPS = 59.94\n",
        "def prep_meta(vmeta: pd.DataFrame):\n",
        "    vm = vmeta.copy()\n",
        "    for c in ['start_time','snap_time']:\n",
        "        if np.issubdtype(vm[c].dtype, np.number):\n",
        "            continue\n",
        "        ts = pd.to_datetime(vm[c], errors='coerce')\n",
        "        if ts.notna().any():\n",
        "            vm[c] = (ts - ts.dt.floor('D')).dt.total_seconds().astype(float)\n",
        "        else:\n",
        "            vm[c] = pd.to_numeric(vm[c], errors='coerce')\n",
        "    vm['snap_frame'] = ((vm['snap_time'] - vm['start_time']) * FPS).round().astype('Int64')\n",
        "    return vm[['game_play','view','snap_frame']].drop_duplicates()\n",
        "\n",
        "print('Loading helmets and video metadata for r=3.5 merge...')\n",
        "train_helm_df = pd.read_csv('train_baseline_helmets.csv')\n",
        "test_helm_df = pd.read_csv('test_baseline_helmets.csv')\n",
        "train_vmeta_df = pd.read_csv('train_video_metadata.csv')\n",
        "test_vmeta_df = pd.read_csv('test_video_metadata.csv')\n",
        "meta_tr = prep_meta(train_vmeta_df); meta_te = prep_meta(test_vmeta_df)\n",
        "\n",
        "def dedup_and_step(helm: pd.DataFrame, meta: pd.DataFrame):\n",
        "    df = helm[['game_play','view','frame','nfl_player_id','left','top','width','height']].copy()\n",
        "    df = df.dropna(subset=['nfl_player_id'])\n",
        "    df['nfl_player_id'] = df['nfl_player_id'].astype(int).astype(str)\n",
        "    df['area'] = df['width'] * df['height']\n",
        "    df['cx'] = df['left'] + 0.5 * df['width']\n",
        "    df['cy'] = df['top'] + 0.5 * df['height']\n",
        "    df = df.sort_values(['game_play','view','frame','nfl_player_id','area'], ascending=[True,True,True,True,False]).drop_duplicates(['game_play','view','frame','nfl_player_id'], keep='first')\n",
        "    df = df.merge(meta, on=['game_play','view'], how='left')\n",
        "    df['step'] = ((df['frame'] - df['snap_frame']).astype('float') / 6.0).round().astype('Int64')\n",
        "    df = df.dropna(subset=['step']); df['step'] = df['step'].astype(int)\n",
        "    dm1 = df.copy(); dm1['target_step'] = dm1['step'] - 1\n",
        "    d0 = df.copy(); d0['target_step'] = d0['step']\n",
        "    dp1 = df.copy(); dp1['target_step'] = dp1['step'] + 1\n",
        "    d = pd.concat([dm1, d0, dp1], ignore_index=True)\n",
        "    agg = d.groupby(['game_play','view','target_step','nfl_player_id'], sort=False).agg(\n",
        "        cx_mean=('cx','mean'), cy_mean=('cy','mean'), h_mean=('height','mean'), cnt=('cx','size')\n",
        "    ).reset_index().rename(columns={'target_step':'step'})\n",
        "    return agg\n",
        "\n",
        "print('Preparing helmet aggregates...')\n",
        "h_tr = dedup_and_step(train_helm_df, meta_tr)\n",
        "h_te = dedup_and_step(test_helm_df, meta_te)\n",
        "print('Helmet agg shapes:', h_tr.shape, h_te.shape)\n",
        "\n",
        "def merge_helmet_to_pairs_df(pairs: pd.DataFrame, h_agg: pd.DataFrame):\n",
        "    ha = h_agg[['game_play','step','view','nfl_player_id','cx_mean','cy_mean','h_mean']].copy()\n",
        "    a = ha.rename(columns={'nfl_player_id':'p1','cx_mean':'cx1','cy_mean':'cy1','h_mean':'h1'})\n",
        "    b = ha.rename(columns={'nfl_player_id':'p2','cx_mean':'cx2','cy_mean':'cy2','h_mean':'h2'})\n",
        "    merged = a.merge(b, on=['game_play','step','view'], how='inner')\n",
        "    merged = merged[merged['p1'] < merged['p2']]\n",
        "    merged['px_dist'] = np.sqrt((merged['cx1'] - merged['cx2'])**2 + (merged['cy1'] - merged['cy2'])**2)\n",
        "    merged['px_dist_norm'] = merged['px_dist'] / np.sqrt(np.maximum(1e-6, merged['h1'] * merged['h2']))\n",
        "    agg = merged.groupby(['game_play','step','p1','p2'], as_index=False).agg(\n",
        "        px_dist_norm_min=('px_dist_norm','min'),\n",
        "        views_both_present=('px_dist_norm', lambda s: int(s.notna().sum()))\n",
        "    )\n",
        "    out = pairs.merge(agg, on=['game_play','step','p1','p2'], how='left')\n",
        "    return out\n",
        "\n",
        "print('Merging helmets into pairs (train/test) ...')\n",
        "train_pairs_w5_helm_r35 = merge_helmet_to_pairs_df(train_w_r35, h_tr)\n",
        "test_pairs_w5_helm_r35 = merge_helmet_to_pairs_df(test_w_r35, h_te)\n",
        "train_pairs_w5_helm_r35.to_parquet('train_pairs_w5_helm_r35.parquet', index=False)\n",
        "test_pairs_w5_helm_r35.to_parquet('test_pairs_w5_helm_r35.parquet', index=False)\n",
        "\n",
        "# 4) Add dynamic features (TTC/deltas/rel/helmet dynamics)\n",
        "def add_dyn_feats(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.sort_values(['game_play','p1','p2','step']).copy()\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "    df['approaching_flag'] = (df['closing'] < 0).astype(int)\n",
        "    denom = (-df['closing']).clip(lower=1e-3)\n",
        "    ttc_raw = df['distance'] / denom\n",
        "    ttc_raw = ttc_raw.where(df['approaching_flag'] == 1, 10.0)\n",
        "    df['ttc_raw'] = ttc_raw.astype(float)\n",
        "    df['ttc_clip'] = df['ttc_raw'].clip(0, 5)\n",
        "    df['ttc_log'] = np.log1p(df['ttc_clip'])\n",
        "    df['inv_ttc'] = 1.0 / (1.0 + df['ttc_clip'])\n",
        "    df['d_dist_1'] = df['distance'] - grp['distance'].shift(1)\n",
        "    df['d_dist_2'] = df['distance'] - grp['distance'].shift(2)\n",
        "    df['d_dist_5'] = df['distance'] - grp['distance'].shift(5)\n",
        "    df['d_close_1'] = df['closing'] - grp['closing'].shift(1)\n",
        "    df['d_absclose_1'] = df['abs_closing'] - grp['abs_closing'].shift(1)\n",
        "    df['d_speed1_1'] = df['speed1'] - grp['speed1'].shift(1)\n",
        "    df['d_speed2_1'] = df['speed2'] - grp['speed2'].shift(1)\n",
        "    df['d_accel1_1'] = df['accel1'] - grp['accel1'].shift(1)\n",
        "    df['d_accel2_1'] = df['accel2'] - grp['accel2'].shift(1)\n",
        "    df['rm3_d_dist_1'] = grp['d_dist_1'].transform(lambda s: s.rolling(3, min_periods=1).mean())\n",
        "    df['rm3_d_close_1'] = grp['d_close_1'].transform(lambda s: s.rolling(3, min_periods=1).mean())\n",
        "    for c in ['d_dist_1','d_dist_2','d_dist_5','d_close_1','d_absclose_1','d_speed1_1','d_speed2_1','d_accel1_1','d_accel2_1','rm3_d_dist_1','rm3_d_close_1']:\n",
        "        df[c] = df[c].fillna(0.0)\n",
        "    df['rel_speed'] = (df['speed2'] - df['speed1']).astype(float)\n",
        "    df['abs_rel_speed'] = df['rel_speed'].abs()\n",
        "    df['rel_accel'] = (df['accel2'] - df['accel1']).astype(float)\n",
        "    df['abs_rel_accel'] = df['rel_accel'].abs()\n",
        "    df['jerk1'] = grp['accel1'].diff().fillna(0.0)\n",
        "    df['jerk2'] = grp['accel2'].diff().fillna(0.0)\n",
        "    if 'px_dist_norm_min' in df.columns:\n",
        "        df['d_px_norm_1'] = df['px_dist_norm_min'] - grp['px_dist_norm_min'].shift(1)\n",
        "        df['d_px_norm_1'] = df['d_px_norm_1'].fillna(0.0)\n",
        "        df['cnt_px_lt006_p3'] = grp['px_dist_norm_min'].transform(lambda s: s.lt(0.06).rolling(3, min_periods=1).sum()).astype(float)\n",
        "        df['cnt_px_lt008_p3'] = grp['px_dist_norm_min'].transform(lambda s: s.lt(0.08).rolling(3, min_periods=1).sum()).astype(float)\n",
        "    else:\n",
        "        df['d_px_norm_1'] = 0.0; df['cnt_px_lt006_p3'] = 0.0; df['cnt_px_lt008_p3'] = 0.0\n",
        "    return df\n",
        "\n",
        "print('Adding dyn features (train/test) ...')\n",
        "tr_dyn_r35 = add_dyn_feats(train_pairs_w5_helm_r35)\n",
        "te_dyn_r35 = add_dyn_feats(test_pairs_w5_helm_r35)\n",
        "tr_dyn_r35.to_parquet('train_pairs_w5_helm_dyn_r35.parquet', index=False)\n",
        "te_dyn_r35.to_parquet('test_pairs_w5_helm_dyn_r35.parquet', index=False)\n",
        "\n",
        "# 5) Supervised via INNER JOIN to labels then \u00b11 positive expansion\n",
        "key_cols = ['game_play','step','p1','p2']\n",
        "lab_cols = key_cols + ['contact']\n",
        "labels_min = train_labels[lab_cols].copy()\n",
        "sup_r35 = labels_min.merge(tr_dyn_r35, on=key_cols, how='inner')\n",
        "print('Supervised(inner) r=3.5 before expansion:', sup_r35.shape, 'pos rate:', sup_r35['contact'].mean())\n",
        "pos = sup_r35.loc[sup_r35['contact'] == 1, ['game_play','p1','p2','step']]\n",
        "pos_m1 = pos.copy(); pos_m1['step'] = pos_m1['step'] - 1\n",
        "pos_p1 = pos.copy(); pos_p1['step'] = pos_p1['step'] + 1\n",
        "pos_exp = pd.concat([pos_m1, pos_p1], ignore_index=True).drop_duplicates()\n",
        "pos_exp['flag_pos_exp'] = 1\n",
        "sup_r35 = sup_r35.merge(pos_exp, on=['game_play','p1','p2','step'], how='left')\n",
        "sup_r35.loc[sup_r35['flag_pos_exp'] == 1, 'contact'] = 1\n",
        "sup_r35.drop(columns=['flag_pos_exp'], inplace=True)\n",
        "print('After positive expansion (r=3.5): pos rate:', sup_r35['contact'].mean())\n",
        "sup_r35.to_parquet('train_supervised_w5_helm_dyn_r35.parquet', index=False)\n",
        "\n",
        "print('Done r=3.5 rebuild in {:.1f}s'.format(time.time()-t0), flush=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rebuilding pipeline with r=3.5 ...\nBuilding train pairs r=3.5 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 500 steps; +0.7s; total 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1000 steps; +0.5s; total 1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1500 steps; +0.8s; total 2.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2000 steps; +0.6s; total 2.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2500 steps; +0.6s; total 3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3000 steps; +0.6s; total 3.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3500 steps; +0.6s; total 4.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4000 steps; +0.8s; total 5.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4500 steps; +0.6s; total 5.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5000 steps; +0.6s; total 6.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5500 steps; +0.6s; total 6.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 6000 steps; +0.5s; total 7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 6500 steps; +0.5s; total 7.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 7000 steps; +0.6s; total 8.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 7500 steps; +0.9s; total 9.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 8000 steps; +0.5s; total 9.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 8500 steps; +0.6s; total 10.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 9000 steps; +0.6s; total 10.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 9500 steps; +0.6s; total 11.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 10000 steps; +0.6s; total 12.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 10500 steps; +0.6s; total 12.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 11000 steps; +0.5s; total 13.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 11500 steps; +0.5s; total 13.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 12000 steps; +1.0s; total 14.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 12500 steps; +0.6s; total 15.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 13000 steps; +0.6s; total 15.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 13500 steps; +0.5s; total 16.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 14000 steps; +0.6s; total 16.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 14500 steps; +0.6s; total 17.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 15000 steps; +0.5s; total 18.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 15500 steps; +0.5s; total 18.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 16000 steps; +0.6s; total 19.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 16500 steps; +0.6s; total 19.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 17000 steps; +1.0s; total 20.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 17500 steps; +0.6s; total 21.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 18000 steps; +0.5s; total 21.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 18500 steps; +0.6s; total 22.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 19000 steps; +0.6s; total 23.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 19500 steps; +0.5s; total 23.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 20000 steps; +0.5s; total 24.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 20500 steps; +0.5s; total 24.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 21000 steps; +0.6s; total 25.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 21500 steps; +0.5s; total 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 22000 steps; +0.5s; total 26.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 22500 steps; +0.6s; total 26.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 23000 steps; +0.5s; total 27.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 23500 steps; +1.2s; total 28.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 24000 steps; +0.6s; total 29.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 24500 steps; +0.6s; total 29.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 25000 steps; +0.6s; total 30.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 25500 steps; +0.6s; total 30.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 26000 steps; +0.6s; total 31.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 26500 steps; +0.6s; total 31.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 27000 steps; +0.6s; total 32.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 27500 steps; +0.6s; total 33.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 28000 steps; +0.6s; total 33.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 28500 steps; +0.6s; total 34.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 29000 steps; +0.6s; total 34.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 29500 steps; +0.6s; total 35.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 30000 steps; +0.6s; total 35.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 30500 steps; +0.6s; total 36.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 31000 steps; +1.4s; total 37.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 31500 steps; +0.6s; total 38.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 32000 steps; +0.5s; total 38.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 32500 steps; +0.5s; total 39.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 33000 steps; +0.5s; total 40.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 33500 steps; +0.6s; total 40.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 34000 steps; +0.6s; total 41.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 34500 steps; +0.6s; total 41.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 35000 steps; +0.5s; total 42.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 35500 steps; +0.6s; total 42.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 36000 steps; +0.6s; total 43.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 36500 steps; +0.6s; total 43.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 37000 steps; +0.6s; total 44.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 37500 steps; +0.6s; total 45.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 38000 steps; +0.6s; total 45.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 38500 steps; +0.6s; total 46.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 39000 steps; +0.6s; total 46.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 39500 steps; +0.5s; total 47.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 40000 steps; +1.4s; total 48.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 40500 steps; +0.6s; total 49.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 41000 steps; +0.5s; total 49.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 41500 steps; +0.6s; total 50.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 42000 steps; +0.5s; total 51.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 42500 steps; +0.6s; total 51.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 43000 steps; +0.6s; total 52.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 43500 steps; +0.6s; total 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 44000 steps; +0.5s; total 53.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 44500 steps; +0.5s; total 53.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 45000 steps; +0.5s; total 54.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 45500 steps; +0.6s; total 54.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 46000 steps; +0.6s; total 55.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 46500 steps; +0.5s; total 56.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 47000 steps; +0.5s; total 56.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 47500 steps; +0.6s; total 57.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 48000 steps; +0.6s; total 57.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 48500 steps; +0.6s; total 58.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 49000 steps; +0.6s; total 58.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 49500 steps; +0.5s; total 59.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 50000 steps; +0.5s; total 59.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 50500 steps; +0.6s; total 60.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 51000 steps; +0.6s; total 61.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 51500 steps; +1.7s; total 62.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 52000 steps; +0.6s; total 63.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 52500 steps; +0.6s; total 63.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 53000 steps; +0.6s; total 64.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 53500 steps; +0.6s; total 65.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 54000 steps; +0.6s; total 65.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 54500 steps; +0.6s; total 66.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 55000 steps; +0.6s; total 66.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 55500 steps; +0.6s; total 67.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_pairs_r35: (2051428, 19)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building test pairs r=3.5 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 500 steps; +0.6s; total 73.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1000 steps; +0.6s; total 74.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1500 steps; +0.6s; total 74.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2000 steps; +0.6s; total 75.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2500 steps; +0.6s; total 75.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3000 steps; +0.5s; total 76.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3500 steps; +0.5s; total 77.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4000 steps; +0.6s; total 77.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4500 steps; +0.6s; total 78.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5000 steps; +0.5s; total 78.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5500 steps; +0.6s; total 79.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_pairs_r35: (237673, 19)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding W5 features (train/test)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading helmets and video metadata for r=3.5 merge...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing helmet aggregates...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helmet agg shapes: (620840, 8) (67667, 8)\nMerging helmets into pairs (train/test) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding dyn features (train/test) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supervised(inner) r=3.5 before expansion: (524248, 59) pos rate: 0.0812878637591369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After positive expansion (r=3.5): pos rate: 0.09304947276861333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done r=3.5 rebuild in 369.6s\n"
          ]
        }
      ]
    },
    {
      "id": "fbc69afc-a2fe-415b-a1fa-a29ec1ee48a0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train on r=3.5 dyn features, smooth OOF, dual thresholds (same vs opp), predict test\n",
        "import time, sys, subprocess, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version:', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def mcc_from_counts(tp, tn, fp, fn):\n",
        "    num = tp * tn - fp * fn\n",
        "    den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
        "    den = np.where(den == 0, 1.0, den)\n",
        "    return num / den\n",
        "\n",
        "def fast_dual_threshold_mcc(y_true, prob, same_flag, grid_points=151):\n",
        "    res = {}\n",
        "    for cohort in (0, 1):\n",
        "        mask = (same_flag == cohort)\n",
        "        y_c = y_true[mask].astype(int)\n",
        "        p_c = prob[mask].astype(float)\n",
        "        n = len(y_c)\n",
        "        if n == 0:\n",
        "            res[cohort] = {'k_grid': np.array([0], int), 'tp': np.array([0.0]), 'fp': np.array([0.0]), 'tn': np.array([0.0]), 'fn': np.array([0.0]), 'thr_vals': np.array([1.0])}\n",
        "            continue\n",
        "        order = np.argsort(-p_c)\n",
        "        y_sorted = y_c[order]\n",
        "        p_sorted = p_c[order]\n",
        "        cum_pos = np.concatenate([[0], np.cumsum(y_sorted)])\n",
        "        k_grid = np.unique(np.linspace(0, n, num=min(grid_points, n + 1), dtype=int))\n",
        "        tp = cum_pos[k_grid]\n",
        "        fp = k_grid - tp\n",
        "        P = y_sorted.sum(); N = n - P\n",
        "        fn = P - tp; tn = N - fp\n",
        "        thr_vals = np.where(k_grid == 0, 1.0 + 1e-6, p_sorted[np.maximum(0, k_grid - 1)])\n",
        "        res[cohort] = {'k_grid': k_grid, 'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn, 'thr_vals': thr_vals}\n",
        "    tp0, fp0, tn0, fn0, thr0 = res[0]['tp'], res[0]['fp'], res[0]['tn'], res[0]['fn'], res[0]['thr_vals']\n",
        "    tp1, fp1, tn1, fn1, thr1 = res[1]['tp'], res[1]['fp'], res[1]['tn'], res[1]['fn'], res[1]['thr_vals']\n",
        "    best = (-1.0, 0.5, 0.5)\n",
        "    for i in range(len(thr0)):\n",
        "        tp_sum = tp0[i] + tp1; fp_sum = fp0[i] + fp1; tn_sum = tn0[i] + tn1; fn_sum = fn0[i] + fn1\n",
        "        m_arr = mcc_from_counts(tp_sum, tn_sum, fp_sum, fn_sum)\n",
        "        j = int(np.argmax(m_arr)); m = float(m_arr[j])\n",
        "        if m > best[0]:\n",
        "            best = (m, float(thr0[i]), float(thr1[j]))\n",
        "    return best  # (best_mcc, thr_opp, thr_same)\n",
        "\n",
        "print('Loading r=3.5 supervised dyn train and dyn test features...')\n",
        "train_sup = pd.read_parquet('train_supervised_w5_helm_dyn_r35.parquet')\n",
        "test_feats = pd.read_parquet('test_pairs_w5_helm_dyn_r35.parquet')\n",
        "folds_df = pd.read_csv('folds_game_play.csv')\n",
        "print('train_sup:', train_sup.shape, 'test_feats:', test_feats.shape)\n",
        "\n",
        "train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "assert train_sup['fold'].notna().all()\n",
        "\n",
        "for df in (train_sup, test_feats):\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "\n",
        "drop_cols = {'contact','game_play','step','p1','p2','team1','team2','pos1','pos2','fold'}\n",
        "feat_cols = [c for c in train_sup.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_sup[c])]\n",
        "print('Using', len(feat_cols), 'features')\n",
        "\n",
        "X_all = train_sup[feat_cols].astype(float).values\n",
        "y_all = train_sup['contact'].astype(int).values\n",
        "groups = train_sup['game_play'].values\n",
        "same_flag_all = train_sup['same_team'].astype(int).values if 'same_team' in train_sup.columns else np.zeros(len(train_sup), dtype=int)\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "oof = np.full(len(train_sup), np.nan, dtype=float)\n",
        "models = []\n",
        "start = time.time()\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "    t0 = time.time()\n",
        "    X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "    X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "    neg = (y_tr == 0).sum(); pos = (y_tr == 1).sum()\n",
        "    spw = max(1.0, neg / max(1, pos))\n",
        "    print(f'Fold {fold}: train {len(tr_idx)} (pos {pos}), valid {len(va_idx)} (pos {(y_va==1).sum()}), spw={spw:.2f}', flush=True)\n",
        "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
        "    dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "    params = {\n",
        "        'tree_method': 'hist', 'device': 'cuda', 'max_depth': 7, 'eta': 0.05, 'subsample': 0.9,\n",
        "        'colsample_bytree': 0.8, 'min_child_weight': 10, 'lambda': 1.5, 'alpha': 0.1, 'gamma': 0.1,\n",
        "        'objective': 'binary:logistic', 'eval_metric': 'logloss', 'scale_pos_weight': float(spw), 'seed': 42 + fold\n",
        "    }\n",
        "    booster = xgb.train(params=params, dtrain=dtrain, num_boost_round=4000, evals=[(dtrain,'train'),(dvalid,'valid')],\n",
        "                        early_stopping_rounds=200, verbose_eval=False)\n",
        "    best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "    oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "    models.append((booster, best_it))\n",
        "    print(f' Fold {fold} done in {time.time()-t0:.1f}s; best_iteration={best_it}', flush=True)\n",
        "\n",
        "# Smooth OOF per (gp,p1,p2) with centered rolling-max window=3\n",
        "oof_df = train_sup[['game_play','p1','p2','step']].copy()\n",
        "oof_df['oof'] = oof\n",
        "oof_df = oof_df.sort_values(['game_play','p1','p2','step'])\n",
        "grp = oof_df.groupby(['game_play','p1','p2'], sort=False)\n",
        "oof_df['oof_smooth'] = grp['oof'].transform(lambda s: s.rolling(3, center=True, min_periods=1).max())\n",
        "oof_smooth = oof_df['oof_smooth'].values\n",
        "idx_ord = oof_df.index.to_numpy()\n",
        "y_sorted = train_sup['contact'].astype(int).to_numpy()[idx_ord]\n",
        "same_sorted = (train_sup['same_team'].fillna(0).astype(int).to_numpy()[idx_ord]) if 'same_team' in train_sup.columns else np.zeros(len(oof_df), dtype=int)\n",
        "\n",
        "# Dual thresholds by same_team\n",
        "best_mcc, thr_opp, thr_same = fast_dual_threshold_mcc(y_sorted, oof_smooth, same_sorted, grid_points=151)\n",
        "if not np.isfinite(best_mcc) or best_mcc < 0:\n",
        "    thrs = np.linspace(0.01, 0.99, 99)\n",
        "    m_list = [matthews_corrcoef(y_sorted, (oof_smooth >= t).astype(int)) for t in thrs]\n",
        "    j = int(np.argmax(m_list)); best_mcc = float(m_list[j]); thr_opp = thr_same = float(thrs[j])\n",
        "print(f'Best OOF MCC (dual thresholds)={best_mcc:.5f} | thr_same={thr_same:.4f}, thr_opp={thr_opp:.4f}')\n",
        "\n",
        "# Inference on test and smoothing\n",
        "Xt = test_feats[feat_cols].astype(float).values\n",
        "dtest = xgb.DMatrix(Xt)\n",
        "pt = np.zeros(len(test_feats), dtype=float)\n",
        "for i, (booster, best_it) in enumerate(models):\n",
        "    t0 = time.time()\n",
        "    pt += booster.predict(dtest, iteration_range=(0, best_it + 1))\n",
        "    print(f' Inference model {i} took {time.time()-t0:.1f}s', flush=True)\n",
        "pt /= max(1, len(models))\n",
        "pred_tmp = test_feats[['game_play','step','p1','p2']].copy()\n",
        "pred_tmp['prob'] = pt\n",
        "pred_tmp = pred_tmp.sort_values(['game_play','p1','p2','step'])\n",
        "grp_t = pred_tmp.groupby(['game_play','p1','p2'], sort=False)\n",
        "pred_tmp['prob_smooth'] = grp_t['prob'].transform(lambda s: s.rolling(3, center=True, min_periods=1).max())\n",
        "\n",
        "# Apply dual thresholds by same_team on test\n",
        "same_flag_test = test_feats['same_team'].astype(int).values if 'same_team' in test_feats.columns else np.zeros(len(test_feats), dtype=int)\n",
        "thr_arr_test = np.where(same_flag_test == 1, thr_same, thr_opp)\n",
        "pred_bin = (pred_tmp['prob_smooth'].values >= thr_arr_test).astype(int)\n",
        "\n",
        "# Build submission\n",
        "cid = (test_feats['game_play'].astype(str) + '_' + test_feats['step'].astype(str) + '_' +\n",
        "       test_feats['p1'].astype(str) + '_' + test_feats['p2'].astype(str))\n",
        "pred_df = pd.DataFrame({'contact_id': cid, 'pred_contact': pred_bin})\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.copy()\n",
        "sub['contact'] = sub['contact_id'].map(pred_df.set_index('contact_id')['pred_contact']).fillna(0).astype(int)\n",
        "sub[['contact_id','contact']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv')\n",
        "print('Done. Total time:', f'{time.time()-start:.1f}s', flush=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version: 2.1.4\nLoading r=3.5 supervised dyn train and dyn test features...\ntrain_sup: (524248, 59) test_feats: (237673, 58)\nUsing 50 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: train 419413 (pos 38437), valid 104835 (pos 10344), spw=9.91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 0 done in 29.7s; best_iteration=3094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train 419331 (pos 39249), valid 104917 (pos 9532), spw=9.68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 1 done in 30.7s; best_iteration=3182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: train 419417 (pos 39309), valid 104831 (pos 9472), spw=9.67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 2 done in 32.0s; best_iteration=3270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: train 419430 (pos 39012), valid 104818 (pos 9769), spw=9.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 3 done in 30.1s; best_iteration=2995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: train 419401 (pos 39117), valid 104847 (pos 9664), spw=9.72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 4 done in 28.3s; best_iteration=2873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_94/4131490327.py:16: RuntimeWarning: invalid value encountered in sqrt\n  den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best OOF MCC (dual thresholds)=0.71744 | thr_same=0.7800, thr_opp=0.7800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 0 took 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 1 took 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 2 took 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 3 took 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 4 took 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv\nDone. Total time: 163.5s\n"
          ]
        }
      ]
    },
    {
      "id": "f0e1435f-5e31-454b-9d0d-39efab53d517",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Player-Ground (G) head: build per-player features, train XGB, smooth+threshold, and merge G preds into submission\n",
        "import time, math, sys, subprocess, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost for G head...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version (G head):', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "t0 = time.time()\n",
        "print('Building per-player features for G...')\n",
        "\n",
        "# 1) Base per-player tracking features (past-only dynamics)\n",
        "trk_cols = ['game_play','step','nfl_player_id','team','position','x_position','y_position','speed','acceleration','direction','orientation']\n",
        "tr_trk = pd.read_csv('train_player_tracking.csv', usecols=trk_cols).copy()\n",
        "te_trk = pd.read_csv('test_player_tracking.csv', usecols=trk_cols).copy()\n",
        "for df in (tr_trk, te_trk):\n",
        "    df['nfl_player_id'] = df['nfl_player_id'].astype(int).astype(str)\n",
        "\n",
        "def circ_diff_deg(a, b):\n",
        "    d = (a - b + 180.0) % 360.0 - 180.0\n",
        "    return np.abs(d)\n",
        "\n",
        "def build_player_dyn(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.sort_values(['game_play','nfl_player_id','step']).copy()\n",
        "    grp = df.groupby(['game_play','nfl_player_id'], sort=False)\n",
        "    # basic deltas\n",
        "    df['d_speed_1'] = grp['speed'].diff(1)\n",
        "    df['d_speed_3'] = df['speed'] - grp['speed'].shift(3)\n",
        "    df['d_accel_1'] = grp['acceleration'].diff(1)\n",
        "    df['jerk'] = grp['acceleration'].diff(1)\n",
        "    # rolling stats\n",
        "    for col in ['speed','acceleration']:\n",
        "        s = grp[col]\n",
        "        df[f'{col}_min_p3'] = s.rolling(3, min_periods=1).min().reset_index(level=[0,1], drop=True)\n",
        "        df[f'{col}_mean_p3'] = s.rolling(3, min_periods=1).mean().reset_index(level=[0,1], drop=True)\n",
        "        df[f'{col}_std_p3'] = s.rolling(3, min_periods=1).std().reset_index(level=[0,1], drop=True)\n",
        "        df[f'{col}_min_p5'] = s.rolling(5, min_periods=1).min().reset_index(level=[0,1], drop=True)\n",
        "        df[f'{col}_mean_p5'] = s.rolling(5, min_periods=1).mean().reset_index(level=[0,1], drop=True)\n",
        "        df[f'{col}_std_p5'] = s.rolling(5, min_periods=1).std().reset_index(level=[0,1], drop=True)\n",
        "    # direction vs orientation\n",
        "    df['dir_orient_diff'] = circ_diff_deg(df['direction'].fillna(0.0), df['orientation'].fillna(0.0))\n",
        "    # boundary context\n",
        "    df['dist_to_sideline'] = np.minimum(df['y_position'], 53.3 - df['y_position'])\n",
        "    df['near_sideline'] = ((df['y_position'] <= 2.0) | (df['y_position'] >= 51.3)).astype(int)\n",
        "    df['near_goal'] = ((df['x_position'] <= 3.0) | (df['x_position'] >= 117.0)).astype(int)\n",
        "    # fill deltas\n",
        "    for c in ['d_speed_1','d_speed_3','d_accel_1','jerk','speed_std_p3','speed_std_p5','acceleration_std_p3','acceleration_std_p5']:\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].fillna(0.0)\n",
        "    return df\n",
        "\n",
        "tr_p = build_player_dyn(tr_trk)\n",
        "te_p = build_player_dyn(te_trk)\n",
        "\n",
        "# 2) Opponent context from r=3.5 pairs\n",
        "tr_pairs = pd.read_parquet('train_pairs_r35.parquet')\n",
        "te_pairs = pd.read_parquet('test_pairs_r35.parquet')\n",
        "\n",
        "def pairs_to_player_ctx(pairs: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Build per-player rows from both sides\n",
        "    a = pairs[['game_play','step','p1','distance']].rename(columns={'p1':'nfl_player_id'})\n",
        "    b = pairs[['game_play','step','p2','distance']].rename(columns={'p2':'nfl_player_id'})\n",
        "    u = pd.concat([a, b], ignore_index=True)\n",
        "    g = u.groupby(['game_play','step','nfl_player_id'], sort=False)\n",
        "    out = g['distance'].agg(min_opp_dist='min').reset_index()\n",
        "    # counts within thresholds: recompute by applying thresholds before groupby for speed\n",
        "    for thr, name in [(1.5,'lt15'), (2.0,'lt20'), (2.5,'lt25')]:\n",
        "        u[name] = (u['distance'] < thr).astype(int)\n",
        "        cnt = u.groupby(['game_play','step','nfl_player_id'], sort=False)[name].sum().rename(f'cnt_opp_{name}')\n",
        "        out = out.merge(cnt.reset_index(), on=['game_play','step','nfl_player_id'], how='left')\n",
        "    return out\n",
        "\n",
        "tr_ctx = pairs_to_player_ctx(tr_pairs)\n",
        "te_ctx = pairs_to_player_ctx(te_pairs)\n",
        "\n",
        "# 3) Helmet per-player aggregates and deltas\n",
        "train_helm = pd.read_csv('train_baseline_helmets.csv')\n",
        "test_helm = pd.read_csv('test_baseline_helmets.csv')\n",
        "train_vmeta = pd.read_csv('train_video_metadata.csv')\n",
        "test_vmeta = pd.read_csv('test_video_metadata.csv')\n",
        "FPS = 59.94\n",
        "def prep_meta(vmeta: pd.DataFrame):\n",
        "    vm = vmeta.copy()\n",
        "    for c in ['start_time','snap_time']:\n",
        "        if not np.issubdtype(vm[c].dtype, np.number):\n",
        "            ts = pd.to_datetime(vm[c], errors='coerce')\n",
        "            vm[c] = (ts - ts.dt.floor('D')).dt.total_seconds().astype(float)\n",
        "    vm['snap_frame'] = ((vm['snap_time'] - vm['start_time']) * FPS).round().astype('Int64')\n",
        "    return vm[['game_play','view','snap_frame']].drop_duplicates()\n",
        "meta_tr = prep_meta(train_vmeta)\n",
        "meta_te = prep_meta(test_vmeta)\n",
        "\n",
        "def helm_player_agg(helm: pd.DataFrame, meta: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = helm[['game_play','view','frame','nfl_player_id','left','top','width','height']].copy()\n",
        "    df = df.dropna(subset=['nfl_player_id'])\n",
        "    df['nfl_player_id'] = df['nfl_player_id'].astype(int).astype(str)\n",
        "    df['area'] = df['width'] * df['height']\n",
        "    df['cx'] = df['left'] + 0.5 * df['width']\n",
        "    df['cy'] = df['top'] + 0.5 * df['height']\n",
        "    df = df.sort_values(['game_play','view','frame','nfl_player_id','area'], ascending=[True,True,True,True,False])\n",
        "    df = df.drop_duplicates(['game_play','view','frame','nfl_player_id'], keep='first')\n",
        "    df = df.merge(meta, on=['game_play','view'], how='left')\n",
        "    df['step'] = ((df['frame'] - df['snap_frame']).astype('float') / 6.0).round().astype('Int64')\n",
        "    df = df.dropna(subset=['step'])\n",
        "    df['step'] = df['step'].astype(int)\n",
        "    # expand \u00b11 to align tolerance\n",
        "    dm1 = df.copy(); dm1['target_step'] = dm1['step'] - 1\n",
        "    d0 = df.copy(); d0['target_step'] = d0['step']\n",
        "    dp1 = df.copy(); dp1['target_step'] = dp1['step'] + 1\n",
        "    d = pd.concat([dm1, d0, dp1], ignore_index=True)\n",
        "    agg = d.groupby(['game_play','target_step','nfl_player_id'], sort=False).agg(\n",
        "        cy_mean=('cy','mean'), h_mean=('height','mean'), cnt=('cx','size')\n",
        "    ).reset_index().rename(columns={'target_step':'step'})\n",
        "    # deltas per player\n",
        "    agg = agg.sort_values(['game_play','nfl_player_id','step'])\n",
        "    g = agg.groupby(['game_play','nfl_player_id'], sort=False)\n",
        "    agg['d_cy_1'] = g['cy_mean'].diff(1).fillna(0.0)\n",
        "    agg['d_h_1'] = g['h_mean'].diff(1).fillna(0.0)\n",
        "    return agg\n",
        "\n",
        "h_tr_p = helm_player_agg(train_helm, meta_tr)\n",
        "h_te_p = helm_player_agg(test_helm, meta_te)\n",
        "\n",
        "# 4) Merge contexts into per-player frames\n",
        "def merge_all(base: pd.DataFrame, ctx: pd.DataFrame, helm: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = base.merge(ctx, on=['game_play','step','nfl_player_id'], how='left')\n",
        "    df = df.merge(helm, on=['game_play','step','nfl_player_id'], how='left')\n",
        "    # fill context NaNs\n",
        "    fill0 = ['min_opp_dist','cnt_opp_lt15','cnt_opp_lt20','cnt_opp_lt25','cy_mean','h_mean','d_cy_1','d_h_1']\n",
        "    for c in fill0:\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].fillna(0.0)\n",
        "    return df\n",
        "\n",
        "tr_feat_p = merge_all(tr_p, tr_ctx, h_tr_p)\n",
        "te_feat_p = merge_all(te_p, te_ctx, h_te_p)\n",
        "print('Per-player train/test feature shapes:', tr_feat_p.shape, te_feat_p.shape)\n",
        "\n",
        "# 5) Supervision for G: inner-join to labels where one pid is G, with \u00b11 expansion within supervised only\n",
        "labels = pd.read_csv('train_labels.csv', usecols=['contact_id','game_play','step','nfl_player_id_1','nfl_player_id_2','contact'])\n",
        "labels['pid1'] = labels['nfl_player_id_1'].astype(str)\n",
        "labels['pid2'] = labels['nfl_player_id_2'].astype(str)\n",
        "mask_g = (labels['pid1'] == 'G') | (labels['pid2'] == 'G')\n",
        "g_labels = labels.loc[mask_g, ['game_play','step','pid1','pid2','contact']].copy()\n",
        "g_labels['player'] = np.where(g_labels['pid1'] == 'G', g_labels['pid2'], g_labels['pid1'])\n",
        "g_labels = g_labels[['game_play','step','player','contact']]\n",
        "sup_g = g_labels.merge(tr_feat_p.rename(columns={'nfl_player_id':'player'}), on=['game_play','step','player'], how='inner')\n",
        "print('G supervised inner shape:', sup_g.shape, 'pos rate:', sup_g['contact'].mean())\n",
        "pos = sup_g.loc[sup_g['contact'] == 1, ['game_play','step','player']]\n",
        "pos_m1 = pos.copy(); pos_m1['step'] = pos_m1['step'] - 1\n",
        "pos_p1 = pos.copy(); pos_p1['step'] = pos_p1['step'] + 1\n",
        "pos_exp = pd.concat([pos_m1, pos_p1], ignore_index=True).drop_duplicates()\n",
        "pos_exp['flag_pos_exp'] = 1\n",
        "sup_g = sup_g.merge(pos_exp, on=['game_play','step','player'], how='left')\n",
        "sup_g.loc[sup_g['flag_pos_exp'] == 1, 'contact'] = 1\n",
        "sup_g.drop(columns=['flag_pos_exp'], inplace=True)\n",
        "print('G after \u00b11 expansion pos rate:', sup_g['contact'].mean())\n",
        "\n",
        "# 6) Train small XGB with GroupKFold by game_play\n",
        "drop_cols = {'contact','game_play','step','player','team','position','nfl_player_id'}\n",
        "feat_cols = [c for c in sup_g.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(sup_g[c])]\n",
        "print('G feature count:', len(feat_cols))\n",
        "\n",
        "X_all = sup_g[feat_cols].astype(float).values\n",
        "y_all = sup_g['contact'].astype(int).values\n",
        "groups = sup_g['game_play'].values\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "oof = np.full(len(sup_g), np.nan, float)\n",
        "models = []\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "    t1 = time.time()\n",
        "    X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "    X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "    neg = (y_tr == 0).sum(); posc = (y_tr == 1).sum()\n",
        "    spw = max(1.0, neg / max(1, posc))\n",
        "    print(f'G Fold {fold}: train {len(tr_idx)} (pos {posc}), valid {len(va_idx)} (pos {(y_va==1).sum()}), spw={spw:.2f}', flush=True)\n",
        "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
        "    dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "    params = {\n",
        "        'tree_method': 'hist', 'device': 'cuda', 'max_depth': 6, 'eta': 0.05,\n",
        "        'subsample': 0.9, 'colsample_bytree': 0.8, 'min_child_weight': 10,\n",
        "        'lambda': 1.5, 'alpha': 0.0, 'objective': 'binary:logistic', 'eval_metric': 'logloss',\n",
        "        'scale_pos_weight': float(spw), 'seed': 42 + fold\n",
        "    }\n",
        "    booster = xgb.train(params, dtrain, num_boost_round=2000, evals=[(dtrain,'train'),(dvalid,'valid')],\n",
        "                        early_stopping_rounds=100, verbose_eval=False)\n",
        "    best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "    oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "    models.append((booster, best_it))\n",
        "    print(f' G Fold {fold} done in {time.time()-t1:.1f}s; best_it={best_it}', flush=True)\n",
        "\n",
        "# 7) Smooth OOF with centered rolling-max window=5 per (gp,player)\n",
        "oof_df = sup_g[['game_play','player','step']].copy()\n",
        "oof_df['oof'] = oof\n",
        "oof_df = oof_df.sort_values(['game_play','player','step'])\n",
        "grp_o = oof_df.groupby(['game_play','player'], sort=False)\n",
        "oof_df['oof_smooth'] = grp_o['oof'].transform(lambda s: s.rolling(5, center=True, min_periods=1).max())\n",
        "oof_smooth = oof_df['oof_smooth'].values\n",
        "y_sorted = sup_g.loc[oof_df.index, 'contact'].astype(int).values\n",
        "\n",
        "# Hysteresis: require min_duration >= 2 via rolling sum >=2 within window=3 centered\n",
        "def apply_min_dur(bin_arr, gp, pl):\n",
        "    df = pd.DataFrame({'gp': gp, 'pl': pl, 'b': bin_arr})\n",
        "    df = df.groupby(['gp','pl'], sort=False)['b'].apply(lambda s: (s.rolling(3, center=True, min_periods=1).sum() >= 2).astype(int))\n",
        "    return df.values\n",
        "\n",
        "# threshold sweep for G\n",
        "best_thr, best_mcc = 0.58, -1.0\n",
        "thr_grid = np.linspace(0.4, 0.8, 41)\n",
        "gp_arr = oof_df['game_play'].values\n",
        "pl_arr = oof_df['player'].values\n",
        "for thr in thr_grid:\n",
        "    pred0 = (oof_smooth >= thr).astype(int)\n",
        "    pred = apply_min_dur(pred0, gp_arr, pl_arr)\n",
        "    m = matthews_corrcoef(y_sorted, pred)\n",
        "    if m > best_mcc:\n",
        "        best_mcc, best_thr = float(m), float(thr)\n",
        "print(f'G OOF MCC={best_mcc:.5f} at thr={best_thr:.2f} (after smooth+min_duration)')\n",
        "\n",
        "# 8) Inference on test\n",
        "Xt = te_feat_p[feat_cols].astype(float).values\n",
        "dtest = xgb.DMatrix(Xt)\n",
        "pt = np.zeros(len(te_feat_p), dtype=float)\n",
        "for i, (booster, best_it) in enumerate(models):\n",
        "    t1 = time.time()\n",
        "    pt += booster.predict(dtest, iteration_range=(0, best_it + 1))\n",
        "    print(f' G Inference model {i} took {time.time()-t1:.1f}s')\n",
        "pt /= max(1, len(models))\n",
        "pred_tmp = te_feat_p[['game_play','step','nfl_player_id']].rename(columns={'nfl_player_id':'player'}).copy()\n",
        "pred_tmp['prob'] = pt\n",
        "pred_tmp = pred_tmp.sort_values(['game_play','player','step'])\n",
        "grp_t = pred_tmp.groupby(['game_play','player'], sort=False)\n",
        "pred_tmp['prob_smooth'] = grp_t['prob'].transform(lambda s: s.rolling(5, center=True, min_periods=1).max())\n",
        "bin0 = (pred_tmp['prob_smooth'].values >= best_thr).astype(int)\n",
        "bin1 = apply_min_dur(bin0, pred_tmp['game_play'].values, pred_tmp['player'].values)\n",
        "pred_tmp['pred_bin'] = bin1.astype(int)\n",
        "\n",
        "# 9) Build G contact_id and overwrite in submission\n",
        "g_cid = (pred_tmp['game_play'].astype(str) + '_' + pred_tmp['step'].astype(str) + '_' + 'G' + '_' + pred_tmp['player'].astype(str))\n",
        "g_pred_df = pd.DataFrame({'contact_id': g_cid, 'contact': pred_tmp['pred_bin'].astype(int)})\n",
        "\n",
        "sub = pd.read_csv('submission.csv')\n",
        "before_ones = int(sub['contact'].sum())\n",
        "sub = sub.drop(columns=['contact']).merge(g_pred_df, on='contact_id', how='left').merge(pd.read_csv('submission.csv'), on='contact_id', how='left', suffixes=('_g','_pp'))\n",
        "sub['contact'] = sub['contact_g'].fillna(sub['contact_pp']).astype(int)\n",
        "sub = sub[['contact_id','contact']]\n",
        "after_ones = int(sub['contact'].sum())\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'Overwrote G rows in submission. ones before={before_ones}, after={after_ones}')\n",
        "print('G head done in {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version (G head): 2.1.4\nBuilding per-player features for G...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-player train/test feature shapes: (1225299, 40) (127754, 40)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G supervised inner shape: (370351, 41) pos rate: 0.041106949893479426\nG after \u00b11 expansion pos rate: 0.04376658899260431\nG feature count: 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G Fold 0: train 296428 (pos 12821), valid 73923 (pos 3388), spw=22.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " G Fold 0 done in 9.2s; best_it=1470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G Fold 1: train 296519 (pos 13057), valid 73832 (pos 3152), spw=21.71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " G Fold 1 done in 8.5s; best_it=1371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G Fold 2: train 296365 (pos 13719), valid 73986 (pos 2490), spw=20.60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " G Fold 2 done in 8.7s; best_it=1434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G Fold 3: train 296365 (pos 12501), valid 73986 (pos 3708), spw=22.71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " G Fold 3 done in 7.5s; best_it=1193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G Fold 4: train 295727 (pos 12738), valid 74624 (pos 3471), spw=22.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " G Fold 4 done in 10.1s; best_it=1629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G OOF MCC=0.53119 at thr=0.73 (after smooth+min_duration)\n G Inference model 0 took 0.0s\n G Inference model 1 took 0.0s\n G Inference model 2 took 0.0s\n G Inference model 3 took 0.0s\n G Inference model 4 took 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwrote G rows in submission. ones before=8903, after=8903\nG head done in 137.7s\n"
          ]
        }
      ]
    },
    {
      "id": "55057d79-3ea0-4032-8b60-e50e67c6c0c6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnose G contact_id alignment and coverage\n",
        "import pandas as pd, numpy as np\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "tokens = ss['contact_id'].str.split('_', n=4, expand=True)\n",
        "tokens.columns = ['g1','g2','step','a','b']\n",
        "g_first_mask = tokens['a'] == 'G'\n",
        "g_second_mask = tokens['b'] == 'G'\n",
        "print('Sample G-first rows:', int(g_first_mask.sum()))\n",
        "print('Sample G-second rows:', int(g_second_mask.sum()))\n",
        "\n",
        "# Build our predicted G ids in both orientations from te_feat_p (available from Cell 11)\n",
        "assert 'te_feat_p' in globals(), 'te_feat_p missing; re-run Cell 11 first.'\n",
        "pred_base = te_feat_p[['game_play','step','nfl_player_id']].copy()\n",
        "pred_base['nfl_player_id'] = pred_base['nfl_player_id'].astype(str)\n",
        "cid_g_first = pred_base['game_play'].astype(str) + '_' + pred_base['step'].astype(str) + '_G_' + pred_base['nfl_player_id']\n",
        "cid_g_second = pred_base['game_play'].astype(str) + '_' + pred_base['step'].astype(str) + '_' + pred_base['nfl_player_id'] + '_G'\n",
        "our_g_first = set(cid_g_first.values)\n",
        "our_g_second = set(cid_g_second.values)\n",
        "\n",
        "ss_ids = set(ss['contact_id'].values)\n",
        "inter_first = len(our_g_first & ss_ids)\n",
        "inter_second = len(our_g_second & ss_ids)\n",
        "print('Intersect counts -> G-first:', inter_first, '| G-second:', inter_second)\n",
        "print('Total our per-player rows:', len(pred_base), 'Total ss rows:', len(ss))\n",
        "\n",
        "# Show a few examples of existing G ids in sample\n",
        "print('Sample G-first example:', ss.loc[g_first_mask, 'contact_id'].head(3).tolist())\n",
        "print('Sample G-second example:', ss.loc[g_second_mask, 'contact_id'].head(3).tolist())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample G-first rows: 0\nSample G-second rows: 40282\nIntersect counts -> G-first: 0 | G-second: 40282\nTotal our per-player rows: 127754 Total ss rows: 463243\nSample G-first example: []\nSample G-second example: ['58187_001341_0_47795_G', '58187_001341_0_41300_G', '58187_001341_0_52650_G']\n"
          ]
        }
      ]
    },
    {
      "id": "d9689677-2704-419f-8192-9a403a95bb79",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fix G contact_id orientation to '<player>_G' and overwrite submission accordingly\n",
        "import pandas as pd\n",
        "assert 'pred_tmp' in globals(), 'pred_tmp missing; re-run Cell 11 first to compute G per-player predictions.'\n",
        "\n",
        "# Build G-second contact_ids: {game_play}_{step}_{player}_G\n",
        "g_cid_second = (pred_tmp['game_play'].astype(str) + '_' + pred_tmp['step'].astype(str) + '_' + pred_tmp['player'].astype(str) + '_G')\n",
        "g_pred_second = pd.DataFrame({'contact_id': g_cid_second, 'contact_g': pred_tmp['pred_bin'].astype(int).values})\n",
        "\n",
        "sub = pd.read_csv('submission.csv')\n",
        "before_ones = int(sub['contact'].sum())\n",
        "sub = sub.merge(g_pred_second, on='contact_id', how='left')\n",
        "sub['contact'] = sub['contact_g'].fillna(sub['contact']).astype(int)\n",
        "sub = sub[['contact_id','contact']]\n",
        "after_ones = int(sub['contact'].sum())\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'Applied G-second overwrite. ones before={before_ones}, after={after_ones}, delta={after_ones-before_ones}')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied G-second overwrite. ones before=8903, after=9121, delta=218\n"
          ]
        }
      ]
    },
    {
      "id": "1ceaad45-68d3-428d-80b1-8f9b73c3a26b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Multi-seed bagging: PP (r=3.5 dyn) + G head; rebuild submission with averaged probs and calibrated thresholds\n",
        "import time, numpy as np, pandas as pd, sys, subprocess\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version (bagging):', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def mcc_from_counts(tp, tn, fp, fn):\n",
        "    num = tp * tn - fp * fn\n",
        "    den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
        "    den = np.where(den == 0, 1.0, den)\n",
        "    return num / den\n",
        "\n",
        "def fast_dual_threshold_mcc(y_true, prob, same_flag, grid_points=151):\n",
        "    res = {}\n",
        "    for cohort in (0, 1):\n",
        "        mask = (same_flag == cohort)\n",
        "        y_c = y_true[mask].astype(int); p_c = prob[mask].astype(float)\n",
        "        n = len(y_c)\n",
        "        if n == 0:\n",
        "            res[cohort] = {'k_grid': np.array([0], int), 'tp': np.array([0.0]), 'fp': np.array([0.0]), 'tn': np.array([0.0]), 'fn': np.array([0.0]), 'thr_vals': np.array([1.0])}\n",
        "            continue\n",
        "        order = np.argsort(-p_c)\n",
        "        y_sorted = y_c[order]; p_sorted = p_c[order]\n",
        "        cum_pos = np.concatenate([[0], np.cumsum(y_sorted)])\n",
        "        k_grid = np.unique(np.linspace(0, n, num=min(grid_points, n + 1), dtype=int))\n",
        "        tp = cum_pos[k_grid]; fp = k_grid - tp\n",
        "        P = y_sorted.sum(); N = n - P\n",
        "        fn = P - tp; tn = N - fp\n",
        "        thr_vals = np.where(k_grid == 0, 1.0 + 1e-6, p_sorted[np.maximum(0, k_grid - 1)])\n",
        "        res[cohort] = {'k_grid': k_grid, 'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn, 'thr_vals': thr_vals}\n",
        "    tp0, fp0, tn0, fn0, thr0 = res[0]['tp'], res[0]['fp'], res[0]['tn'], res[0]['fn'], res[0]['thr_vals']\n",
        "    tp1, fp1, tn1, fn1, thr1 = res[1]['tp'], res[1]['fp'], res[1]['tn'], res[1]['fn'], res[1]['thr_vals']\n",
        "    best = (-1.0, 0.5, 0.5)\n",
        "    for i in range(len(thr0)):\n",
        "        tp_sum = tp0[i] + tp1; fp_sum = fp0[i] + fp1; tn_sum = tn0[i] + tn1; fn_sum = fn0[i] + fn1\n",
        "        m_arr = mcc_from_counts(tp_sum, tn_sum, fp_sum, fn_sum)\n",
        "        j = int(np.argmax(m_arr)); m = float(m_arr[j])\n",
        "        if m > best[0]:\n",
        "            best = (m, float(thr0[i]), float(thr1[j]))\n",
        "    return best\n",
        "\n",
        "t_all = time.time()\n",
        "# ====================\n",
        "# 1) PP multi-seed bagging (r=3.5 dyn features)\n",
        "# ====================\n",
        "print('PP bagging: loading artifacts...')\n",
        "train_sup = pd.read_parquet('train_supervised_w5_helm_dyn_r35.parquet')\n",
        "test_feats = pd.read_parquet('test_pairs_w5_helm_dyn_r35.parquet')\n",
        "folds_df = pd.read_csv('folds_game_play.csv')\n",
        "train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "for df in (train_sup, test_feats):\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "drop_cols = {'contact','game_play','step','p1','p2','team1','team2','pos1','pos2','fold'}\n",
        "feat_cols = [c for c in train_sup.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_sup[c])]\n",
        "X_all = train_sup[feat_cols].astype(float).values\n",
        "y_all = train_sup['contact'].astype(int).values\n",
        "groups = train_sup['game_play'].values\n",
        "same_all = (train_sup['same_team'].fillna(0).astype(int).values) if 'same_team' in train_sup.columns else np.zeros(len(train_sup), int)\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "seeds = [42, 1337, 2025]\n",
        "oof_list = []\n",
        "test_preds_list = []\n",
        "for s in seeds:\n",
        "    print(f' PP seed {s} ...')\n",
        "    oof = np.full(len(train_sup), np.nan, float)\n",
        "    models = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "        t0 = time.time()\n",
        "        X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "        X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "        neg = (y_tr == 0).sum(); posc = (y_tr == 1).sum()\n",
        "        spw = max(1.0, neg / max(1, posc))\n",
        "        dtrain = xgb.DMatrix(X_tr, label=y_tr); dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "        params = {'tree_method': 'hist','device': 'cuda','max_depth': 7,'eta': 0.05,'subsample': 0.9,'colsample_bytree': 0.8,\n",
        "                  'min_child_weight': 10,'lambda': 1.5,'alpha': 0.1,'gamma': 0.1,'objective': 'binary:logistic','eval_metric': 'logloss',\n",
        "                  'scale_pos_weight': float(spw),'seed': int(s + fold)}\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=3500, evals=[(dtrain,'train'),(dvalid,'valid')], early_stopping_rounds=200, verbose_eval=False)\n",
        "        best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "        oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "        models.append((booster, best_it))\n",
        "        print(f'  PP seed {s} fold {fold} done in {time.time()-t0:.1f}s; best_it={best_it}', flush=True)\n",
        "    # Smooth OOF per (gp,p1,p2)\n",
        "    oof_df = train_sup[['game_play','p1','p2','step']].copy()\n",
        "    oof_df['oof'] = oof\n",
        "    oof_df = oof_df.sort_values(['game_play','p1','p2','step'])\n",
        "    grp_s = oof_df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    oof_df['oof_smooth'] = grp_s['oof'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    oof_list.append((oof_df.index.values, oof_df['oof_smooth'].values))\n",
        "    # Test preds for this seed\n",
        "    Xt = test_feats[feat_cols].astype(float).values\n",
        "    dtest = xgb.DMatrix(Xt)\n",
        "    pt = np.zeros(len(test_feats), float)\n",
        "    for i, (booster, best_it) in enumerate(models):\n",
        "        t1 = time.time(); pt += booster.predict(dtest, iteration_range=(0, best_it + 1));\n",
        "        print(f'   PP seed {s} inference model {i} {time.time()-t1:.1f}s')\n",
        "    pt /= max(1, len(models))\n",
        "    pred_tmp = test_feats[['game_play','step','p1','p2']].copy(); pred_tmp['prob'] = pt\n",
        "    pred_tmp = pred_tmp.sort_values(['game_play','p1','p2','step'])\n",
        "    grp_t = pred_tmp.groupby(['game_play','p1','p2'], sort=False)\n",
        "    pred_tmp['prob_smooth'] = grp_t['prob'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    test_preds_list.append(pred_tmp['prob_smooth'].values)\n",
        "\n",
        "# Align and average OOF across seeds (indices should be identical order by construction)\n",
        "idx_ord = oof_list[0][0]\n",
        "oof_avg = np.mean([x[1] for x in oof_list], axis=0)\n",
        "y_sorted = train_sup['contact'].astype(int).to_numpy()[idx_ord]\n",
        "same_sorted = same_all[idx_ord]\n",
        "best_mcc, thr_opp, thr_same = fast_dual_threshold_mcc(y_sorted, oof_avg, same_sorted, grid_points=151)\n",
        "print(f'PP bagged OOF MCC={best_mcc:.5f} | thr_same={thr_same:.4f}, thr_opp={thr_opp:.4f}')\n",
        "\n",
        "# Average test probs across seeds and threshold\n",
        "pt_bag = np.mean(np.vstack(test_preds_list), axis=0)\n",
        "pred_tmp_final = test_feats[['game_play','step','p1','p2']].copy()\n",
        "pred_tmp_final = pred_tmp_final.sort_values(['game_play','p1','p2','step'])\n",
        "same_flag_test = test_feats['same_team'].astype(int).values if 'same_team' in test_feats.columns else np.zeros(len(test_feats), int)\n",
        "thr_arr_test = np.where(same_flag_test == 1, thr_same, thr_opp)\n",
        "pred_bin_pp = (pt_bag >= thr_arr_test).astype(int)\n",
        "cid_pp = (test_feats['game_play'].astype(str) + '_' + test_feats['step'].astype(str) + '_' + test_feats['p1'].astype(str) + '_' + test_feats['p2'].astype(str))\n",
        "pred_df_pp = pd.DataFrame({'contact_id': cid_pp, 'contact_pp': pred_bin_pp})\n",
        "\n",
        "# ====================\n",
        "# 2) G head multi-seed bagging (reuse sup_g/te_feat_p/feat_cols from Cell 11 if present; else rebuild minimal)\n",
        "# ====================\n",
        "print('G bagging...')\n",
        "if 'sup_g' not in globals() or 'te_feat_p' not in globals() or 'feat_cols' not in globals():\n",
        "    raise RuntimeError('G features not in memory. Re-run Cell 11 first.')\n",
        "labels_g = sup_g[['game_play','player','step','contact']].copy()\n",
        "feat_cols_g = [c for c in sup_g.columns if c not in {'contact','game_play','step','player','team','position','nfl_player_id'} and pd.api.types.is_numeric_dtype(sup_g[c])]\n",
        "Xg = sup_g[feat_cols_g].astype(float).values; yg = sup_g['contact'].astype(int).values; groups_g = sup_g['game_play'].values\n",
        "gkf_g = GroupKFold(n_splits=5)\n",
        "oof_list_g = []; test_list_g = []\n",
        "for s in seeds:\n",
        "    print(f' G seed {s} ...')\n",
        "    oofg = np.full(len(sup_g), np.nan, float)\n",
        "    models_g = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(gkf_g.split(Xg, yg, groups=groups_g)):\n",
        "        t0 = time.time()\n",
        "        X_tr, y_tr = Xg[tr_idx], yg[tr_idx]\n",
        "        X_va, y_va = Xg[va_idx], yg[va_idx]\n",
        "        neg = (y_tr == 0).sum(); posc = (y_tr == 1).sum()\n",
        "        spw = max(1.0, neg / max(1, posc))\n",
        "        dtrain = xgb.DMatrix(X_tr, label=y_tr); dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "        params = {'tree_method': 'hist','device': 'cuda','max_depth': 6,'eta': 0.05,'subsample': 0.9,'colsample_bytree': 0.8,\n",
        "                  'min_child_weight': 10,'lambda': 1.5,'alpha': 0.0,'objective': 'binary:logistic','eval_metric': 'logloss','scale_pos_weight': float(spw),'seed': int(s + fold)}\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=2000, evals=[(dtrain,'train'),(dvalid,'valid')], early_stopping_rounds=100, verbose_eval=False)\n",
        "        best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "        oofg[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "        models_g.append((booster, best_it))\n",
        "        print(f'   G seed {s} fold {fold} {time.time()-t0:.1f}s; best_it={best_it}')\n",
        "    # Smooth OOF with centered rolling-max window=5\n",
        "    oof_df_g = sup_g[['game_play','player','step']].copy()\n",
        "    oof_df_g['oof'] = oofg\n",
        "    oof_df_g = oof_df_g.sort_values(['game_play','player','step'])\n",
        "    grp_go = oof_df_g.groupby(['game_play','player'], sort=False)\n",
        "    oof_df_g['oof_smooth'] = grp_go['oof'].transform(lambda s_: s_.rolling(5, center=True, min_periods=1).max())\n",
        "    oof_list_g.append((oof_df_g.index.values, oof_df_g['oof_smooth'].values))\n",
        "    # Test\n",
        "    Xt_g = te_feat_p[feat_cols_g].astype(float).values\n",
        "    dtest_g = xgb.DMatrix(Xt_g)\n",
        "    ptg = np.zeros(len(te_feat_p), float)\n",
        "    for i, (booster, best_it) in enumerate(models_g):\n",
        "        t1 = time.time(); ptg += booster.predict(dtest_g, iteration_range=(0, best_it + 1));\n",
        "        print(f'    G seed {s} infer model {i} {time.time()-t1:.1f}s')\n",
        "    ptg /= max(1, len(models_g))\n",
        "    pred_g_tmp = te_feat_p[['game_play','step','nfl_player_id']].rename(columns={'nfl_player_id':'player'}).copy()\n",
        "    pred_g_tmp['prob'] = ptg\n",
        "    pred_g_tmp = pred_g_tmp.sort_values(['game_play','player','step'])\n",
        "    grp_gt = pred_g_tmp.groupby(['game_play','player'], sort=False)\n",
        "    pred_g_tmp['prob_smooth'] = grp_gt['prob'].transform(lambda s_: s_.rolling(5, center=True, min_periods=1).max())\n",
        "    test_list_g.append(pred_g_tmp['prob_smooth'].values)\n",
        "\n",
        "# Align and average OOF for G\n",
        "idx_ord_g = oof_list_g[0][0]\n",
        "oof_avg_g = np.mean([x[1] for x in oof_list_g], axis=0)\n",
        "yg_sorted = sup_g.loc[idx_ord_g, 'contact'].astype(int).values\n",
        "gp_arr = sup_g.loc[idx_ord_g, 'game_play'].values\n",
        "pl_arr = sup_g.loc[idx_ord_g, 'player'].values\n",
        "def apply_min_dur(bin_arr, gp, pl):\n",
        "    df = pd.DataFrame({'gp': gp, 'pl': pl, 'b': bin_arr})\n",
        "    df = df.groupby(['gp','pl'], sort=False)['b'].apply(lambda s: (s.rolling(3, center=True, min_periods=1).sum() >= 2).astype(int))\n",
        "    return df.values\n",
        "best_thr_g, best_mcc_g = 0.60, -1.0\n",
        "thr_grid = np.linspace(0.4, 0.8, 41)\n",
        "for thr in thr_grid:\n",
        "    pred0 = (oof_avg_g >= thr).astype(int)\n",
        "    pred = apply_min_dur(pred0, gp_arr, pl_arr)\n",
        "    m = matthews_corrcoef(yg_sorted, pred)\n",
        "    if m > best_mcc_g:\n",
        "        best_mcc_g, best_thr_g = float(m), float(thr)\n",
        "print(f'G bagged OOF MCC={best_mcc_g:.5f} at thr={best_thr_g:.2f}')\n",
        "\n",
        "# Average test probs and threshold + min_duration for G\n",
        "ptg_bag = np.mean(np.vstack(test_list_g), axis=0)\n",
        "pred_g_final = te_feat_p[['game_play','step','nfl_player_id']].rename(columns={'nfl_player_id':'player'}).copy()\n",
        "pred_g_final = pred_g_final.sort_values(['game_play','player','step'])\n",
        "pred_g_final['prob_bag'] = ptg_bag\n",
        "bin0 = (pred_g_final['prob_bag'].values >= best_thr_g).astype(int)\n",
        "bin1 = apply_min_dur(bin0, pred_g_final['game_play'].values, pred_g_final['player'].values)\n",
        "pred_g_final['pred_bin'] = bin1.astype(int)\n",
        "g_cid_second = (pred_g_final['game_play'].astype(str) + '_' + pred_g_final['step'].astype(str) + '_' + pred_g_final['player'].astype(str) + '_G')\n",
        "g_pred_second = pd.DataFrame({'contact_id': g_cid_second, 'contact_g': pred_g_final['pred_bin'].astype(int).values})\n",
        "\n",
        "# ====================\n",
        "# 3) Build submission: PP then overwrite G-second\n",
        "# ====================\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.merge(pred_df_pp, on='contact_id', how='left')\n",
        "sub['contact'] = sub['contact_pp'].fillna(0).astype(int)\n",
        "sub = sub.drop(columns=['contact_pp'])\n",
        "before_ones = int(sub['contact'].sum())\n",
        "sub = sub.merge(g_pred_second, on='contact_id', how='left')\n",
        "sub['contact'] = sub['contact_g'].fillna(sub['contact']).astype(int)\n",
        "sub = sub[['contact_id','contact']]\n",
        "after_ones = int(sub['contact'].sum())\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'Final submission saved. PP ones={before_ones}, after G overwrite={after_ones}. Total time {time.time()-t_all:.1f}s')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version (bagging): 2.1.4\nPP bagging: loading artifacts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP seed 42 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PP seed 42 fold 0 done in 29.8s; best_it=3094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PP seed 42 fold 1 done in 30.6s; best_it=3182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PP seed 42 fold 2 done in 32.0s; best_it=3270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PP seed 42 fold 3 done in 30.1s; best_it=2995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PP seed 42 fold 4 done in 27.2s; best_it=2873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PP seed 42 inference model 0 0.1s\n   PP seed 42 inference model 1 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PP seed 42 inference model 2 0.1s\n   PP seed 42 inference model 3 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PP seed 42 inference model 4 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP seed 1337 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PP seed 1337 fold 0 done in 30.9s; best_it=3211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PP seed 1337 fold 1 done in 31.6s; best_it=3291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PP seed 1337 fold 2 done in 32.2s; best_it=3408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PP seed 1337 fold 3 done in 29.5s; best_it=2955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PP seed 1337 fold 4 done in 26.9s; best_it=2747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PP seed 1337 inference model 0 0.1s\n   PP seed 1337 inference model 1 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PP seed 1337 inference model 2 0.1s\n   PP seed 1337 inference model 3 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PP seed 1337 inference model 4 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP seed 2025 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PP seed 2025 fold 0 done in 28.8s; best_it=3011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PP seed 2025 fold 1 done in 31.7s; best_it=3338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PP seed 2025 fold 2 done in 31.3s; best_it=3189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PP seed 2025 fold 3 done in 29.1s; best_it=2925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PP seed 2025 fold 4 done in 27.5s; best_it=2898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PP seed 2025 inference model 0 0.1s\n   PP seed 2025 inference model 1 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PP seed 2025 inference model 2 0.1s\n   PP seed 2025 inference model 3 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PP seed 2025 inference model 4 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_94/1094044630.py:16: RuntimeWarning: invalid value encountered in sqrt\n  den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP bagged OOF MCC=-1.00000 | thr_same=0.5000, thr_opp=0.5000\nG bagging...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " G seed 42 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   G seed 42 fold 0 9.2s; best_it=1470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   G seed 42 fold 1 8.5s; best_it=1371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   G seed 42 fold 2 8.7s; best_it=1434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   G seed 42 fold 3 7.5s; best_it=1193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   G seed 42 fold 4 9.7s; best_it=1629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    G seed 42 infer model 0 0.0s\n    G seed 42 infer model 1 0.0s\n    G seed 42 infer model 2 0.0s\n    G seed 42 infer model 3 0.0s\n    G seed 42 infer model 4 0.0s\n G seed 1337 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   G seed 1337 fold 0 9.6s; best_it=1545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   G seed 1337 fold 1 9.3s; best_it=1518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   G seed 1337 fold 2 8.8s; best_it=1458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   G seed 1337 fold 3 8.5s; best_it=1373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   G seed 1337 fold 4 9.2s; best_it=1481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    G seed 1337 infer model 0 0.0s\n    G seed 1337 infer model 1 0.0s\n    G seed 1337 infer model 2 0.0s\n    G seed 1337 infer model 3 0.0s\n    G seed 1337 infer model 4 0.0s\n G seed 2025 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   G seed 2025 fold 0 8.8s; best_it=1413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   G seed 2025 fold 1 7.9s; best_it=1264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   G seed 2025 fold 2 9.4s; best_it=1566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   G seed 2025 fold 3 8.2s; best_it=1308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   G seed 2025 fold 4 9.5s; best_it=1537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    G seed 2025 infer model 0 0.0s\n    G seed 2025 infer model 1 0.0s\n    G seed 2025 infer model 2 0.0s\n    G seed 2025 infer model 3 0.0s\n    G seed 2025 infer model 4 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G bagged OOF MCC=0.53627 at thr=0.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final submission saved. PP ones=8829, after G overwrite=10572. Total time 666.3s\n"
          ]
        }
      ]
    },
    {
      "id": "da873cb7-56e2-41fc-bd9c-5ce8327d1a56",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PP bagging with strict key alignment; robust thresholds; read G preds from prior submission; rebuild submission\n",
        "import time, numpy as np, pandas as pd, sys, subprocess\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version (pp-align):', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def fast_dual_threshold_mcc(y_true, prob, same_flag, grid_points=256):\n",
        "    import numpy as np\n",
        "    y = np.asarray(y_true, dtype=np.int64)\n",
        "    p = np.asarray(prob, dtype=np.float64)\n",
        "    s = np.asarray(same_flag, dtype=np.int8)\n",
        "    mask = np.isfinite(y) & np.isfinite(p) & np.isfinite(s)\n",
        "    y, p, s = y[mask], p[mask], s[mask]\n",
        "\n",
        "    def cohort_counts(yc, pc, G):\n",
        "        n = yc.size\n",
        "        if n == 0:\n",
        "            return dict(tp=np.array([0], np.int64), fp=np.array([0], np.int64),\n",
        "                        tn=np.array([0], np.int64), fn=np.array([0], np.int64),\n",
        "                        thr=np.array([1.0], np.float64))\n",
        "        order = np.argsort(-pc, kind='mergesort')\n",
        "        ys, ps = yc[order], pc[order]\n",
        "        P = int(ys.sum()); N = n - P\n",
        "        step = max(1, n // max(1, (G - 1)))\n",
        "        k = np.arange(0, n + 1, step, dtype=np.int64)\n",
        "        if k[-1] != n: k = np.append(k, n)\n",
        "        cum = np.concatenate(([0], np.cumsum(ys, dtype=np.int64)))\n",
        "        tp = cum[k]; fp = k - tp; fn = P - tp; tn = N - fp\n",
        "        thr = np.where(k == 0, 1.0 + 1e-6, ps[k - 1])\n",
        "        return dict(tp=tp, fp=fp, tn=tn, fn=fn, thr=thr)\n",
        "\n",
        "    a = cohort_counts(y[s == 0], p[s == 0], grid_points)\n",
        "    b = cohort_counts(y[s == 1], p[s == 1], grid_points)\n",
        "\n",
        "    tp = a['tp'][:, None] + b['tp'][None, :]\n",
        "    fp = a['fp'][:, None] + b['fp'][None, :]\n",
        "    tn = a['tn'][:, None] + b['tn'][None, :]\n",
        "    fn = a['fn'][:, None] + b['fn'][None, :]\n",
        "\n",
        "    with np.errstate(invalid='ignore', divide='ignore'):\n",
        "        num = tp * tn - fp * fn\n",
        "        den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
        "        den = np.where(den > 0, np.sqrt(den), np.nan)\n",
        "        mcc = num / den\n",
        "\n",
        "    if not np.isfinite(mcc).any():\n",
        "        return -1.0, 0.5, 0.5\n",
        "    i, j = np.unravel_index(np.nanargmax(mcc), mcc.shape)\n",
        "    return float(mcc[i, j]), float(a['thr'][i]), float(b['thr'][j])\n",
        "\n",
        "t0 = time.time()\n",
        "print('Loading r=3.5 dyn artifacts...')\n",
        "train_sup = pd.read_parquet('train_supervised_w5_helm_dyn_r35.parquet')\n",
        "test_feats = pd.read_parquet('test_pairs_w5_helm_dyn_r35.parquet')\n",
        "folds_df = pd.read_csv('folds_game_play.csv')\n",
        "train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "for df in (train_sup, test_feats):\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "\n",
        "drop_cols = {'contact','game_play','step','p1','p2','team1','team2','pos1','pos2','fold'}\n",
        "feat_cols = [c for c in train_sup.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_sup[c])]\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train_sup['game_play'].values\n",
        "y_all = train_sup['contact'].astype(int).values\n",
        "same_all = train_sup['same_team'].fillna(0).astype(int).values if 'same_team' in train_sup.columns else np.zeros(len(train_sup), int)\n",
        "\n",
        "seeds = [42,1337,2025]\n",
        "oof_frames = []\n",
        "test_frames = []\n",
        "\n",
        "for s in seeds:\n",
        "    print(f'PP aligned bagging seed {s} ...')\n",
        "    X_all = train_sup[feat_cols].astype(float).values\n",
        "    oof = np.full(len(train_sup), np.nan, float)\n",
        "    models = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "        t1 = time.time()\n",
        "        X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "        X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "        neg = (y_tr == 0).sum(); posc = (y_tr == 1).sum()\n",
        "        spw = max(1.0, neg / max(1, posc))\n",
        "        dtrain = xgb.DMatrix(X_tr, label=y_tr); dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "        params = {'tree_method':'hist','device':'cuda','max_depth':7,'eta':0.05,'subsample':0.9,'colsample_bytree':0.8,\n",
        "                  'min_child_weight':10,'lambda':1.5,'alpha':0.1,'gamma':0.1,'objective':'binary:logistic','eval_metric':'logloss',\n",
        "                  'scale_pos_weight': float(spw), 'seed': int(s + fold)}\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=3500, evals=[(dtrain,'train'),(dvalid,'valid')], early_stopping_rounds=200, verbose_eval=False)\n",
        "        best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "        oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "        models.append((booster, best_it))\n",
        "        print(f'  seed {s} fold {fold} done in {time.time()-t1:.1f}s; best_it={best_it}', flush=True)\n",
        "    # Smooth and attach keys for alignment\n",
        "    oof_df = train_sup[['game_play','p1','p2','step']].copy()\n",
        "    oof_df['oof'] = oof\n",
        "    oof_df = oof_df.sort_values(['game_play','p1','p2','step'])\n",
        "    grp = oof_df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    oof_df['oof_smooth'] = grp['oof'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    oof_df['key'] = (oof_df['game_play'].astype(str) + '|' + oof_df['p1'].astype(str) + '|' + oof_df['p2'].astype(str) + '|' + oof_df['step'].astype(str))\n",
        "    oof_frames.append(oof_df[['key','oof_smooth']].rename(columns={'oof_smooth': f'oof_{s}'}))\n",
        "    # Test probs for this seed\n",
        "    Xt = test_feats[feat_cols].astype(float).values\n",
        "    dtest = xgb.DMatrix(Xt)\n",
        "    pt = np.zeros(len(test_feats), float)\n",
        "    for i, (booster, best_it) in enumerate(models):\n",
        "        t1 = time.time(); pt += booster.predict(dtest, iteration_range=(0, best_it + 1));\n",
        "        print(f'   seed {s} test model {i} {time.time()-t1:.1f}s')\n",
        "    pt /= max(1, len(models))\n",
        "    pred_tmp = test_feats[['game_play','p1','p2','step']].copy()\n",
        "    pred_tmp['prob'] = pt\n",
        "    pred_tmp = pred_tmp.sort_values(['game_play','p1','p2','step'])\n",
        "    grp_t = pred_tmp.groupby(['game_play','p1','p2'], sort=False)\n",
        "    pred_tmp['prob_smooth'] = grp_t['prob'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    pred_tmp['key'] = (pred_tmp['game_play'].astype(str) + '|' + pred_tmp['p1'].astype(str) + '|' + pred_tmp['p2'].astype(str) + '|' + pred_tmp['step'].astype(str))\n",
        "    test_frames.append(pred_tmp[['key','prob_smooth']].rename(columns={'prob_smooth': f'pt_{s}'}))\n",
        "\n",
        "# Align by key and average (outer to avoid row loss), fill missing to neutral 0.5\n",
        "oof_join = oof_frames[0]\n",
        "for df in oof_frames[1:]:\n",
        "    oof_join = oof_join.merge(df, on='key', how='outer')\n",
        "oof_join = oof_join.fillna(0.5)\n",
        "oof_join['oof_avg'] = oof_join[[c for c in oof_join.columns if c.startswith('oof_')]].mean(axis=1)\n",
        "\n",
        "keys_split = oof_join['key'].str.split('|', expand=True)\n",
        "keys_split.columns = ['game_play','p1','p2','step']\n",
        "oof_join = pd.concat([oof_join[['key','oof_avg']], keys_split], axis=1)\n",
        "oof_join['step'] = oof_join['step'].astype(int)\n",
        "\n",
        "# Bring labels and same_flag aligned to keys\n",
        "lab_df = train_sup[['game_play','p1','p2','step','contact','same_team']].copy()\n",
        "lab_df['key'] = (lab_df['game_play'].astype(str) + '|' + lab_df['p1'].astype(str) + '|' + lab_df['p2'].astype(str) + '|' + lab_df['step'].astype(str))\n",
        "eval_df = oof_join.merge(lab_df[['key','contact','same_team']], on='key', how='inner')\n",
        "y_sorted = eval_df['contact'].astype(int).to_numpy()\n",
        "p_sorted = eval_df['oof_avg'].astype(float).to_numpy()\n",
        "same_sorted = eval_df['same_team'].fillna(0).astype(int).to_numpy() if 'same_team' in eval_df.columns else np.zeros(len(eval_df), int)\n",
        "\n",
        "# Robust dual-threshold search with fallback\n",
        "best_mcc, thr_opp, thr_same = fast_dual_threshold_mcc(y_sorted, p_sorted, same_sorted, grid_points=256)\n",
        "if (not np.isfinite(best_mcc)) or best_mcc < 0.6:\n",
        "    thrs = np.linspace(0.6, 0.9, 31)\n",
        "    m_list = [matthews_corrcoef(y_sorted, (p_sorted >= t).astype(int)) for t in thrs]\n",
        "    j = int(np.argmax(m_list))\n",
        "    thr_opp = thr_same = float(thrs[j])  # fallback single threshold\n",
        "    best_mcc = float(m_list[j])\n",
        "print(f'PP aligned bagged OOF MCC={best_mcc:.5f} | thr_same={thr_same:.4f}, thr_opp={thr_opp:.4f}')\n",
        "\n",
        "# Test: align and average (outer), fill to 0.5\n",
        "test_join = test_frames[0]\n",
        "for df in test_frames[1:]:\n",
        "    test_join = test_join.merge(df, on='key', how='outer')\n",
        "test_join = test_join.fillna(0.5)\n",
        "test_join['pt_avg'] = test_join[[c for c in test_join.columns if c.startswith('pt_')]].mean(axis=1)\n",
        "tj = test_join.copy()\n",
        "split_t = tj['key'].str.split('|', expand=True)\n",
        "split_t.columns = ['game_play','p1','p2','step']\n",
        "tj = pd.concat([tj[['key','pt_avg']], split_t], axis=1)\n",
        "tj['step'] = tj['step'].astype(int)\n",
        "\n",
        "# Merge same_team for thresholding\n",
        "st = test_feats[['game_play','p1','p2','step','same_team']].copy()\n",
        "st['key'] = (st['game_play'].astype(str) + '|' + st['p1'].astype(str) + '|' + st['p2'].astype(str) + '|' + st['step'].astype(str))\n",
        "tj = tj.merge(st[['key','same_team']], on='key', how='left')\n",
        "same_flag_test = tj['same_team'].fillna(0).astype(int).to_numpy()\n",
        "thr_arr_test = np.where(same_flag_test == 1, thr_same, thr_opp)\n",
        "tj['pred_bin'] = (tj['pt_avg'].to_numpy() >= thr_arr_test).astype(int)\n",
        "cid_pp = (tj['game_play'].astype(str) + '_' + tj['step'].astype(str) + '_' + tj['p1'].astype(str) + '_' + tj['p2'].astype(str))\n",
        "pred_df_pp = pd.DataFrame({'contact_id': cid_pp, 'contact_pp': tj['pred_bin'].astype(int).values})\n",
        "\n",
        "# Build final submission: PP then overwrite G-second, read G from prior submission or CSV\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.merge(pred_df_pp, on='contact_id', how='left')\n",
        "sub['contact'] = sub['contact_pp'].fillna(0).astype(int)\n",
        "sub = sub.drop(columns=['contact_pp'])\n",
        "before_ones = int(sub['contact'].sum())\n",
        "\n",
        "# Option B: read G rows from existing submission.csv\n",
        "try:\n",
        "    prev_sub = pd.read_csv('submission.csv')\n",
        "    g_pred_second = prev_sub[prev_sub['contact_id'].str.endswith('_G')][['contact_id','contact']].rename(columns={'contact':'contact_g'})\n",
        "except Exception as e:\n",
        "    print('Warning: could not read previous submission for G; defaulting to no G overwrite:', e)\n",
        "    g_pred_second = pd.DataFrame(columns=['contact_id','contact_g'])\n",
        "\n",
        "sub = sub.merge(g_pred_second, on='contact_id', how='left')\n",
        "sub['contact'] = sub['contact_g'].fillna(sub['contact']).astype(int)\n",
        "sub = sub[['contact_id','contact']]\n",
        "after_ones = int(sub['contact'].sum())\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'Final aligned-bag submission saved. PP ones={before_ones}, after G overwrite={after_ones}. Took {time.time()-t0:.1f}s')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version (pp-align): 2.1.4\nLoading r=3.5 dyn artifacts...\nPP aligned bagging seed 42 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  seed 42 fold 0 done in 29.7s; best_it=3094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  seed 42 fold 1 done in 30.8s; best_it=3182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  seed 42 fold 2 done in 32.0s; best_it=3270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  seed 42 fold 3 done in 30.1s; best_it=2995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  seed 42 fold 4 done in 28.3s; best_it=2873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 test model 0 0.1s\n   seed 42 test model 1 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 test model 2 0.1s\n   seed 42 test model 3 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 test model 4 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP aligned bagging seed 1337 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  seed 1337 fold 0 done in 30.9s; best_it=3211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  seed 1337 fold 1 done in 31.7s; best_it=3291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  seed 1337 fold 2 done in 32.2s; best_it=3408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  seed 1337 fold 3 done in 29.6s; best_it=2955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  seed 1337 fold 4 done in 27.3s; best_it=2747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 test model 0 0.2s\n   seed 1337 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 test model 2 0.2s\n   seed 1337 test model 3 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 test model 4 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP aligned bagging seed 2025 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  seed 2025 fold 0 done in 28.9s; best_it=3011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  seed 2025 fold 1 done in 31.8s; best_it=3338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  seed 2025 fold 2 done in 31.3s; best_it=3189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  seed 2025 fold 3 done in 29.3s; best_it=2925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  seed 2025 fold 4 done in 28.7s; best_it=2898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 test model 0 0.1s\n   seed 2025 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 test model 2 0.1s\n   seed 2025 test model 3 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 test model 4 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP aligned bagged OOF MCC=2122.94447 | thr_same=0.0048, thr_opp=0.1520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final aligned-bag submission saved. PP ones=15681, after G overwrite=17733. Took 470.3s\n"
          ]
        }
      ]
    },
    {
      "id": "501e7af9-8131-4d5e-9942-7a94ca2a288d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Rebuild full pipeline with candidate radius r=4.0 and save *_r40 artifacts\n",
        "import pandas as pd, numpy as np, time, math\n",
        "from itertools import combinations\n",
        "\n",
        "t0 = time.time()\n",
        "print('Rebuilding pipeline with r=4.0 ...')\n",
        "\n",
        "def build_pairs_for_group_r(gdf, r=4.0):\n",
        "    rows = []\n",
        "    arr = gdf[['nfl_player_id','team','position','x_position','y_position','speed','acceleration','direction']].values\n",
        "    n = arr.shape[0]\n",
        "    for i, j in combinations(range(n), 2):\n",
        "        pid_i, team_i, pos_i, xi, yi, si, ai, diri = arr[i]\n",
        "        pid_j, team_j, pos_j, xj, yj, sj, aj, dirj = arr[j]\n",
        "        dx = xj - xi; dy = yj - yi\n",
        "        dist = math.hypot(dx, dy)\n",
        "        if dist > r:\n",
        "            continue\n",
        "        a = int(pid_i); b = int(pid_j)\n",
        "        p1, p2 = (str(a), str(b)) if a <= b else (str(b), str(a))\n",
        "        vxi = si * math.cos(math.radians(diri)) if not pd.isna(diri) else 0.0\n",
        "        vyi = si * math.sin(math.radians(diri)) if not pd.isna(diri) else 0.0\n",
        "        vxj = sj * math.cos(math.radians(dirj)) if not pd.isna(dirj) else 0.0\n",
        "        vyj = sj * math.sin(math.radians(dirj)) if not pd.isna(dirj) else 0.0\n",
        "        rvx = vxj - vxi; rvy = vyj - vyi\n",
        "        if dist > 0:\n",
        "            ux = dx / dist; uy = dy / dist\n",
        "            closing = rvx * ux + rvy * uy\n",
        "        else:\n",
        "            closing = 0.0\n",
        "        if pd.isna(diri) or pd.isna(dirj):\n",
        "            hd = np.nan\n",
        "        else:\n",
        "            d = (diri - dirj + 180) % 360 - 180\n",
        "            hd = abs(d)\n",
        "        rows.append((p1, p2, dist, dx, dy, si, sj, ai, aj, closing, abs(closing), hd, int(team_i == team_j), str(team_i), str(team_j), str(pos_i), str(pos_j)))\n",
        "    if not rows:\n",
        "        return pd.DataFrame(columns=['p1','p2','distance','rel_dx','rel_dy','speed1','speed2','accel1','accel2','closing','abs_closing','abs_d_heading','same_team','team1','team2','pos1','pos2'])\n",
        "    return pd.DataFrame(rows, columns=['p1','p2','distance','rel_dx','rel_dy','speed1','speed2','accel1','accel2','closing','abs_closing','abs_d_heading','same_team','team1','team2','pos1','pos2'])\n",
        "\n",
        "def build_feature_table_r(track_df, r=4.0):\n",
        "    feats = []\n",
        "    cnt = 0\n",
        "    last = time.time()\n",
        "    for (gp, step), gdf in track_df.groupby(['game_play','step'], sort=False):\n",
        "        f = build_pairs_for_group_r(gdf, r=r)\n",
        "        if not f.empty:\n",
        "            f.insert(0, 'step', step)\n",
        "            f.insert(0, 'game_play', gp)\n",
        "            feats.append(f)\n",
        "        cnt += 1\n",
        "        if cnt % 500 == 0:\n",
        "            now = time.time()\n",
        "            print(f' processed {cnt} steps; +{now-last:.1f}s; total {now-t0:.1f}s', flush=True)\n",
        "            last = now\n",
        "    if feats:\n",
        "        return pd.concat(feats, ignore_index=True)\n",
        "    return pd.DataFrame(columns=['game_play','step','p1','p2','distance','rel_dx','rel_dy','speed1','speed2','accel1','accel2','closing','abs_closing','abs_d_heading','same_team','team1','team2','pos1','pos2'])\n",
        "\n",
        "print('Building train pairs r=4.0 ...')\n",
        "train_pairs_r40 = build_feature_table_r(train_track_idx, r=4.0)\n",
        "print('train_pairs_r40:', train_pairs_r40.shape)\n",
        "train_pairs_r40.to_parquet('train_pairs_r40.parquet', index=False)\n",
        "print('Building test pairs r=4.0 ...')\n",
        "test_pairs_r40 = build_feature_table_r(test_track_idx, r=4.0)\n",
        "print('test_pairs_r40:', test_pairs_r40.shape)\n",
        "test_pairs_r40.to_parquet('test_pairs_r40.parquet', index=False)\n",
        "\n",
        "def add_window_feats_local(df: pd.DataFrame, W: int = 5):\n",
        "    df = df.sort_values(['game_play','p1','p2','step']).copy()\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    df['dist_min_p5'] = grp['distance'].rolling(W, min_periods=1).min().reset_index(level=[0,1,2], drop=True)\n",
        "    df['dist_mean_p5'] = grp['distance'].rolling(W, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n",
        "    df['dist_max_p5'] = grp['distance'].rolling(W, min_periods=1).max().reset_index(level=[0,1,2], drop=True)\n",
        "    df['dist_std_p5'] = grp['distance'].rolling(W, min_periods=1).std().reset_index(level=[0,1,2], drop=True)\n",
        "    df['abs_close_min_p5'] = grp['abs_closing'].rolling(W, min_periods=1).min().reset_index(level=[0,1,2], drop=True)\n",
        "    df['abs_close_mean_p5'] = grp['abs_closing'].rolling(W, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n",
        "    df['abs_close_max_p5'] = grp['abs_closing'].rolling(W, min_periods=1).max().reset_index(level=[0,1,2], drop=True)\n",
        "    df['abs_close_std_p5'] = grp['abs_closing'].rolling(W, min_periods=1).std().reset_index(level=[0,1,2], drop=True)\n",
        "    for thr, name in [(1.5,'lt15'), (2.0,'lt20'), (2.5,'lt25')]:\n",
        "        key = f'cnt_dist_{name}_p5'\n",
        "        df[key] = grp['distance'].apply(lambda s: s.lt(thr).rolling(W, min_periods=1).sum()).reset_index(level=[0,1,2], drop=True)\n",
        "    df['dist_delta_p5'] = df['distance'] - grp['distance'].shift(W)\n",
        "    return df\n",
        "\n",
        "print('Adding W5 features (train/test) for r=4.0 ...')\n",
        "train_w_r40 = add_window_feats_local(train_pairs_r40, W=5)\n",
        "test_w_r40 = add_window_feats_local(test_pairs_r40, W=5)\n",
        "train_w_r40.to_parquet('train_pairs_w5_r40.parquet', index=False)\n",
        "test_w_r40.to_parquet('test_pairs_w5_r40.parquet', index=False)\n",
        "\n",
        "FPS = 59.94\n",
        "def prep_meta(vmeta: pd.DataFrame):\n",
        "    vm = vmeta.copy()\n",
        "    for c in ['start_time','snap_time']:\n",
        "        if np.issubdtype(vm[c].dtype, np.number):\n",
        "            continue\n",
        "        ts = pd.to_datetime(vm[c], errors='coerce')\n",
        "        if ts.notna().any():\n",
        "            vm[c] = (ts - ts.dt.floor('D')).dt.total_seconds().astype(float)\n",
        "        else:\n",
        "            vm[c] = pd.to_numeric(vm[c], errors='coerce')\n",
        "    vm['snap_frame'] = ((vm['snap_time'] - vm['start_time']) * FPS).round().astype('Int64')\n",
        "    return vm[['game_play','view','snap_frame']].drop_duplicates()\n",
        "\n",
        "print('Loading helmets and video metadata...')\n",
        "train_helm_df = pd.read_csv('train_baseline_helmets.csv')\n",
        "test_helm_df = pd.read_csv('test_baseline_helmets.csv')\n",
        "train_vmeta_df = pd.read_csv('train_video_metadata.csv')\n",
        "test_vmeta_df = pd.read_csv('test_video_metadata.csv')\n",
        "meta_tr = prep_meta(train_vmeta_df); meta_te = prep_meta(test_vmeta_df)\n",
        "\n",
        "def dedup_and_step(helm: pd.DataFrame, meta: pd.DataFrame):\n",
        "    df = helm[['game_play','view','frame','nfl_player_id','left','top','width','height']].copy()\n",
        "    df = df.dropna(subset=['nfl_player_id'])\n",
        "    df['nfl_player_id'] = df['nfl_player_id'].astype(int).astype(str)\n",
        "    df['area'] = df['width'] * df['height']\n",
        "    df['cx'] = df['left'] + 0.5 * df['width']\n",
        "    df['cy'] = df['top'] + 0.5 * df['height']\n",
        "    df = df.sort_values(['game_play','view','frame','nfl_player_id','area'], ascending=[True,True,True,True,False]).drop_duplicates(['game_play','view','frame','nfl_player_id'], keep='first')\n",
        "    df = df.merge(meta, on=['game_play','view'], how='left')\n",
        "    df['step'] = ((df['frame'] - df['snap_frame']).astype('float') / 6.0).round().astype('Int64')\n",
        "    df = df.dropna(subset=['step']); df['step'] = df['step'].astype(int)\n",
        "    dm1 = df.copy(); dm1['target_step'] = dm1['step'] - 1\n",
        "    d0 = df.copy(); d0['target_step'] = df['step']\n",
        "    dp1 = df.copy(); dp1['target_step'] = df['step'] + 1\n",
        "    d = pd.concat([dm1, d0, dp1], ignore_index=True)\n",
        "    agg = d.groupby(['game_play','view','target_step','nfl_player_id'], sort=False).agg(\n",
        "        cx_mean=('cx','mean'), cy_mean=('cy','mean'), h_mean=('height','mean'), cnt=('cx','size')\n",
        "    ).reset_index().rename(columns={'target_step':'step'})\n",
        "    return agg\n",
        "\n",
        "print('Preparing helmet aggregates...')\n",
        "h_tr = dedup_and_step(train_helm_df, meta_tr)\n",
        "h_te = dedup_and_step(test_helm_df, meta_te)\n",
        "print('Helmet agg shapes:', h_tr.shape, h_te.shape)\n",
        "\n",
        "def merge_helmet_to_pairs_df(pairs: pd.DataFrame, h_agg: pd.DataFrame):\n",
        "    ha = h_agg[['game_play','step','view','nfl_player_id','cx_mean','cy_mean','h_mean']].copy()\n",
        "    a = ha.rename(columns={'nfl_player_id':'p1','cx_mean':'cx1','cy_mean':'cy1','h_mean':'h1'})\n",
        "    b = ha.rename(columns={'nfl_player_id':'p2','cx_mean':'cx2','cy_mean':'cy2','h_mean':'h2'})\n",
        "    merged = a.merge(b, on=['game_play','step','view'], how='inner')\n",
        "    merged = merged[merged['p1'] < merged['p2']]\n",
        "    merged['px_dist'] = np.sqrt((merged['cx1'] - merged['cx2'])**2 + (merged['cy1'] - merged['cy2'])**2)\n",
        "    merged['px_dist_norm'] = merged['px_dist'] / np.sqrt(np.maximum(1e-6, merged['h1'] * merged['h2']))\n",
        "    agg = merged.groupby(['game_play','step','p1','p2'], as_index=False).agg(\n",
        "        px_dist_norm_min=('px_dist_norm','min'),\n",
        "        views_both_present=('px_dist_norm', lambda s: int(s.notna().sum()))\n",
        "    )\n",
        "    out = pairs.merge(agg, on=['game_play','step','p1','p2'], how='left')\n",
        "    return out\n",
        "\n",
        "print('Merging helmets into pairs (train/test) ...')\n",
        "train_pairs_w5_helm_r40 = merge_helmet_to_pairs_df(train_w_r40, h_tr)\n",
        "test_pairs_w5_helm_r40 = merge_helmet_to_pairs_df(test_w_r40, h_te)\n",
        "train_pairs_w5_helm_r40.to_parquet('train_pairs_w5_helm_r40.parquet', index=False)\n",
        "test_pairs_w5_helm_r40.to_parquet('test_pairs_w5_helm_r40.parquet', index=False)\n",
        "\n",
        "def add_dyn_feats(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.sort_values(['game_play','p1','p2','step']).copy()\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "    df['approaching_flag'] = (df['closing'] < 0).astype(int)\n",
        "    denom = (-df['closing']).clip(lower=1e-3)\n",
        "    ttc_raw = df['distance'] / denom\n",
        "    ttc_raw = ttc_raw.where(df['approaching_flag'] == 1, 10.0)\n",
        "    df['ttc_raw'] = ttc_raw.astype(float)\n",
        "    df['ttc_clip'] = df['ttc_raw'].clip(0, 5)\n",
        "    df['ttc_log'] = np.log1p(df['ttc_clip'])\n",
        "    df['inv_ttc'] = 1.0 / (1.0 + df['ttc_clip'])\n",
        "    df['d_dist_1'] = df['distance'] - grp['distance'].shift(1)\n",
        "    df['d_dist_2'] = df['distance'] - grp['distance'].shift(2)\n",
        "    df['d_dist_5'] = df['distance'] - grp['distance'].shift(5)\n",
        "    df['d_close_1'] = df['closing'] - grp['closing'].shift(1)\n",
        "    df['d_absclose_1'] = df['abs_closing'] - grp['abs_closing'].shift(1)\n",
        "    df['d_speed1_1'] = df['speed1'] - grp['speed1'].shift(1)\n",
        "    df['d_speed2_1'] = df['speed2'] - grp['speed2'].shift(1)\n",
        "    df['d_accel1_1'] = df['accel1'] - grp['accel1'].shift(1)\n",
        "    df['d_accel2_1'] = df['accel2'] - grp['accel2'].shift(1)\n",
        "    df['rm3_d_dist_1'] = grp['d_dist_1'].transform(lambda s: s.rolling(3, min_periods=1).mean())\n",
        "    df['rm3_d_close_1'] = grp['d_close_1'].transform(lambda s: s.rolling(3, min_periods=1).mean())\n",
        "    for c in ['d_dist_1','d_dist_2','d_dist_5','d_close_1','d_absclose_1','d_speed1_1','d_speed2_1','d_accel1_1','d_accel2_1','rm3_d_dist_1','rm3_d_close_1']:\n",
        "        df[c] = df[c].fillna(0.0)\n",
        "    df['rel_speed'] = (df['speed2'] - df['speed1']).astype(float)\n",
        "    df['abs_rel_speed'] = df['rel_speed'].abs()\n",
        "    df['rel_accel'] = (df['accel2'] - df['accel1']).astype(float)\n",
        "    df['abs_rel_accel'] = df['rel_accel'].abs()\n",
        "    df['jerk1'] = grp['accel1'].diff().fillna(0.0)\n",
        "    df['jerk2'] = grp['accel2'].diff().fillna(0.0)\n",
        "    if 'px_dist_norm_min' in df.columns:\n",
        "        df['d_px_norm_1'] = df['px_dist_norm_min'] - grp['px_dist_norm_min'].shift(1)\n",
        "        df['d_px_norm_1'] = df['d_px_norm_1'].fillna(0.0)\n",
        "        df['cnt_px_lt006_p3'] = grp['px_dist_norm_min'].transform(lambda s: s.lt(0.06).rolling(3, min_periods=1).sum()).astype(float)\n",
        "        df['cnt_px_lt008_p3'] = grp['px_dist_norm_min'].transform(lambda s: s.lt(0.08).rolling(3, min_periods=1).sum()).astype(float)\n",
        "    else:\n",
        "        df['d_px_norm_1'] = 0.0; df['cnt_px_lt006_p3'] = 0.0; df['cnt_px_lt008_p3'] = 0.0\n",
        "    return df\n",
        "\n",
        "print('Adding dyn features (train/test) ...')\n",
        "tr_dyn_r40 = add_dyn_feats(train_pairs_w5_helm_r40)\n",
        "te_dyn_r40 = add_dyn_feats(test_pairs_w5_helm_r40)\n",
        "tr_dyn_r40.to_parquet('train_pairs_w5_helm_dyn_r40.parquet', index=False)\n",
        "te_dyn_r40.to_parquet('test_pairs_w5_helm_dyn_r40.parquet', index=False)\n",
        "\n",
        "key_cols = ['game_play','step','p1','p2']\n",
        "lab_cols = key_cols + ['contact']\n",
        "labels_min = train_labels[lab_cols].copy()\n",
        "sup_r40 = labels_min.merge(tr_dyn_r40, on=key_cols, how='inner')\n",
        "print('Supervised(inner) r=4.0 before expansion:', sup_r40.shape, 'pos rate:', sup_r40['contact'].mean())\n",
        "pos = sup_r40.loc[sup_r40['contact'] == 1, ['game_play','p1','p2','step']]\n",
        "pos_m1 = pos.copy(); pos_m1['step'] = pos_m1['step'] - 1\n",
        "pos_p1 = pos.copy(); pos_p1['step'] = pos_p1['step'] + 1\n",
        "pos_exp = pd.concat([pos_m1, pos_p1], ignore_index=True).drop_duplicates()\n",
        "pos_exp['flag_pos_exp'] = 1\n",
        "sup_r40 = sup_r40.merge(pos_exp, on=['game_play','p1','p2','step'], how='left')\n",
        "sup_r40.loc[sup_r40['flag_pos_exp'] == 1, 'contact'] = 1\n",
        "sup_r40.drop(columns=['flag_pos_exp'], inplace=True)\n",
        "print('After positive expansion (r=4.0): pos rate:', sup_r40['contact'].mean())\n",
        "sup_r40.to_parquet('train_supervised_w5_helm_dyn_r40.parquet', index=False)\n",
        "\n",
        "print('Done r=4.0 rebuild in {:.1f}s'.format(time.time()-t0), flush=True)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rebuilding pipeline with r=4.0 ...\nBuilding train pairs r=4.0 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 500 steps; +0.7s; total 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1000 steps; +0.8s; total 1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1500 steps; +0.6s; total 2.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2000 steps; +0.6s; total 2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2500 steps; +0.6s; total 3.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3000 steps; +0.6s; total 3.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3500 steps; +0.6s; total 4.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4000 steps; +0.9s; total 5.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4500 steps; +0.6s; total 5.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5000 steps; +0.6s; total 6.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5500 steps; +0.6s; total 7.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 6000 steps; +0.6s; total 7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 6500 steps; +0.6s; total 8.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 7000 steps; +0.6s; total 8.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 7500 steps; +1.0s; total 9.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 8000 steps; +0.6s; total 10.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 8500 steps; +0.6s; total 10.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 9000 steps; +0.6s; total 11.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 9500 steps; +0.6s; total 12.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 10000 steps; +0.6s; total 12.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 10500 steps; +0.6s; total 13.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 11000 steps; +0.6s; total 13.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 11500 steps; +1.1s; total 14.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 12000 steps; +0.6s; total 15.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 12500 steps; +0.6s; total 15.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 13000 steps; +0.6s; total 16.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 13500 steps; +0.6s; total 17.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 14000 steps; +0.6s; total 17.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 14500 steps; +0.6s; total 18.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 15000 steps; +0.6s; total 18.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 15500 steps; +0.6s; total 19.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 16000 steps; +0.6s; total 19.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 16500 steps; +0.6s; total 20.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 17000 steps; +1.1s; total 21.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 17500 steps; +0.6s; total 22.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 18000 steps; +0.6s; total 22.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 18500 steps; +0.6s; total 23.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 19000 steps; +0.6s; total 23.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 19500 steps; +0.6s; total 24.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 20000 steps; +0.6s; total 25.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 20500 steps; +0.6s; total 25.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 21000 steps; +0.6s; total 26.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 21500 steps; +0.6s; total 26.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 22000 steps; +0.6s; total 27.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 22500 steps; +0.6s; total 27.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 23000 steps; +1.2s; total 29.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 23500 steps; +0.6s; total 29.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 24000 steps; +0.6s; total 30.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 24500 steps; +0.6s; total 30.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 25000 steps; +0.6s; total 31.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 25500 steps; +0.6s; total 31.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 26000 steps; +0.6s; total 32.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 26500 steps; +0.6s; total 33.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 27000 steps; +0.6s; total 33.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 27500 steps; +0.6s; total 34.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 28000 steps; +0.6s; total 34.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 28500 steps; +0.6s; total 35.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 29000 steps; +0.6s; total 35.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 29500 steps; +0.6s; total 36.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 30000 steps; +0.6s; total 37.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 30500 steps; +0.6s; total 37.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 31000 steps; +1.4s; total 39.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 31500 steps; +0.6s; total 39.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 32000 steps; +0.6s; total 40.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 32500 steps; +0.6s; total 40.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 33000 steps; +0.6s; total 41.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 33500 steps; +0.6s; total 41.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 34000 steps; +0.6s; total 42.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 34500 steps; +0.6s; total 43.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 35000 steps; +0.6s; total 43.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 35500 steps; +0.6s; total 44.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 36000 steps; +0.6s; total 44.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 36500 steps; +0.6s; total 45.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 37000 steps; +0.6s; total 45.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 37500 steps; +0.6s; total 46.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 38000 steps; +0.6s; total 47.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 38500 steps; +0.6s; total 47.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 39000 steps; +0.6s; total 48.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 39500 steps; +0.6s; total 48.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 40000 steps; +1.6s; total 50.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 40500 steps; +0.6s; total 50.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 41000 steps; +0.6s; total 51.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 41500 steps; +0.6s; total 52.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 42000 steps; +0.6s; total 52.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 42500 steps; +0.6s; total 53.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 43000 steps; +0.6s; total 53.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 43500 steps; +0.6s; total 54.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 44000 steps; +0.6s; total 54.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 44500 steps; +0.6s; total 55.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 45000 steps; +0.6s; total 55.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 45500 steps; +0.6s; total 56.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 46000 steps; +0.6s; total 57.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 46500 steps; +0.6s; total 57.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 47000 steps; +0.6s; total 58.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 47500 steps; +0.6s; total 58.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 48000 steps; +0.6s; total 59.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 48500 steps; +0.6s; total 59.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 49000 steps; +0.6s; total 60.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 49500 steps; +0.6s; total 61.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 50000 steps; +0.6s; total 61.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 50500 steps; +0.6s; total 62.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 51000 steps; +0.6s; total 62.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 51500 steps; +1.7s; total 64.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 52000 steps; +0.6s; total 65.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 52500 steps; +0.6s; total 65.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 53000 steps; +0.6s; total 66.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 53500 steps; +0.6s; total 66.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 54000 steps; +0.6s; total 67.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 54500 steps; +0.6s; total 67.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 55000 steps; +0.6s; total 68.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 55500 steps; +0.6s; total 69.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_pairs_r40: (2430943, 19)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building test pairs r=4.0 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 500 steps; +0.6s; total 75.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1000 steps; +0.6s; total 76.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1500 steps; +0.6s; total 76.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2000 steps; +0.6s; total 77.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2500 steps; +0.6s; total 78.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3000 steps; +0.6s; total 78.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3500 steps; +0.6s; total 79.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4000 steps; +0.6s; total 79.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4500 steps; +0.6s; total 80.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5000 steps; +0.5s; total 80.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5500 steps; +0.6s; total 81.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_pairs_r40: (278492, 19)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding W5 features (train/test) for r=4.0 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading helmets and video metadata...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing helmet aggregates...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helmet agg shapes: (620840, 8) (67667, 8)\nMerging helmets into pairs (train/test) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding dyn features (train/test) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supervised(inner) r=4.0 before expansion: (634192, 59) pos rate: 0.06721308373489417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After positive expansion (r=4.0): pos rate: 0.07693884501854328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done r=4.0 rebuild in 386.5s\n"
          ]
        }
      ]
    },
    {
      "id": "c8eed906-c1ab-49fa-8ae5-7af4978c33f9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train on r=4.0 dyn features, smooth OOF, dual thresholds (same vs opp), predict test\n",
        "import time, sys, subprocess, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version:', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def mcc_from_counts(tp, tn, fp, fn):\n",
        "    num = tp * tn - fp * fn\n",
        "    den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
        "    den = np.where(den == 0, 1.0, den)\n",
        "    return num / den\n",
        "\n",
        "def fast_dual_threshold_mcc(y_true, prob, same_flag, grid_points=151):\n",
        "    res = {}\n",
        "    for cohort in (0, 1):\n",
        "        mask = (same_flag == cohort)\n",
        "        y_c = y_true[mask].astype(int)\n",
        "        p_c = prob[mask].astype(float)\n",
        "        n = len(y_c)\n",
        "        if n == 0:\n",
        "            res[cohort] = {'k_grid': np.array([0], int), 'tp': np.array([0.0]), 'fp': np.array([0.0]), 'tn': np.array([0.0]), 'fn': np.array([0.0]), 'thr_vals': np.array([1.0])}\n",
        "            continue\n",
        "        order = np.argsort(-p_c)\n",
        "        y_sorted = y_c[order]\n",
        "        p_sorted = p_c[order]\n",
        "        cum_pos = np.concatenate([[0], np.cumsum(y_sorted)])\n",
        "        k_grid = np.unique(np.linspace(0, n, num=min(grid_points, n + 1), dtype=int))\n",
        "        tp = cum_pos[k_grid]\n",
        "        fp = k_grid - tp\n",
        "        P = y_sorted.sum(); N = n - P\n",
        "        fn = P - tp; tn = N - fp\n",
        "        thr_vals = np.where(k_grid == 0, 1.0 + 1e-6, p_sorted[np.maximum(0, k_grid - 1)])\n",
        "        res[cohort] = {'k_grid': k_grid, 'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn, 'thr_vals': thr_vals}\n",
        "    tp0, fp0, tn0, fn0, thr0 = res[0]['tp'], res[0]['fp'], res[0]['tn'], res[0]['fn'], res[0]['thr_vals']\n",
        "    tp1, fp1, tn1, fn1, thr1 = res[1]['tp'], res[1]['fp'], res[1]['tn'], res[1]['fn'], res[1]['thr_vals']\n",
        "    best = (-1.0, 0.5, 0.5)\n",
        "    for i in range(len(thr0)):\n",
        "        tp_sum = tp0[i] + tp1; fp_sum = fp0[i] + fp1; tn_sum = tn0[i] + tn1; fn_sum = fn0[i] + fn1\n",
        "        m_arr = mcc_from_counts(tp_sum, tn_sum, fp_sum, fn_sum)\n",
        "        j = int(np.argmax(m_arr)); m = float(m_arr[j])\n",
        "        if m > best[0]:\n",
        "            best = (m, float(thr0[i]), float(thr1[j]))\n",
        "    return best  # (best_mcc, thr_opp, thr_same)\n",
        "\n",
        "print('Loading r=4.0 supervised dyn train and dyn test features...')\n",
        "train_sup = pd.read_parquet('train_supervised_w5_helm_dyn_r40.parquet')\n",
        "test_feats = pd.read_parquet('test_pairs_w5_helm_dyn_r40.parquet')\n",
        "folds_df = pd.read_csv('folds_game_play.csv')\n",
        "print('train_sup:', train_sup.shape, 'test_feats:', test_feats.shape)\n",
        "\n",
        "train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "assert train_sup['fold'].notna().all()\n",
        "\n",
        "for df in (train_sup, test_feats):\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "\n",
        "drop_cols = {'contact','game_play','step','p1','p2','team1','team2','pos1','pos2','fold'}\n",
        "feat_cols = [c for c in train_sup.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_sup[c])]\n",
        "print('Using', len(feat_cols), 'features')\n",
        "\n",
        "X_all = train_sup[feat_cols].astype(float).values\n",
        "y_all = train_sup['contact'].astype(int).values\n",
        "groups = train_sup['game_play'].values\n",
        "same_flag_all = train_sup['same_team'].astype(int).values if 'same_team' in train_sup.columns else np.zeros(len(train_sup), dtype=int)\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "oof = np.full(len(train_sup), np.nan, dtype=float)\n",
        "models = []\n",
        "start = time.time()\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "    t0 = time.time()\n",
        "    X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "    X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "    neg = (y_tr == 0).sum(); pos = (y_tr == 1).sum()\n",
        "    spw = max(1.0, neg / max(1, pos))\n",
        "    print(f'Fold {fold}: train {len(tr_idx)} (pos {pos}), valid {len(va_idx)} (pos {(y_va==1).sum()}), spw={spw:.2f}', flush=True)\n",
        "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
        "    dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "    params = {\n",
        "        'tree_method': 'hist', 'device': 'cuda', 'max_depth': 7, 'eta': 0.05, 'subsample': 0.9,\n",
        "        'colsample_bytree': 0.8, 'min_child_weight': 10, 'lambda': 1.5, 'alpha': 0.1, 'gamma': 0.1,\n",
        "        'objective': 'binary:logistic', 'eval_metric': 'logloss', 'scale_pos_weight': float(spw), 'seed': 42 + fold\n",
        "    }\n",
        "    booster = xgb.train(params=params, dtrain=dtrain, num_boost_round=4000, evals=[(dtrain,'train'),(dvalid,'valid')],\n",
        "                        early_stopping_rounds=200, verbose_eval=False)\n",
        "    best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "    oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "    models.append((booster, best_it))\n",
        "    print(f' Fold {fold} done in {time.time()-t0:.1f}s; best_iteration={best_it}', flush=True)\n",
        "\n",
        "# Smooth OOF per (gp,p1,p2) with centered rolling-max window=3\n",
        "oof_df = train_sup[['game_play','p1','p2','step']].copy()\n",
        "oof_df['oof'] = oof\n",
        "oof_df = oof_df.sort_values(['game_play','p1','p2','step'])\n",
        "grp = oof_df.groupby(['game_play','p1','p2'], sort=False)\n",
        "oof_df['oof_smooth'] = grp['oof'].transform(lambda s: s.rolling(3, center=True, min_periods=1).max())\n",
        "oof_smooth = oof_df['oof_smooth'].values\n",
        "idx_ord = oof_df.index.to_numpy()\n",
        "y_sorted = train_sup['contact'].astype(int).to_numpy()[idx_ord]\n",
        "same_sorted = (train_sup['same_team'].fillna(0).astype(int).to_numpy()[idx_ord]) if 'same_team' in train_sup.columns else np.zeros(len(oof_df), dtype=int)\n",
        "\n",
        "best_mcc, thr_opp, thr_same = fast_dual_threshold_mcc(y_sorted, oof_smooth, same_sorted, grid_points=151)\n",
        "if not np.isfinite(best_mcc) or best_mcc < 0:\n",
        "    thrs = np.linspace(0.6, 0.9, 31)\n",
        "    m_list = [matthews_corrcoef(y_sorted, (oof_smooth >= t).astype(int)) for t in thrs]\n",
        "    j = int(np.argmax(m_list)); best_mcc = float(m_list[j]); thr_opp = thr_same = float(thrs[j])\n",
        "print(f'Best OOF MCC (dual thresholds)={best_mcc:.5f} | thr_same={thr_same:.4f}, thr_opp={thr_opp:.4f}')\n",
        "\n",
        "# Inference on test and smoothing\n",
        "Xt = test_feats[feat_cols].astype(float).values\n",
        "dtest = xgb.DMatrix(Xt)\n",
        "pt = np.zeros(len(test_feats), dtype=float)\n",
        "for i, (booster, best_it) in enumerate(models):\n",
        "    t0 = time.time()\n",
        "    pt += booster.predict(dtest, iteration_range=(0, best_it + 1))\n",
        "    print(f' Inference model {i} took {time.time()-t0:.1f}s', flush=True)\n",
        "pt /= max(1, len(models))\n",
        "pred_tmp = test_feats[['game_play','step','p1','p2']].copy()\n",
        "pred_tmp['prob'] = pt\n",
        "pred_tmp = pred_tmp.sort_values(['game_play','p1','p2','step'])\n",
        "grp_t = pred_tmp.groupby(['game_play','p1','p2'], sort=False)\n",
        "pred_tmp['prob_smooth'] = grp_t['prob'].transform(lambda s: s.rolling(3, center=True, min_periods=1).max())\n",
        "\n",
        "# Apply dual thresholds by same_team on test\n",
        "same_flag_test = test_feats['same_team'].astype(int).values if 'same_team' in test_feats.columns else np.zeros(len(test_feats), dtype=int)\n",
        "thr_arr_test = np.where(same_flag_test == 1, thr_same, thr_opp)\n",
        "pred_bin = (pred_tmp['prob_smooth'].values >= thr_arr_test).astype(int)\n",
        "\n",
        "# Build submission\n",
        "cid = (test_feats['game_play'].astype(str) + '_' + test_feats['step'].astype(str) + '_' + test_feats['p1'].astype(str) + '_' + test_feats['p2'].astype(str))\n",
        "pred_df = pd.DataFrame({'contact_id': cid, 'pred_contact': pred_bin})\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.copy()\n",
        "sub['contact'] = sub['contact_id'].map(pred_df.set_index('contact_id')['pred_contact']).fillna(0).astype(int)\n",
        "sub[['contact_id','contact']].to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv')\n",
        "print('Done. Total time:', f'{time.time()-start:.1f}s', flush=True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version: 2.1.4\nLoading r=4.0 supervised dyn train and dyn test features...\ntrain_sup: (634192, 59) test_feats: (278492, 58)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 50 features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: train 507383 (pos 38276), valid 126809 (pos 10518), spw=12.26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 0 done in 34.9s; best_iteration=3253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train 507362 (pos 38941), valid 126830 (pos 9853), spw=12.03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 1 done in 40.1s; best_iteration=3632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: train 507403 (pos 39474), valid 126789 (pos 9320), spw=11.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 2 done in 37.9s; best_iteration=3326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: train 507360 (pos 39212), valid 126832 (pos 9582), spw=11.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 3 done in 37.4s; best_iteration=3446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: train 507260 (pos 39273), valid 126932 (pos 9521), spw=11.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 4 done in 37.3s; best_iteration=3468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_94/2177737263.py:16: RuntimeWarning: invalid value encountered in sqrt\n  den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best OOF MCC (dual thresholds)=0.72401 | thr_same=0.7900, thr_opp=0.7900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 0 took 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 1 took 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 2 took 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 3 took 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference model 4 took 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv\nDone. Total time: 195.8s\n"
          ]
        }
      ]
    },
    {
      "id": "3346d586-f569-43ab-91ff-74660709e86a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PP multi-seed bagging (r=4.0) with strict alignment and float-safe dual-thresholds; then optional G overwrite from prior submission\n",
        "import time, numpy as np, pandas as pd, sys, subprocess\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.4'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version (pp-bag-r40):', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def fast_dual_threshold_mcc(y_true, prob, same_flag, grid_points=256):\n",
        "    import numpy as np\n",
        "    y = np.asarray(y_true, dtype=np.int64)\n",
        "    p = np.asarray(prob, dtype=np.float64)\n",
        "    s = np.asarray(same_flag, dtype=np.int8)\n",
        "    mask = np.isfinite(y) & np.isfinite(p) & np.isfinite(s)\n",
        "    y, p, s = y[mask], p[mask], s[mask]\n",
        "\n",
        "    def cohort_counts(yc, pc, G):\n",
        "        n = yc.size\n",
        "        if n == 0:\n",
        "            return dict(tp=np.array([0], np.float64), fp=np.array([0], np.float64),\n",
        "                        tn=np.array([0], np.float64), fn=np.array([0], np.float64),\n",
        "                        thr=np.array([1.0], np.float64))\n",
        "        order = np.argsort(-pc, kind='mergesort')\n",
        "        ys, ps = yc[order], pc[order]\n",
        "        P = float(ys.sum()); N = float(n - ys.sum())\n",
        "        # top-k grid\n",
        "        step = max(1, n // max(1, (G - 1)))\n",
        "        k = np.arange(0, n + 1, step, dtype=np.int64)\n",
        "        if k[-1] != n: k = np.append(k, n)\n",
        "        cum = np.concatenate(([0], np.cumsum(ys, dtype=np.int64)))\n",
        "        tp = cum[k].astype(np.float64); fp = (k - cum[k]).astype(np.float64)\n",
        "        fn = P - tp; tn = N - fp\n",
        "        thr = np.where(k == 0, 1.0 + 1e-6, ps[np.maximum(0, k - 1)])\n",
        "        return dict(tp=tp, fp=fp, tn=tn, fn=fn, thr=thr)\n",
        "\n",
        "    a = cohort_counts(y[s == 0], p[s == 0], grid_points)\n",
        "    b = cohort_counts(y[s == 1], p[s == 1], grid_points)\n",
        "\n",
        "    tp = a['tp'][:, None] + b['tp'][None, :]\n",
        "    fp = a['fp'][:, None] + b['fp'][None, :]\n",
        "    tn = a['tn'][:, None] + b['tn'][None, :]\n",
        "    fn = a['fn'][:, None] + b['fn'][None, :]\n",
        "\n",
        "    with np.errstate(invalid='ignore', divide='ignore'):\n",
        "        num = tp * tn - fp * fn\n",
        "        den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
        "        den = np.where(den > 0, np.sqrt(den), np.nan)\n",
        "        mcc = num / den\n",
        "\n",
        "    if not np.isfinite(mcc).any():\n",
        "        return -1.0, 0.79, 0.79\n",
        "    i, j = np.unravel_index(np.nanargmax(mcc), mcc.shape)\n",
        "    return float(mcc[i, j]), float(a['thr'][i]), float(b['thr'][j])\n",
        "\n",
        "t0 = time.time()\n",
        "print('PP bagging r=4.0: loading artifacts...')\n",
        "train_sup = pd.read_parquet('train_supervised_w5_helm_dyn_r40.parquet')\n",
        "test_feats = pd.read_parquet('test_pairs_w5_helm_dyn_r40.parquet')\n",
        "folds_df = pd.read_csv('folds_game_play.csv')\n",
        "train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "assert train_sup['fold'].notna().all()\n",
        "for df in (train_sup, test_feats):\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "\n",
        "drop_cols = {'contact','game_play','step','p1','p2','team1','team2','pos1','pos2','fold'}\n",
        "feat_cols = [c for c in train_sup.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_sup[c])]\n",
        "print('Using', len(feat_cols), 'features')\n",
        "\n",
        "# Canonical key order used by ALL seeds\n",
        "key_df = train_sup[['game_play','p1','p2','step']].copy()\n",
        "key_df = key_df.assign(row=np.arange(len(key_df)))\n",
        "ord_idx = key_df.sort_values(['game_play','p1','p2','step']).index.to_numpy()\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train_sup['game_play'].values\n",
        "y_all = train_sup['contact'].astype(int).values\n",
        "same_all = train_sup['same_team'].fillna(0).astype(int).values if 'same_team' in train_sup.columns else np.zeros(len(train_sup), np.int8)\n",
        "\n",
        "seeds = [42, 1337, 2025]\n",
        "oof_s_list = []\n",
        "test_s_list = []\n",
        "\n",
        "for s in seeds:\n",
        "    print(f' PP r=4.0 seed {s} ...', flush=True)\n",
        "    X_all = train_sup[feat_cols].astype(float).values\n",
        "    oof = np.full(len(train_sup), np.nan, float)\n",
        "    models = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "        t1 = time.time()\n",
        "        X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "        X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "        neg = (y_tr == 0).sum(); posc = (y_tr == 1).sum()\n",
        "        spw = max(1.0, neg / max(1, posc))\n",
        "        dtrain = xgb.DMatrix(X_tr, label=y_tr); dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "        params = {'tree_method':'hist','device':'cuda','max_depth':7,'eta':0.05,'subsample':0.9,'colsample_bytree':0.8,\n",
        "                  'min_child_weight':10,'lambda':1.5,'alpha':0.1,'gamma':0.1,'objective':'binary:logistic','eval_metric':'logloss',\n",
        "                  'scale_pos_weight': float(spw), 'seed': int(s + fold)}\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=3800, evals=[(dtrain,'train'),(dvalid,'valid')], early_stopping_rounds=200, verbose_eval=False)\n",
        "        best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "        oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "        models.append((booster, best_it))\n",
        "        print(f'   seed {s} fold {fold} done in {time.time()-t1:.1f}s; best_it={best_it}', flush=True)\n",
        "    # Smooth OOF on canonical order\n",
        "    df = train_sup[['game_play','p1','p2','step']].iloc[ord_idx].copy()\n",
        "    df['oof'] = oof[ord_idx]\n",
        "    df = df.sort_values(['game_play','p1','p2','step'])\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    df['oof_smooth'] = grp['oof'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    oof_s_list.append(df['oof_smooth'].to_numpy())\n",
        "\n",
        "    # Test predictions, smooth on canonical order for test\n",
        "    Xt = test_feats[feat_cols].astype(float).values\n",
        "    dtest = xgb.DMatrix(Xt)\n",
        "    pt = np.zeros(len(test_feats), float)\n",
        "    for i, (booster, best_it) in enumerate(models):\n",
        "        t1 = time.time(); pt += booster.predict(dtest, iteration_range=(0, best_it + 1));\n",
        "        print(f'    seed {s} test model {i} {time.time()-t1:.1f}s', flush=True)\n",
        "    pt /= max(1, len(models))\n",
        "    dt = test_feats[['game_play','p1','p2','step']].copy()\n",
        "    dt = dt.sort_values(['game_play','p1','p2','step'])\n",
        "    dt['prob'] = pt[dt.index.values]  # already aligned but safe\n",
        "    grp_t = dt.groupby(['game_play','p1','p2'], sort=False)\n",
        "    dt['prob_smooth'] = grp_t['prob'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    test_s_list.append(dt['prob_smooth'].to_numpy())\n",
        "\n",
        "# Average OOF across seeds in the same canonical order\n",
        "oof_avg = np.mean(np.vstack(oof_s_list), axis=0)\n",
        "y_sorted = train_sup['contact'].astype(int).to_numpy()[ord_idx]\n",
        "same_sorted = train_sup['same_team'].fillna(0).astype(int).to_numpy()[ord_idx] if 'same_team' in train_sup.columns else np.zeros_like(y_sorted, np.int8)\n",
        "best_mcc, thr_opp, thr_same = fast_dual_threshold_mcc(y_sorted, oof_avg, same_sorted, grid_points=256)\n",
        "if (not np.isfinite(best_mcc)) or best_mcc < 0:\n",
        "    thrs = np.linspace(0.7, 0.85, 31)\n",
        "    m_list = [matthews_corrcoef(y_sorted, (oof_avg >= t).astype(int)) for t in thrs]\n",
        "    j = int(np.argmax(m_list)); best_mcc = float(m_list[j]); thr_opp = thr_same = float(thrs[j])\n",
        "print(f'PP bagged r=4.0 OOF MCC={best_mcc:.5f} | thr_same={thr_same:.4f}, thr_opp={thr_opp:.4f}')\n",
        "\n",
        "# Average test probs across seeds and threshold\n",
        "pt_bag = np.mean(np.vstack(test_s_list), axis=0)\n",
        "dt_keys = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step'])\n",
        "same_flag_test = test_feats[['game_play','p1','p2','step','same_team']].copy()\n",
        "same_flag_test = same_flag_test.merge(dt_keys.reset_index().rename(columns={'index':'ord'}), on=['game_play','p1','p2','step'], how='right')\n",
        "same_flag_test = same_flag_test.sort_values('ord')\n",
        "same_flag_test_arr = same_flag_test['same_team'].fillna(0).astype(int).to_numpy() if 'same_team' in same_flag_test.columns else np.zeros(len(dt_keys), int)\n",
        "thr_arr_test = np.where(same_flag_test_arr == 1, thr_same, thr_opp)\n",
        "pred_bin_sorted = (pt_bag >= thr_arr_test).astype(int)\n",
        "\n",
        "# Map back to contact_id\n",
        "cid_sorted = (dt_keys['game_play'].astype(str) + '_' + dt_keys['step'].astype(str) + '_' + dt_keys['p1'].astype(str) + '_' + dt_keys['p2'].astype(str))\n",
        "pred_df_pp = pd.DataFrame({'contact_id': cid_sorted.values, 'contact_pp': pred_bin_sorted})\n",
        "\n",
        "# Build submission from sample and PP preds\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.merge(pred_df_pp, on='contact_id', how='left')\n",
        "sub['contact'] = sub['contact_pp'].fillna(0).astype(int)\n",
        "sub = sub.drop(columns=['contact_pp'])\n",
        "pp_ones = int(sub['contact'].sum())\n",
        "print('PP bagged ones before G overwrite:', pp_ones)\n",
        "\n",
        "# Optional: overwrite G-second rows from previous submission if available\n",
        "try:\n",
        "    prev_sub = pd.read_csv('submission.csv')\n",
        "    g_pred_second = prev_sub[prev_sub['contact_id'].str.endswith('_G')][['contact_id','contact']].rename(columns={'contact':'contact_g'})\n",
        "    sub = sub.merge(g_pred_second, on='contact_id', how='left')\n",
        "    sub['contact'] = sub['contact_g'].fillna(sub['contact']).astype(int)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "    after_ones = int(sub['contact'].sum())\n",
        "    print(f'Applied prior G overwrite. ones after={after_ones}, delta={after_ones-pp_ones}')\n",
        "except Exception as e:\n",
        "    print('No prior submission with G rows found; skipping G overwrite.', e)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Final PP-bagged (r=4.0) submission saved. Took {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version (pp-bag-r40): 2.1.4\nPP bagging r=4.0: loading artifacts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 50 features\n PP r=4.0 seed 42 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 0 done in 34.9s; best_it=3253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 1 done in 39.7s; best_it=3632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 2 done in 37.8s; best_it=3326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 3 done in 37.3s; best_it=3446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 4 done in 37.2s; best_it=3468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.0 seed 1337 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 0 done in 38.1s; best_it=3385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 1 done in 39.6s; best_it=3608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 2 done in 36.0s; best_it=3140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 3 done in 38.0s; best_it=3378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 4 done in 39.1s; best_it=3609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.0 seed 2025 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 0 done in 39.0s; best_it=3453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 1 done in 38.4s; best_it=3408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 2 done in 37.9s; best_it=3284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 3 done in 40.6s; best_it=3573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 4 done in 37.1s; best_it=3388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP bagged r=4.0 OOF MCC=0.72502 | thr_same=0.7626, thr_opp=0.7713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP bagged ones before G overwrite: 6810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied prior G overwrite. ones after=8862, delta=2052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final PP-bagged (r=4.0) submission saved. Took 586.2s\n"
          ]
        }
      ]
    },
    {
      "id": "fa80faa0-2eeb-4abd-b8ac-02fec4c550cb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Post-processing: apply PP hysteresis (2-of-3 min-duration) on binary predictions in submission.csv; keep G rows intact\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "sub_path = Path('submission.csv')\n",
        "assert sub_path.exists(), 'submission.csv not found. Run bagging cell first.'\n",
        "sub = pd.read_csv(sub_path)\n",
        "before_ones = int(sub['contact'].sum())\n",
        "\n",
        "# Parse contact_id\n",
        "tok = sub['contact_id'].str.split('_', n=4, expand=True)\n",
        "tok.columns = ['g1','g2','step','a','b']\n",
        "tok['game_play'] = tok['g1'] + '_' + tok['g2']\n",
        "tok['step'] = tok['step'].astype(int)\n",
        "\n",
        "# Identify PP vs G rows (G rows end with _G as b == 'G')\n",
        "is_g = tok['b'] == 'G'\n",
        "pp_df = pd.DataFrame({\n",
        "    'game_play': tok['game_play'],\n",
        "    'p1': tok['a'],\n",
        "    'p2': tok['b'],\n",
        "    'step': tok['step'],\n",
        "    'contact': sub['contact'].values\n",
        "})\n",
        "\n",
        "# Keep only player-player pairs (neither side is G). In sample, G appears only as second token.\n",
        "pp_mask = (~is_g) & (pp_df['p1'] != 'G')\n",
        "pp = pp_df.loc[pp_mask, ['game_play','p1','p2','step','contact']].copy()\n",
        "\n",
        "# Apply centered rolling majority (2-of-3) per (game_play, p1, p2)\n",
        "pp = pp.sort_values(['game_play','p1','p2','step'])\n",
        "grp = pp.groupby(['game_play','p1','p2'], sort=False)['contact']\n",
        "pp['contact_hyst'] = grp.transform(lambda s: (s.rolling(3, center=True, min_periods=1).sum() >= 2).astype(int))\n",
        "\n",
        "# Map back to submission\n",
        "pp_key = (pp['game_play'] + '_' + pp['step'].astype(str) + '_' + pp['p1'] + '_' + pp['p2'])\n",
        "sub_key = (tok['game_play'] + '_' + tok['step'].astype(str) + '_' + tok['a'] + '_' + tok['b'])\n",
        "map_h = pd.Series(pp['contact_hyst'].values, index=pp_key.values)\n",
        "sub_h = sub_key.map(map_h)\n",
        "\n",
        "# Overwrite only PP rows with hysteresis; keep G rows untouched\n",
        "sub.loc[pp_mask, 'contact'] = sub_h.loc[pp_mask].fillna(sub.loc[pp_mask, 'contact']).astype(int)\n",
        "\n",
        "after_ones = int(sub['contact'].sum())\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'Applied PP hysteresis (2-of-3). ones before={before_ones}, after={after_ones}, delta={after_ones-before_ones}')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied PP hysteresis (2-of-3). ones before=8862, after=8885, delta=23\n"
          ]
        }
      ]
    },
    {
      "id": "c2ede705-6c7a-4679-818b-bcf4ae15d079",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PP r=4.0 multi-seed bagging with 4 thresholds (same/opponent x distance<=1.8), strict alignment; optional G overwrite; save submission\n",
        "import time, numpy as np, pandas as pd, sys, subprocess\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.4'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version (pp-bag-r40-dist4):', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def fast_dual_threshold_mcc(y_true, prob, same_flag, grid_points=256):\n",
        "    import numpy as np\n",
        "    y = np.asarray(y_true, dtype=np.int64)\n",
        "    p = np.asarray(prob, dtype=np.float64)\n",
        "    s = np.asarray(same_flag, dtype=np.int8)\n",
        "    mask = np.isfinite(y) & np.isfinite(p) & np.isfinite(s)\n",
        "    y, p, s = y[mask], p[mask], s[mask]\n",
        "    def cohort_counts(yc, pc, G):\n",
        "        n = yc.size\n",
        "        if n == 0:\n",
        "            return dict(tp=np.array([0], np.float64), fp=np.array([0], np.float64),\n",
        "                        tn=np.array([0], np.float64), fn=np.array([0], np.float64),\n",
        "                        thr=np.array([1.0], np.float64))\n",
        "        order = np.argsort(-pc, kind='mergesort')\n",
        "        ys, ps = yc[order], pc[order]\n",
        "        P = float(ys.sum()); N = float(n - ys.sum())\n",
        "        step = max(1, n // max(1, (G - 1)))\n",
        "        k = np.arange(0, n + 1, step, dtype=np.int64)\n",
        "        if k[-1] != n: k = np.append(k, n)\n",
        "        cum = np.concatenate(([0], np.cumsum(ys, dtype=np.int64)))\n",
        "        tp = cum[k].astype(np.float64); fp = (k - cum[k]).astype(np.float64)\n",
        "        fn = P - tp; tn = N - fp\n",
        "        thr = np.where(k == 0, 1.0 + 1e-6, ps[np.maximum(0, k - 1)])\n",
        "        return dict(tp=tp, fp=fp, tn=tn, fn=fn, thr=thr)\n",
        "    a = cohort_counts(y[s == 0], p[s == 0], grid_points)\n",
        "    b = cohort_counts(y[s == 1], p[s == 1], grid_points)\n",
        "    tp = a['tp'][:, None] + b['tp'][None, :]\n",
        "    fp = a['fp'][:, None] + b['fp'][None, :]\n",
        "    tn = a['tn'][:, None] + b['tn'][None, :]\n",
        "    fn = a['fn'][:, None] + b['fn'][None, :]\n",
        "    with np.errstate(invalid='ignore', divide='ignore'):\n",
        "        num = tp * tn - fp * fn\n",
        "        den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
        "        den = np.where(den > 0, np.sqrt(den), np.nan)\n",
        "        mcc = num / den\n",
        "    if not np.isfinite(mcc).any():\n",
        "        return -1.0, 0.79, 0.79\n",
        "    i, j = np.unravel_index(np.nanargmax(mcc), mcc.shape)\n",
        "    return float(mcc[i, j]), float(a['thr'][i]), float(b['thr'][j])\n",
        "\n",
        "t0 = time.time()\n",
        "print('PP bagging r=4.0 (4-thr by distance): loading artifacts...')\n",
        "train_sup = pd.read_parquet('train_supervised_w5_helm_dyn_r40.parquet')\n",
        "test_feats = pd.read_parquet('test_pairs_w5_helm_dyn_r40.parquet')\n",
        "folds_df = pd.read_csv('folds_game_play.csv')\n",
        "train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "assert train_sup['fold'].notna().all()\n",
        "for df in (train_sup, test_feats):\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "\n",
        "drop_cols = {'contact','game_play','step','p1','p2','team1','team2','pos1','pos2','fold'}\n",
        "feat_cols = [c for c in train_sup.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_sup[c])]\n",
        "print('Using', len(feat_cols), 'features')\n",
        "\n",
        "# Canonical order\n",
        "key_df = train_sup[['game_play','p1','p2','step']].copy()\n",
        "ord_idx = key_df.sort_values(['game_play','p1','p2','step']).index.to_numpy()\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train_sup['game_play'].values\n",
        "y_all = train_sup['contact'].astype(int).values\n",
        "same_all = train_sup['same_team'].fillna(0).astype(int).values if 'same_team' in train_sup.columns else np.zeros(len(train_sup), np.int8)\n",
        "\n",
        "seeds = [42, 1337, 2025]\n",
        "oof_list = []\n",
        "test_list = []\n",
        "\n",
        "for s in seeds:\n",
        "    print(f' PP r=4.0 seed {s} ...', flush=True)\n",
        "    X_all = train_sup[feat_cols].astype(float).values\n",
        "    oof = np.full(len(train_sup), np.nan, float)\n",
        "    models = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "        t1 = time.time()\n",
        "        X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "        X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "        neg = (y_tr == 0).sum(); posc = (y_tr == 1).sum()\n",
        "        spw = max(1.0, neg / max(1, posc))\n",
        "        dtrain = xgb.DMatrix(X_tr, label=y_tr); dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "        params = {'tree_method':'hist','device':'cuda','max_depth':7,'eta':0.05,'subsample':0.9,'colsample_bytree':0.8,\n",
        "                  'min_child_weight':10,'lambda':1.5,'alpha':0.1,'gamma':0.1,'objective':'binary:logistic','eval_metric':'logloss',\n",
        "                  'scale_pos_weight': float(spw), 'seed': int(s + fold)}\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=3800, evals=[(dtrain,'train'),(dvalid,'valid')], early_stopping_rounds=200, verbose_eval=False)\n",
        "        best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "        oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "        models.append((booster, best_it))\n",
        "        print(f'   seed {s} fold {fold} done in {time.time()-t1:.1f}s; best_it={best_it}', flush=True)\n",
        "    # store unsmoothed OOF in canonical order\n",
        "    oof_list.append(oof[ord_idx])\n",
        "    # Test predictions (unsmoothed) in canonical order of test\n",
        "    Xt = test_feats[feat_cols].astype(float).values\n",
        "    dtest = xgb.DMatrix(Xt)\n",
        "    pt = np.zeros(len(test_feats), float)\n",
        "    for i, (booster, best_it) in enumerate(models):\n",
        "        t1 = time.time(); pt += booster.predict(dtest, iteration_range=(0, best_it + 1));\n",
        "        print(f'    seed {s} test model {i} {time.time()-t1:.1f}s', flush=True)\n",
        "    pt /= max(1, len(models))\n",
        "    dt = test_feats[['game_play','p1','p2','step']].copy()\n",
        "    dt = dt.sort_values(['game_play','p1','p2','step'])\n",
        "    test_list.append(pt[dt.index.values])\n",
        "\n",
        "# Average across seeds, then smooth by (gp,p1,p2) with window=3 centered\n",
        "oof_avg = np.mean(np.vstack(oof_list), axis=0)\n",
        "ts_keys = train_sup[['game_play','p1','p2','step']].iloc[ord_idx].copy()\n",
        "df_o = ts_keys.copy(); df_o['prob'] = oof_avg\n",
        "df_o = df_o.sort_values(['game_play','p1','p2','step'])\n",
        "grp = df_o.groupby(['game_play','p1','p2'], sort=False)\n",
        "df_o['prob_smooth'] = grp['prob'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "oof_smooth = df_o['prob_smooth'].to_numpy()\n",
        "\n",
        "y_sorted = train_sup['contact'].astype(int).to_numpy()[ord_idx]\n",
        "same_sorted = train_sup['same_team'].fillna(0).astype(int).to_numpy()[ord_idx] if 'same_team' in train_sup.columns else np.zeros_like(y_sorted, np.int8)\n",
        "dist_sorted = train_sup['distance'].astype(float).to_numpy()[ord_idx] if 'distance' in train_sup.columns else np.full_like(y_sorted, 10.0, float)\n",
        "bin_sorted = (dist_sorted <= 1.8).astype(np.int8)  # 1=close, 0=far\n",
        "\n",
        "# 4 thresholds: by distance bin (close/far) x same_team\n",
        "thr_dict = {}  # bin -> (thr_opp, thr_same)\n",
        "mcc_parts = []\n",
        "for b in (0, 1):\n",
        "    mask = (bin_sorted == b)\n",
        "    if mask.sum() == 0:\n",
        "        thr_dict[b] = (0.79, 0.79)\n",
        "        continue\n",
        "    mcc_b, t_opp_b, t_same_b = fast_dual_threshold_mcc(y_sorted[mask], oof_smooth[mask], same_sorted[mask], grid_points=256)\n",
        "    if (not np.isfinite(mcc_b)) or mcc_b < 0:\n",
        "        thrs = np.linspace(0.7, 0.85, 31)\n",
        "        ml = [matthews_corrcoef(y_sorted[mask], (oof_smooth[mask] >= t).astype(int)) for t in thrs]\n",
        "        j = int(np.argmax(ml)); t_opp_b = t_same_b = float(thrs[j])\n",
        "    thr_dict[b] = (float(t_opp_b), float(t_same_b))\n",
        "print('Thresholds by dist bin (0=far,1=close):', thr_dict)\n",
        "\n",
        "# Evaluate combined OOF MCC\n",
        "thr_arr = np.empty(len(oof_smooth), dtype=float)\n",
        "for b in (0, 1):\n",
        "    m = (bin_sorted == b)\n",
        "    t_opp, t_same = thr_dict[b]\n",
        "    thr_arr[m] = np.where(same_sorted[m] == 1, t_same, t_opp)\n",
        "pred_oof = (oof_smooth >= thr_arr).astype(int)\n",
        "oof_mcc_all = matthews_corrcoef(y_sorted, pred_oof)\n",
        "print(f'OOF MCC with 4 thresholds (dist bins): {oof_mcc_all:.5f}')\n",
        "\n",
        "# Test: average across seeds, then smooth, then apply 4 thresholds\n",
        "pt_avg = np.mean(np.vstack(test_list), axis=0)\n",
        "dt_keys = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step'])\n",
        "df_t = dt_keys.copy()\n",
        "df_t['prob'] = pt_avg\n",
        "grp_t = df_t.groupby(['game_play','p1','p2'], sort=False)\n",
        "df_t['prob_smooth'] = grp_t['prob'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "same_flag_test = test_feats[['game_play','p1','p2','step','same_team']].copy()\n",
        "same_flag_test = same_flag_test.merge(dt_keys.reset_index().rename(columns={'index':'ord'}), on=['game_play','p1','p2','step'], how='right').sort_values('ord')\n",
        "same_flag_test_arr = same_flag_test['same_team'].fillna(0).astype(int).to_numpy() if 'same_team' in same_flag_test.columns else np.zeros(len(df_t), int)\n",
        "dist_test = test_feats[['game_play','p1','p2','step','distance']].copy()\n",
        "dist_test = dist_test.merge(dt_keys.reset_index().rename(columns={'index':'ord'}), on=['game_play','p1','p2','step'], how='right').sort_values('ord')\n",
        "bin_test = (dist_test['distance'].astype(float).to_numpy() <= 1.8).astype(np.int8)\n",
        "thr_arr_test = np.where(same_flag_test_arr == 1,\n",
        "                        np.where(bin_test == 1, thr_dict[1][1], thr_dict[0][1]),\n",
        "                        np.where(bin_test == 1, thr_dict[1][0], thr_dict[0][0]))\n",
        "pred_bin_sorted = (df_t['prob_smooth'].to_numpy() >= thr_arr_test).astype(int)\n",
        "\n",
        "# Build submission and optional G overwrite from prior submission.csv\n",
        "cid_sorted = (df_t['game_play'].astype(str) + '_' + df_t['step'].astype(str) + '_' + df_t['p1'].astype(str) + '_' + df_t['p2'].astype(str))\n",
        "pred_df_pp = pd.DataFrame({'contact_id': cid_sorted.values, 'contact_pp': pred_bin_sorted})\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.merge(pred_df_pp, on='contact_id', how='left')\n",
        "sub['contact'] = sub['contact_pp'].fillna(0).astype(int)\n",
        "sub = sub.drop(columns=['contact_pp'])\n",
        "pp_ones = int(sub['contact'].sum())\n",
        "print('PP bagged (r40, 4-thr) ones before G overwrite:', pp_ones)\n",
        "try:\n",
        "    prev_sub = pd.read_csv('submission.csv')\n",
        "    g_pred_second = prev_sub[prev_sub['contact_id'].str.endswith('_G')][['contact_id','contact']].rename(columns={'contact':'contact_g'})\n",
        "    sub = sub.merge(g_pred_second, on='contact_id', how='left')\n",
        "    sub['contact'] = sub['contact_g'].fillna(sub['contact']).astype(int)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "    after_ones = int(sub['contact'].sum())\n",
        "    print(f'Applied prior G overwrite. ones after={after_ones}, delta={after_ones-pp_ones}')\n",
        "except Exception as e:\n",
        "    print('No prior submission with G rows found; skipping G overwrite.', e)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv. Took {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version (pp-bag-r40-dist4): 2.1.4\nPP bagging r=4.0 (4-thr by distance): loading artifacts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 50 features\n PP r=4.0 seed 42 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 0 done in 36.7s; best_it=3253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 1 done in 39.7s; best_it=3632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 2 done in 37.9s; best_it=3326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 3 done in 38.8s; best_it=3446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 4 done in 37.2s; best_it=3468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.0 seed 1337 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 0 done in 38.1s; best_it=3385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 1 done in 39.7s; best_it=3608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 2 done in 36.0s; best_it=3140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 3 done in 38.1s; best_it=3378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 4 done in 39.1s; best_it=3609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.0 seed 2025 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 0 done in 39.2s; best_it=3453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 1 done in 38.7s; best_it=3408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 2 done in 38.1s; best_it=3284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 3 done in 40.8s; best_it=3573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 4 done in 37.2s; best_it=3388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thresholds by dist bin (0=far,1=close): {0: (0.5131852726141611, 1.000001), 1: (0.8557006319363912, 0.7985922495524088)}\nOOF MCC with 4 thresholds (dist bins): 0.72028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP bagged (r40, 4-thr) ones before G overwrite: 6097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied prior G overwrite. ones after=8149, delta=2052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv. Took 584.3s\n"
          ]
        }
      ]
    },
    {
      "id": "53108f8f-2e20-49e0-8537-327f28876014",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blend r=3.5 and r=4.0 PP bagged probabilities (0.6/0.4), smooth, dual-threshold, apply PP hysteresis, keep prior G overwrite\n",
        "import time, numpy as np, pandas as pd, sys, subprocess\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.4'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version (blend r35+r40):', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def fast_dual_threshold_mcc(y_true, prob, same_flag, grid_points=256):\n",
        "    import numpy as np\n",
        "    y = np.asarray(y_true, dtype=np.int64)\n",
        "    p = np.asarray(prob, dtype=np.float64)\n",
        "    s = np.asarray(same_flag, dtype=np.int8)\n",
        "    mask = np.isfinite(y) & np.isfinite(p) & np.isfinite(s)\n",
        "    y, p, s = y[mask], p[mask], s[mask]\n",
        "    def cohort_counts(yc, pc, G):\n",
        "        n = yc.size\n",
        "        if n == 0:\n",
        "            return dict(tp=np.array([0], np.float64), fp=np.array([0], np.float64), tn=np.array([0], np.float64), fn=np.array([0], np.float64), thr=np.array([1.0], np.float64))\n",
        "        order = np.argsort(-pc, kind='mergesort')\n",
        "        ys, ps = yc[order], pc[order]\n",
        "        P = float(ys.sum()); N = float(n - ys.sum())\n",
        "        step = max(1, n // max(1, (G - 1)))\n",
        "        k = np.arange(0, n + 1, step, dtype=np.int64)\n",
        "        if k[-1] != n: k = np.append(k, n)\n",
        "        cum = np.concatenate(([0], np.cumsum(ys, dtype=np.int64)))\n",
        "        tp = cum[k].astype(np.float64); fp = (k - cum[k]).astype(np.float64)\n",
        "        fn = P - tp; tn = N - fp\n",
        "        thr = np.where(k == 0, 1.0 + 1e-6, ps[np.maximum(0, k - 1)])\n",
        "        return dict(tp=tp, fp=fp, tn=tn, fn=fn, thr=thr)\n",
        "    a = cohort_counts(y[s == 0], p[s == 0], grid_points)\n",
        "    b = cohort_counts(y[s == 1], p[s == 1], grid_points)\n",
        "    tp = a['tp'][:, None] + b['tp'][None, :]\n",
        "    fp = a['fp'][:, None] + b['fp'][None, :]\n",
        "    tn = a['tn'][:, None] + b['tn'][None, :]\n",
        "    fn = a['fn'][:, None] + b['fn'][None, :]\n",
        "    with np.errstate(invalid='ignore', divide='ignore'):\n",
        "        num = tp * tn - fp * fn\n",
        "        den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
        "        den = np.where(den > 0, np.sqrt(den), np.nan)\n",
        "        mcc = num / den\n",
        "    if not np.isfinite(mcc).any():\n",
        "        return -1.0, 0.79, 0.79\n",
        "    i, j = np.unravel_index(np.nanargmax(mcc), mcc.shape)\n",
        "    return float(mcc[i, j]), float(a['thr'][i]), float(b['thr'][j])\n",
        "\n",
        "def run_bag(radius_tag, train_sup_path, test_feats_path, seeds=(42,1337,2025)):\n",
        "    print(f'PP bagging {radius_tag}: load...')\n",
        "    train_sup = pd.read_parquet(train_sup_path)\n",
        "    test_feats = pd.read_parquet(test_feats_path)\n",
        "    folds_df = pd.read_csv('folds_game_play.csv')\n",
        "    train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "    assert train_sup['fold'].notna().all()\n",
        "    for df in (train_sup, test_feats):\n",
        "        if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "        if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "    drop_cols = {'contact','game_play','step','p1','p2','team1','team2','pos1','pos2','fold'}\n",
        "    feat_cols = [c for c in train_sup.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_sup[c])]\n",
        "    print(f' Using {len(feat_cols)} features')\n",
        "    key_df = train_sup[['game_play','p1','p2','step']].copy()\n",
        "    ord_idx = key_df.sort_values(['game_play','p1','p2','step']).index.to_numpy()\n",
        "    gkf = GroupKFold(n_splits=5)\n",
        "    groups = train_sup['game_play'].values\n",
        "    y_all = train_sup['contact'].astype(int).values\n",
        "    oof_list = []; test_list = []\n",
        "    for s in seeds:\n",
        "        print(f'  {radius_tag} seed {s} ...', flush=True)\n",
        "        X_all = train_sup[feat_cols].astype(float).values\n",
        "        oof = np.full(len(train_sup), np.nan, float)\n",
        "        models = []\n",
        "        for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "            t1 = time.time()\n",
        "            X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "            X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "            neg = (y_tr == 0).sum(); posc = (y_tr == 1).sum()\n",
        "            spw = max(1.0, neg / max(1, posc))\n",
        "            dtrain = xgb.DMatrix(X_tr, label=y_tr); dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "            params = {'tree_method':'hist','device':'cuda','max_depth':7,'eta':0.05,'subsample':0.9,'colsample_bytree':0.8,\n",
        "                      'min_child_weight':10,'lambda':1.5,'alpha':0.1,'gamma':0.1,'objective':'binary:logistic','eval_metric':'logloss',\n",
        "                      'scale_pos_weight': float(spw), 'seed': int(s + fold)}\n",
        "            booster = xgb.train(params, dtrain, num_boost_round=3800, evals=[(dtrain,'train'),(dvalid,'valid')], early_stopping_rounds=200, verbose_eval=False)\n",
        "            best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "            oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "            models.append((booster, best_it))\n",
        "            print(f'   {radius_tag} seed {s} fold {fold} {time.time()-t1:.1f}s; best_it={best_it}', flush=True)\n",
        "        oof_list.append(oof[ord_idx])\n",
        "        Xt = test_feats[feat_cols].astype(float).values\n",
        "        dtest = xgb.DMatrix(Xt)\n",
        "        pt = np.zeros(len(test_feats), float)\n",
        "        for i, (booster, best_it) in enumerate(models):\n",
        "            t1 = time.time(); pt += booster.predict(dtest, iteration_range=(0, best_it + 1));\n",
        "            print(f'    {radius_tag} seed {s} test model {i} {time.time()-t1:.1f}s', flush=True)\n",
        "        pt /= max(1, len(models))\n",
        "        dt = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step'])\n",
        "        test_list.append(pt[dt.index.values])\n",
        "    # pack outputs and keys\n",
        "    keys_tr = train_sup[['game_play','p1','p2','step']].iloc[ord_idx].copy()\n",
        "    keys_te = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step'])\n",
        "    return keys_tr, np.mean(np.vstack(oof_list), axis=0), keys_te, np.mean(np.vstack(test_list), axis=0), train_sup, test_feats, ord_idx\n",
        "\n",
        "t0 = time.time()\n",
        "print('Running bagging for r=3.5 and r=4.0...')\n",
        "keys_tr35, oof35, keys_te35, pt35, tr35, te35, ord35 = run_bag('r3.5', 'train_supervised_w5_helm_dyn_r35.parquet', 'test_pairs_w5_helm_dyn_r35.parquet')\n",
        "keys_tr40, oof40, keys_te40, pt40, tr40, te40, ord40 = run_bag('r4.0', 'train_supervised_w5_helm_dyn_r40.parquet', 'test_pairs_w5_helm_dyn_r40.parquet')\n",
        "\n",
        "# Ensure same key domains for train to blend OOF: inner-join on keys\n",
        "kt35 = keys_tr35.copy(); kt35['key'] = kt35['game_play'] + '|' + kt35['p1'] + '|' + kt35['p2'] + '|' + kt35['step'].astype(str)\n",
        "kt40 = keys_tr40.copy(); kt40['key'] = kt40['game_play'] + '|' + kt40['p1'] + '|' + kt40['p2'] + '|' + kt40['step'].astype(str)\n",
        "df35 = kt35[['key']].copy(); df35['p35'] = oof35\n",
        "df40 = kt40[['key']].copy(); df40['p40'] = oof40\n",
        "blend_tr = df35.merge(df40, on='key', how='inner')\n",
        "w35, w40 = 0.6, 0.4\n",
        "blend_tr['p_blend'] = w35 * blend_tr['p35'] + w40 * blend_tr['p40']\n",
        "\n",
        "# Smooth blended OOF by (gp,p1,p2)\n",
        "keys_split = blend_tr['key'].str.split('|', expand=True)\n",
        "keys_split.columns = ['game_play','p1','p2','step']\n",
        "tmp = keys_split.copy()\n",
        "tmp['step'] = tmp['step'].astype(int)\n",
        "tmp['p'] = blend_tr['p_blend'].values\n",
        "tmp = tmp.sort_values(['game_play','p1','p2','step'])\n",
        "grp = tmp.groupby(['game_play','p1','p2'], sort=False)\n",
        "tmp['p_smooth'] = grp['p'].transform(lambda s: s.rolling(3, center=True, min_periods=1).max())\n",
        "oof_blend_smooth = tmp['p_smooth'].to_numpy()\n",
        "y_map = tr40[['game_play','p1','p2','step','contact','same_team']].copy()\n",
        "y_map['key'] = (y_map['game_play'] + '|' + y_map['p1'] + '|' + y_map['p2'] + '|' + y_map['step'].astype(str))\n",
        "eval_df = tmp.copy()\n",
        "eval_df['key'] = (eval_df['game_play'] + '|' + eval_df['p1'] + '|' + eval_df['p2'] + '|' + eval_df['step'].astype(str))\n",
        "eval_df = eval_df.merge(y_map[['key','contact','same_team']], on='key', how='inner')\n",
        "y_sorted = eval_df['contact'].astype(int).to_numpy()\n",
        "same_sorted = eval_df['same_team'].fillna(0).astype(int).to_numpy() if 'same_team' in eval_df.columns else np.zeros(len(eval_df), np.int8)\n",
        "\n",
        "best_mcc, thr_opp, thr_same = fast_dual_threshold_mcc(y_sorted, oof_blend_smooth, same_sorted, grid_points=256)\n",
        "if (not np.isfinite(best_mcc)) or best_mcc < 0:\n",
        "    thrs = np.linspace(0.7, 0.85, 31)\n",
        "    m_list = [matthews_corrcoef(y_sorted, (oof_blend_smooth >= t).astype(int)) for t in thrs]\n",
        "    j = int(np.argmax(m_list)); best_mcc = float(m_list[j]); thr_opp = thr_same = float(thrs[j])\n",
        "print(f'Blended OOF MCC={best_mcc:.5f} | thr_same={thr_same:.4f}, thr_opp={thr_opp:.4f}')\n",
        "\n",
        "# Test: align r3.5 and r4.0 by keys then blend\n",
        "kte35 = keys_te35.copy(); kte35['key'] = kte35['game_play'] + '|' + kte35['p1'] + '|' + kte35['p2'] + '|' + kte35['step'].astype(str)\n",
        "kte40 = keys_te40.copy(); kte40['key'] = kte40['game_play'] + '|' + kte40['p1'] + '|' + kte40['p2'] + '|' + kte40['step'].astype(str)\n",
        "te35_df = kte35[['key']].copy(); te35_df['p35'] = pt35\n",
        "te40_df = kte40[['key']].copy(); te40_df['p40'] = pt40\n",
        "blend_te = te40_df.merge(te35_df, on='key', how='inner')  # use inner to ensure both available\n",
        "blend_te['p_blend'] = w35 * blend_te['p35'] + w40 * blend_te['p40']\n",
        "ks = blend_te['key'].str.split('|', expand=True); ks.columns = ['game_play','p1','p2','step']\n",
        "df_t = ks.copy(); df_t['step'] = df_t['step'].astype(int); df_t['prob'] = blend_te['p_blend'].values\n",
        "df_t = df_t.sort_values(['game_play','p1','p2','step'])\n",
        "grp_t = df_t.groupby(['game_play','p1','p2'], sort=False)\n",
        "df_t['prob_smooth'] = grp_t['prob'].transform(lambda s: s.rolling(3, center=True, min_periods=1).max())\n",
        "\n",
        "# Apply dual thresholds by same_team on test\n",
        "st = te40[['game_play','p1','p2','step','same_team']].copy()\n",
        "st['key'] = (st['game_play'] + '|' + st['p1'] + '|' + st['p2'] + '|' + st['step'].astype(str))\n",
        "df_t['key'] = (df_t['game_play'] + '|' + df_t['p1'] + '|' + df_t['p2'] + '|' + df_t['step'].astype(str))\n",
        "df_t = df_t.merge(st[['key','same_team']], on='key', how='left')\n",
        "same_flag_test = df_t['same_team'].fillna(0).astype(int).to_numpy() if 'same_team' in df_t.columns else np.zeros(len(df_t), int)\n",
        "thr_arr_test = np.where(same_flag_test == 1, thr_same, thr_opp)\n",
        "df_t['pred_bin'] = (df_t['prob_smooth'].to_numpy() >= thr_arr_test).astype(int)\n",
        "\n",
        "# Build submission from sample and PP preds\n",
        "cid_sorted = (df_t['game_play'].astype(str) + '_' + df_t['step'].astype(str) + '_' + df_t['p1'].astype(str) + '_' + df_t['p2'].astype(str))\n",
        "pred_df_pp = pd.DataFrame({'contact_id': cid_sorted.values, 'contact_pp': df_t['pred_bin'].astype(int).values})\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.merge(pred_df_pp, on='contact_id', how='left')\n",
        "sub['contact'] = sub['contact_pp'].fillna(0).astype(int)\n",
        "sub = sub.drop(columns=['contact_pp'])\n",
        "pp_ones = int(sub['contact'].sum())\n",
        "print('PP blended (r35*0.6 + r40*0.4) ones before G overwrite:', pp_ones)\n",
        "\n",
        "# Optional: overwrite G-second rows from previous submission if available\n",
        "try:\n",
        "    prev_sub = pd.read_csv('submission.csv')\n",
        "    g_pred_second = prev_sub[prev_sub['contact_id'].str.endswith('_G')][['contact_id','contact']].rename(columns={'contact':'contact_g'})\n",
        "    sub = sub.merge(g_pred_second, on='contact_id', how='left')\n",
        "    sub['contact'] = sub['contact_g'].fillna(sub['contact']).astype(int)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "    after_ones = int(sub['contact'].sum())\n",
        "    print(f'Applied prior G overwrite. ones after={after_ones}, delta={after_ones-pp_ones}')\n",
        "except Exception as e:\n",
        "    print('No prior submission with G rows found; skipping G overwrite.', e)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "\n",
        "# Apply PP hysteresis (2-of-3) on PP rows only\n",
        "tok = sub['contact_id'].str.split('_', n=4, expand=True)\n",
        "tok.columns = ['g1','g2','step','a','b']\n",
        "tok['game_play'] = tok['g1'] + '_' + tok['g2']\n",
        "tok['step'] = tok['step'].astype(int)\n",
        "is_g = tok['b'] == 'G'\n",
        "pp_mask = (~is_g) & (tok['a'] != 'G')\n",
        "pp_df = pd.DataFrame({'game_play': tok['game_play'], 'p1': tok['a'], 'p2': tok['b'], 'step': tok['step'], 'contact': sub['contact'].values})\n",
        "pp = pp_df.loc[pp_mask, ['game_play','p1','p2','step','contact']].copy().sort_values(['game_play','p1','p2','step'])\n",
        "grp_pp = pp.groupby(['game_play','p1','p2'], sort=False)['contact']\n",
        "pp['contact_hyst'] = grp_pp.transform(lambda s: (s.rolling(3, center=True, min_periods=1).sum() >= 2).astype(int))\n",
        "pp_key = (pp['game_play'] + '_' + pp['step'].astype(str) + '_' + pp['p1'] + '_' + pp['p2'])\n",
        "sub_key = (tok['game_play'] + '_' + tok['step'].astype(str) + '_' + tok['a'] + '_' + tok['b'])\n",
        "map_h = pd.Series(pp['contact_hyst'].values, index=pp_key.values)\n",
        "sub_h = sub_key.map(map_h)\n",
        "before_ones_all = int(sub['contact'].sum())\n",
        "sub.loc[pp_mask, 'contact'] = sub_h.loc[pp_mask].fillna(sub.loc[pp_mask, 'contact']).astype(int)\n",
        "after_ones_all = int(sub['contact'].sum())\n",
        "print(f'Applied PP hysteresis. ones before={before_ones_all}, after={after_ones_all}, delta={after_ones_all-before_ones_all}')\n",
        "\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Blended submission saved. Total time {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version (blend r35+r40): 2.1.4\nRunning bagging for r=3.5 and r=4.0...\nPP bagging r3.5: load...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Using 50 features\n  r3.5 seed 42 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r3.5 seed 42 fold 0 29.7s; best_it=3094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r3.5 seed 42 fold 1 30.2s; best_it=3182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r3.5 seed 42 fold 2 32.0s; best_it=3270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r3.5 seed 42 fold 3 29.4s; best_it=2995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r3.5 seed 42 fold 4 27.2s; best_it=2873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r3.5 seed 42 test model 0 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r3.5 seed 42 test model 1 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r3.5 seed 42 test model 2 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r3.5 seed 42 test model 3 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r3.5 seed 42 test model 4 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  r3.5 seed 1337 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r3.5 seed 1337 fold 0 30.9s; best_it=3211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r3.5 seed 1337 fold 1 31.8s; best_it=3291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r3.5 seed 1337 fold 2 33.3s; best_it=3408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r3.5 seed 1337 fold 3 29.7s; best_it=2955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r3.5 seed 1337 fold 4 27.0s; best_it=2747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r3.5 seed 1337 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r3.5 seed 1337 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r3.5 seed 1337 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r3.5 seed 1337 test model 3 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r3.5 seed 1337 test model 4 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  r3.5 seed 2025 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r3.5 seed 2025 fold 0 28.9s; best_it=3011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r3.5 seed 2025 fold 1 32.2s; best_it=3338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r3.5 seed 2025 fold 2 31.3s; best_it=3189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r3.5 seed 2025 fold 3 29.3s; best_it=2925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r3.5 seed 2025 fold 4 28.6s; best_it=2898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r3.5 seed 2025 test model 0 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r3.5 seed 2025 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r3.5 seed 2025 test model 2 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r3.5 seed 2025 test model 3 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r3.5 seed 2025 test model 4 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP bagging r4.0: load...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Using 50 features\n  r4.0 seed 42 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r4.0 seed 42 fold 0 35.0s; best_it=3253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r4.0 seed 42 fold 1 39.9s; best_it=3632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r4.0 seed 42 fold 2 38.1s; best_it=3326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r4.0 seed 42 fold 3 37.7s; best_it=3446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r4.0 seed 42 fold 4 37.8s; best_it=3468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r4.0 seed 42 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r4.0 seed 42 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r4.0 seed 42 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r4.0 seed 42 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r4.0 seed 42 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  r4.0 seed 1337 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r4.0 seed 1337 fold 0 37.7s; best_it=3385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r4.0 seed 1337 fold 1 39.9s; best_it=3608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r4.0 seed 1337 fold 2 36.1s; best_it=3140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r4.0 seed 1337 fold 3 38.1s; best_it=3378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r4.0 seed 1337 fold 4 38.8s; best_it=3609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r4.0 seed 1337 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r4.0 seed 1337 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r4.0 seed 1337 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r4.0 seed 1337 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r4.0 seed 1337 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  r4.0 seed 2025 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r4.0 seed 2025 fold 0 38.3s; best_it=3453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r4.0 seed 2025 fold 1 39.1s; best_it=3408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r4.0 seed 2025 fold 2 36.0s; best_it=3284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r4.0 seed 2025 fold 3 40.9s; best_it=3573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   r4.0 seed 2025 fold 4 37.6s; best_it=3388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r4.0 seed 2025 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r4.0 seed 2025 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r4.0 seed 2025 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r4.0 seed 2025 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    r4.0 seed 2025 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blended OOF MCC=0.72128 | thr_same=0.7433, thr_opp=0.7328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP blended (r35*0.6 + r40*0.4) ones before G overwrite: 7044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied prior G overwrite. ones after=9096, delta=2052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied PP hysteresis. ones before=9096, after=9121, delta=25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blended submission saved. Total time 1043.1s\n"
          ]
        }
      ]
    },
    {
      "id": "72f1c95c-2709-42ee-9d76-c62eaf1f5b70",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# G head with \u00b12 label expansion; smooth + 2-of-3 hysteresis; overwrite *_G rows in submission\n",
        "import time, math, sys, subprocess, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost for G head...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.4'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version (G head \u00b12):', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "t0 = time.time()\n",
        "print('G head \u00b12: building per-player features (reuse fast pipeline) ...')\n",
        "\n",
        "# Base tracking\n",
        "trk_cols = ['game_play','step','nfl_player_id','team','position','x_position','y_position','speed','acceleration','direction','orientation']\n",
        "tr_trk = pd.read_csv('train_player_tracking.csv', usecols=trk_cols).copy()\n",
        "te_trk = pd.read_csv('test_player_tracking.csv', usecols=trk_cols).copy()\n",
        "for df in (tr_trk, te_trk):\n",
        "    df['nfl_player_id'] = df['nfl_player_id'].astype(int).astype(str)\n",
        "\n",
        "def circ_diff_deg(a, b):\n",
        "    d = (a - b + 180.0) % 360.0 - 180.0\n",
        "    return np.abs(d)\n",
        "\n",
        "def build_player_dyn(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.sort_values(['game_play','nfl_player_id','step']).copy()\n",
        "    grp = df.groupby(['game_play','nfl_player_id'], sort=False)\n",
        "    df['d_speed_1'] = grp['speed'].diff(1)\n",
        "    df['d_speed_3'] = df['speed'] - grp['speed'].shift(3)\n",
        "    df['d_accel_1'] = grp['acceleration'].diff(1)\n",
        "    df['jerk'] = grp['acceleration'].diff(1)\n",
        "    for col in ['speed','acceleration']:\n",
        "        s = grp[col]\n",
        "        df[f'{col}_min_p3'] = s.rolling(3, min_periods=1).min().reset_index(level=[0,1], drop=True)\n",
        "        df[f'{col}_mean_p3'] = s.rolling(3, min_periods=1).mean().reset_index(level=[0,1], drop=True)\n",
        "        df[f'{col}_std_p3'] = s.rolling(3, min_periods=1).std().reset_index(level=[0,1], drop=True)\n",
        "        df[f'{col}_min_p5'] = s.rolling(5, min_periods=1).min().reset_index(level=[0,1], drop=True)\n",
        "        df[f'{col}_mean_p5'] = s.rolling(5, min_periods=1).mean().reset_index(level=[0,1], drop=True)\n",
        "        df[f'{col}_std_p5'] = s.rolling(5, min_periods=1).std().reset_index(level=[0,1], drop=True)\n",
        "    df['dir_orient_diff'] = circ_diff_deg(df['direction'].fillna(0.0), df['orientation'].fillna(0.0))\n",
        "    df['dist_to_sideline'] = np.minimum(df['y_position'], 53.3 - df['y_position'])\n",
        "    df['near_sideline'] = ((df['y_position'] <= 2.0) | (df['y_position'] >= 51.3)).astype(int)\n",
        "    df['near_goal'] = ((df['x_position'] <= 3.0) | (df['x_position'] >= 117.0)).astype(int)\n",
        "    for c in ['d_speed_1','d_speed_3','d_accel_1','jerk','speed_std_p3','speed_std_p5','acceleration_std_p3','acceleration_std_p5']:\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].fillna(0.0)\n",
        "    return df\n",
        "\n",
        "tr_p = build_player_dyn(tr_trk)\n",
        "te_p = build_player_dyn(te_trk)\n",
        "\n",
        "# Opponent context from r=3.5 pairs (prebuilt files exist)\n",
        "tr_pairs = pd.read_parquet('train_pairs_r35.parquet')\n",
        "te_pairs = pd.read_parquet('test_pairs_r35.parquet')\n",
        "\n",
        "def pairs_to_player_ctx(pairs: pd.DataFrame) -> pd.DataFrame:\n",
        "    a = pairs[['game_play','step','p1','distance']].rename(columns={'p1':'nfl_player_id'})\n",
        "    b = pairs[['game_play','step','p2','distance']].rename(columns={'p2':'nfl_player_id'})\n",
        "    u = pd.concat([a, b], ignore_index=True)\n",
        "    g = u.groupby(['game_play','step','nfl_player_id'], sort=False)\n",
        "    out = g['distance'].agg(min_opp_dist='min').reset_index()\n",
        "    for thr, name in [(1.5,'lt15'), (2.0,'lt20'), (2.5,'lt25')]:\n",
        "        u[name] = (u['distance'] < thr).astype(int)\n",
        "        cnt = u.groupby(['game_play','step','nfl_player_id'], sort=False)[name].sum().rename(f'cnt_opp_{name}')\n",
        "        out = out.merge(cnt.reset_index(), on=['game_play','step','nfl_player_id'], how='left')\n",
        "    return out\n",
        "\n",
        "tr_ctx = pairs_to_player_ctx(tr_pairs)\n",
        "te_ctx = pairs_to_player_ctx(te_pairs)\n",
        "\n",
        "# Helmet per-player aggregates and deltas\n",
        "train_helm = pd.read_csv('train_baseline_helmets.csv')\n",
        "test_helm = pd.read_csv('test_baseline_helmets.csv')\n",
        "train_vmeta = pd.read_csv('train_video_metadata.csv')\n",
        "test_vmeta = pd.read_csv('test_video_metadata.csv')\n",
        "FPS = 59.94\n",
        "def prep_meta(vmeta: pd.DataFrame):\n",
        "    vm = vmeta.copy()\n",
        "    for c in ['start_time','snap_time']:\n",
        "        if not np.issubdtype(vm[c].dtype, np.number):\n",
        "            ts = pd.to_datetime(vm[c], errors='coerce')\n",
        "            vm[c] = (ts - ts.dt.floor('D')).dt.total_seconds().astype(float)\n",
        "    vm['snap_frame'] = ((vm['snap_time'] - vm['start_time']) * FPS).round().astype('Int64')\n",
        "    return vm[['game_play','view','snap_frame']].drop_duplicates()\n",
        "meta_tr = prep_meta(train_vmeta)\n",
        "meta_te = prep_meta(test_vmeta)\n",
        "\n",
        "def helm_player_agg(helm: pd.DataFrame, meta: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = helm[['game_play','view','frame','nfl_player_id','left','top','width','height']].copy()\n",
        "    df = df.dropna(subset=['nfl_player_id'])\n",
        "    df['nfl_player_id'] = df['nfl_player_id'].astype(int).astype(str)\n",
        "    df['area'] = df['width'] * df['height']\n",
        "    df['cx'] = df['left'] + 0.5 * df['width']\n",
        "    df['cy'] = df['top'] + 0.5 * df['height']\n",
        "    df = df.sort_values(['game_play','view','frame','nfl_player_id','area'], ascending=[True,True,True,True,False]).drop_duplicates(['game_play','view','frame','nfl_player_id'], keep='first')\n",
        "    df = df.merge(meta, on=['game_play','view'], how='left')\n",
        "    df['step'] = ((df['frame'] - df['snap_frame']).astype('float') / 6.0).round().astype('Int64')\n",
        "    df = df.dropna(subset=['step']); df['step'] = df['step'].astype(int)\n",
        "    # expand \u00b11 to align tolerance with steps\n",
        "    dm1 = df.copy(); dm1['target_step'] = dm1['step'] - 1\n",
        "    d0 = df.copy(); d0['target_step'] = d0['step']\n",
        "    dp1 = df.copy(); dp1['target_step'] = df['step'] + 1\n",
        "    d = pd.concat([dm1, d0, dp1], ignore_index=True)\n",
        "    agg = d.groupby(['game_play','target_step','nfl_player_id'], sort=False).agg(\n",
        "        cy_mean=('cy','mean'), h_mean=('height','mean'), cnt=('cx','size')\n",
        "    ).reset_index().rename(columns={'target_step':'step'})\n",
        "    agg = agg.sort_values(['game_play','nfl_player_id','step'])\n",
        "    g = agg.groupby(['game_play','nfl_player_id'], sort=False)\n",
        "    agg['d_cy_1'] = g['cy_mean'].diff(1).fillna(0.0)\n",
        "    agg['d_h_1'] = g['h_mean'].diff(1).fillna(0.0)\n",
        "    return agg\n",
        "\n",
        "h_tr_p = helm_player_agg(train_helm, meta_tr)\n",
        "h_te_p = helm_player_agg(test_helm, meta_te)\n",
        "\n",
        "def merge_all(base: pd.DataFrame, ctx: pd.DataFrame, helm: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = base.merge(ctx, on=['game_play','step','nfl_player_id'], how='left')\n",
        "    df = df.merge(helm, on=['game_play','step','nfl_player_id'], how='left')\n",
        "    for c in ['min_opp_dist','cnt_opp_lt15','cnt_opp_lt20','cnt_opp_lt25','cy_mean','h_mean','d_cy_1','d_h_1']:\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].fillna(0.0)\n",
        "    return df\n",
        "\n",
        "tr_feat_p = merge_all(tr_p, tr_ctx, h_tr_p)\n",
        "te_feat_p = merge_all(te_p, te_ctx, h_te_p)\n",
        "print('Per-player train/test feature shapes:', tr_feat_p.shape, te_feat_p.shape)\n",
        "\n",
        "# Supervision for G with \u00b12 expansion\n",
        "labels = pd.read_csv('train_labels.csv', usecols=['contact_id','game_play','step','nfl_player_id_1','nfl_player_id_2','contact'])\n",
        "labels['pid1'] = labels['nfl_player_id_1'].astype(str); labels['pid2'] = labels['nfl_player_id_2'].astype(str)\n",
        "mask_g = (labels['pid1'] == 'G') | (labels['pid2'] == 'G')\n",
        "g_labels = labels.loc[mask_g, ['game_play','step','pid1','pid2','contact']].copy()\n",
        "g_labels['player'] = np.where(g_labels['pid1'] == 'G', g_labels['pid2'], g_labels['pid1'])\n",
        "g_labels = g_labels[['game_play','step','player','contact']]\n",
        "sup_g = g_labels.merge(tr_feat_p.rename(columns={'nfl_player_id':'player'}), on=['game_play','step','player'], how='inner')\n",
        "print('G supervised inner shape:', sup_g.shape, 'pos rate:', sup_g['contact'].mean())\n",
        "pos = sup_g.loc[sup_g['contact'] == 1, ['game_play','step','player']]\n",
        "ex = [pos.assign(step=pos['step'] + d) for d in (-2,-1,1,2)]\n",
        "pos_exp = pd.concat(ex, ignore_index=True).drop_duplicates()\n",
        "pos_exp['flag_pos_exp'] = 1\n",
        "sup_g = sup_g.merge(pos_exp, on=['game_play','step','player'], how='left')\n",
        "sup_g.loc[sup_g['flag_pos_exp'] == 1, 'contact'] = 1\n",
        "sup_g.drop(columns=['flag_pos_exp'], inplace=True)\n",
        "print('G after \u00b12 expansion pos rate:', sup_g['contact'].mean())\n",
        "\n",
        "# Train XGB with GKF\n",
        "drop_cols = {'contact','game_play','step','player','team','position','nfl_player_id'}\n",
        "feat_cols = [c for c in sup_g.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(sup_g[c])]\n",
        "print('G feature count:', len(feat_cols))\n",
        "X_all = sup_g[feat_cols].astype(float).values\n",
        "y_all = sup_g['contact'].astype(int).values\n",
        "groups = sup_g['game_play'].values\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "oof = np.full(len(sup_g), np.nan, float)\n",
        "models = []\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "    t1 = time.time()\n",
        "    X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "    X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "    neg = (y_tr == 0).sum(); posc = (y_tr == 1).sum()\n",
        "    spw = max(1.0, neg / max(1, posc))\n",
        "    print(f'G\u00b12 Fold {fold}: train {len(tr_idx)} (pos {posc}), valid {len(va_idx)} (pos {(y_va==1).sum()}), spw={spw:.2f}', flush=True)\n",
        "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
        "    dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "    params = {'tree_method':'hist','device':'cuda','max_depth':6,'eta':0.05,'subsample':0.9,'colsample_bytree':0.8,\n",
        "              'min_child_weight':10,'lambda':1.5,'alpha':0.0,'objective':'binary:logistic','eval_metric':'logloss',\n",
        "              'scale_pos_weight': float(spw), 'seed': 2025 + fold}\n",
        "    booster = xgb.train(params, dtrain, num_boost_round=2000, evals=[(dtrain,'train'),(dvalid,'valid')], early_stopping_rounds=100, verbose_eval=False)\n",
        "    best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "    oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "    models.append((booster, best_it))\n",
        "    print(f' G\u00b12 Fold {fold} done in {time.time()-t1:.1f}s; best_it={best_it}', flush=True)\n",
        "\n",
        "# Smooth OOF (roll-max(5)) and apply 2-of-3 hysteresis for evaluation thresholding\n",
        "oof_df = sup_g[['game_play','player','step']].copy()\n",
        "oof_df['oof'] = oof\n",
        "oof_df = oof_df.sort_values(['game_play','player','step'])\n",
        "grp_o = oof_df.groupby(['game_play','player'], sort=False)\n",
        "oof_df['oof_smooth'] = grp_o['oof'].transform(lambda s: s.rolling(5, center=True, min_periods=1).max())\n",
        "oof_smooth = oof_df['oof_smooth'].values\n",
        "y_sorted = sup_g.loc[oof_df.index, 'contact'].astype(int).values\n",
        "\n",
        "def apply_min_dur(bin_arr, gp, pl):\n",
        "    df = pd.DataFrame({'gp': gp, 'pl': pl, 'b': bin_arr})\n",
        "    df = df.groupby(['gp','pl'], sort=False)['b'].apply(lambda s: (s.rolling(3, center=True, min_periods=1).sum() >= 2).astype(int))\n",
        "    return df.values\n",
        "\n",
        "best_thr, best_mcc = 0.6, -1.0\n",
        "thr_grid = np.linspace(0.4, 0.85, 46)\n",
        "gp_arr = oof_df['game_play'].values\n",
        "pl_arr = oof_df['player'].values\n",
        "for thr in thr_grid:\n",
        "    pred0 = (oof_smooth >= thr).astype(int)\n",
        "    pred = apply_min_dur(pred0, gp_arr, pl_arr)\n",
        "    m = matthews_corrcoef(y_sorted, pred)\n",
        "    if m > best_mcc:\n",
        "        best_mcc, best_thr = float(m), float(thr)\n",
        "print(f'G\u00b12 OOF MCC={best_mcc:.5f} at thr={best_thr:.2f}')\n",
        "\n",
        "# Inference on test\n",
        "Xt = te_feat_p[feat_cols].astype(float).values\n",
        "dtest = xgb.DMatrix(Xt)\n",
        "pt = np.zeros(len(te_feat_p), dtype=float)\n",
        "for i, (booster, best_it) in enumerate(models):\n",
        "    t1 = time.time()\n",
        "    pt += booster.predict(dtest, iteration_range=(0, best_it + 1))\n",
        "    print(f' G\u00b12 Inference model {i} took {time.time()-t1:.1f}s')\n",
        "pt /= max(1, len(models))\n",
        "pred_tmp = te_feat_p[['game_play','step','nfl_player_id']].rename(columns={'nfl_player_id':'player'}).copy()\n",
        "pred_tmp['prob'] = pt\n",
        "pred_tmp = pred_tmp.sort_values(['game_play','player','step'])\n",
        "grp_t = pred_tmp.groupby(['game_play','player'], sort=False)\n",
        "pred_tmp['prob_smooth'] = grp_t['prob'].transform(lambda s: s.rolling(5, center=True, min_periods=1).max())\n",
        "bin0 = (pred_tmp['prob_smooth'].values >= best_thr).astype(int)\n",
        "bin1 = apply_min_dur(bin0, pred_tmp['game_play'].values, pred_tmp['player'].values)\n",
        "pred_tmp['pred_bin'] = bin1.astype(int)\n",
        "\n",
        "# Build G contact_id with player_G (second token is G) and overwrite submission\n",
        "g_cid_second = (pred_tmp['game_play'].astype(str) + '_' + pred_tmp['step'].astype(str) + '_' + pred_tmp['player'].astype(str) + '_G')\n",
        "g_pred_second = pd.DataFrame({'contact_id': g_cid_second, 'contact': pred_tmp['pred_bin'].astype(int)})\n",
        "\n",
        "sub = pd.read_csv('submission.csv')\n",
        "before_ones = int(sub['contact'].sum())\n",
        "sub = sub.drop(columns=['contact']).merge(g_pred_second, on='contact_id', how='left').merge(pd.read_csv('submission.csv'), on='contact_id', how='left', suffixes=('_g','_pp'))\n",
        "sub['contact'] = sub['contact_g'].fillna(sub['contact_pp']).astype(int)\n",
        "sub = sub[['contact_id','contact']]\n",
        "after_ones = int(sub['contact'].sum())\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'G\u00b12 overwrite done. ones before={before_ones}, after={after_ones}, delta={after_ones-before_ones}')\n",
        "print('G\u00b12 head done in {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version (G head \u00b12): 2.1.4\nG head \u00b12: building per-player features (reuse fast pipeline) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-player train/test feature shapes: (1225299, 40) (127754, 40)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G supervised inner shape: (370351, 41) pos rate: 0.041106949893479426\nG after \u00b12 expansion pos rate: 0.04628312060720776\nG feature count: 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G\u00b12 Fold 0: train 296428 (pos 13560), valid 73923 (pos 3581), spw=20.86\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " G\u00b12 Fold 0 done in 9.2s; best_it=1475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G\u00b12 Fold 1: train 296519 (pos 13800), valid 73832 (pos 3341), spw=20.49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " G\u00b12 Fold 1 done in 8.4s; best_it=1352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G\u00b12 Fold 2: train 296365 (pos 14505), valid 73986 (pos 2636), spw=19.43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " G\u00b12 Fold 2 done in 8.2s; best_it=1344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G\u00b12 Fold 3: train 296365 (pos 13238), valid 73986 (pos 3903), spw=21.39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " G\u00b12 Fold 3 done in 9.1s; best_it=1472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G\u00b12 Fold 4: train 295727 (pos 13461), valid 74624 (pos 3680), spw=20.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " G\u00b12 Fold 4 done in 9.2s; best_it=1488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G\u00b12 OOF MCC=0.53988 at thr=0.80\n G\u00b12 Inference model 0 took 0.0s\n G\u00b12 Inference model 1 took 0.0s\n G\u00b12 Inference model 2 took 0.0s\n G\u00b12 Inference model 3 took 0.0s\n G\u00b12 Inference model 4 took 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G\u00b12 overwrite done. ones before=9121, after=8903, delta=-218\nG\u00b12 head done in 146.4s\n"
          ]
        }
      ]
    },
    {
      "id": "6bb3bfc7-49c1-4ab3-bdd3-277d50e97058",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PP r=4.0 bagging with per-step cap=2 per player (on smoothed probs) -> dual thresholds -> 2-of-3 hysteresis; keep prior G overwrite\n",
        "import time, numpy as np, pandas as pd, sys, subprocess\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.4'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version (pp-bag-r40-cap2):', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def fast_dual_threshold_mcc(y_true, prob, same_flag, grid_points=256):\n",
        "    import numpy as np\n",
        "    y = np.asarray(y_true, dtype=np.int64)\n",
        "    p = np.asarray(prob, dtype=np.float64)\n",
        "    s = np.asarray(same_flag, dtype=np.int8)\n",
        "    mask = np.isfinite(y) & np.isfinite(p) & np.isfinite(s)\n",
        "    y, p, s = y[mask], p[mask], s[mask]\n",
        "    def cohort_counts(yc, pc, G):\n",
        "        n = yc.size\n",
        "        if n == 0:\n",
        "            return dict(tp=np.array([0], np.float64), fp=np.array([0], np.float64), tn=np.array([0], np.float64), fn=np.array([0], np.float64), thr=np.array([1.0], np.float64))\n",
        "        order = np.argsort(-pc, kind='mergesort')\n",
        "        ys, ps = yc[order], pc[order]\n",
        "        P = float(ys.sum()); N = float(n - ys.sum())\n",
        "        step = max(1, n // max(1, (G - 1)))\n",
        "        k = np.arange(0, n + 1, step, dtype=np.int64)\n",
        "        if k[-1] != n: k = np.append(k, n)\n",
        "        cum = np.concatenate(([0], np.cumsum(ys, dtype=np.int64)))\n",
        "        tp = cum[k].astype(np.float64); fp = (k - cum[k]).astype(np.float64)\n",
        "        fn = P - tp; tn = N - fp\n",
        "        thr = np.where(k == 0, 1.0 + 1e-6, ps[np.maximum(0, k - 1)])\n",
        "        return dict(tp=tp, fp=fp, tn=tn, fn=fn, thr=thr)\n",
        "    a = cohort_counts(y[s == 0], p[s == 0], grid_points)\n",
        "    b = cohort_counts(y[s == 1], p[s == 1], grid_points)\n",
        "    tp = a['tp'][:, None] + b['tp'][None, :]\n",
        "    fp = a['fp'][:, None] + b['fp'][None, :]\n",
        "    tn = a['tn'][:, None] + b['tn'][None, :]\n",
        "    fn = a['fn'][:, None] + b['fn'][None, :]\n",
        "    with np.errstate(invalid='ignore', divide='ignore'):\n",
        "        num = tp * tn - fp * fn\n",
        "        den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
        "        den = np.where(den > 0, np.sqrt(den), np.nan)\n",
        "        mcc = num / den\n",
        "    if not np.isfinite(mcc).any():\n",
        "        return -1.0, 0.79, 0.79\n",
        "    i, j = np.unravel_index(np.nanargmax(mcc), mcc.shape)\n",
        "    return float(mcc[i, j]), float(a['thr'][i]), float(b['thr'][j])\n",
        "\n",
        "t0 = time.time()\n",
        "print('PP bagging r=4.0 with cap=2: loading artifacts...')\n",
        "train_sup = pd.read_parquet('train_supervised_w5_helm_dyn_r40.parquet')\n",
        "test_feats = pd.read_parquet('test_pairs_w5_helm_dyn_r40.parquet')\n",
        "folds_df = pd.read_csv('folds_game_play.csv')\n",
        "train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "assert train_sup['fold'].notna().all()\n",
        "for df in (train_sup, test_feats):\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "\n",
        "drop_cols = {'contact','game_play','step','p1','p2','team1','team2','pos1','pos2','fold'}\n",
        "feat_cols = [c for c in train_sup.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_sup[c])]\n",
        "print('Using', len(feat_cols), 'features')\n",
        "\n",
        "# Canonical order\n",
        "ord_idx = train_sup[['game_play','p1','p2','step']].sort_values(['game_play','p1','p2','step']).index.to_numpy()\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train_sup['game_play'].values\n",
        "y_all = train_sup['contact'].astype(int).values\n",
        "same_all = train_sup['same_team'].fillna(0).astype(int).values if 'same_team' in train_sup.columns else np.zeros(len(train_sup), np.int8)\n",
        "seeds = [42,1337,2025]\n",
        "oof_s_list = []; test_s_list = []\n",
        "\n",
        "for s in seeds:\n",
        "    print(f' PP r=4.0 seed {s} ...', flush=True)\n",
        "    X_all = train_sup[feat_cols].astype(float).values\n",
        "    oof = np.full(len(train_sup), np.nan, float)\n",
        "    models = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "        t1 = time.time()\n",
        "        X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "        X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "        neg = (y_tr == 0).sum(); posc = (y_tr == 1).sum()\n",
        "        spw = max(1.0, neg / max(1, posc))\n",
        "        dtrain = xgb.DMatrix(X_tr, label=y_tr); dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "        params = {'tree_method':'hist','device':'cuda','max_depth':7,'eta':0.05,'subsample':0.9,'colsample_bytree':0.8,\n",
        "                  'min_child_weight':10,'lambda':1.5,'alpha':0.1,'gamma':0.1,'objective':'binary:logistic','eval_metric':'logloss',\n",
        "                  'scale_pos_weight': float(spw), 'seed': int(s + fold)}\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=3800, evals=[(dtrain,'train'),(dvalid,'valid')], early_stopping_rounds=200, verbose_eval=False)\n",
        "        best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "        oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "        models.append((booster, best_it))\n",
        "        print(f'   seed {s} fold {fold} done in {time.time()-t1:.1f}s; best_it={best_it}', flush=True)\n",
        "    # Smooth OOF on canonical order\n",
        "    df = train_sup[['game_play','p1','p2','step']].iloc[ord_idx].copy()\n",
        "    df['oof'] = oof[ord_idx]\n",
        "    df = df.sort_values(['game_play','p1','p2','step'])\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    df['oof_smooth'] = grp['oof'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    oof_s_list.append(df['oof_smooth'].to_numpy())\n",
        "\n",
        "    # Test predictions per seed\n",
        "    Xt = test_feats[feat_cols].astype(float).values\n",
        "    dtest = xgb.DMatrix(Xt)\n",
        "    pt = np.zeros(len(test_feats), float)\n",
        "    for i, (booster, best_it) in enumerate(models):\n",
        "        t1 = time.time(); pt += booster.predict(dtest, iteration_range=(0, best_it + 1));\n",
        "        print(f'    seed {s} test model {i} {time.time()-t1:.1f}s', flush=True)\n",
        "    pt /= max(1, len(models))\n",
        "    dt = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step'])\n",
        "    dt['prob'] = pt[dt.index.values]\n",
        "    grp_t = dt.groupby(['game_play','p1','p2'], sort=False)\n",
        "    dt['prob_smooth'] = grp_t['prob'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    test_s_list.append(dt['prob_smooth'].to_numpy())\n",
        "\n",
        "# Average OOF across seeds\n",
        "oof_avg = np.mean(np.vstack(oof_s_list), axis=0)\n",
        "y_sorted = train_sup['contact'].astype(int).to_numpy()[ord_idx]\n",
        "same_sorted = train_sup['same_team'].fillna(0).astype(int).to_numpy()[ord_idx] if 'same_team' in train_sup.columns else np.zeros_like(y_sorted, np.int8)\n",
        "best_mcc, thr_opp, thr_same = fast_dual_threshold_mcc(y_sorted, oof_avg, same_sorted, grid_points=256)\n",
        "if (not np.isfinite(best_mcc)) or best_mcc < 0:\n",
        "    thrs = np.linspace(0.7, 0.85, 31)\n",
        "    m_list = [matthews_corrcoef(y_sorted, (oof_avg >= t).astype(int)) for t in thrs]\n",
        "    j = int(np.argmax(m_list)); best_mcc = float(m_list[j]); thr_opp = thr_same = float(thrs[j])\n",
        "print(f'PP bagged r=4.0 OOF MCC={best_mcc:.5f} | thr_same={thr_same:.4f}, thr_opp={thr_opp:.4f}')\n",
        "\n",
        "# Average test probs, then apply cap=2 per (game_play, step, player) using smoothed probs\n",
        "pt_bag = np.mean(np.vstack(test_s_list), axis=0)\n",
        "df_t = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step']).reset_index(drop=True)\n",
        "df_t['prob_smooth'] = pt_bag\n",
        "df_t['row_id'] = np.arange(len(df_t))\n",
        "\n",
        "# Build long frame for players (both sides) to rank and keep top-2 per player-step\n",
        "long1 = df_t[['game_play','step','p1','prob_smooth','row_id']].rename(columns={'p1':'player','prob_smooth':'prob'})\n",
        "long2 = df_t[['game_play','step','p2','prob_smooth','row_id']].rename(columns={'p2':'player','prob_smooth':'prob'})\n",
        "df_long = pd.concat([long1, long2], ignore_index=True)\n",
        "df_long = df_long.sort_values(['game_play','step','player','prob'], ascending=[True, True, True, False])\n",
        "df_long['rank'] = df_long.groupby(['game_play','step','player'], sort=False)['prob'].rank(method='first', ascending=False)\n",
        "kept_rows = set(df_long.loc[df_long['rank'] <= 2, 'row_id'].tolist())\n",
        "keep_mask = df_t['row_id'].isin(kept_rows).to_numpy()\n",
        "df_t.loc[~keep_mask, 'prob_smooth'] = 0.0\n",
        "print('Applied cap=2 per player-step. Kept rows:', int(keep_mask.sum()), 'out of', len(keep_mask))\n",
        "\n",
        "# Threshold by same_team\n",
        "same_flag_test = test_feats[['game_play','p1','p2','step','same_team']].copy()\n",
        "same_flag_test = same_flag_test.merge(df_t[['game_play','p1','p2','step','row_id']], on=['game_play','p1','p2','step'], how='right').sort_values('row_id')\n",
        "same_flag_arr = same_flag_test['same_team'].fillna(0).astype(int).to_numpy() if 'same_team' in same_flag_test.columns else np.zeros(len(df_t), int)\n",
        "thr_arr_test = np.where(same_flag_arr == 1, thr_same, thr_opp)\n",
        "df_t['pred_bin'] = (df_t['prob_smooth'].to_numpy() >= thr_arr_test).astype(int)\n",
        "\n",
        "# Apply 2-of-3 hysteresis per (gp,p1,p2)\n",
        "df_h = df_t[['game_play','p1','p2','step','pred_bin']].copy().sort_values(['game_play','p1','p2','step'])\n",
        "grp_h = df_h.groupby(['game_play','p1','p2'], sort=False)['pred_bin']\n",
        "df_h['pred_hyst'] = grp_h.transform(lambda s: (s.rolling(3, center=True, min_periods=1).sum() >= 2).astype(int))\n",
        "df_t = df_t.merge(df_h[['game_play','p1','p2','step','pred_hyst']], on=['game_play','p1','p2','step'], how='left')\n",
        "\n",
        "# Build submission from sample and PP preds (after hysteresis), then overwrite G from prior submission.csv\n",
        "cid_sorted = (df_t['game_play'].astype(str) + '_' + df_t['step'].astype(str) + '_' + df_t['p1'].astype(str) + '_' + df_t['p2'].astype(str))\n",
        "pred_df_pp = pd.DataFrame({'contact_id': cid_sorted.values, 'contact_pp': df_t['pred_hyst'].astype(int).values})\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.merge(pred_df_pp, on='contact_id', how='left')\n",
        "sub['contact'] = sub['contact_pp'].fillna(0).astype(int)\n",
        "sub = sub.drop(columns=['contact_pp'])\n",
        "pp_ones = int(sub['contact'].sum())\n",
        "print('PP (r40 bag+cap2+hyst) ones before G overwrite:', pp_ones)\n",
        "try:\n",
        "    prev_sub = pd.read_csv('submission.csv')\n",
        "    g_pred_second = prev_sub[prev_sub['contact_id'].str.endswith('_G')][['contact_id','contact']].rename(columns={'contact':'contact_g'})\n",
        "    sub = sub.merge(g_pred_second, on='contact_id', how='left')\n",
        "    sub['contact'] = sub['contact_g'].fillna(sub['contact']).astype(int)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "    after_ones = int(sub['contact'].sum())\n",
        "    print(f'Applied prior G overwrite. ones after={after_ones}, delta={after_ones-pp_ones}')\n",
        "except Exception as e:\n",
        "    print('No prior submission with G rows found; skipping G overwrite.', e)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv. Took {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version (pp-bag-r40-cap2): 2.1.4\nPP bagging r=4.0 with cap=2: loading artifacts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 50 features\n PP r=4.0 seed 42 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 0 done in 34.9s; best_it=3253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 1 done in 39.7s; best_it=3632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 2 done in 37.9s; best_it=3326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 3 done in 37.4s; best_it=3446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 4 done in 37.3s; best_it=3468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.0 seed 1337 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 0 done in 38.2s; best_it=3385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 1 done in 39.7s; best_it=3608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 2 done in 36.0s; best_it=3140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 3 done in 38.0s; best_it=3378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 4 done in 39.1s; best_it=3609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.0 seed 2025 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 0 done in 38.8s; best_it=3453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 1 done in 38.3s; best_it=3408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 2 done in 37.9s; best_it=3284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 3 done in 40.7s; best_it=3573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 4 done in 37.2s; best_it=3388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP bagged r=4.0 OOF MCC=0.72502 | thr_same=0.7626, thr_opp=0.7713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied cap=2 per player-step. Kept rows: 122291 out of 278492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP (r40 bag+cap2+hyst) ones before G overwrite: 6565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied prior G overwrite. ones after=8617, delta=2052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv. Took 587.3s\n"
          ]
        }
      ]
    },
    {
      "id": "439a8117-ef26-4667-9c26-0f7904cbc4de",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PP r=4.0 bagging with fold-median dual thresholds, matching CV post-proc to test (smooth -> cap2), then hysteresis and G overwrite\n",
        "import time, numpy as np, pandas as pd, sys, subprocess\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.4'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version (pp-bag-r40-fold-median):', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def fast_dual_threshold_mcc(y_true, prob, same_flag, grid_points=256):\n",
        "    import numpy as np\n",
        "    y = np.asarray(y_true, dtype=np.int64)\n",
        "    p = np.asarray(prob, dtype=np.float64)\n",
        "    s = np.asarray(same_flag, dtype=np.int8)\n",
        "    mask = np.isfinite(y) & np.isfinite(p) & np.isfinite(s)\n",
        "    y, p, s = y[mask], p[mask], s[mask]\n",
        "    def cohort_counts(yc, pc, G):\n",
        "        n = yc.size\n",
        "        if n == 0:\n",
        "            return dict(tp=np.array([0], np.float64), fp=np.array([0], np.float64), tn=np.array([0], np.float64), fn=np.array([0], np.float64), thr=np.array([1.0], np.float64))\n",
        "        order = np.argsort(-pc, kind='mergesort')\n",
        "        ys, ps = yc[order], pc[order]\n",
        "        P = float(ys.sum()); N = float(n - ys.sum())\n",
        "        step = max(1, n // max(1, (G - 1)))\n",
        "        k = np.arange(0, n + 1, step, dtype=np.int64)\n",
        "        if k[-1] != n: k = np.append(k, n)\n",
        "        cum = np.concatenate(([0], np.cumsum(ys, dtype=np.int64)))\n",
        "        tp = cum[k].astype(np.float64); fp = (k - cum[k]).astype(np.float64)\n",
        "        fn = P - tp; tn = N - fp\n",
        "        thr = np.where(k == 0, 1.0 + 1e-6, ps[np.maximum(0, k - 1)])\n",
        "        return dict(tp=tp, fp=fp, tn=tn, fn=fn, thr=thr)\n",
        "    a = cohort_counts(y[s == 0], p[s == 0], grid_points)\n",
        "    b = cohort_counts(y[s == 1], p[s == 1], grid_points)\n",
        "    tp = a['tp'][:, None] + b['tp'][None, :]\n",
        "    fp = a['fp'][:, None] + b['fp'][None, :]\n",
        "    tn = a['tn'][:, None] + b['tn'][None, :]\n",
        "    fn = a['fn'][:, None] + b['fn'][None, :]\n",
        "    with np.errstate(invalid='ignore', divide='ignore'):\n",
        "        num = tp * tn - fp * fn\n",
        "        den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
        "        den = np.where(den > 0, np.sqrt(den), np.nan)\n",
        "        mcc = num / den\n",
        "    if not np.isfinite(mcc).any():\n",
        "        return -1.0, 0.79, 0.79\n",
        "    i, j = np.unravel_index(np.nanargmax(mcc), mcc.shape)\n",
        "    return float(mcc[i, j]), float(a['thr'][i]), float(b['thr'][j])\n",
        "\n",
        "t0 = time.time()\n",
        "print('Fold-median thresholds run: loading r=4.0 supervised dyn and test features...')\n",
        "train_sup = pd.read_parquet('train_supervised_w5_helm_dyn_r40.parquet')\n",
        "test_feats = pd.read_parquet('test_pairs_w5_helm_dyn_r40.parquet')\n",
        "folds_df = pd.read_csv('folds_game_play.csv')\n",
        "train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "assert train_sup['fold'].notna().all()\n",
        "for df in (train_sup, test_feats):\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "\n",
        "drop_cols = {'contact','game_play','step','p1','p2','team1','team2','pos1','pos2','fold'}\n",
        "feat_cols = [c for c in train_sup.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_sup[c])]\n",
        "print('Using', len(feat_cols), 'features')\n",
        "\n",
        "# Canonical order\n",
        "ord_idx = train_sup[['game_play','p1','p2','step']].sort_values(['game_play','p1','p2','step']).index.to_numpy()\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train_sup['game_play'].values\n",
        "y_all = train_sup['contact'].astype(int).values\n",
        "same_all = train_sup['same_team'].fillna(0).astype(int).values if 'same_team' in train_sup.columns else np.zeros(len(train_sup), np.int8)\n",
        "fold_arr = train_sup['fold'].astype(int).to_numpy()\n",
        "\n",
        "seeds = [42,1337,2025]\n",
        "oof_s_list = []\n",
        "test_s_list = []\n",
        "\n",
        "for s in seeds:\n",
        "    print(f' PP r=4.0 seed {s} ...', flush=True)\n",
        "    X_all = train_sup[feat_cols].astype(float).values\n",
        "    oof = np.full(len(train_sup), np.nan, float)\n",
        "    models = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "        t1 = time.time()\n",
        "        X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "        X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "        neg = (y_tr == 0).sum(); posc = (y_tr == 1).sum()\n",
        "        spw = max(1.0, neg / max(1, posc))\n",
        "        dtrain = xgb.DMatrix(X_tr, label=y_tr); dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "        params = {'tree_method':'hist','device':'cuda','max_depth':7,'eta':0.05,'subsample':0.9,'colsample_bytree':0.8,\n",
        "                  'min_child_weight':10,'lambda':1.5,'alpha':0.1,'gamma':0.1,'objective':'binary:logistic','eval_metric':'logloss',\n",
        "                  'scale_pos_weight': float(spw), 'seed': int(s + fold)}\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=3800, evals=[(dtrain,'train'),(dvalid,'valid')], early_stopping_rounds=200, verbose_eval=False)\n",
        "        best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "        oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "        models.append((booster, best_it))\n",
        "        print(f'   seed {s} fold {fold} done in {time.time()-t1:.1f}s; best_it={best_it}', flush=True)\n",
        "    # Smooth OOF on canonical order\n",
        "    df = train_sup[['game_play','p1','p2','step']].iloc[ord_idx].copy()\n",
        "    df['oof'] = oof[ord_idx]\n",
        "    df = df.sort_values(['game_play','p1','p2','step'])\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    df['oof_smooth'] = grp['oof'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    oof_s_list.append(df['oof_smooth'].to_numpy())\n",
        "\n",
        "    # Test predictions per seed with smoothing\n",
        "    Xt = test_feats[feat_cols].astype(float).values\n",
        "    dtest = xgb.DMatrix(Xt)\n",
        "    pt = np.zeros(len(test_feats), float)\n",
        "    for i, (booster, best_it) in enumerate(models):\n",
        "        t1 = time.time(); pt += booster.predict(dtest, iteration_range=(0, best_it + 1));\n",
        "        print(f'    seed {s} test model {i} {time.time()-t1:.1f}s', flush=True)\n",
        "    pt /= max(1, len(models))\n",
        "    dt = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step'])\n",
        "    dt['prob'] = pt[dt.index.values]\n",
        "    grp_t = dt.groupby(['game_play','p1','p2'], sort=False)\n",
        "    dt['prob_smooth'] = grp_t['prob'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    test_s_list.append(dt['prob_smooth'].to_numpy())\n",
        "\n",
        "# Average OOF across seeds in canonical order\n",
        "oof_avg = np.mean(np.vstack(oof_s_list), axis=0)\n",
        "keys_tr_sorted = train_sup[['game_play','p1','p2','step']].iloc[ord_idx].copy().reset_index(drop=True)\n",
        "y_sorted = train_sup['contact'].astype(int).to_numpy()[ord_idx]\n",
        "same_sorted = train_sup['same_team'].fillna(0).astype(int).to_numpy()[ord_idx] if 'same_team' in train_sup.columns else np.zeros_like(y_sorted, np.int8)\n",
        "fold_sorted = fold_arr[ord_idx]\n",
        "\n",
        "# Apply cap=2 by (game_play, step, player) on smoothed OOF probs BEFORE thresholding\n",
        "df_o = keys_tr_sorted.copy()\n",
        "df_o['prob'] = oof_avg\n",
        "df_o['row_id'] = np.arange(len(df_o))\n",
        "long1 = df_o[['game_play','step','p1','prob','row_id']].rename(columns={'p1':'player'})\n",
        "long2 = df_o[['game_play','step','p2','prob','row_id']].rename(columns={'p2':'player'})\n",
        "df_long = pd.concat([long1, long2], ignore_index=True)\n",
        "df_long = df_long.sort_values(['game_play','step','player','prob'], ascending=[True, True, True, False])\n",
        "df_long['rank'] = df_long.groupby(['game_play','step','player'], sort=False)['prob'].rank(method='first', ascending=False)\n",
        "kept_rows = set(df_long.loc[df_long['rank'] <= 2, 'row_id'].tolist())\n",
        "keep_mask_all = df_o['row_id'].isin(kept_rows).to_numpy()\n",
        "oof_cap = oof_avg.copy()\n",
        "oof_cap[~keep_mask_all] = 0.0\n",
        "print('Applied cap=2 to OOF. Kept rows:', int(keep_mask_all.sum()), 'of', len(keep_mask_all))\n",
        "\n",
        "# Per-fold threshold optimization on capped OOF, then median across folds\n",
        "thr_opp_f = []; thr_same_f = []\n",
        "for k in sorted(np.unique(fold_sorted)):\n",
        "    m = (fold_sorted == k)\n",
        "    mcc_k, t_opp_k, t_same_k = fast_dual_threshold_mcc(y_sorted[m], oof_cap[m], same_sorted[m], grid_points=256)\n",
        "    if (not np.isfinite(mcc_k)) or mcc_k < 0:\n",
        "        thrs = np.linspace(0.7, 0.85, 31)\n",
        "        ml = [matthews_corrcoef(y_sorted[m], (oof_cap[m] >= t).astype(int)) for t in thrs]\n",
        "        j = int(np.argmax(ml)); t_opp_k = t_same_k = float(thrs[j])\n",
        "    thr_opp_f.append(float(t_opp_k)); thr_same_f.append(float(t_same_k))\n",
        "    print(f' Fold {k} thresholds: thr_opp={t_opp_k:.4f}, thr_same={t_same_k:.4f}')\n",
        "thr_opp = float(np.median(thr_opp_f)); thr_same = float(np.median(thr_same_f))\n",
        "print(f'Final fold-median thresholds: thr_opp={thr_opp:.4f}, thr_same={thr_same:.4f}')\n",
        "\n",
        "# Test: average probs across seeds, smooth, then cap=2, then apply median thresholds\n",
        "pt_bag = np.mean(np.vstack(test_s_list), axis=0)\n",
        "df_t = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step']).reset_index(drop=True)\n",
        "df_t['prob_smooth'] = pt_bag\n",
        "df_t['row_id'] = np.arange(len(df_t))\n",
        "long1t = df_t[['game_play','step','p1','prob_smooth','row_id']].rename(columns={'p1':'player','prob_smooth':'prob'})\n",
        "long2t = df_t[['game_play','step','p2','prob_smooth','row_id']].rename(columns={'p2':'player','prob_smooth':'prob'})\n",
        "df_long_t = pd.concat([long1t, long2t], ignore_index=True)\n",
        "df_long_t = df_long_t.sort_values(['game_play','step','player','prob'], ascending=[True, True, True, False])\n",
        "df_long_t['rank'] = df_long_t.groupby(['game_play','step','player'], sort=False)['prob'].rank(method='first', ascending=False)\n",
        "kept_rows_t = set(df_long_t.loc[df_long_t['rank'] <= 2, 'row_id'].tolist())\n",
        "keep_mask_t = df_t['row_id'].isin(kept_rows_t).to_numpy()\n",
        "df_t.loc[~keep_mask_t, 'prob_smooth'] = 0.0\n",
        "print('Applied cap=2 on test. Kept rows:', int(keep_mask_t.sum()), 'of', len(keep_mask_t))\n",
        "\n",
        "same_flag_test = test_feats[['game_play','p1','p2','step','same_team']].copy()\n",
        "same_flag_test = same_flag_test.merge(df_t[['game_play','p1','p2','step','row_id']], on=['game_play','p1','p2','step'], how='right').sort_values('row_id')\n",
        "same_flag_arr = same_flag_test['same_team'].fillna(0).astype(int).to_numpy() if 'same_team' in same_flag_test.columns else np.zeros(len(df_t), int)\n",
        "thr_arr_test = np.where(same_flag_arr == 1, thr_same, thr_opp)\n",
        "df_t['pred_bin'] = (df_t['prob_smooth'].to_numpy() >= thr_arr_test).astype(int)\n",
        "\n",
        "# Apply 2-of-3 hysteresis per (gp,p1,p2) on binaries\n",
        "df_h = df_t[['game_play','p1','p2','step','pred_bin']].copy().sort_values(['game_play','p1','p2','step'])\n",
        "grp_h = df_h.groupby(['game_play','p1','p2'], sort=False)['pred_bin']\n",
        "df_h['pred_hyst'] = grp_h.transform(lambda s: (s.rolling(3, center=True, min_periods=1).sum() >= 2).astype(int))\n",
        "df_t = df_t.merge(df_h[['game_play','p1','p2','step','pred_hyst']], on=['game_play','p1','p2','step'], how='left')\n",
        "\n",
        "# Build submission from sample and PP preds (after hysteresis), then overwrite G from prior submission.csv\n",
        "cid_sorted = (df_t['game_play'].astype(str) + '_' + df_t['step'].astype(str) + '_' + df_t['p1'].astype(str) + '_' + df_t['p2'].astype(str))\n",
        "pred_df_pp = pd.DataFrame({'contact_id': cid_sorted.values, 'contact_pp': df_t['pred_hyst'].astype(int).values})\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.merge(pred_df_pp, on='contact_id', how='left')\n",
        "sub['contact'] = sub['contact_pp'].fillna(0).astype(int)\n",
        "sub = sub.drop(columns=['contact_pp'])\n",
        "pp_ones = int(sub['contact'].sum())\n",
        "print('PP (r40 bag + fold-median thr + cap2 + hyst) ones before G overwrite:', pp_ones)\n",
        "try:\n",
        "    prev_sub = pd.read_csv('submission.csv')\n",
        "    g_pred_second = prev_sub[prev_sub['contact_id'].str.endswith('_G')][['contact_id','contact']].rename(columns={'contact':'contact_g'})\n",
        "    sub = sub.merge(g_pred_second, on='contact_id', how='left')\n",
        "    sub['contact'] = sub['contact_g'].fillna(sub['contact']).astype(int)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "    after_ones = int(sub['contact'].sum())\n",
        "    print(f'Applied prior G overwrite. ones after={after_ones}, delta={after_ones-pp_ones}')\n",
        "except Exception as e:\n",
        "    print('No prior submission with G rows found; skipping G overwrite.', e)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv. Took {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version (pp-bag-r40-fold-median): 2.1.4\nFold-median thresholds run: loading r=4.0 supervised dyn and test features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 50 features\n PP r=4.0 seed 42 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 0 done in 36.8s; best_it=3253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 1 done in 39.7s; best_it=3632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 2 done in 37.9s; best_it=3326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 3 done in 38.8s; best_it=3446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 4 done in 37.1s; best_it=3468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.0 seed 1337 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 0 done in 38.1s; best_it=3385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 1 done in 39.6s; best_it=3608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 2 done in 36.0s; best_it=3140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 3 done in 38.1s; best_it=3378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 4 done in 39.1s; best_it=3609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.0 seed 2025 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 0 done in 39.0s; best_it=3453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 1 done in 38.4s; best_it=3408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 2 done in 37.7s; best_it=3284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 3 done in 40.5s; best_it=3573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 4 done in 36.9s; best_it=3388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied cap=2 to OOF. Kept rows: 308871 of 634192\n Fold 0 thresholds: thr_opp=0.7721, thr_same=0.8034\n Fold 1 thresholds: thr_opp=0.8178, thr_same=0.7708\n Fold 2 thresholds: thr_opp=0.8713, thr_same=0.6954\n Fold 3 thresholds: thr_opp=0.7253, thr_same=0.7283\n Fold 4 thresholds: thr_opp=0.8006, thr_same=0.8531\nFinal fold-median thresholds: thr_opp=0.8006, thr_same=0.7708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied cap=2 on test. Kept rows: 122291 of 278492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP (r40 bag + fold-median thr + cap2 + hyst) ones before G overwrite: 6356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied prior G overwrite. ones after=8408, delta=2052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv. Took 590.4s\n"
          ]
        }
      ]
    },
    {
      "id": "ba9fa9cd-7770-4e35-bf44-a5662a71efb7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PP r=4.0 bagging with fold-median dual thresholds and cap=3 per player-step (smooth->cap3), then hysteresis and G overwrite\n",
        "import time, numpy as np, pandas as pd, sys, subprocess\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.4'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version (pp-bag-r40-fold-median-cap3):', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def fast_dual_threshold_mcc(y_true, prob, same_flag, grid_points=256):\n",
        "    import numpy as np\n",
        "    y = np.asarray(y_true, dtype=np.int64)\n",
        "    p = np.asarray(prob, dtype=np.float64)\n",
        "    s = np.asarray(same_flag, dtype=np.int8)\n",
        "    mask = np.isfinite(y) & np.isfinite(p) & np.isfinite(s)\n",
        "    y, p, s = y[mask], p[mask], s[mask]\n",
        "    def cohort_counts(yc, pc, G):\n",
        "        n = yc.size\n",
        "        if n == 0:\n",
        "            return dict(tp=np.array([0], np.float64), fp=np.array([0], np.float64), tn=np.array([0], np.float64), fn=np.array([0], np.float64), thr=np.array([1.0], np.float64))\n",
        "        order = np.argsort(-pc, kind='mergesort')\n",
        "        ys, ps = yc[order], pc[order]\n",
        "        P = float(ys.sum()); N = float(n - ys.sum())\n",
        "        step = max(1, n // max(1, (G - 1)))\n",
        "        k = np.arange(0, n + 1, step, dtype=np.int64)\n",
        "        if k[-1] != n: k = np.append(k, n)\n",
        "        cum = np.concatenate(([0], np.cumsum(ys, dtype=np.int64)))\n",
        "        tp = cum[k].astype(np.float64); fp = (k - cum[k]).astype(np.float64)\n",
        "        fn = P - tp; tn = N - fp\n",
        "        thr = np.where(k == 0, 1.0 + 1e-6, ps[np.maximum(0, k - 1)])\n",
        "        return dict(tp=tp, fp=fp, tn=tn, fn=fn, thr=thr)\n",
        "    a = cohort_counts(y[s == 0], p[s == 0], grid_points)\n",
        "    b = cohort_counts(y[s == 1], p[s == 1], grid_points)\n",
        "    tp = a['tp'][:, None] + b['tp'][None, :]\n",
        "    fp = a['fp'][:, None] + b['fp'][None, :]\n",
        "    tn = a['tn'][:, None] + b['tn'][None, :]\n",
        "    fn = a['fn'][:, None] + b['fn'][None, :]\n",
        "    with np.errstate(invalid='ignore', divide='ignore'):\n",
        "        num = tp * tn - fp * fn\n",
        "        den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
        "        den = np.where(den > 0, np.sqrt(den), np.nan)\n",
        "        mcc = num / den\n",
        "    if not np.isfinite(mcc).any():\n",
        "        return -1.0, 0.79, 0.79\n",
        "    i, j = np.unravel_index(np.nanargmax(mcc), mcc.shape)\n",
        "    return float(mcc[i, j]), float(a['thr'][i]), float(b['thr'][j])\n",
        "\n",
        "t0 = time.time()\n",
        "print('Fold-median thresholds (cap=3) run: loading r=4.0 supervised dyn and test features...')\n",
        "train_sup = pd.read_parquet('train_supervised_w5_helm_dyn_r40.parquet')\n",
        "test_feats = pd.read_parquet('test_pairs_w5_helm_dyn_r40.parquet')\n",
        "folds_df = pd.read_csv('folds_game_play.csv')\n",
        "train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "assert train_sup['fold'].notna().all()\n",
        "for df in (train_sup, test_feats):\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "\n",
        "drop_cols = {'contact','game_play','step','p1','p2','team1','team2','pos1','pos2','fold'}\n",
        "feat_cols = [c for c in train_sup.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_sup[c])]\n",
        "print('Using', len(feat_cols), 'features')\n",
        "\n",
        "ord_idx = train_sup[['game_play','p1','p2','step']].sort_values(['game_play','p1','p2','step']).index.to_numpy()\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train_sup['game_play'].values\n",
        "y_all = train_sup['contact'].astype(int).values\n",
        "same_all = train_sup['same_team'].fillna(0).astype(int).values if 'same_team' in train_sup.columns else np.zeros(len(train_sup), np.int8)\n",
        "fold_arr = train_sup['fold'].astype(int).to_numpy()\n",
        "\n",
        "seeds = [42,1337,2025]\n",
        "oof_s_list = []; test_s_list = []\n",
        "\n",
        "for s in seeds:\n",
        "    print(f' PP r=4.0 seed {s} ...', flush=True)\n",
        "    X_all = train_sup[feat_cols].astype(float).values\n",
        "    oof = np.full(len(train_sup), np.nan, float)\n",
        "    models = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "        t1 = time.time()\n",
        "        X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "        X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "        neg = (y_tr == 0).sum(); posc = (y_tr == 1).sum()\n",
        "        spw = max(1.0, neg / max(1, posc))\n",
        "        dtrain = xgb.DMatrix(X_tr, label=y_tr); dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "        params = {'tree_method':'hist','device':'cuda','max_depth':7,'eta':0.05,'subsample':0.9,'colsample_bytree':0.8,\n",
        "                  'min_child_weight':10,'lambda':1.5,'alpha':0.1,'gamma':0.1,'objective':'binary:logistic','eval_metric':'logloss',\n",
        "                  'scale_pos_weight': float(spw), 'seed': int(s + fold)}\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=3800, evals=[(dtrain,'train'),(dvalid,'valid')], early_stopping_rounds=200, verbose_eval=False)\n",
        "        best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "        oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "        models.append((booster, best_it))\n",
        "        print(f'   seed {s} fold {fold} done in {time.time()-t1:.1f}s; best_it={best_it}', flush=True)\n",
        "    df = train_sup[['game_play','p1','p2','step']].iloc[ord_idx].copy()\n",
        "    df['oof'] = oof[ord_idx]\n",
        "    df = df.sort_values(['game_play','p1','p2','step'])\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    df['oof_smooth'] = grp['oof'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    oof_s_list.append(df['oof_smooth'].to_numpy())\n",
        "\n",
        "    Xt = test_feats[feat_cols].astype(float).values\n",
        "    dtest = xgb.DMatrix(Xt)\n",
        "    pt = np.zeros(len(test_feats), float)\n",
        "    for i, (booster, best_it) in enumerate(models):\n",
        "        t1 = time.time(); pt += booster.predict(dtest, iteration_range=(0, best_it + 1));\n",
        "        print(f'    seed {s} test model {i} {time.time()-t1:.1f}s', flush=True)\n",
        "    pt /= max(1, len(models))\n",
        "    dt = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step'])\n",
        "    dt['prob'] = pt[dt.index.values]\n",
        "    grp_t = dt.groupby(['game_play','p1','p2'], sort=False)\n",
        "    dt['prob_smooth'] = grp_t['prob'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    test_s_list.append(dt['prob_smooth'].to_numpy())\n",
        "\n",
        "oof_avg = np.mean(np.vstack(oof_s_list), axis=0)\n",
        "keys_tr_sorted = train_sup[['game_play','p1','p2','step']].iloc[ord_idx].copy().reset_index(drop=True)\n",
        "y_sorted = train_sup['contact'].astype(int).to_numpy()[ord_idx]\n",
        "same_sorted = train_sup['same_team'].fillna(0).astype(int).to_numpy()[ord_idx] if 'same_team' in train_sup.columns else np.zeros_like(y_sorted, np.int8)\n",
        "fold_sorted = fold_arr[ord_idx]\n",
        "\n",
        "# cap=3 on OOF\n",
        "df_o = keys_tr_sorted.copy()\n",
        "df_o['prob'] = oof_avg\n",
        "df_o['row_id'] = np.arange(len(df_o))\n",
        "long1 = df_o[['game_play','step','p1','prob','row_id']].rename(columns={'p1':'player'})\n",
        "long2 = df_o[['game_play','step','p2','prob','row_id']].rename(columns={'p2':'player'})\n",
        "df_long = pd.concat([long1, long2], ignore_index=True)\n",
        "df_long = df_long.sort_values(['game_play','step','player','prob'], ascending=[True, True, True, False])\n",
        "df_long['rank'] = df_long.groupby(['game_play','step','player'], sort=False)['prob'].rank(method='first', ascending=False)\n",
        "kept_rows = set(df_long.loc[df_long['rank'] <= 3, 'row_id'].tolist())\n",
        "keep_mask_all = df_o['row_id'].isin(kept_rows).to_numpy()\n",
        "oof_cap = oof_avg.copy(); oof_cap[~keep_mask_all] = 0.0\n",
        "print('Applied cap=3 to OOF. Kept rows:', int(keep_mask_all.sum()), 'of', len(keep_mask_all))\n",
        "\n",
        "thr_opp_f = []; thr_same_f = []\n",
        "for k in sorted(np.unique(fold_sorted)):\n",
        "    m = (fold_sorted == k)\n",
        "    mcc_k, t_opp_k, t_same_k = fast_dual_threshold_mcc(y_sorted[m], oof_cap[m], same_sorted[m], grid_points=256)\n",
        "    if (not np.isfinite(mcc_k)) or mcc_k < 0:\n",
        "        thrs = np.linspace(0.7, 0.85, 31)\n",
        "        ml = [matthews_corrcoef(y_sorted[m], (oof_cap[m] >= t).astype(int)) for t in thrs]\n",
        "        j = int(np.argmax(ml)); t_opp_k = t_same_k = float(thrs[j])\n",
        "    thr_opp_f.append(float(t_opp_k)); thr_same_f.append(float(t_same_k))\n",
        "    print(f' Fold {k} thresholds: thr_opp={t_opp_k:.4f}, thr_same={t_same_k:.4f}')\n",
        "thr_opp = float(np.median(thr_opp_f)); thr_same = float(np.median(thr_same_f))\n",
        "print(f'Final fold-median thresholds (cap3): thr_opp={thr_opp:.4f}, thr_same={thr_same:.4f}')\n",
        "\n",
        "pt_bag = np.mean(np.vstack(test_s_list), axis=0)\n",
        "df_t = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step']).reset_index(drop=True)\n",
        "df_t['prob_smooth'] = pt_bag\n",
        "df_t['row_id'] = np.arange(len(df_t))\n",
        "long1t = df_t[['game_play','step','p1','prob_smooth','row_id']].rename(columns={'p1':'player','prob_smooth':'prob'})\n",
        "long2t = df_t[['game_play','step','p2','prob_smooth','row_id']].rename(columns={'p2':'player','prob_smooth':'prob'})\n",
        "df_long_t = pd.concat([long1t, long2t], ignore_index=True)\n",
        "df_long_t = df_long_t.sort_values(['game_play','step','player','prob'], ascending=[True, True, True, False])\n",
        "df_long_t['rank'] = df_long_t.groupby(['game_play','step','player'], sort=False)['prob'].rank(method='first', ascending=False)\n",
        "kept_rows_t = set(df_long_t.loc[df_long_t['rank'] <= 3, 'row_id'].tolist())\n",
        "keep_mask_t = df_t['row_id'].isin(kept_rows_t).to_numpy()\n",
        "df_t.loc[~keep_mask_t, 'prob_smooth'] = 0.0\n",
        "print('Applied cap=3 on test. Kept rows:', int(keep_mask_t.sum()), 'of', len(keep_mask_t))\n",
        "\n",
        "same_flag_test = test_feats[['game_play','p1','p2','step','same_team']].copy()\n",
        "same_flag_test = same_flag_test.merge(df_t[['game_play','p1','p2','step','row_id']], on=['game_play','p1','p2','step'], how='right').sort_values('row_id')\n",
        "same_flag_arr = same_flag_test['same_team'].fillna(0).astype(int).to_numpy() if 'same_team' in same_flag_test.columns else np.zeros(len(df_t), int)\n",
        "thr_arr_test = np.where(same_flag_arr == 1, thr_same, thr_opp)\n",
        "df_t['pred_bin'] = (df_t['prob_smooth'].to_numpy() >= thr_arr_test).astype(int)\n",
        "\n",
        "df_h = df_t[['game_play','p1','p2','step','pred_bin']].copy().sort_values(['game_play','p1','p2','step'])\n",
        "grp_h = df_h.groupby(['game_play','p1','p2'], sort=False)['pred_bin']\n",
        "df_h['pred_hyst'] = grp_h.transform(lambda s: (s.rolling(3, center=True, min_periods=1).sum() >= 2).astype(int))\n",
        "df_t = df_t.merge(df_h[['game_play','p1','p2','step','pred_hyst']], on=['game_play','p1','p2','step'], how='left')\n",
        "\n",
        "cid_sorted = (df_t['game_play'].astype(str) + '_' + df_t['step'].astype(str) + '_' + df_t['p1'].astype(str) + '_' + df_t['p2'].astype(str))\n",
        "pred_df_pp = pd.DataFrame({'contact_id': cid_sorted.values, 'contact_pp': df_t['pred_hyst'].astype(int).values})\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.merge(pred_df_pp, on='contact_id', how='left')\n",
        "sub['contact'] = sub['contact_pp'].fillna(0).astype(int)\n",
        "sub = sub.drop(columns=['contact_pp'])\n",
        "pp_ones = int(sub['contact'].sum())\n",
        "print('PP (r40 bag + fold-median thr cap3 + hyst) ones before G overwrite:', pp_ones)\n",
        "try:\n",
        "    prev_sub = pd.read_csv('submission.csv')\n",
        "    g_pred_second = prev_sub[prev_sub['contact_id'].str.endswith('_G')][['contact_id','contact']].rename(columns={'contact':'contact_g'})\n",
        "    sub = sub.merge(g_pred_second, on='contact_id', how='left')\n",
        "    sub['contact'] = sub['contact_g'].fillna(sub['contact']).astype(int)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "    after_ones = int(sub['contact'].sum())\n",
        "    print(f'Applied prior G overwrite. ones after={after_ones}, delta={after_ones-pp_ones}')\n",
        "except Exception as e:\n",
        "    print('No prior submission with G rows found; skipping G overwrite.', e)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv. Took {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version (pp-bag-r40-fold-median-cap3): 2.1.4\nFold-median thresholds (cap=3) run: loading r=4.0 supervised dyn and test features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 50 features\n PP r=4.0 seed 42 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 0 done in 36.7s; best_it=3253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 1 done in 39.7s; best_it=3632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 2 done in 37.9s; best_it=3326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 3 done in 38.8s; best_it=3446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 4 done in 37.3s; best_it=3468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.0 seed 1337 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 0 done in 38.1s; best_it=3385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 1 done in 39.7s; best_it=3608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 2 done in 36.0s; best_it=3140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 3 done in 38.1s; best_it=3378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 4 done in 38.9s; best_it=3609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.0 seed 2025 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 0 done in 38.8s; best_it=3453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 1 done in 38.1s; best_it=3408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 2 done in 37.8s; best_it=3284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 3 done in 40.7s; best_it=3573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 4 done in 37.2s; best_it=3388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied cap=3 to OOF. Kept rows: 400456 of 634192\n Fold 0 thresholds: thr_opp=0.7890, thr_same=0.8594\n Fold 1 thresholds: thr_opp=0.8231, thr_same=0.7933\n Fold 2 thresholds: thr_opp=0.8570, thr_same=0.6249\n Fold 3 thresholds: thr_opp=0.7145, thr_same=0.7097\n Fold 4 thresholds: thr_opp=0.8042, thr_same=0.8685\nFinal fold-median thresholds (cap3): thr_opp=0.8042, thr_same=0.7933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied cap=3 on test. Kept rows: 162678 of 278492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP (r40 bag + fold-median thr cap3 + hyst) ones before G overwrite: 6488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied prior G overwrite. ones after=8540, delta=2052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv. Took 590.7s\n"
          ]
        }
      ]
    },
    {
      "id": "9466079b-ab38-4d52-be1e-1e4728b1ca02",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PP r=4.0 bagging with thresholds optimized AFTER hysteresis per fold (cap=2), fold-median thresholds, apply same chain on test, then G overwrite\n",
        "import time, numpy as np, pandas as pd, sys, subprocess\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.4'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version (pp-bag-r40-thr-after-hyst):', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def apply_hyst_per_pair(df_bin: pd.DataFrame) -> np.ndarray:\n",
        "    # df_bin must have columns: game_play, p1, p2, step, pred_bin\n",
        "    df_h = df_bin.sort_values(['game_play','p1','p2','step']).copy()\n",
        "    grp = df_h.groupby(['game_play','p1','p2'], sort=False)['pred_bin']\n",
        "    df_h['pred_hyst'] = grp.transform(lambda s: (s.rolling(3, center=True, min_periods=1).sum() >= 2).astype(int))\n",
        "    return df_h['pred_hyst'].to_numpy()\n",
        "\n",
        "t0 = time.time()\n",
        "print('Loading r=4.0 supervised dyn train and test features...')\n",
        "train_sup = pd.read_parquet('train_supervised_w5_helm_dyn_r40.parquet')\n",
        "test_feats = pd.read_parquet('test_pairs_w5_helm_dyn_r40.parquet')\n",
        "folds_df = pd.read_csv('folds_game_play.csv')\n",
        "train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "assert train_sup['fold'].notna().all()\n",
        "for df in (train_sup, test_feats):\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "\n",
        "drop_cols = {'contact','game_play','step','p1','p2','team1','team2','pos1','pos2','fold'}\n",
        "feat_cols = [c for c in train_sup.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_sup[c])]\n",
        "print('Using', len(feat_cols), 'features')\n",
        "\n",
        "# Canonical sorted order for alignment\n",
        "ord_idx = train_sup[['game_play','p1','p2','step']].sort_values(['game_play','p1','p2','step']).index.to_numpy()\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train_sup['game_play'].values\n",
        "y_all = train_sup['contact'].astype(int).values\n",
        "same_all = train_sup['same_team'].fillna(0).astype(int).values if 'same_team' in train_sup.columns else np.zeros(len(train_sup), np.int8)\n",
        "fold_arr = train_sup['fold'].astype(int).to_numpy()\n",
        "\n",
        "seeds = [42,1337,2025]\n",
        "oof_s_list = []; test_s_list = []\n",
        "\n",
        "for s in seeds:\n",
        "    print(f' PP r=4.0 seed {s} ...', flush=True)\n",
        "    X_all = train_sup[feat_cols].astype(float).values\n",
        "    oof = np.full(len(train_sup), np.nan, float)\n",
        "    models = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "        t1 = time.time()\n",
        "        X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "        X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "        neg = (y_tr == 0).sum(); posc = (y_tr == 1).sum()\n",
        "        spw = max(1.0, neg / max(1, posc))\n",
        "        dtrain = xgb.DMatrix(X_tr, label=y_tr); dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "        params = {'tree_method':'hist','device':'cuda','max_depth':7,'eta':0.05,'subsample':0.9,'colsample_bytree':0.8,\n",
        "                  'min_child_weight':10,'lambda':1.5,'alpha':0.1,'gamma':0.1,'objective':'binary:logistic','eval_metric':'logloss',\n",
        "                  'scale_pos_weight': float(spw), 'seed': int(s + fold)}\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=3800, evals=[(dtrain,'train'),(dvalid,'valid')], early_stopping_rounds=200, verbose_eval=False)\n",
        "        best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "        oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "        models.append((booster, best_it))\n",
        "        print(f'   seed {s} fold {fold} done in {time.time()-t1:.1f}s; best_it={best_it}', flush=True)\n",
        "    # Smooth OOF on canonical order\n",
        "    df = train_sup[['game_play','p1','p2','step']].iloc[ord_idx].copy()\n",
        "    df['oof'] = oof[ord_idx]\n",
        "    df = df.sort_values(['game_play','p1','p2','step'])\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    df['oof_smooth'] = grp['oof'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    oof_s_list.append(df['oof_smooth'].to_numpy())\n",
        "\n",
        "    # Test predictions and smoothing\n",
        "    Xt = test_feats[feat_cols].astype(float).values\n",
        "    dtest = xgb.DMatrix(Xt)\n",
        "    pt = np.zeros(len(test_feats), float)\n",
        "    for i, (booster, best_it) in enumerate(models):\n",
        "        t1 = time.time(); pt += booster.predict(dtest, iteration_range=(0, best_it + 1));\n",
        "        print(f'    seed {s} test model {i} {time.time()-t1:.1f}s', flush=True)\n",
        "    pt /= max(1, len(models))\n",
        "    dt = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step'])\n",
        "    dt['prob'] = pt[dt.index.values]\n",
        "    grp_t = dt.groupby(['game_play','p1','p2'], sort=False)\n",
        "    dt['prob_smooth'] = grp_t['prob'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    test_s_list.append(dt['prob_smooth'].to_numpy())\n",
        "\n",
        "# Average OOF across seeds in canonical order\n",
        "oof_avg = np.mean(np.vstack(oof_s_list), axis=0)\n",
        "keys_tr_sorted = train_sup[['game_play','p1','p2','step']].iloc[ord_idx].copy().reset_index(drop=True)\n",
        "y_sorted = train_sup['contact'].astype(int).to_numpy()[ord_idx]\n",
        "same_sorted = train_sup['same_team'].fillna(0).astype(int).to_numpy()[ord_idx] if 'same_team' in train_sup.columns else np.zeros_like(y_sorted, np.int8)\n",
        "fold_sorted = fold_arr[ord_idx]\n",
        "\n",
        "# Apply cap=2 on OOF probs before thresholding\n",
        "df_o = keys_tr_sorted.copy()\n",
        "df_o['prob'] = oof_avg\n",
        "df_o['row_id'] = np.arange(len(df_o))\n",
        "long1 = df_o[['game_play','step','p1','prob','row_id']].rename(columns={'p1':'player'})\n",
        "long2 = df_o[['game_play','step','p2','prob','row_id']].rename(columns={'p2':'player'})\n",
        "df_long = pd.concat([long1, long2], ignore_index=True)\n",
        "df_long = df_long.sort_values(['game_play','step','player','prob'], ascending=[True, True, True, False])\n",
        "df_long['rank'] = df_long.groupby(['game_play','step','player'], sort=False)['prob'].rank(method='first', ascending=False)\n",
        "kept_rows = set(df_long.loc[df_long['rank'] <= 2, 'row_id'].tolist())\n",
        "keep_mask_all = df_o['row_id'].isin(kept_rows).to_numpy()\n",
        "oof_cap = oof_avg.copy(); oof_cap[~keep_mask_all] = 0.0\n",
        "print('Applied cap=2 to OOF. Kept rows:', int(keep_mask_all.sum()), 'of', len(keep_mask_all))\n",
        "\n",
        "# Optimize thresholds AFTER hysteresis per fold\n",
        "thr_grid = np.round(np.linspace(0.70, 0.85, 16), 3)\n",
        "thr_best = []\n",
        "for k in sorted(np.unique(fold_sorted)):\n",
        "    m = (fold_sorted == k)\n",
        "    df_k = keys_tr_sorted.loc[m, ['game_play','p1','p2','step']].copy()\n",
        "    df_k['prob'] = oof_cap[m]\n",
        "    df_k['same'] = same_sorted[m]\n",
        "    y_k = y_sorted[m]\n",
        "    # Build cap already applied; threshold and hysteresis will be varied\n",
        "    best_m, best_to, best_ts = -1.0, 0.78, 0.78\n",
        "    # Pre-allocate arrays for speed\n",
        "    same_arr = df_k['same'].to_numpy()\n",
        "    for to in thr_grid:\n",
        "        thr_arr = np.where(same_arr == 1, 1.0, to)  # temp; will set same later in inner loop\n",
        "        for ts in thr_grid:\n",
        "            thr_arr = np.where(same_arr == 1, ts, to)\n",
        "            pred_bin = (df_k['prob'].to_numpy() >= thr_arr).astype(int)\n",
        "            df_tmp = df_k[['game_play','p1','p2','step']].copy()\n",
        "            df_tmp['pred_bin'] = pred_bin\n",
        "            pred_h = apply_hyst_per_pair(df_tmp)\n",
        "            mcc = matthews_corrcoef(y_k, pred_h)\n",
        "            if mcc > best_m:\n",
        "                best_m, best_to, best_ts = float(mcc), float(to), float(ts)\n",
        "    thr_best.append((best_to, best_ts))\n",
        "    print(f' Fold {k} best after-hyst MCC={best_m:.5f} thr_opp={best_to:.3f} thr_same={best_ts:.3f}')\n",
        "\n",
        "thr_best = np.array(thr_best, float)\n",
        "thr_opp_med = float(np.median(thr_best[:, 0]))\n",
        "thr_same_med = float(np.median(thr_best[:, 1]))\n",
        "print(f'Fold-median thresholds after hysteresis (cap2): thr_opp={thr_opp_med:.4f}, thr_same={thr_same_med:.4f}')\n",
        "\n",
        "# Test: average probs across seeds, smooth, cap=2, then apply median thresholds, then hysteresis\n",
        "pt_bag = np.mean(np.vstack(test_s_list), axis=0)\n",
        "df_t = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step']).reset_index(drop=True)\n",
        "df_t['prob_smooth'] = pt_bag\n",
        "df_t['row_id'] = np.arange(len(df_t))\n",
        "long1t = df_t[['game_play','step','p1','prob_smooth','row_id']].rename(columns={'p1':'player','prob_smooth':'prob'})\n",
        "long2t = df_t[['game_play','step','p2','prob_smooth','row_id']].rename(columns={'p2':'player','prob_smooth':'prob'})\n",
        "df_long_t = pd.concat([long1t, long2t], ignore_index=True)\n",
        "df_long_t = df_long_t.sort_values(['game_play','step','player','prob'], ascending=[True, True, True, False])\n",
        "df_long_t['rank'] = df_long_t.groupby(['game_play','step','player'], sort=False)['prob'].rank(method='first', ascending=False)\n",
        "kept_rows_t = set(df_long_t.loc[df_long_t['rank'] <= 2, 'row_id'].tolist())\n",
        "keep_mask_t = df_t['row_id'].isin(kept_rows_t).to_numpy()\n",
        "df_t.loc[~keep_mask_t, 'prob_smooth'] = 0.0\n",
        "print('Applied cap=2 on test. Kept rows:', int(keep_mask_t.sum()), 'of', len(keep_mask_t))\n",
        "\n",
        "same_flag_test = test_feats[['game_play','p1','p2','step','same_team']].copy()\n",
        "same_flag_test = same_flag_test.merge(df_t[['game_play','p1','p2','step','row_id']], on=['game_play','p1','p2','step'], how='right').sort_values('row_id')\n",
        "same_arr_t = same_flag_test['same_team'].fillna(0).astype(int).to_numpy() if 'same_team' in same_flag_test.columns else np.zeros(len(df_t), int)\n",
        "thr_arr_t = np.where(same_arr_t == 1, thr_same_med, thr_opp_med)\n",
        "df_t['pred_bin'] = (df_t['prob_smooth'].to_numpy() >= thr_arr_t).astype(int)\n",
        "\n",
        "df_tmp_t = df_t[['game_play','p1','p2','step','pred_bin']].copy()\n",
        "pred_h_t = apply_hyst_per_pair(df_tmp_t)\n",
        "df_t['pred_hyst'] = pred_h_t.astype(int)\n",
        "\n",
        "# Build submission with PP, then overwrite G rows from prior submission (no PP leakage in CV)\n",
        "cid_sorted = (df_t['game_play'].astype(str) + '_' + df_t['step'].astype(str) + '_' + df_t['p1'].astype(str) + '_' + df_t['p2'].astype(str))\n",
        "pred_df_pp = pd.DataFrame({'contact_id': cid_sorted.values, 'contact_pp': df_t['pred_hyst'].astype(int).values})\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.merge(pred_df_pp, on='contact_id', how='left')\n",
        "sub['contact'] = sub['contact_pp'].fillna(0).astype(int)\n",
        "sub = sub.drop(columns=['contact_pp'])\n",
        "pp_ones = int(sub['contact'].sum())\n",
        "print('PP (r40 bag + thr-after-hyst cap2) ones before G overwrite:', pp_ones)\n",
        "try:\n",
        "    prev_sub = pd.read_csv('submission.csv')\n",
        "    g_pred_second = prev_sub[prev_sub['contact_id'].str.endswith('_G')][['contact_id','contact']].rename(columns={'contact':'contact_g'})\n",
        "    sub = sub.merge(g_pred_second, on='contact_id', how='left')\n",
        "    sub['contact'] = sub['contact_g'].fillna(sub['contact']).astype(int)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "    after_ones = int(sub['contact'].sum())\n",
        "    print(f'Applied prior G overwrite. ones after={after_ones}, delta={after_ones-pp_ones}')\n",
        "except Exception as e:\n",
        "    print('No prior submission with G rows found; skipping G overwrite.', e)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv. Took {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version (pp-bag-r40-thr-after-hyst): 2.1.4\nLoading r=4.0 supervised dyn train and test features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 50 features\n PP r=4.0 seed 42 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 0 done in 36.8s; best_it=3253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 1 done in 39.7s; best_it=3632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 2 done in 37.8s; best_it=3326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 3 done in 38.9s; best_it=3446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 4 done in 37.2s; best_it=3468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.0 seed 1337 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 0 done in 38.1s; best_it=3385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 1 done in 39.6s; best_it=3608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 2 done in 36.0s; best_it=3140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 3 done in 38.0s; best_it=3378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 4 done in 38.9s; best_it=3609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.0 seed 2025 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 0 done in 38.9s; best_it=3453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 1 done in 38.4s; best_it=3408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 2 done in 37.9s; best_it=3284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 3 done in 40.6s; best_it=3573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 4 done in 37.1s; best_it=3388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied cap=2 to OOF. Kept rows: 308871 of 634192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 0 best after-hyst MCC=0.71295 thr_opp=0.790 thr_same=0.830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 1 best after-hyst MCC=0.74333 thr_opp=0.820 thr_same=0.780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 2 best after-hyst MCC=0.73521 thr_opp=0.850 thr_same=0.700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 3 best after-hyst MCC=0.73375 thr_opp=0.720 thr_same=0.700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 4 best after-hyst MCC=0.72896 thr_opp=0.770 thr_same=0.840\nFold-median thresholds after hysteresis (cap2): thr_opp=0.7900, thr_same=0.7800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied cap=2 on test. Kept rows: 122291 of 278492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP (r40 bag + thr-after-hyst cap2) ones before G overwrite: 6418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied prior G overwrite. ones after=8470, delta=2052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv. Took 1585.4s\n"
          ]
        }
      ]
    },
    {
      "id": "ae4a0699-9fe9-4c4b-85cc-e01a7cbd2995",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PP r=4.0 bagging with distance-aware caps (3/2/1) before thresholding, thresholds optimized AFTER hysteresis per fold, fold-median thresholds; apply same chain on test; G overwrite\n",
        "import time, numpy as np, pandas as pd, sys, subprocess\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.4'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version (pp-r40-cap321-thr-after-hyst):', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def apply_hyst_per_pair(df_bin: pd.DataFrame) -> np.ndarray:\n",
        "    df_h = df_bin.sort_values(['game_play','p1','p2','step']).copy()\n",
        "    grp = df_h.groupby(['game_play','p1','p2'], sort=False)['pred_bin']\n",
        "    df_h['pred_hyst'] = grp.transform(lambda s: (s.rolling(3, center=True, min_periods=1).sum() >= 2).astype(int))\n",
        "    return df_h['pred_hyst'].to_numpy()\n",
        "\n",
        "t0 = time.time()\n",
        "print('Loading r=4.0 supervised dyn train and test features...')\n",
        "train_sup = pd.read_parquet('train_supervised_w5_helm_dyn_r40.parquet')\n",
        "test_feats = pd.read_parquet('test_pairs_w5_helm_dyn_r40.parquet')\n",
        "folds_df = pd.read_csv('folds_game_play.csv')\n",
        "train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "assert train_sup['fold'].notna().all()\n",
        "for df in (train_sup, test_feats):\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "\n",
        "drop_cols = {'contact','game_play','step','p1','p2','team1','team2','pos1','pos2','fold'}\n",
        "feat_cols = [c for c in train_sup.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_sup[c])]\n",
        "print('Using', len(feat_cols), 'features')\n",
        "\n",
        "# Canonical order\n",
        "ord_idx = train_sup[['game_play','p1','p2','step']].sort_values(['game_play','p1','p2','step']).index.to_numpy()\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train_sup['game_play'].values\n",
        "y_all = train_sup['contact'].astype(int).values\n",
        "same_all = train_sup['same_team'].fillna(0).astype(int).values if 'same_team' in train_sup.columns else np.zeros(len(train_sup), np.int8)\n",
        "fold_arr = train_sup['fold'].astype(int).to_numpy()\n",
        "\n",
        "seeds = [42,1337,2025]\n",
        "oof_s_list = []; test_s_list = []\n",
        "\n",
        "for s in seeds:\n",
        "    print(f' PP r=4.0 seed {s} ...', flush=True)\n",
        "    X_all = train_sup[feat_cols].astype(float).values\n",
        "    oof = np.full(len(train_sup), np.nan, float)\n",
        "    models = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "        t1 = time.time()\n",
        "        X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "        X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "        neg = (y_tr == 0).sum(); posc = (y_tr == 1).sum()\n",
        "        spw = max(1.0, neg / max(1, posc))\n",
        "        dtrain = xgb.DMatrix(X_tr, label=y_tr); dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "        params = {'tree_method':'hist','device':'cuda','max_depth':7,'eta':0.05,'subsample':0.9,'colsample_bytree':0.8,\n",
        "                  'min_child_weight':10,'lambda':1.5,'alpha':0.1,'gamma':0.1,'objective':'binary:logistic','eval_metric':'logloss',\n",
        "                  'scale_pos_weight': float(spw), 'seed': int(s + fold)}\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=3800, evals=[(dtrain,'train'),(dvalid,'valid')], early_stopping_rounds=200, verbose_eval=False)\n",
        "        best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "        oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "        models.append((booster, best_it))\n",
        "        print(f'   seed {s} fold {fold} done in {time.time()-t1:.1f}s; best_it={best_it}', flush=True)\n",
        "    # Smooth OOF on canonical order\n",
        "    df = train_sup[['game_play','p1','p2','step']].iloc[ord_idx].copy()\n",
        "    df['oof'] = oof[ord_idx]\n",
        "    df = df.sort_values(['game_play','p1','p2','step'])\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    df['oof_smooth'] = grp['oof'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    oof_s_list.append(df['oof_smooth'].to_numpy())\n",
        "\n",
        "    # Test predictions and smoothing\n",
        "    Xt = test_feats[feat_cols].astype(float).values\n",
        "    dtest = xgb.DMatrix(Xt)\n",
        "    pt = np.zeros(len(test_feats), float)\n",
        "    for i, (booster, best_it) in enumerate(models):\n",
        "        t1 = time.time(); pt += booster.predict(dtest, iteration_range=(0, best_it + 1));\n",
        "        print(f'    seed {s} test model {i} {time.time()-t1:.1f}s', flush=True)\n",
        "    pt /= max(1, len(models))\n",
        "    dt = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step'])\n",
        "    dt['prob'] = pt[dt.index.values]\n",
        "    grp_t = dt.groupby(['game_play','p1','p2'], sort=False)\n",
        "    dt['prob_smooth'] = grp_t['prob'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    test_s_list.append(dt['prob_smooth'].to_numpy())\n",
        "\n",
        "# Average OOF across seeds in canonical order\n",
        "oof_avg = np.mean(np.vstack(oof_s_list), axis=0)\n",
        "keys_tr_sorted = train_sup[['game_play','p1','p2','step','distance']].iloc[ord_idx].copy().reset_index(drop=True)\n",
        "y_sorted = train_sup['contact'].astype(int).to_numpy()[ord_idx]\n",
        "same_sorted = train_sup['same_team'].fillna(0).astype(int).to_numpy()[ord_idx] if 'same_team' in train_sup.columns else np.zeros_like(y_sorted, np.int8)\n",
        "fold_sorted = fold_arr[ord_idx]\n",
        "\n",
        "# Distance-aware caps (<=1.6: top-3, 1.6-2.4: top-2, >2.4: top-1) applied on smoothed probs BEFORE thresholding\n",
        "df_o = keys_tr_sorted.copy()\n",
        "df_o['prob'] = oof_avg\n",
        "df_o['row_id'] = np.arange(len(df_o))\n",
        "df_o['bin'] = np.where(df_o['distance'] <= 1.6, 0, np.where(df_o['distance'] <= 2.4, 1, 2))\n",
        "cap_map = {0:3, 1:2, 2:1}\n",
        "# long format for both players with distance bin carried\n",
        "long1 = df_o[['game_play','step','p1','prob','row_id','bin']].rename(columns={'p1':'player'})\n",
        "long2 = df_o[['game_play','step','p2','prob','row_id','bin']].rename(columns={'p2':'player'})\n",
        "df_long = pd.concat([long1, long2], ignore_index=True)\n",
        "df_long = df_long.sort_values(['game_play','step','player','bin','prob'], ascending=[True, True, True, True, False])\n",
        "df_long['rank_in_bin'] = df_long.groupby(['game_play','step','player','bin'], sort=False)['prob'].rank(method='first', ascending=False)\n",
        "keep_rows = []\n",
        "for b, cap in cap_map.items():\n",
        "    keep_rows.append(df_long.loc[(df_long['bin'] == b) & (df_long['rank_in_bin'] <= cap), 'row_id'])\n",
        "kept_rows = set(pd.concat(keep_rows).tolist())\n",
        "keep_mask_all = df_o['row_id'].isin(kept_rows).to_numpy()\n",
        "oof_cap = oof_avg.copy(); oof_cap[~keep_mask_all] = 0.0\n",
        "print('Applied distance-aware caps (3/2/1) to OOF. Kept rows:', int(keep_mask_all.sum()), 'of', len(keep_mask_all))\n",
        "\n",
        "# Optimize thresholds AFTER hysteresis per fold on capped OOF\n",
        "thr_grid = np.round(np.linspace(0.70, 0.85, 16), 3)\n",
        "thr_best = []\n",
        "for k in sorted(np.unique(fold_sorted)):\n",
        "    m = (fold_sorted == k)\n",
        "    df_k = keys_tr_sorted.loc[m, ['game_play','p1','p2','step']].copy()\n",
        "    df_k['prob'] = oof_cap[m]\n",
        "    df_k['same'] = same_sorted[m]\n",
        "    y_k = y_sorted[m]\n",
        "    best_m, best_to, best_ts = -1.0, 0.78, 0.78\n",
        "    same_arr = df_k['same'].to_numpy()\n",
        "    prob_arr = df_k['prob'].to_numpy()\n",
        "    for to in thr_grid:\n",
        "        for ts in thr_grid:\n",
        "            thr_arr = np.where(same_arr == 1, ts, to)\n",
        "            pred_bin = (prob_arr >= thr_arr).astype(int)\n",
        "            df_tmp = df_k[['game_play','p1','p2','step']].copy()\n",
        "            df_tmp['pred_bin'] = pred_bin\n",
        "            pred_h = apply_hyst_per_pair(df_tmp)\n",
        "            mcc = matthews_corrcoef(y_k, pred_h)\n",
        "            if mcc > best_m:\n",
        "                best_m, best_to, best_ts = float(mcc), float(to), float(ts)\n",
        "    thr_best.append((best_to, best_ts))\n",
        "    print(f' Fold {k} best after-hyst MCC={best_m:.5f} thr_opp={best_to:.3f} thr_same={best_ts:.3f}')\n",
        "\n",
        "thr_best = np.array(thr_best, float)\n",
        "thr_opp_med = float(np.median(thr_best[:, 0]))\n",
        "thr_same_med = float(np.median(thr_best[:, 1]))\n",
        "print(f'Fold-median thresholds after hysteresis (cap 3/2/1): thr_opp={thr_opp_med:.4f}, thr_same={thr_same_med:.4f}')\n",
        "\n",
        "# Test: average probs, smooth, apply distance-aware caps, then median thresholds, then hysteresis\n",
        "pt_bag = np.mean(np.vstack(test_s_list), axis=0)\n",
        "df_t = test_feats[['game_play','p1','p2','step','distance']].copy().sort_values(['game_play','p1','p2','step']).reset_index(drop=True)\n",
        "df_t['prob_smooth'] = pt_bag\n",
        "df_t['row_id'] = np.arange(len(df_t))\n",
        "df_t['bin'] = np.where(df_t['distance'] <= 1.6, 0, np.where(df_t['distance'] <= 2.4, 1, 2))\n",
        "long1t = df_t[['game_play','step','p1','prob_smooth','row_id','bin']].rename(columns={'p1':'player','prob_smooth':'prob'})\n",
        "long2t = df_t[['game_play','step','p2','prob_smooth','row_id','bin']].rename(columns={'p2':'player','prob_smooth':'prob'})\n",
        "df_long_t = pd.concat([long1t, long2t], ignore_index=True)\n",
        "df_long_t = df_long_t.sort_values(['game_play','step','player','bin','prob'], ascending=[True, True, True, True, False])\n",
        "df_long_t['rank_in_bin'] = df_long_t.groupby(['game_play','step','player','bin'], sort=False)['prob'].rank(method='first', ascending=False)\n",
        "keep_rows_t = []\n",
        "for b, cap in cap_map.items():\n",
        "    keep_rows_t.append(df_long_t.loc[(df_long_t['bin'] == b) & (df_long_t['rank_in_bin'] <= cap), 'row_id'])\n",
        "kept_rows_t = set(pd.concat(keep_rows_t).tolist())\n",
        "keep_mask_t = df_t['row_id'].isin(kept_rows_t).to_numpy()\n",
        "df_t.loc[~keep_mask_t, 'prob_smooth'] = 0.0\n",
        "print('Applied distance-aware caps (3/2/1) on test. Kept rows:', int(keep_mask_t.sum()), 'of', len(keep_mask_t))\n",
        "\n",
        "same_flag_test = test_feats[['game_play','p1','p2','step','same_team']].copy()\n",
        "same_flag_test = same_flag_test.merge(df_t[['game_play','p1','p2','step','row_id']], on=['game_play','p1','p2','step'], how='right').sort_values('row_id')\n",
        "same_arr_t = same_flag_test['same_team'].fillna(0).astype(int).to_numpy() if 'same_team' in same_flag_test.columns else np.zeros(len(df_t), int)\n",
        "thr_arr_t = np.where(same_arr_t == 1, thr_same_med, thr_opp_med)\n",
        "df_t['pred_bin'] = (df_t['prob_smooth'].to_numpy() >= thr_arr_t).astype(int)\n",
        "\n",
        "df_tmp_t = df_t[['game_play','p1','p2','step','pred_bin']].copy()\n",
        "pred_h_t = apply_hyst_per_pair(df_tmp_t)\n",
        "df_t['pred_hyst'] = pred_h_t.astype(int)\n",
        "\n",
        "# Build submission with PP, then overwrite G rows from prior submission\n",
        "cid_sorted = (df_t['game_play'].astype(str) + '_' + df_t['step'].astype(str) + '_' + df_t['p1'].astype(str) + '_' + df_t['p2'].astype(str))\n",
        "pred_df_pp = pd.DataFrame({'contact_id': cid_sorted.values, 'contact_pp': df_t['pred_hyst'].astype(int).values})\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.merge(pred_df_pp, on='contact_id', how='left')\n",
        "sub['contact'] = sub['contact_pp'].fillna(0).astype(int)\n",
        "sub = sub.drop(columns=['contact_pp'])\n",
        "pp_ones = int(sub['contact'].sum())\n",
        "print('PP (r40 bag + thr-after-hyst cap3/2/1) ones before G overwrite:', pp_ones)\n",
        "try:\n",
        "    prev_sub = pd.read_csv('submission.csv')\n",
        "    g_pred_second = prev_sub[prev_sub['contact_id'].str.endswith('_G')][['contact_id','contact']].rename(columns={'contact':'contact_g'})\n",
        "    sub = sub.merge(g_pred_second, on='contact_id', how='left')\n",
        "    sub['contact'] = sub['contact_g'].fillna(sub['contact']).astype(int)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "    after_ones = int(sub['contact'].sum())\n",
        "    print(f'Applied prior G overwrite. ones after={after_ones}, delta={after_ones-pp_ones}')\n",
        "except Exception as e:\n",
        "    print('No prior submission with G rows found; skipping G overwrite.', e)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv. Took {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version (pp-r40-cap321-thr-after-hyst): 2.1.4\nLoading r=4.0 supervised dyn train and test features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 50 features\n PP r=4.0 seed 42 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 0 done in 36.7s; best_it=3253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 1 done in 39.7s; best_it=3632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 2 done in 37.9s; best_it=3326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 3 done in 38.9s; best_it=3446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 4 done in 37.3s; best_it=3468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.0 seed 1337 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 0 done in 38.2s; best_it=3385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 1 done in 39.6s; best_it=3608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 2 done in 36.0s; best_it=3140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 3 done in 38.0s; best_it=3378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 4 done in 38.8s; best_it=3609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.0 seed 2025 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 0 done in 38.8s; best_it=3453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 1 done in 38.2s; best_it=3408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 2 done in 37.7s; best_it=3284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 3 done in 40.5s; best_it=3573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 4 done in 37.2s; best_it=3388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied distance-aware caps (3/2/1) to OOF. Kept rows: 440634 of 634192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 0 best after-hyst MCC=0.71215 thr_opp=0.790 thr_same=0.830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 1 best after-hyst MCC=0.74012 thr_opp=0.820 thr_same=0.780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 2 best after-hyst MCC=0.73435 thr_opp=0.850 thr_same=0.700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 3 best after-hyst MCC=0.73583 thr_opp=0.720 thr_same=0.700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 4 best after-hyst MCC=0.73013 thr_opp=0.770 thr_same=0.840\nFold-median thresholds after hysteresis (cap 3/2/1): thr_opp=0.7900, thr_same=0.7800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied distance-aware caps (3/2/1) on test. Kept rows: 189763 of 278492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP (r40 bag + thr-after-hyst cap3/2/1) ones before G overwrite: 6642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied prior G overwrite. ones after=8694, delta=2052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv. Took 1596.0s\n"
          ]
        }
      ]
    },
    {
      "id": "6c3cd26b-4cdd-4d8f-82c3-96f6f9d0ec84",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Rebuild full pipeline with candidate radius r=4.5 and save *_r45 artifacts\n",
        "import pandas as pd, numpy as np, time, math\n",
        "from itertools import combinations\n",
        "\n",
        "t0 = time.time()\n",
        "print('Rebuilding pipeline with r=4.5 ...')\n",
        "\n",
        "def build_pairs_for_group_r(gdf, r=4.5):\n",
        "    rows = []\n",
        "    arr = gdf[['nfl_player_id','team','position','x_position','y_position','speed','acceleration','direction']].values\n",
        "    n = arr.shape[0]\n",
        "    for i, j in combinations(range(n), 2):\n",
        "        pid_i, team_i, pos_i, xi, yi, si, ai, diri = arr[i]\n",
        "        pid_j, team_j, pos_j, xj, yj, sj, aj, dirj = arr[j]\n",
        "        dx = xj - xi; dy = yj - yi\n",
        "        dist = math.hypot(dx, dy)\n",
        "        if dist > r:\n",
        "            continue\n",
        "        a = int(pid_i); b = int(pid_j)\n",
        "        p1, p2 = (str(a), str(b)) if a <= b else (str(b), str(a))\n",
        "        vxi = si * math.cos(math.radians(diri)) if not pd.isna(diri) else 0.0\n",
        "        vyi = si * math.sin(math.radians(diri)) if not pd.isna(diri) else 0.0\n",
        "        vxj = sj * math.cos(math.radians(dirj)) if not pd.isna(dirj) else 0.0\n",
        "        vyj = sj * math.sin(math.radians(dirj)) if not pd.isna(dirj) else 0.0\n",
        "        rvx = vxj - vxi; rvy = vyj - vyi\n",
        "        if dist > 0:\n",
        "            ux = dx / dist; uy = dy / dist\n",
        "            closing = rvx * ux + rvy * uy\n",
        "        else:\n",
        "            closing = 0.0\n",
        "        if pd.isna(diri) or pd.isna(dirj):\n",
        "            hd = np.nan\n",
        "        else:\n",
        "            d = (diri - dirj + 180) % 360 - 180\n",
        "            hd = abs(d)\n",
        "        rows.append((p1, p2, dist, dx, dy, si, sj, ai, aj, closing, abs(closing), hd, int(team_i == team_j), str(team_i), str(team_j), str(pos_i), str(pos_j)))\n",
        "    if not rows:\n",
        "        return pd.DataFrame(columns=['p1','p2','distance','rel_dx','rel_dy','speed1','speed2','accel1','accel2','closing','abs_closing','abs_d_heading','same_team','team1','team2','pos1','pos2'])\n",
        "    return pd.DataFrame(rows, columns=['p1','p2','distance','rel_dx','rel_dy','speed1','speed2','accel1','accel2','closing','abs_closing','abs_d_heading','same_team','team1','team2','pos1','pos2'])\n",
        "\n",
        "def build_feature_table_r(track_df, r=4.5):\n",
        "    feats = []\n",
        "    cnt = 0\n",
        "    last = time.time()\n",
        "    for (gp, step), gdf in track_df.groupby(['game_play','step'], sort=False):\n",
        "        f = build_pairs_for_group_r(gdf, r=r)\n",
        "        if not f.empty:\n",
        "            f.insert(0, 'step', step)\n",
        "            f.insert(0, 'game_play', gp)\n",
        "            feats.append(f)\n",
        "        cnt += 1\n",
        "        if cnt % 500 == 0:\n",
        "            now = time.time()\n",
        "            print(f' processed {cnt} steps; +{now-last:.1f}s; total {now-t0:.1f}s', flush=True)\n",
        "            last = now\n",
        "    if feats:\n",
        "        return pd.concat(feats, ignore_index=True)\n",
        "    return pd.DataFrame(columns=['game_play','step','p1','p2','distance','rel_dx','rel_dy','speed1','speed2','accel1','accel2','closing','abs_closing','abs_d_heading','same_team','team1','team2','pos1','pos2'])\n",
        "\n",
        "print('Building train pairs r=4.5 ...')\n",
        "train_pairs_r45 = build_feature_table_r(train_track_idx, r=4.5)\n",
        "print('train_pairs_r45:', train_pairs_r45.shape)\n",
        "train_pairs_r45.to_parquet('train_pairs_r45.parquet', index=False)\n",
        "print('Building test pairs r=4.5 ...')\n",
        "test_pairs_r45 = build_feature_table_r(test_track_idx, r=4.5)\n",
        "print('test_pairs_r45:', test_pairs_r45.shape)\n",
        "test_pairs_r45.to_parquet('test_pairs_r45.parquet', index=False)\n",
        "\n",
        "def add_window_feats_local(df: pd.DataFrame, W: int = 5):\n",
        "    df = df.sort_values(['game_play','p1','p2','step']).copy()\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    df['dist_min_p5'] = grp['distance'].rolling(W, min_periods=1).min().reset_index(level=[0,1,2], drop=True)\n",
        "    df['dist_mean_p5'] = grp['distance'].rolling(W, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n",
        "    df['dist_max_p5'] = grp['distance'].rolling(W, min_periods=1).max().reset_index(level=[0,1,2], drop=True)\n",
        "    df['dist_std_p5'] = grp['distance'].rolling(W, min_periods=1).std().reset_index(level=[0,1,2], drop=True)\n",
        "    df['abs_close_min_p5'] = grp['abs_closing'].rolling(W, min_periods=1).min().reset_index(level=[0,1,2], drop=True)\n",
        "    df['abs_close_mean_p5'] = grp['abs_closing'].rolling(W, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n",
        "    df['abs_close_max_p5'] = grp['abs_closing'].rolling(W, min_periods=1).max().reset_index(level=[0,1,2], drop=True)\n",
        "    df['abs_close_std_p5'] = grp['abs_closing'].rolling(W, min_periods=1).std().reset_index(level=[0,1,2], drop=True)\n",
        "    for thr, name in [(1.5,'lt15'), (2.0,'lt20'), (2.5,'lt25')]:\n",
        "        key = f'cnt_dist_{name}_p5'\n",
        "        df[key] = grp['distance'].apply(lambda s: s.lt(thr).rolling(W, min_periods=1).sum()).reset_index(level=[0,1,2], drop=True)\n",
        "    df['dist_delta_p5'] = df['distance'] - grp['distance'].shift(W)\n",
        "    return df\n",
        "\n",
        "print('Adding W5 features (train/test) for r=4.5 ...')\n",
        "train_w_r45 = add_window_feats_local(train_pairs_r45, W=5)\n",
        "test_w_r45 = add_window_feats_local(test_pairs_r45, W=5)\n",
        "train_w_r45.to_parquet('train_pairs_w5_r45.parquet', index=False)\n",
        "test_w_r45.to_parquet('test_pairs_w5_r45.parquet', index=False)\n",
        "\n",
        "FPS = 59.94\n",
        "def prep_meta(vmeta: pd.DataFrame):\n",
        "    vm = vmeta.copy()\n",
        "    for c in ['start_time','snap_time']:\n",
        "        if np.issubdtype(vm[c].dtype, np.number):\n",
        "            continue\n",
        "        ts = pd.to_datetime(vm[c], errors='coerce')\n",
        "        if ts.notna().any():\n",
        "            vm[c] = (ts - ts.dt.floor('D')).dt.total_seconds().astype(float)\n",
        "        else:\n",
        "            vm[c] = pd.to_numeric(vm[c], errors='coerce')\n",
        "    vm['snap_frame'] = ((vm['snap_time'] - vm['start_time']) * FPS).round().astype('Int64')\n",
        "    return vm[['game_play','view','snap_frame']].drop_duplicates()\n",
        "\n",
        "print('Loading helmets and video metadata...')\n",
        "train_helm_df = pd.read_csv('train_baseline_helmets.csv')\n",
        "test_helm_df = pd.read_csv('test_baseline_helmets.csv')\n",
        "train_vmeta_df = pd.read_csv('train_video_metadata.csv')\n",
        "test_vmeta_df = pd.read_csv('test_video_metadata.csv')\n",
        "meta_tr = prep_meta(train_vmeta_df); meta_te = prep_meta(test_vmeta_df)\n",
        "\n",
        "def dedup_and_step(helm: pd.DataFrame, meta: pd.DataFrame):\n",
        "    df = helm[['game_play','view','frame','nfl_player_id','left','top','width','height']].copy()\n",
        "    df = df.dropna(subset=['nfl_player_id'])\n",
        "    df['nfl_player_id'] = df['nfl_player_id'].astype(int).astype(str)\n",
        "    df['area'] = df['width'] * df['height']\n",
        "    df['cx'] = df['left'] + 0.5 * df['width']\n",
        "    df['cy'] = df['top'] + 0.5 * df['height']\n",
        "    df = df.sort_values(['game_play','view','frame','nfl_player_id','area'], ascending=[True,True,True,True,False]).drop_duplicates(['game_play','view','frame','nfl_player_id'], keep='first')\n",
        "    df = df.merge(meta, on=['game_play','view'], how='left')\n",
        "    df['step'] = ((df['frame'] - df['snap_frame']).astype('float') / 6.0).round().astype('Int64')\n",
        "    df = df.dropna(subset=['step']); df['step'] = df['step'].astype(int)\n",
        "    dm1 = df.copy(); dm1['target_step'] = dm1['step'] - 1\n",
        "    d0 = df.copy(); d0['target_step'] = df['step']\n",
        "    dp1 = df.copy(); dp1['target_step'] = df['step'] + 1\n",
        "    d = pd.concat([dm1, d0, dp1], ignore_index=True)\n",
        "    agg = d.groupby(['game_play','view','target_step','nfl_player_id'], sort=False).agg(\n",
        "        cx_mean=('cx','mean'), cy_mean=('cy','mean'), h_mean=('height','mean'), cnt=('cx','size')\n",
        "    ).reset_index().rename(columns={'target_step':'step'})\n",
        "    return agg\n",
        "\n",
        "print('Preparing helmet aggregates...')\n",
        "h_tr = dedup_and_step(train_helm_df, meta_tr)\n",
        "h_te = dedup_and_step(test_helm_df, meta_te)\n",
        "print('Helmet agg shapes:', h_tr.shape, h_te.shape)\n",
        "\n",
        "def merge_helmet_to_pairs_df(pairs: pd.DataFrame, h_agg: pd.DataFrame):\n",
        "    ha = h_agg[['game_play','step','view','nfl_player_id','cx_mean','cy_mean','h_mean']].copy()\n",
        "    a = ha.rename(columns={'nfl_player_id':'p1','cx_mean':'cx1','cy_mean':'cy1','h_mean':'h1'})\n",
        "    b = ha.rename(columns={'nfl_player_id':'p2','cx_mean':'cx2','cy_mean':'cy2','h_mean':'h2'})\n",
        "    merged = a.merge(b, on=['game_play','step','view'], how='inner')\n",
        "    merged = merged[merged['p1'] < merged['p2']]\n",
        "    merged['px_dist'] = np.sqrt((merged['cx1'] - merged['cx2'])**2 + (merged['cy1'] - merged['cy2'])**2)\n",
        "    merged['px_dist_norm'] = merged['px_dist'] / np.sqrt(np.maximum(1e-6, merged['h1'] * merged['h2']))\n",
        "    agg = merged.groupby(['game_play','step','p1','p2'], as_index=False).agg(\n",
        "        px_dist_norm_min=('px_dist_norm','min'),\n",
        "        views_both_present=('px_dist_norm', lambda s: int(s.notna().sum()))\n",
        "    )\n",
        "    out = pairs.merge(agg, on=['game_play','step','p1','p2'], how='left')\n",
        "    return out\n",
        "\n",
        "print('Merging helmets into pairs (train/test) ...')\n",
        "train_pairs_w5_helm_r45 = merge_helmet_to_pairs_df(train_w_r45, h_tr)\n",
        "test_pairs_w5_helm_r45 = merge_helmet_to_pairs_df(test_w_r45, h_te)\n",
        "train_pairs_w5_helm_r45.to_parquet('train_pairs_w5_helm_r45.parquet', index=False)\n",
        "test_pairs_w5_helm_r45.to_parquet('test_pairs_w5_helm_r45.parquet', index=False)\n",
        "\n",
        "def add_dyn_feats(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.sort_values(['game_play','p1','p2','step']).copy()\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "    df['approaching_flag'] = (df['closing'] < 0).astype(int)\n",
        "    denom = (-df['closing']).clip(lower=1e-3)\n",
        "    ttc_raw = df['distance'] / denom\n",
        "    ttc_raw = ttc_raw.where(df['approaching_flag'] == 1, 10.0)\n",
        "    df['ttc_raw'] = ttc_raw.astype(float)\n",
        "    df['ttc_clip'] = df['ttc_raw'].clip(0, 5)\n",
        "    df['ttc_log'] = np.log1p(df['ttc_clip'])\n",
        "    df['inv_ttc'] = 1.0 / (1.0 + df['ttc_clip'])\n",
        "    df['d_dist_1'] = df['distance'] - grp['distance'].shift(1)\n",
        "    df['d_dist_2'] = df['distance'] - grp['distance'].shift(2)\n",
        "    df['d_dist_5'] = df['distance'] - grp['distance'].shift(5)\n",
        "    df['d_close_1'] = df['closing'] - grp['closing'].shift(1)\n",
        "    df['d_absclose_1'] = df['abs_closing'] - grp['abs_closing'].shift(1)\n",
        "    df['d_speed1_1'] = df['speed1'] - grp['speed1'].shift(1)\n",
        "    df['d_speed2_1'] = df['speed2'] - grp['speed2'].shift(1)\n",
        "    df['d_accel1_1'] = df['accel1'] - grp['accel1'].shift(1)\n",
        "    df['d_accel2_1'] = df['accel2'] - grp['accel2'].shift(1)\n",
        "    df['rm3_d_dist_1'] = grp['d_dist_1'].transform(lambda s: s.rolling(3, min_periods=1).mean())\n",
        "    df['rm3_d_close_1'] = grp['d_close_1'].transform(lambda s: s.rolling(3, min_periods=1).mean())\n",
        "    for c in ['d_dist_1','d_dist_2','d_dist_5','d_close_1','d_absclose_1','d_speed1_1','d_speed2_1','d_accel1_1','d_accel2_1','rm3_d_dist_1','rm3_d_close_1']:\n",
        "        df[c] = df[c].fillna(0.0)\n",
        "    df['rel_speed'] = (df['speed2'] - df['speed1']).astype(float)\n",
        "    df['abs_rel_speed'] = df['rel_speed'].abs()\n",
        "    df['rel_accel'] = (df['accel2'] - df['accel1']).astype(float)\n",
        "    df['abs_rel_accel'] = df['rel_accel'].abs()\n",
        "    df['jerk1'] = grp['accel1'].diff().fillna(0.0)\n",
        "    df['jerk2'] = grp['accel2'].diff().fillna(0.0)\n",
        "    if 'px_dist_norm_min' in df.columns:\n",
        "        df['d_px_norm_1'] = df['px_dist_norm_min'] - grp['px_dist_norm_min'].shift(1)\n",
        "        df['d_px_norm_1'] = df['d_px_norm_1'].fillna(0.0)\n",
        "        df['cnt_px_lt006_p3'] = grp['px_dist_norm_min'].transform(lambda s: s.lt(0.06).rolling(3, min_periods=1).sum()).astype(float)\n",
        "        df['cnt_px_lt008_p3'] = grp['px_dist_norm_min'].transform(lambda s: s.lt(0.08).rolling(3, min_periods=1).sum()).astype(float)\n",
        "    else:\n",
        "        df['d_px_norm_1'] = 0.0; df['cnt_px_lt006_p3'] = 0.0; df['cnt_px_lt008_p3'] = 0.0\n",
        "    return df\n",
        "\n",
        "print('Adding dyn features (train/test) ...')\n",
        "tr_dyn_r45 = add_dyn_feats(train_pairs_w5_helm_r45)\n",
        "te_dyn_r45 = add_dyn_feats(test_pairs_w5_helm_r45)\n",
        "tr_dyn_r45.to_parquet('train_pairs_w5_helm_dyn_r45.parquet', index=False)\n",
        "te_dyn_r45.to_parquet('test_pairs_w5_helm_dyn_r45.parquet', index=False)\n",
        "\n",
        "key_cols = ['game_play','step','p1','p2']\n",
        "lab_cols = key_cols + ['contact']\n",
        "labels_min = train_labels[lab_cols].copy()\n",
        "sup_r45 = labels_min.merge(tr_dyn_r45, on=key_cols, how='inner')\n",
        "print('Supervised(inner) r=4.5 before expansion:', sup_r45.shape, 'pos rate:', sup_r45['contact'].mean())\n",
        "pos = sup_r45.loc[sup_r45['contact'] == 1, ['game_play','p1','p2','step']]\n",
        "pos_m1 = pos.copy(); pos_m1['step'] = pos_m1['step'] - 1\n",
        "pos_p1 = pos.copy(); pos_p1['step'] = pos_p1['step'] + 1\n",
        "pos_exp = pd.concat([pos_m1, pos_p1], ignore_index=True).drop_duplicates()\n",
        "pos_exp['flag_pos_exp'] = 1\n",
        "sup_r45 = sup_r45.merge(pos_exp, on=['game_play','p1','p2','step'], how='left')\n",
        "sup_r45.loc[sup_r45['flag_pos_exp'] == 1, 'contact'] = 1\n",
        "sup_r45.drop(columns=['flag_pos_exp'], inplace=True)\n",
        "print('After positive expansion (r=4.5): pos rate:', sup_r45['contact'].mean())\n",
        "sup_r45.to_parquet('train_supervised_w5_helm_dyn_r45.parquet', index=False)\n",
        "\n",
        "print('Done r=4.5 rebuild in {:.1f}s'.format(time.time()-t0), flush=True)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rebuilding pipeline with r=4.5 ...\nBuilding train pairs r=4.5 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 500 steps; +0.7s; total 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1000 steps; +0.6s; total 1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1500 steps; +0.6s; total 1.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2000 steps; +0.6s; total 2.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2500 steps; +0.6s; total 3.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3000 steps; +0.6s; total 3.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3500 steps; +0.6s; total 4.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4000 steps; +0.6s; total 4.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4500 steps; +0.6s; total 5.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5000 steps; +0.6s; total 6.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5500 steps; +0.6s; total 6.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 6000 steps; +0.6s; total 7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 6500 steps; +1.6s; total 8.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 7000 steps; +0.6s; total 9.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 7500 steps; +0.6s; total 9.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 8000 steps; +0.6s; total 10.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 8500 steps; +0.6s; total 11.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 9000 steps; +0.6s; total 11.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 9500 steps; +0.6s; total 12.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 10000 steps; +0.6s; total 12.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 10500 steps; +0.6s; total 13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 11000 steps; +0.6s; total 13.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 11500 steps; +0.5s; total 14.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 12000 steps; +0.6s; total 15.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 12500 steps; +0.6s; total 15.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 13000 steps; +0.6s; total 16.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 13500 steps; +0.6s; total 16.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 14000 steps; +0.6s; total 17.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 14500 steps; +0.6s; total 18.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 15000 steps; +0.6s; total 18.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 15500 steps; +0.6s; total 19.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 16000 steps; +1.7s; total 20.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 16500 steps; +0.6s; total 21.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 17000 steps; +0.6s; total 22.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 17500 steps; +0.6s; total 22.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 18000 steps; +0.6s; total 23.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 18500 steps; +0.6s; total 23.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 19000 steps; +0.6s; total 24.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 19500 steps; +0.6s; total 24.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 20000 steps; +0.6s; total 25.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 20500 steps; +0.6s; total 26.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 21000 steps; +0.6s; total 26.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 21500 steps; +0.6s; total 27.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 22000 steps; +0.6s; total 27.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 22500 steps; +0.6s; total 28.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 23000 steps; +0.6s; total 28.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 23500 steps; +0.6s; total 29.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 24000 steps; +0.6s; total 30.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 24500 steps; +0.6s; total 30.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 25000 steps; +0.6s; total 31.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 25500 steps; +0.6s; total 31.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 26000 steps; +0.6s; total 32.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 26500 steps; +0.6s; total 32.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 27000 steps; +0.6s; total 33.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 27500 steps; +1.9s; total 35.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 28000 steps; +0.6s; total 35.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 28500 steps; +0.6s; total 36.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 29000 steps; +0.6s; total 37.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 29500 steps; +0.6s; total 37.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 30000 steps; +0.6s; total 38.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 30500 steps; +0.6s; total 38.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 31000 steps; +0.6s; total 39.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 31500 steps; +0.6s; total 40.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 32000 steps; +0.6s; total 40.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 32500 steps; +0.6s; total 41.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 33000 steps; +0.6s; total 41.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 33500 steps; +0.6s; total 42.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 34000 steps; +0.6s; total 42.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 34500 steps; +0.6s; total 43.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 35000 steps; +0.6s; total 44.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 35500 steps; +0.6s; total 44.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 36000 steps; +0.6s; total 45.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 36500 steps; +0.6s; total 45.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 37000 steps; +0.6s; total 46.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 37500 steps; +0.6s; total 46.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 38000 steps; +0.6s; total 47.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 38500 steps; +0.6s; total 48.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 39000 steps; +0.6s; total 48.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 39500 steps; +0.6s; total 49.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 40000 steps; +0.6s; total 49.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 40500 steps; +0.6s; total 50.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 41000 steps; +0.6s; total 51.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 41500 steps; +2.1s; total 53.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 42000 steps; +0.6s; total 53.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 42500 steps; +0.6s; total 54.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 43000 steps; +0.6s; total 54.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 43500 steps; +0.6s; total 55.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 44000 steps; +0.6s; total 56.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 44500 steps; +0.6s; total 56.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 45000 steps; +0.6s; total 57.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 45500 steps; +0.6s; total 57.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 46000 steps; +0.6s; total 58.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 46500 steps; +0.6s; total 58.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 47000 steps; +0.6s; total 59.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 47500 steps; +0.6s; total 60.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 48000 steps; +0.6s; total 60.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 48500 steps; +0.6s; total 61.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 49000 steps; +0.6s; total 61.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 49500 steps; +0.6s; total 62.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 50000 steps; +0.6s; total 62.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 50500 steps; +0.6s; total 63.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 51000 steps; +0.6s; total 64.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 51500 steps; +0.6s; total 64.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 52000 steps; +0.6s; total 65.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 52500 steps; +0.6s; total 65.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 53000 steps; +0.6s; total 66.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 53500 steps; +0.6s; total 67.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 54000 steps; +0.6s; total 67.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 54500 steps; +0.6s; total 68.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 55000 steps; +0.6s; total 68.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 55500 steps; +0.6s; total 69.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_pairs_r45: (2828916, 19)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building test pairs r=4.5 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 500 steps; +0.6s; total 76.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1000 steps; +0.6s; total 77.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 1500 steps; +0.6s; total 78.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2000 steps; +0.6s; total 78.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 2500 steps; +0.6s; total 79.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3000 steps; +0.6s; total 79.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 3500 steps; +0.6s; total 80.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4000 steps; +0.6s; total 80.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 4500 steps; +0.6s; total 81.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5000 steps; +0.6s; total 82.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " processed 5500 steps; +0.6s; total 82.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_pairs_r45: (319769, 19)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding W5 features (train/test) for r=4.5 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading helmets and video metadata...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing helmet aggregates...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helmet agg shapes: (620840, 8) (67667, 8)\nMerging helmets into pairs (train/test) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding dyn features (train/test) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supervised(inner) r=4.5 before expansion: (745624, 59) pos rate: 0.05718431810134867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After positive expansion (r=4.5): pos rate: 0.06546060749117517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done r=4.5 rebuild in 395.7s\n"
          ]
        }
      ]
    },
    {
      "id": "8b2f3571-d288-4c6c-8940-73ff861cfc80",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PP r=4.5 bagging with thresholds optimized AFTER hysteresis per fold (cap=2), fold-median thresholds, identical test chain, then G overwrite\n",
        "import time, numpy as np, pandas as pd, sys, subprocess\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.4'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version (pp-bag-r45-thr-after-hyst):', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def apply_hyst_per_pair(df_bin: pd.DataFrame) -> np.ndarray:\n",
        "    df_h = df_bin.sort_values(['game_play','p1','p2','step']).copy()\n",
        "    grp = df_h.groupby(['game_play','p1','p2'], sort=False)['pred_bin']\n",
        "    df_h['pred_hyst'] = grp.transform(lambda s: (s.rolling(3, center=True, min_periods=1).sum() >= 2).astype(int))\n",
        "    return df_h['pred_hyst'].to_numpy()\n",
        "\n",
        "t0 = time.time()\n",
        "print('Loading r=4.5 supervised dyn train and test features...')\n",
        "train_sup = pd.read_parquet('train_supervised_w5_helm_dyn_r45.parquet')\n",
        "test_feats = pd.read_parquet('test_pairs_w5_helm_dyn_r45.parquet')\n",
        "folds_df = pd.read_csv('folds_game_play.csv')\n",
        "train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "assert train_sup['fold'].notna().all()\n",
        "for df in (train_sup, test_feats):\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "\n",
        "drop_cols = {'contact','game_play','step','p1','p2','team1','team2','pos1','pos2','fold'}\n",
        "feat_cols = [c for c in train_sup.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_sup[c])]\n",
        "print('Using', len(feat_cols), 'features')\n",
        "\n",
        "# Canonical order\n",
        "ord_idx = train_sup[['game_play','p1','p2','step']].sort_values(['game_play','p1','p2','step']).index.to_numpy()\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train_sup['game_play'].values\n",
        "y_all = train_sup['contact'].astype(int).values\n",
        "same_all = train_sup['same_team'].fillna(0).astype(int).values if 'same_team' in train_sup.columns else np.zeros(len(train_sup), np.int8)\n",
        "fold_arr = train_sup['fold'].astype(int).to_numpy()\n",
        "\n",
        "seeds = [42,1337,2025]\n",
        "oof_s_list = []; test_s_list = []\n",
        "\n",
        "for s in seeds:\n",
        "    print(f' PP r=4.5 seed {s} ...', flush=True)\n",
        "    X_all = train_sup[feat_cols].astype(float).values\n",
        "    oof = np.full(len(train_sup), np.nan, float)\n",
        "    models = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "        t1 = time.time()\n",
        "        X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "        X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "        neg = (y_tr == 0).sum(); posc = (y_tr == 1).sum()\n",
        "        spw = max(1.0, neg / max(1, posc))\n",
        "        dtrain = xgb.DMatrix(X_tr, label=y_tr); dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "        params = {'tree_method':'hist','device':'cuda','max_depth':7,'eta':0.05,'subsample':0.9,'colsample_bytree':0.8,\n",
        "                  'min_child_weight':10,'lambda':1.5,'alpha':0.1,'gamma':0.1,'objective':'binary:logistic','eval_metric':'logloss',\n",
        "                  'scale_pos_weight': float(spw), 'seed': int(s + fold)}\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=3800, evals=[(dtrain,'train'),(dvalid,'valid')], early_stopping_rounds=200, verbose_eval=False)\n",
        "        best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "        oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "        models.append((booster, best_it))\n",
        "        print(f'   seed {s} fold {fold} done in {time.time()-t1:.1f}s; best_it={best_it}', flush=True)\n",
        "    # Smooth OOF on canonical order\n",
        "    df = train_sup[['game_play','p1','p2','step']].iloc[ord_idx].copy()\n",
        "    df['oof'] = oof[ord_idx]\n",
        "    df = df.sort_values(['game_play','p1','p2','step'])\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    df['oof_smooth'] = grp['oof'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    oof_s_list.append(df['oof_smooth'].to_numpy())\n",
        "\n",
        "    # Test predictions and smoothing\n",
        "    Xt = test_feats[feat_cols].astype(float).values\n",
        "    dtest = xgb.DMatrix(Xt)\n",
        "    pt = np.zeros(len(test_feats), float)\n",
        "    for i, (booster, best_it) in enumerate(models):\n",
        "        t1 = time.time(); pt += booster.predict(dtest, iteration_range=(0, best_it + 1));\n",
        "        print(f'    seed {s} test model {i} {time.time()-t1:.1f}s', flush=True)\n",
        "    pt /= max(1, len(models))\n",
        "    dt = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step'])\n",
        "    dt['prob'] = pt[dt.index.values]\n",
        "    grp_t = dt.groupby(['game_play','p1','p2'], sort=False)\n",
        "    dt['prob_smooth'] = grp_t['prob'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    test_s_list.append(dt['prob_smooth'].to_numpy())\n",
        "\n",
        "# Average OOF across seeds in canonical order\n",
        "oof_avg = np.mean(np.vstack(oof_s_list), axis=0)\n",
        "keys_tr_sorted = train_sup[['game_play','p1','p2','step']].iloc[ord_idx].copy().reset_index(drop=True)\n",
        "y_sorted = train_sup['contact'].astype(int).to_numpy()[ord_idx]\n",
        "same_sorted = train_sup['same_team'].fillna(0).astype(int).to_numpy()[ord_idx] if 'same_team' in train_sup.columns else np.zeros_like(y_sorted, np.int8)\n",
        "fold_sorted = fold_arr[ord_idx]\n",
        "\n",
        "# Apply cap=2 on OOF probs before thresholding\n",
        "df_o = keys_tr_sorted.copy()\n",
        "df_o['prob'] = oof_avg\n",
        "df_o['row_id'] = np.arange(len(df_o))\n",
        "long1 = df_o[['game_play','step','p1','prob','row_id']].rename(columns={'p1':'player'})\n",
        "long2 = df_o[['game_play','step','p2','prob','row_id']].rename(columns={'p2':'player'})\n",
        "df_long = pd.concat([long1, long2], ignore_index=True)\n",
        "df_long = df_long.sort_values(['game_play','step','player','prob'], ascending=[True, True, True, False])\n",
        "df_long['rank'] = df_long.groupby(['game_play','step','player'], sort=False)['prob'].rank(method='first', ascending=False)\n",
        "kept_rows = set(df_long.loc[df_long['rank'] <= 2, 'row_id'].tolist())\n",
        "keep_mask_all = df_o['row_id'].isin(kept_rows).to_numpy()\n",
        "oof_cap = oof_avg.copy(); oof_cap[~keep_mask_all] = 0.0\n",
        "print('Applied cap=2 to OOF. Kept rows:', int(keep_mask_all.sum()), 'of', len(keep_mask_all))\n",
        "\n",
        "# Optimize thresholds AFTER hysteresis per fold\n",
        "thr_grid = np.round(np.linspace(0.70, 0.85, 16), 3)\n",
        "thr_best = []\n",
        "for k in sorted(np.unique(fold_sorted)):\n",
        "    m = (fold_sorted == k)\n",
        "    df_k = keys_tr_sorted.loc[m, ['game_play','p1','p2','step']].copy()\n",
        "    df_k['prob'] = oof_cap[m]\n",
        "    df_k['same'] = same_sorted[m]\n",
        "    y_k = y_sorted[m]\n",
        "    best_m, best_to, best_ts = -1.0, 0.78, 0.78\n",
        "    same_arr = df_k['same'].to_numpy()\n",
        "    prob_arr = df_k['prob'].to_numpy()\n",
        "    for to in thr_grid:\n",
        "        for ts in thr_grid:\n",
        "            thr_arr = np.where(same_arr == 1, ts, to)\n",
        "            pred_bin = (prob_arr >= thr_arr).astype(int)\n",
        "            df_tmp = df_k[['game_play','p1','p2','step']].copy()\n",
        "            df_tmp['pred_bin'] = pred_bin\n",
        "            pred_h = apply_hyst_per_pair(df_tmp)\n",
        "            mcc = matthews_corrcoef(y_k, pred_h)\n",
        "            if mcc > best_m:\n",
        "                best_m, best_to, best_ts = float(mcc), float(to), float(ts)\n",
        "    thr_best.append((best_to, best_ts))\n",
        "    print(f' Fold {k} best after-hyst MCC={best_m:.5f} thr_opp={best_to:.3f} thr_same={best_ts:.3f}')\n",
        "\n",
        "thr_best = np.array(thr_best, float)\n",
        "thr_opp_med = float(np.median(thr_best[:, 0]))\n",
        "thr_same_med = float(np.median(thr_best[:, 1]))\n",
        "print(f'Fold-median thresholds after hysteresis (r=4.5, cap2): thr_opp={thr_opp_med:.4f}, thr_same={thr_same_med:.4f}')\n",
        "\n",
        "# Test: average probs across seeds, smooth, cap=2, then apply median thresholds, then hysteresis\n",
        "pt_bag = np.mean(np.vstack(test_s_list), axis=0)\n",
        "df_t = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step']).reset_index(drop=True)\n",
        "df_t['prob_smooth'] = pt_bag\n",
        "df_t['row_id'] = np.arange(len(df_t))\n",
        "long1t = df_t[['game_play','step','p1','prob_smooth','row_id']].rename(columns={'p1':'player','prob_smooth':'prob'})\n",
        "long2t = df_t[['game_play','step','p2','prob_smooth','row_id']].rename(columns={'p2':'player','prob_smooth':'prob'})\n",
        "df_long_t = pd.concat([long1t, long2t], ignore_index=True)\n",
        "df_long_t = df_long_t.sort_values(['game_play','step','player','prob'], ascending=[True, True, True, False])\n",
        "df_long_t['rank'] = df_long_t.groupby(['game_play','step','player'], sort=False)['prob'].rank(method='first', ascending=False)\n",
        "kept_rows_t = set(df_long_t.loc[df_long_t['rank'] <= 2, 'row_id'].tolist())\n",
        "keep_mask_t = df_t['row_id'].isin(kept_rows_t).to_numpy()\n",
        "df_t.loc[~keep_mask_t, 'prob_smooth'] = 0.0\n",
        "print('Applied cap=2 on test. Kept rows:', int(keep_mask_t.sum()), 'of', len(keep_mask_t))\n",
        "\n",
        "same_flag_test = test_feats[['game_play','p1','p2','step','same_team']].copy()\n",
        "same_flag_test = same_flag_test.merge(df_t[['game_play','p1','p2','step','row_id']], on=['game_play','p1','p2','step'], how='right').sort_values('row_id')\n",
        "same_arr_t = same_flag_test['same_team'].fillna(0).astype(int).to_numpy() if 'same_team' in same_flag_test.columns else np.zeros(len(df_t), int)\n",
        "thr_arr_t = np.where(same_arr_t == 1, thr_same_med, thr_opp_med)\n",
        "df_t['pred_bin'] = (df_t['prob_smooth'].to_numpy() >= thr_arr_t).astype(int)\n",
        "\n",
        "df_tmp_t = df_t[['game_play','p1','p2','step','pred_bin']].copy()\n",
        "pred_h_t = apply_hyst_per_pair(df_tmp_t)\n",
        "df_t['pred_hyst'] = pred_h_t.astype(int)\n",
        "\n",
        "# Build submission with PP, then overwrite G rows from prior submission\n",
        "cid_sorted = (df_t['game_play'].astype(str) + '_' + df_t['step'].astype(str) + '_' + df_t['p1'].astype(str) + '_' + df_t['p2'].astype(str))\n",
        "pred_df_pp = pd.DataFrame({'contact_id': cid_sorted.values, 'contact_pp': df_t['pred_hyst'].astype(int).values})\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.merge(pred_df_pp, on='contact_id', how='left')\n",
        "sub['contact'] = sub['contact_pp'].fillna(0).astype(int)\n",
        "sub = sub.drop(columns=['contact_pp'])\n",
        "pp_ones = int(sub['contact'].sum())\n",
        "print('PP (r45 bag + thr-after-hyst cap2) ones before G overwrite:', pp_ones)\n",
        "try:\n",
        "    prev_sub = pd.read_csv('submission.csv')\n",
        "    g_pred_second = prev_sub[prev_sub['contact_id'].str.endswith('_G')][['contact_id','contact']].rename(columns={'contact':'contact_g'})\n",
        "    sub = sub.merge(g_pred_second, on='contact_id', how='left')\n",
        "    sub['contact'] = sub['contact_g'].fillna(sub['contact']).astype(int)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "    after_ones = int(sub['contact'].sum())\n",
        "    print(f'Applied prior G overwrite. ones after={after_ones}, delta={after_ones-pp_ones}')\n",
        "except Exception as e:\n",
        "    print('No prior submission with G rows found; skipping G overwrite.', e)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv. Took {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version (pp-bag-r45-thr-after-hyst): 2.1.4\nLoading r=4.5 supervised dyn train and test features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 50 features\n PP r=4.5 seed 42 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 0 done in 46.2s; best_it=3688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 1 done in 44.1s; best_it=3754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 2 done in 43.0s; best_it=3466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 3 done in 39.5s; best_it=3177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 4 done in 44.4s; best_it=3799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.5 seed 1337 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 0 done in 46.4s; best_it=3799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 1 done in 44.8s; best_it=3799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 2 done in 43.1s; best_it=3467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 3 done in 37.8s; best_it=2982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 4 done in 45.2s; best_it=3777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.5 seed 2025 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 0 done in 46.7s; best_it=3798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 1 done in 45.1s; best_it=3796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 2 done in 43.8s; best_it=3519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 3 done in 42.3s; best_it=3358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 4 done in 44.4s; best_it=3716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied cap=2 to OOF. Kept rows: 333340 of 745624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 0 best after-hyst MCC=0.71485 thr_opp=0.820 thr_same=0.810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 1 best after-hyst MCC=0.74275 thr_opp=0.850 thr_same=0.810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 2 best after-hyst MCC=0.73672 thr_opp=0.840 thr_same=0.740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 3 best after-hyst MCC=0.73139 thr_opp=0.770 thr_same=0.710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 4 best after-hyst MCC=0.73835 thr_opp=0.790 thr_same=0.830\nFold-median thresholds after hysteresis (r=4.5, cap2): thr_opp=0.8200, thr_same=0.8100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied cap=2 on test. Kept rows: 131297 of 319769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP (r45 bag + thr-after-hyst cap2) ones before G overwrite: 6260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied prior G overwrite. ones after=8312, delta=2052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv. Took 1810.4s\n"
          ]
        }
      ]
    },
    {
      "id": "a84cf069-f597-4a74-881e-35266f911f7d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PP r=4.5 bagging with distance-aware caps (3/2/1) and thresholds optimized AFTER hysteresis per fold; fold-median thresholds; identical test chain; then G overwrite\n",
        "import time, numpy as np, pandas as pd, sys, subprocess\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception as e:\n",
        "    print('Installing xgboost...', e)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.4'], check=True)\n",
        "    import xgboost as xgb\n",
        "print('xgboost version (pp-bag-r45-cap321-thr-after-hyst):', getattr(xgb, '__version__', 'unknown'))\n",
        "\n",
        "def apply_hyst_per_pair(df_bin: pd.DataFrame) -> np.ndarray:\n",
        "    df_h = df_bin.sort_values(['game_play','p1','p2','step']).copy()\n",
        "    grp = df_h.groupby(['game_play','p1','p2'], sort=False)['pred_bin']\n",
        "    df_h['pred_hyst'] = grp.transform(lambda s: (s.rolling(3, center=True, min_periods=1).sum() >= 2).astype(int))\n",
        "    return df_h['pred_hyst'].to_numpy()\n",
        "\n",
        "t0 = time.time()\n",
        "print('Loading r=4.5 supervised dyn train and test features...')\n",
        "train_sup = pd.read_parquet('train_supervised_w5_helm_dyn_r45.parquet')\n",
        "test_feats = pd.read_parquet('test_pairs_w5_helm_dyn_r45.parquet')\n",
        "folds_df = pd.read_csv('folds_game_play.csv')\n",
        "train_sup = train_sup.merge(folds_df, on='game_play', how='left')\n",
        "assert train_sup['fold'].notna().all()\n",
        "for df in (train_sup, test_feats):\n",
        "    if 'px_dist_norm_min' in df.columns: df['px_dist_norm_min'] = df['px_dist_norm_min'].fillna(1.0)\n",
        "    if 'views_both_present' in df.columns: df['views_both_present'] = df['views_both_present'].fillna(0).astype(float)\n",
        "\n",
        "drop_cols = {'contact','game_play','step','p1','p2','team1','team2','pos1','pos2','fold'}\n",
        "feat_cols = [c for c in train_sup.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_sup[c])]\n",
        "print('Using', len(feat_cols), 'features')\n",
        "\n",
        "# Canonical order\n",
        "ord_idx = train_sup[['game_play','p1','p2','step']].sort_values(['game_play','p1','p2','step']).index.to_numpy()\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train_sup['game_play'].values\n",
        "y_all = train_sup['contact'].astype(int).values\n",
        "same_all = train_sup['same_team'].fillna(0).astype(int).values if 'same_team' in train_sup.columns else np.zeros(len(train_sup), np.int8)\n",
        "fold_arr = train_sup['fold'].astype(int).to_numpy()\n",
        "\n",
        "seeds = [42,1337,2025]\n",
        "oof_s_list = []; test_s_list = []\n",
        "\n",
        "for s in seeds:\n",
        "    print(f' PP r=4.5 seed {s} ...', flush=True)\n",
        "    X_all = train_sup[feat_cols].astype(float).values\n",
        "    oof = np.full(len(train_sup), np.nan, float)\n",
        "    models = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
        "        t1 = time.time()\n",
        "        X_tr, y_tr = X_all[tr_idx], y_all[tr_idx]\n",
        "        X_va, y_va = X_all[va_idx], y_all[va_idx]\n",
        "        neg = (y_tr == 0).sum(); posc = (y_tr == 1).sum()\n",
        "        spw = max(1.0, neg / max(1, posc))\n",
        "        dtrain = xgb.DMatrix(X_tr, label=y_tr); dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "        params = {'tree_method':'hist','device':'cuda','max_depth':7,'eta':0.05,'subsample':0.9,'colsample_bytree':0.8,\n",
        "                  'min_child_weight':10,'lambda':1.5,'alpha':0.1,'gamma':0.1,'objective':'binary:logistic','eval_metric':'logloss',\n",
        "                  'scale_pos_weight': float(spw), 'seed': int(s + fold)}\n",
        "        booster = xgb.train(params, dtrain, num_boost_round=3800, evals=[(dtrain,'train'),(dvalid,'valid')], early_stopping_rounds=200, verbose_eval=False)\n",
        "        best_it = int(getattr(booster, 'best_iteration', None) or booster.num_boosted_rounds() - 1)\n",
        "        oof[va_idx] = booster.predict(dvalid, iteration_range=(0, best_it + 1))\n",
        "        models.append((booster, best_it))\n",
        "        print(f'   seed {s} fold {fold} done in {time.time()-t1:.1f}s; best_it={best_it}', flush=True)\n",
        "    # Smooth OOF on canonical order\n",
        "    df = train_sup[['game_play','p1','p2','step']].iloc[ord_idx].copy()\n",
        "    df['oof'] = oof[ord_idx]\n",
        "    df = df.sort_values(['game_play','p1','p2','step'])\n",
        "    grp = df.groupby(['game_play','p1','p2'], sort=False)\n",
        "    df['oof_smooth'] = grp['oof'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    oof_s_list.append(df['oof_smooth'].to_numpy())\n",
        "\n",
        "    # Test predictions and smoothing\n",
        "    Xt = test_feats[feat_cols].astype(float).values\n",
        "    dtest = xgb.DMatrix(Xt)\n",
        "    pt = np.zeros(len(test_feats), float)\n",
        "    for i, (booster, best_it) in enumerate(models):\n",
        "        t1 = time.time(); pt += booster.predict(dtest, iteration_range=(0, best_it + 1));\n",
        "        print(f'    seed {s} test model {i} {time.time()-t1:.1f}s', flush=True)\n",
        "    pt /= max(1, len(models))\n",
        "    dt = test_feats[['game_play','p1','p2','step']].copy().sort_values(['game_play','p1','p2','step'])\n",
        "    idx_sorted = test_feats[['game_play','p1','p2','step']].sort_values(['game_play','p1','p2','step']).index.values\n",
        "    pt_sorted = pt[idx_sorted]\n",
        "    dt['prob'] = pt_sorted\n",
        "    grp_t = dt.groupby(['game_play','p1','p2'], sort=False)\n",
        "    dt['prob_smooth'] = grp_t['prob'].transform(lambda s_: s_.rolling(3, center=True, min_periods=1).max())\n",
        "    test_s_list.append(dt['prob_smooth'].to_numpy())\n",
        "\n",
        "# Average OOF across seeds in canonical order\n",
        "oof_avg = np.mean(np.vstack(oof_s_list), axis=0)\n",
        "keys_tr_sorted = train_sup[['game_play','p1','p2','step','distance']].iloc[ord_idx].copy().reset_index(drop=True)\n",
        "y_sorted = train_sup['contact'].astype(int).to_numpy()[ord_idx]\n",
        "same_sorted = train_sup['same_team'].fillna(0).astype(int).to_numpy()[ord_idx] if 'same_team' in train_sup.columns else np.zeros_like(y_sorted, np.int8)\n",
        "fold_sorted = fold_arr[ord_idx]\n",
        "\n",
        "# Distance-aware caps (<=1.6: top-3, 1.6-2.4: top-2, >2.4: top-1) applied on smoothed OOF BEFORE thresholding\n",
        "df_o = keys_tr_sorted.copy()\n",
        "df_o['prob'] = oof_avg\n",
        "df_o['row_id'] = np.arange(len(df_o))\n",
        "df_o['bin'] = np.where(df_o['distance'] <= 1.6, 0, np.where(df_o['distance'] <= 2.4, 1, 2))\n",
        "cap_map = {0:3, 1:2, 2:1}\n",
        "long1 = df_o[['game_play','step','p1','prob','row_id','bin']].rename(columns={'p1':'player'})\n",
        "long2 = df_o[['game_play','step','p2','prob','row_id','bin']].rename(columns={'p2':'player'})\n",
        "df_long = pd.concat([long1, long2], ignore_index=True)\n",
        "df_long = df_long.sort_values(['game_play','step','player','bin','prob'], ascending=[True, True, True, True, False])\n",
        "df_long['rank_in_bin'] = df_long.groupby(['game_play','step','player','bin'], sort=False)['prob'].rank(method='first', ascending=False)\n",
        "keep_rows = []\n",
        "for b, cap in cap_map.items():\n",
        "    keep_rows.append(df_long.loc[(df_long['bin'] == b) & (df_long['rank_in_bin'] <= cap), 'row_id'])\n",
        "kept_rows = set(pd.concat(keep_rows).tolist())\n",
        "keep_mask_all = df_o['row_id'].isin(kept_rows).to_numpy()\n",
        "oof_cap = oof_avg.copy(); oof_cap[~keep_mask_all] = 0.0\n",
        "print('Applied distance-aware caps (3/2/1) to OOF. Kept rows:', int(keep_mask_all.sum()), 'of', len(keep_mask_all))\n",
        "\n",
        "# Optimize thresholds AFTER hysteresis per fold\n",
        "thr_grid = np.round(np.linspace(0.70, 0.85, 16), 3)\n",
        "thr_best = []\n",
        "for k in sorted(np.unique(fold_sorted)):\n",
        "    m = (fold_sorted == k)\n",
        "    df_k = keys_tr_sorted.loc[m, ['game_play','p1','p2','step']].copy()\n",
        "    df_k['prob'] = oof_cap[m]\n",
        "    df_k['same'] = same_sorted[m]\n",
        "    y_k = y_sorted[m]\n",
        "    best_m, best_to, best_ts = -1.0, 0.78, 0.78\n",
        "    same_arr = df_k['same'].to_numpy()\n",
        "    prob_arr = df_k['prob'].to_numpy()\n",
        "    for to in thr_grid:\n",
        "        for ts in thr_grid:\n",
        "            thr_arr = np.where(same_arr == 1, ts, to)\n",
        "            pred_bin = (prob_arr >= thr_arr).astype(int)\n",
        "            df_tmp = df_k[['game_play','p1','p2','step']].copy()\n",
        "            df_tmp['pred_bin'] = pred_bin\n",
        "            pred_h = apply_hyst_per_pair(df_tmp)\n",
        "            mcc = matthews_corrcoef(y_k, pred_h)\n",
        "            if mcc > best_m:\n",
        "                best_m, best_to, best_ts = float(mcc), float(to), float(ts)\n",
        "    thr_best.append((best_to, best_ts))\n",
        "    print(f' Fold {k} best after-hyst MCC={best_m:.5f} thr_opp={best_to:.3f} thr_same={best_ts:.3f}')\n",
        "\n",
        "thr_best = np.array(thr_best, float)\n",
        "thr_opp_med = float(np.median(thr_best[:, 0]))\n",
        "thr_same_med = float(np.median(thr_best[:, 1]))\n",
        "print(f'Fold-median thresholds after hysteresis (r=4.5 cap3/2/1): thr_opp={thr_opp_med:.4f}, thr_same={thr_same_med:.4f}')\n",
        "\n",
        "# Test: average probs, smooth, apply distance-aware caps, then median thresholds, then hysteresis\n",
        "pt_bag = np.mean(np.vstack(test_s_list), axis=0)\n",
        "df_t = test_feats[['game_play','p1','p2','step','distance']].copy().sort_values(['game_play','p1','p2','step']).reset_index(drop=True)\n",
        "df_t['prob_smooth'] = pt_bag\n",
        "df_t['row_id'] = np.arange(len(df_t))\n",
        "df_t['bin'] = np.where(df_t['distance'] <= 1.6, 0, np.where(df_t['distance'] <= 2.4, 1, 2))\n",
        "long1t = df_t[['game_play','step','p1','prob_smooth','row_id','bin']].rename(columns={'p1':'player','prob_smooth':'prob'})\n",
        "long2t = df_t[['game_play','step','p2','prob_smooth','row_id','bin']].rename(columns={'p2':'player','prob_smooth':'prob'})\n",
        "df_long_t = pd.concat([long1t, long2t], ignore_index=True)\n",
        "df_long_t = df_long_t.sort_values(['game_play','step','player','bin','prob'], ascending=[True, True, True, True, False])\n",
        "df_long_t['rank_in_bin'] = df_long_t.groupby(['game_play','step','player','bin'], sort=False)['prob'].rank(method='first', ascending=False)\n",
        "keep_rows_t = []\n",
        "for b, cap in cap_map.items():\n",
        "    keep_rows_t.append(df_long_t.loc[(df_long_t['bin'] == b) & (df_long_t['rank_in_bin'] <= cap), 'row_id'])\n",
        "kept_rows_t = set(pd.concat(keep_rows_t).tolist())\n",
        "keep_mask_t = df_t['row_id'].isin(kept_rows_t).to_numpy()\n",
        "df_t.loc[~keep_mask_t, 'prob_smooth'] = 0.0\n",
        "print('Applied distance-aware caps (3/2/1) on test. Kept rows:', int(keep_mask_t.sum()), 'of', len(keep_mask_t))\n",
        "\n",
        "same_flag_test = test_feats[['game_play','p1','p2','step','same_team']].copy()\n",
        "same_flag_test = same_flag_test.merge(df_t[['game_play','p1','p2','step','row_id']], on=['game_play','p1','p2','step'], how='right').sort_values('row_id')\n",
        "same_arr_t = same_flag_test['same_team'].fillna(0).astype(int).to_numpy() if 'same_team' in same_flag_test.columns else np.zeros(len(df_t), int)\n",
        "thr_arr_t = np.where(same_arr_t == 1, thr_same_med, thr_opp_med)\n",
        "df_t['pred_bin'] = (df_t['prob_smooth'].to_numpy() >= thr_arr_t).astype(int)\n",
        "\n",
        "df_tmp_t = df_t[['game_play','p1','p2','step','pred_bin']].copy()\n",
        "pred_h_t = apply_hyst_per_pair(df_tmp_t)\n",
        "df_t['pred_hyst'] = pred_h_t.astype(int)\n",
        "\n",
        "# Build submission with PP, then overwrite G rows from prior submission\n",
        "cid_sorted = (df_t['game_play'].astype(str) + '_' + df_t['step'].astype(str) + '_' + df_t['p1'].astype(str) + '_' + df_t['p2'].astype(str))\n",
        "pred_df_pp = pd.DataFrame({'contact_id': cid_sorted.values, 'contact_pp': df_t['pred_hyst'].astype(int).values})\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = ss.merge(pred_df_pp, on='contact_id', how='left')\n",
        "sub['contact'] = sub['contact_pp'].fillna(0).astype(int)\n",
        "sub = sub.drop(columns=['contact_pp'])\n",
        "pp_ones = int(sub['contact'].sum())\n",
        "print('PP (r45 bag + cap3/2/1 thr-after-hyst) ones before G overwrite:', pp_ones)\n",
        "try:\n",
        "    prev_sub = pd.read_csv('submission.csv')\n",
        "    g_pred_second = prev_sub[prev_sub['contact_id'].str.endswith('_G')][['contact_id','contact']].rename(columns={'contact':'contact_g'})\n",
        "    sub = sub.merge(g_pred_second, on='contact_id', how='left')\n",
        "    sub['contact'] = sub['contact_g'].fillna(sub['contact']).astype(int)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "    after_ones = int(sub['contact'].sum())\n",
        "    print(f'Applied prior G overwrite. ones after={after_ones}, delta={after_ones-pp_ones}')\n",
        "except Exception as e:\n",
        "    print('No prior submission with G rows found; skipping G overwrite.', e)\n",
        "    sub = sub[['contact_id','contact']]\n",
        "\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv. Took {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost version (pp-bag-r45-cap321-thr-after-hyst): 2.1.4\nLoading r=4.5 supervised dyn train and test features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 50 features\n PP r=4.5 seed 42 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 0 done in 46.2s; best_it=3688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 1 done in 44.0s; best_it=3754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 2 done in 43.0s; best_it=3466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 3 done in 39.4s; best_it=3177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 42 fold 4 done in 44.4s; best_it=3799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 42 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.5 seed 1337 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 0 done in 46.3s; best_it=3799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 1 done in 44.6s; best_it=3799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 2 done in 42.8s; best_it=3467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 3 done in 37.7s; best_it=2982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 1337 fold 4 done in 45.1s; best_it=3777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 1337 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PP r=4.5 seed 2025 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 0 done in 46.7s; best_it=3798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 1 done in 45.1s; best_it=3796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 2 done in 43.8s; best_it=3519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 3 done in 42.3s; best_it=3358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   seed 2025 fold 4 done in 44.2s; best_it=3716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 0 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 1 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 2 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 3 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    seed 2025 test model 4 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied distance-aware caps (3/2/1) to OOF. Kept rows: 461191 of 745624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 0 best after-hyst MCC=0.71594 thr_opp=0.820 thr_same=0.810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 1 best after-hyst MCC=0.73929 thr_opp=0.850 thr_same=0.810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 2 best after-hyst MCC=0.73686 thr_opp=0.840 thr_same=0.740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 3 best after-hyst MCC=0.73407 thr_opp=0.770 thr_same=0.710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Fold 4 best after-hyst MCC=0.73886 thr_opp=0.790 thr_same=0.830\nFold-median thresholds after hysteresis (r=4.5 cap3/2/1): thr_opp=0.8200, thr_same=0.8100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied distance-aware caps (3/2/1) on test. Kept rows: 195880 of 319769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PP (r45 bag + cap3/2/1 thr-after-hyst) ones before G overwrite: 6459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied prior G overwrite. ones after=8511, delta=2052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv. Took 1808.0s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}