{
  "cells": [
    {
      "id": "cb5a3885-4eb3-49f2-b7bc-4aaa6a135170",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# INGV - Volcanic Eruption Prediction: Plan\n",
        "\n",
        "Goal: Ship a strong, validated baseline fast, then iterate to medal.\n",
        "\n",
        "Plan v0:\n",
        "- Verify environment (GPU) and repo contents\n",
        "- Inspect data schema (train.csv, sample_submission.csv, train/ and test/ folders)\n",
        "- Establish CV protocol mirroring test (grouped by series, temporal-safe)\n",
        "- Baseline features from raw signals (per-sensor stats, spectral features) with fast model (XGBoost GPU / CatBoost GPU)\n",
        "- Create a fast baseline submission\n",
        "- Iterate: richer features (STFT bands, autocorr, RMS, kurtosis, rolling windows), seed/fold repeats, simple blends\n",
        "\n",
        "Checkpoints to request expert review:\n",
        "1) After this plan\n",
        "2) After EDA + CV choice\n",
        "3) After baseline model + OOF\n",
        "4) After feature set v1/v2\n",
        "5) Before long training jobs / blends\n",
        "\n",
        "Metric: MAE on time_to_eruption. Output: submission.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "f3341ece-b857-4ac0-a95f-0e052f2c55df",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Env + Data schema check\n",
        "import os, sys, glob, subprocess, time, json\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "# 1) GPU check\n",
        "log('Checking GPU with nvidia-smi...')\n",
        "try:\n",
        "    res = subprocess.run(['bash','-lc','nvidia-smi || true'], capture_output=True, text=True)\n",
        "    print(res.stdout)\n",
        "except Exception as e:\n",
        "    log(f'GPU check failed: {e}')\n",
        "\n",
        "# 2) Repo contents\n",
        "log('Listing repo contents...')\n",
        "for p in sorted(os.listdir('.')):\n",
        "    try:\n",
        "        print(p, '->', len(os.listdir(p)) if os.path.isdir(p) else os.path.getsize(p))\n",
        "    except Exception:\n",
        "        print(p)\n",
        "\n",
        "# 3) Load CSVs\n",
        "log('Loading train.csv and sample_submission.csv')\n",
        "train = pd.read_csv('train.csv')\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "log(f'train.shape={train.shape}; columns={list(train.columns)}')\n",
        "log(f'sample_submission.shape={ss.shape}; columns={list(ss.columns)}')\n",
        "print(train.head(3))\n",
        "print(ss.head(3))\n",
        "\n",
        "# Basic target stats\n",
        "if 'time_to_eruption' in train.columns:\n",
        "    y = train['time_to_eruption']\n",
        "    log(f'target describe (secs): min={y.min()}, max={y.max()}, mean={y.mean():.1f}, median={y.median():.1f}')\n",
        "\n",
        "# 4) Train/Test files\n",
        "train_files = sorted(glob.glob('train/*.csv'))\n",
        "test_files = sorted(glob.glob('test/*.csv'))\n",
        "log(f'Found {len(train_files)} train files, {len(test_files)} test files')\n",
        "print('Example train files:', train_files[:3])\n",
        "print('Example test files:', test_files[:3])\n",
        "\n",
        "# Peek one train segment file to infer schema\n",
        "if train_files:\n",
        "    log(f'Loading sample segment file: {train_files[0]}')\n",
        "    df0 = pd.read_csv(train_files[0])\n",
        "    log(f'segment shape: {df0.shape}; cols: {list(df0.columns)[:10]}...')\n",
        "    print(df0.head(3))\n",
        "\n",
        "# 5) Infer series_id groups for CV suggestion\n",
        "if {'segment_id','time_to_eruption'}.issubset(train.columns):\n",
        "    tmp = train[['segment_id','time_to_eruption']].sort_values('segment_id').copy()\n",
        "    # Heuristic: series boundaries where t diff increases vs previous row\n",
        "    d = tmp['time_to_eruption'].diff()\n",
        "    series_id = (d > 0).cumsum().astype(int)\n",
        "    train['series_id_inferred'] = series_id.values\n",
        "    log(f'Inferred series count: {train.series_id_inferred.nunique()}')\n",
        "    print(train[['segment_id','time_to_eruption','series_id_inferred']].head(10))\n",
        "\n",
        "log('Env/data check complete.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:19:20.199431Z] Checking GPU with nvidia-smi...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 27 02:19:20 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     182MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\n[2025-09-27T02:19:20.224045Z] Listing repo contents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".00_eda_and_planning_kernel_state.json -> 182\n00_eda_and_planning.ipynb -> 4911\nagent_metadata -> 10\ndescription.md -> 4726\ndocker_run.log -> 39871\nrequirements.txt -> 2021\nsample_submission.csv -> 5559\nsubmission.csv -> 5559\ntask.txt -> 2836\ntest -> 444\ntrain -> 3987\ntrain.csv -> 76704\n[2025-09-27T02:19:20.227044Z] Loading train.csv and sample_submission.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:19:20.230176Z] train.shape=(3987, 2); columns=['segment_id', 'time_to_eruption']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:19:20.230595Z] sample_submission.shape=(444, 2); columns=['segment_id', 'time_to_eruption']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0  1410693848          20109998\n1   987159268           7202883\n2  1990984540          28138930\n   segment_id  time_to_eruption\n0   951290289                 0\n1   508758258                 0\n2  1566132188                 0\n[2025-09-27T02:19:20.233278Z] target describe (secs): min=6250, max=49046087, mean=22675929.1, median=22300345.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:19:20.239174Z] Found 3987 train files, 444 test files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example train files: ['train/1000015382.csv', 'train/1000554676.csv', 'train/1000745424.csv']\nExample test files: ['test/1003520023.csv', 'test/1004346803.csv', 'test/1007996426.csv']\n[2025-09-27T02:19:20.239677Z] Loading sample segment file: train/1000015382.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:19:20.275681Z] segment shape: (60001, 10); cols: ['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_10']...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sensor_1  sensor_2  sensor_3  sensor_4  sensor_5  sensor_6  sensor_7  \\\n0     260.0      64.0    -232.0     -36.0      -2.0     -35.0     103.0   \n1     233.0     175.0     146.0     160.0      -4.0      29.0    -120.0   \n2     216.0     236.0     321.0     202.0       2.0     113.0    -230.0   \n\n   sensor_8  sensor_9  sensor_10  \n0     389.0      67.0       41.0  \n1     498.0      59.0       63.0  \n2     554.0      97.0       90.0  \n[2025-09-27T02:19:20.282421Z] Inferred series count: 1973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption  series_id_inferred\n0  1410693848          20109998                   0\n1   987159268           7202883                   0\n2  1990984540          28138930                   0\n3   983270799            759643                   1\n4   116548092          47441128                   1\n5  1211468948          20770278                   2\n6   256946052          19237462                   2\n7  1109804924           1153377                   2\n8    48233383          28213217                   3\n9  1305598339          34191056                   4\n[2025-09-27T02:19:20.284125Z] Env/data check complete.\n"
          ]
        }
      ]
    },
    {
      "id": "23bcdf65-fc08-40cd-b1aa-de842addccd5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CV groups (series_id) + Fast feature pipeline smoke test\n",
        "import os, glob, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "# 1) Infer series_id from ORIGINAL train.csv order (per expert guidance)\n",
        "train_csv = pd.read_csv('train.csv')\n",
        "t = train_csv['time_to_eruption']\n",
        "series_id = (t.diff() > 0).cumsum().astype(int)\n",
        "train_csv['series_id_inferred'] = series_id\n",
        "log(f\"Series inferred (orig order): n_series={train_csv.series_id_inferred.nunique()} with sizes head: {train_csv.series_id_inferred.value_counts().head(5).to_dict()}\")\n",
        "\n",
        "# 2) File maps\n",
        "train_map = {int(os.path.splitext(os.path.basename(p))[0]): p for p in glob.glob('train/*.csv')}\n",
        "test_map = {int(os.path.splitext(os.path.basename(p))[0]): p for p in glob.glob('test/*.csv')}\n",
        "assert len(train_map) == len(train_csv), 'Mismatch train files vs train.csv rows'\n",
        "log(f\"Train files mapped: {len(train_map)}; Test files mapped: {len(test_map)}\")\n",
        "\n",
        "# 3) Feature extraction\n",
        "SENSORS = [f'sensor_{i}' for i in range(1,11)]\n",
        "WINDOW_SPECS = {\n",
        "    'full': (0.0, 1.0),\n",
        "    'half': (0.5, 1.0),\n",
        "    'quarter': (0.75, 1.0),\n",
        "}\n",
        "\n",
        "def robust_stats(x):\n",
        "    x = x.astype(np.float32)\n",
        "    med = np.median(x)\n",
        "    mad = np.median(np.abs(x - med))\n",
        "    iqr = np.subtract(*np.percentile(x, [75, 25]))\n",
        "    q05, q95 = np.percentile(x, [5, 95])\n",
        "    return {\n",
        "        'mean': float(np.mean(x)),\n",
        "        'std': float(np.std(x)),\n",
        "        'median': float(med),\n",
        "        'mad': float(mad),\n",
        "        'iqr': float(iqr),\n",
        "        'min': float(np.min(x)),\n",
        "        'max': float(np.max(x)),\n",
        "        'ptp': float(np.ptp(x)),\n",
        "        'q05': float(q05),\n",
        "        'q95': float(q95),\n",
        "    }\n",
        "\n",
        "def zcr(x):\n",
        "    x = x.astype(np.float32)\n",
        "    s = np.signbit(x)\n",
        "    return float(np.count_nonzero(s[1:] != s[:-1]) / (len(x) - 1))\n",
        "\n",
        "def line_length(x):\n",
        "    x = x.astype(np.float32)\n",
        "    return float(np.sum(np.abs(np.diff(x))) / (len(x) - 1))\n",
        "\n",
        "def rms(x):\n",
        "    x = x.astype(np.float32)\n",
        "    return float(np.sqrt(np.mean(x * x)))\n",
        "\n",
        "def extract_features_for_segment(segment_path):\n",
        "    df = pd.read_csv(segment_path)\n",
        "    n = len(df)\n",
        "    feats = {}\n",
        "    for wname, (start_frac, end_frac) in WINDOW_SPECS.items():\n",
        "        s = int(n * start_frac)\n",
        "        e = int(n * end_frac)\n",
        "        win = df.iloc[s:e]\n",
        "        for col in SENSORS:\n",
        "            x = win[col].values\n",
        "            rs = robust_stats(x)\n",
        "            feats.update({f\"{col}__{wname}__{k}\": v for k, v in rs.items()})\n",
        "            feats[f\"{col}__{wname}__rms\"] = rms(x)\n",
        "            feats[f\"{col}__{wname}__zcr\"] = zcr(x)\n",
        "            feats[f\"{col}__{wname}__linelen\"] = line_length(x)\n",
        "    # cross-sensor aggregates (mean/std across sensors for a couple stats on full window)\n",
        "    # Example: mean/std of rms across sensors (full window)\n",
        "    rms_vals = [feats[f\"sensor_{i}__full__rms\"] for i in range(1,11)]\n",
        "    feats['cross_full_rms_mean'] = float(np.mean(rms_vals))\n",
        "    feats['cross_full_rms_std'] = float(np.std(rms_vals))\n",
        "    return feats\n",
        "\n",
        "# 4) Smoke test on a subset for speed\n",
        "FAST_N = 200  # increase later to all\n",
        "train_rows = train_csv.head(FAST_N).copy()\n",
        "test_ids = list(test_map.keys())[:min(100, len(test_map))]  # quick smoke on test\n",
        "\n",
        "log(f\"Extracting features for {len(train_rows)} train segments (smoke)\")\n",
        "train_feat_rows = []\n",
        "for i, row in train_rows.iterrows():\n",
        "    seg_id = int(row['segment_id'])\n",
        "    path = train_map[seg_id]\n",
        "    f = extract_features_for_segment(path)\n",
        "    f['segment_id'] = seg_id\n",
        "    f['series_id'] = int(row['series_id_inferred'])\n",
        "    f['time_to_eruption'] = int(row['time_to_eruption'])\n",
        "    train_feat_rows.append(f)\n",
        "    if (len(train_feat_rows) % 25) == 0:\n",
        "        log(f\"Processed {len(train_feat_rows)}/{len(train_rows)} train segments\")\n",
        "\n",
        "train_feats = pd.DataFrame(train_feat_rows).set_index('segment_id')\n",
        "log(f\"Train features shape: {train_feats.shape}\")\n",
        "\n",
        "log(f\"Extracting features for {len(test_ids)} test segments (smoke)\")\n",
        "test_feat_rows = []\n",
        "for idx, seg_id in enumerate(test_ids, 1):\n",
        "    path = test_map[seg_id]\n",
        "    f = extract_features_for_segment(path)\n",
        "    f['segment_id'] = seg_id\n",
        "    test_feat_rows.append(f)\n",
        "    if (idx % 25) == 0:\n",
        "        log(f\"Processed {idx}/{len(test_ids)} test segments\")\n",
        "\n",
        "test_feats = pd.DataFrame(test_feat_rows).set_index('segment_id')\n",
        "log(f\"Test features shape: {test_feats.shape}\")\n",
        "log(f\"Feature columns: {len(train_feats.columns)}\")\n",
        "print(sorted(train_feats.columns)[:15], '...')\n",
        "\n",
        "gc.collect();\n",
        "log('Smoke feature build complete. Next: scale up to full dataset and train XGBoost GPU with GroupKFold.')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:21.792431Z] Series inferred (orig order): n_series=2003 with sizes head: {1593: 6, 1795: 6, 131: 6, 671: 5, 1268: 5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:21.801259Z] Train files mapped: 3987; Test files mapped: 444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:21.802426Z] Extracting features for 200 train segments (smoke)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:23.648586Z] Processed 25/200 train segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:25.439184Z] Processed 50/200 train segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:27.245325Z] Processed 75/200 train segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:29.082433Z] Processed 100/200 train segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:30.831002Z] Processed 125/200 train segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:32.661424Z] Processed 150/200 train segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:34.439953Z] Processed 175/200 train segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:36.296617Z] Processed 200/200 train segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:36.308461Z] Train features shape: (200, 394)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:36.308899Z] Extracting features for 100 test segments (smoke)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:38.130220Z] Processed 25/100 test segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:39.941790Z] Processed 50/100 test segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:41.753577Z] Processed 75/100 test segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:43.585741Z] Processed 100/100 test segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:43.592514Z] Test features shape: (100, 392)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:20:43.592872Z] Feature columns: 394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cross_full_rms_mean', 'cross_full_rms_std', 'sensor_10__full__iqr', 'sensor_10__full__linelen', 'sensor_10__full__mad', 'sensor_10__full__max', 'sensor_10__full__mean', 'sensor_10__full__median', 'sensor_10__full__min', 'sensor_10__full__ptp', 'sensor_10__full__q05', 'sensor_10__full__q95', 'sensor_10__full__rms', 'sensor_10__full__std', 'sensor_10__full__zcr'] ...\n[2025-09-27T02:20:43.637955Z] Smoke feature build complete. Next: scale up to full dataset and train XGBoost GPU with GroupKFold.\n"
          ]
        }
      ]
    },
    {
      "id": "c2970027-067f-40a7-9a0c-f3c7cd8ec98b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Full feature build with rFFT spectral features + short windows + ACF, caching (CSV)\n",
        "import os, glob, gc, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "# Load train meta and infer series_id from original order\n",
        "train_meta = pd.read_csv('train.csv')\n",
        "t = train_meta['time_to_eruption']\n",
        "train_meta['series_id'] = (t.diff() > 0).cumsum().astype(int)\n",
        "\n",
        "# Maps\n",
        "train_map = {int(os.path.splitext(os.path.basename(p))[0]): p for p in glob.glob('train/*.csv')}\n",
        "test_map = {int(os.path.splitext(os.path.basename(p))[0]): p for p in glob.glob('test/*.csv')}\n",
        "assert len(train_map) == len(train_meta), 'Mismatch train files vs train.csv rows'\n",
        "\n",
        "SENSORS = [f'sensor_{i}' for i in range(1,11)]\n",
        "WINDOW_SPECS = {\n",
        "    'full': (0.0, 1.0),\n",
        "    'quarter': (0.75, 1.0),\n",
        "    'p10': (0.90, 1.0),\n",
        "    'p5': (0.95, 1.0),\n",
        "    'p2': (0.98, 1.0),\n",
        "    'p1': (0.99, 1.0),\n",
        "}\n",
        "\n",
        "# Time-domain feature helpers\n",
        "def robust_stats(x):\n",
        "    x = x.astype(np.float32)\n",
        "    med = np.median(x)\n",
        "    mad = np.median(np.abs(x - med))\n",
        "    iqr = np.subtract(*np.percentile(x, [75, 25]))\n",
        "    q05, q95 = np.percentile(x, [5, 95])\n",
        "    return {\n",
        "        'mean': float(np.mean(x)),\n",
        "        'std': float(np.std(x)),\n",
        "        'median': float(med),\n",
        "        'mad': float(mad),\n",
        "        'iqr': float(iqr),\n",
        "        'min': float(np.min(x)),\n",
        "        'max': float(np.max(x)),\n",
        "        'ptp': float(np.ptp(x)),\n",
        "        'q05': float(q05),\n",
        "        'q95': float(q95),\n",
        "    }\n",
        "\n",
        "def zcr(x):\n",
        "    x = x.astype(np.float32)\n",
        "    s = np.signbit(x)\n",
        "    return float(np.count_nonzero(s[1:] != s[:-1]) / max(1, (len(x) - 1)))\n",
        "\n",
        "def line_length(x):\n",
        "    x = x.astype(np.float32)\n",
        "    return float(np.sum(np.abs(np.diff(x))) / max(1, (len(x) - 1)))\n",
        "\n",
        "def rms(x):\n",
        "    x = x.astype(np.float32)\n",
        "    return float(np.sqrt(np.mean(x * x)))\n",
        "\n",
        "def acf_feats(x, lags=(1,2,4,8,16)):\n",
        "    x = x.astype(np.float32)\n",
        "    n = len(x)\n",
        "    if n < 3:\n",
        "        return {**{f'acf_lag_{l}': 0.0 for l in lags}, 'acf_decay_e': float(lags[-1]), 'acf_decay_0p5': float(lags[-1])}\n",
        "    xm = x - np.mean(x)\n",
        "    var = float(np.sum(xm * xm) / max(1, n))\n",
        "    out = {}\n",
        "    acf_vals = []\n",
        "    for lag in lags:\n",
        "        if lag >= n:\n",
        "            r = 0.0\n",
        "        else:\n",
        "            num = float(np.dot(xm[:-lag], xm[lag:]) / max(1, (n - lag)))\n",
        "            r = num / (var + 1e-8)\n",
        "        out[f'acf_lag_{lag}'] = r\n",
        "        acf_vals.append((lag, r))\n",
        "    # decay: first lag where |acf| < 1/e and < 0.5\n",
        "    decay_e = lags[-1]\n",
        "    decay_05 = lags[-1]\n",
        "    for lag, r in acf_vals:\n",
        "        if decay_e == lags[-1] and abs(r) < (1.0 / math.e):\n",
        "            decay_e = lag\n",
        "        if decay_05 == lags[-1] and abs(r) < 0.5:\n",
        "            decay_05 = lag\n",
        "    out['acf_decay_e'] = float(decay_e)\n",
        "    out['acf_decay_0p5'] = float(decay_05)\n",
        "    return out\n",
        "\n",
        "# Spectral helpers (rFFT, fs=100 Hz, n_fft=4096, Hann)\n",
        "FS = 100.0\n",
        "N_FFT = 4096\n",
        "FREQS = np.fft.rfftfreq(N_FFT, 1.0/FS).astype(np.float32)\n",
        "BANDS = [(0.0,2.0),(2.0,5.0),(5.0,10.0),(10.0,20.0),(20.0,50.0)]\n",
        "HANN = np.hanning(N_FFT).astype(np.float32)\n",
        "\n",
        "def band_indices(freqs, band):\n",
        "    lo, hi = band\n",
        "    return np.where((freqs >= lo) & (freqs < hi))[0]\n",
        "\n",
        "BAND_IDXS = [band_indices(FREQS, b) for b in BANDS]\n",
        "\n",
        "def spectral_feats(x):\n",
        "    x = x.astype(np.float32)\n",
        "    # take last N_FFT samples (pad if needed)\n",
        "    if len(x) >= N_FFT:\n",
        "        xw = x[-N_FFT:]\n",
        "    else:\n",
        "        pad = np.zeros(N_FFT, dtype=np.float32)\n",
        "        pad[-len(x):] = x\n",
        "        xw = pad\n",
        "    xw = xw * HANN\n",
        "    X = np.fft.rfft(xw, n=N_FFT)\n",
        "    P = (np.abs(X) ** 2).astype(np.float32)\n",
        "    total = float(P.sum() + 1e-12)\n",
        "    # bandpowers and fractions\n",
        "    bands_power = [float(P[idx].sum()) for idx in BAND_IDXS]\n",
        "    bands_frac = [bp / total for bp in bands_power]\n",
        "    centroid = float((FREQS * P).sum() / total)\n",
        "    p_norm = P / total\n",
        "    entropy = float(-(p_norm * np.log(p_norm + 1e-12)).sum())\n",
        "    flatness = float(np.exp(np.mean(np.log(P + 1e-12))) / (np.mean(P) + 1e-12))\n",
        "    out = {\n",
        "        'spec_total_power': total,\n",
        "        'spec_centroid': centroid,\n",
        "        'spec_entropy': entropy,\n",
        "        'spec_flatness': flatness,\n",
        "    }\n",
        "    for i, (lo,hi) in enumerate(BANDS):\n",
        "        out[f'spec_band_{int(lo)}_{int(hi)}_power'] = bands_power[i]\n",
        "        out[f'spec_band_{int(lo)}_{int(hi)}_frac'] = bands_frac[i]\n",
        "    return out\n",
        "\n",
        "def extract_one(seg_id, path):\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "        n = len(df)\n",
        "        feats = {}\n",
        "        # per-window, per-sensor\n",
        "        for wname, (start_frac, end_frac) in WINDOW_SPECS.items():\n",
        "            s = int(n * start_frac)\n",
        "            e = int(n * end_frac)\n",
        "            win = df.iloc[s:e]\n",
        "            # time-domain + ACF\n",
        "            for col in SENSORS:\n",
        "                x = win[col].values.astype(np.float32)\n",
        "                rs = robust_stats(x)\n",
        "                for k,v in rs.items():\n",
        "                    feats[f\"{col}__{wname}__{k}\"] = np.float32(v)\n",
        "                feats[f\"{col}__{wname}__rms\"] = np.float32(rms(x))\n",
        "                feats[f\"{col}__{wname}__zcr\"] = np.float32(zcr(x))\n",
        "                feats[f\"{col}__{wname}__linelen\"] = np.float32(line_length(x))\n",
        "                acf = acf_feats(x)\n",
        "                for k,v in acf.items():\n",
        "                    feats[f\"{col}__{wname}__{k}\"] = np.float32(v)\n",
        "            # spectral on last N_FFT samples of this window\n",
        "            total_powers = []\n",
        "            centroids = []\n",
        "            for col in SENSORS:\n",
        "                x = win[col].values.astype(np.float32)\n",
        "                sf = spectral_feats(x)\n",
        "                for k,v in sf.items():\n",
        "                    feats[f\"{col}__{wname}__{k}\"] = np.float32(v)\n",
        "                total_powers.append(sf['spec_total_power'])\n",
        "                centroids.append(sf['spec_centroid'])\n",
        "            feats[f\"cross_{wname}__spec_total_power_mean\"] = np.float32(np.mean(total_powers))\n",
        "            feats[f\"cross_{wname}__spec_total_power_std\"] = np.float32(np.std(total_powers))\n",
        "            feats[f\"cross_{wname}__spec_centroid_mean\"] = np.float32(np.mean(centroids))\n",
        "            feats[f\"cross_{wname}__spec_centroid_std\"] = np.float32(np.std(centroids))\n",
        "        feats['segment_id'] = int(seg_id)\n",
        "        return feats\n",
        "    except Exception as e:\n",
        "        log(f\"Error processing {seg_id}: {e}\")\n",
        "        return {'segment_id': int(seg_id)}\n",
        "\n",
        "def build_features(map_dict, is_train=False):\n",
        "    ids = list(map_dict.keys())\n",
        "    ids.sort()\n",
        "    log(f\"Starting feature extraction for {len(ids)} segments | train={is_train}\")\n",
        "    def _proc(i, seg_id):\n",
        "        feats = extract_one(seg_id, map_dict[seg_id])\n",
        "        if (i+1) % 200 == 0:\n",
        "            log(f\"Processed {i+1}/{len(ids)} segments\")\n",
        "        return feats\n",
        "    res = Parallel(n_jobs=-1, backend='loky')(delayed(_proc)(i, seg_id) for i, seg_id in enumerate(ids))\n",
        "    feats_df = pd.DataFrame(res).set_index('segment_id')\n",
        "    # ensure float32\n",
        "    for c in feats_df.columns:\n",
        "        feats_df[c] = feats_df[c].astype(np.float32)\n",
        "    if is_train:\n",
        "        meta = train_meta.set_index('segment_id').loc[feats_df.index, ['time_to_eruption','series_id']].copy()\n",
        "        feats_df = feats_df.join(meta)\n",
        "    log(f\"Built features shape: {feats_df.shape}\")\n",
        "    return feats_df\n",
        "\n",
        "# Build and cache (overwrite v1 files for simplicity)\n",
        "train_out = 'features_train_v1.csv'\n",
        "test_out = 'features_test_v1.csv'\n",
        "rebuild = True\n",
        "if rebuild or not os.path.exists(train_out):\n",
        "    train_feats = build_features(train_map, is_train=True)\n",
        "    train_feats.to_csv(train_out)\n",
        "    log(f\"Saved {train_out} ({os.path.getsize(train_out)} bytes)\")\n",
        "else:\n",
        "    train_feats = pd.read_csv(train_out).set_index('segment_id')\n",
        "    log(f\"Loaded cached {train_out} -> {train_feats.shape}\")\n",
        "\n",
        "if rebuild or not os.path.exists(test_out):\n",
        "    test_feats = build_features(test_map, is_train=False)\n",
        "    test_feats.to_csv(test_out)\n",
        "    log(f\"Saved {test_out} ({os.path.getsize(test_out)} bytes)\")\n",
        "else:\n",
        "    test_feats = pd.read_csv(test_out).set_index('segment_id')\n",
        "    log(f\"Loaded cached {test_out} -> {test_feats.shape}\")\n",
        "\n",
        "log('Feature build v2 (short windows + ACF + rFFT) complete. Next: retrain XGBoost and blend if time allows.')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:19.765384Z] Starting feature extraction for 3987 segments | train=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:21.834093Z] Processed 200/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:22.899997Z] Processed 400/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:23.954161Z] Processed 600/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:26.051684Z] Processed 1000/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:27.082591Z] Processed 1200/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:28.169339Z] Processed 1400/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:29.184635Z] Processed 1600/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:30.251582Z] Processed 1800/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:31.346414Z] Processed 2000/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:32.378191Z] Processed 2200/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:33.393966Z] Processed 2400/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:34.447941Z] Processed 2600/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:35.495697Z] Processed 2800/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:36.486382Z] Processed 3000/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:37.498437Z] Processed 3200/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:38.507645Z] Processed 3400/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:39.503032Z] Processed 3600/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:40.514907Z] Processed 3800/3987 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:43.376351Z] Built features shape: (3987, 2066)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:47.183811Z] Saved features_train_v1.csv (75307100 bytes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:47.185760Z] Starting feature extraction for 444 segments | train=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:48.209334Z] Processed 200/444 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:49.022586Z] Processed 400/444 segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:49.527580Z] Built features shape: (444, 2064)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:50.244176Z] Saved features_test_v1.csv (8422404 bytes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:38:50.245022Z] Feature build v2 (short windows + ACF + rFFT) complete. Next: retrain XGBoost and blend if time allows.\n"
          ]
        }
      ]
    },
    {
      "id": "7f1b5d7f-b6c3-4881-aa56-5e2ccd5820d7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# XGBoost GPU training with 5-fold GroupKFold using xgb.train (handles early stopping), OOF MAE, clipping sweep, and submission\n",
        "import os, time, subprocess, sys, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "# Ensure xgboost is installed (GPU-capable).\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    log('Installing xgboost...')\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "# Load cached features\n",
        "train_path = 'features_train_v1.csv'\n",
        "test_path = 'features_test_v1.csv'\n",
        "assert os.path.exists(train_path) and os.path.exists(test_path), 'Feature CSVs not found. Run feature build first.'\n",
        "train_feats = pd.read_csv(train_path).set_index('segment_id')\n",
        "test_feats = pd.read_csv(test_path).set_index('segment_id')\n",
        "log(f'Loaded features: train {train_feats.shape}, test {test_feats.shape}')\n",
        "\n",
        "# Split X, y, groups; align columns between train and test\n",
        "y = train_feats['time_to_eruption'].astype(np.float32).values\n",
        "groups = train_feats['series_id'].values\n",
        "feature_cols = [c for c in train_feats.columns if c not in ('time_to_eruption','series_id')]\n",
        "X = train_feats[feature_cols].astype(np.float32).copy()\n",
        "X_test = test_feats.reindex(columns=feature_cols).astype(np.float32).copy()\n",
        "X_test = X_test.fillna(0.0)\n",
        "log(f'Feature columns: {len(feature_cols)}')\n",
        "\n",
        "# CV setup\n",
        "n_splits = 5\n",
        "gkf = GroupKFold(n_splits=n_splits)\n",
        "\n",
        "# XGBoost params for xgb.train\n",
        "params = {\n",
        "    'tree_method': 'gpu_hist',\n",
        "    'predictor': 'gpu_predictor',\n",
        "    'objective': 'reg:squarederror',  # use MAE metric for optimization signal\n",
        "    'max_depth': 6,\n",
        "    'eta': 0.03,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.6,\n",
        "    'min_child_weight': 12,\n",
        "    'alpha': 0.1,\n",
        "    'lambda': 3.0,\n",
        "    'gamma': 0.1,\n",
        "    'eval_metric': 'mae',\n",
        "    'seed': 42,\n",
        "    'verbosity': 0,\n",
        "}\n",
        "\n",
        "oof = np.zeros(len(X), dtype=np.float32)\n",
        "test_preds_folds = []\n",
        "fold_maes = []\n",
        "start_all = time.time()\n",
        "DM_test = xgb.DMatrix(X_test.values, feature_names=feature_cols)\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups), 1):\n",
        "    t0 = time.time()\n",
        "    log(f'Fold {fold}/{n_splits}: train={len(tr_idx)} valid={len(va_idx)}')\n",
        "    X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
        "    X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
        "    DM_tr = xgb.DMatrix(X_tr.values, label=y_tr, feature_names=feature_cols)\n",
        "    DM_va = xgb.DMatrix(X_va.values, label=y_va, feature_names=feature_cols)\n",
        "    evals = [(DM_tr, 'train'), (DM_va, 'valid')]\n",
        "    bst = xgb.train(\n",
        "        params=params,\n",
        "        dtrain=DM_tr,\n",
        "        num_boost_round=10000,\n",
        "        evals=evals,\n",
        "        early_stopping_rounds=300,\n",
        "        verbose_eval=False,\n",
        "    )\n",
        "    best_iter = getattr(bst, 'best_iteration', None)\n",
        "    if best_iter is None:\n",
        "        best_iter = bst.num_boosted_rounds\n",
        "    va_pred = bst.predict(DM_va, iteration_range=(0, int(best_iter)))\n",
        "    oof[va_idx] = va_pred.astype(np.float32)\n",
        "    fold_mae = mean_absolute_error(y_va, va_pred)\n",
        "    fold_maes.append(fold_mae)\n",
        "    log(f'Fold {fold} MAE={fold_mae:,.0f} | best_iter={best_iter}')\n",
        "    tp = bst.predict(DM_test, iteration_range=(0, int(best_iter)))\n",
        "    test_preds_folds.append(tp.astype(np.float32))\n",
        "    log(f'Fold {fold} done in {time.time()-t0:.1f}s')\n",
        "\n",
        "oof_mae = mean_absolute_error(y, oof)\n",
        "log(f'OOF MAE={oof_mae:,.0f}; folds: {[int(m) for m in fold_maes]} | elapsed {time.time()-start_all:.1f}s')\n",
        "\n",
        "# Averaged test preds\n",
        "test_pred = np.mean(test_preds_folds, axis=0).astype(np.float32)\n",
        "\n",
        "# Clipping sweep on OOF to pick upper bound\n",
        "quantiles = [0.99, 0.995, 0.999, 1.0]\n",
        "best_upper = None\n",
        "best_mae = oof_mae\n",
        "for q in quantiles:\n",
        "    upper = float(np.quantile(y, q)) if q < 1.0 else float(np.max(y))\n",
        "    oof_clip = np.clip(oof, 0.0, upper)\n",
        "    mae = mean_absolute_error(y, oof_clip)\n",
        "    log(f'Clip upper {q} -> {upper:,.0f}: OOF MAE {mae:,.0f}')\n",
        "    if mae < best_mae:\n",
        "        best_mae = mae\n",
        "        best_upper = upper\n",
        "if best_upper is None:\n",
        "    best_upper = float(np.max(y))\n",
        "log(f'Chosen clip upper={best_upper:,.0f} (OOF {best_mae:,.0f})')\n",
        "\n",
        "# Save raw XGB test predictions (unaligned, then aligned to sample order) without clipping for later blending\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "test_idx = X_test.index.values\n",
        "pred_series_raw = pd.Series(test_pred, index=test_idx)\n",
        "pred_aligned_raw = pred_series_raw.reindex(ss['segment_id'].values).fillna(pred_series_raw.median()).values\n",
        "pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': pred_aligned_raw.astype(np.float32)}).to_csv('pred_test_xgb_raw.csv', index=False)\n",
        "log(f'Saved pred_test_xgb_raw.csv for blending.')\n",
        "\n",
        "# Create submission from clipped XGB preds\n",
        "pred_aligned = np.clip(pred_aligned_raw, 0.0, best_upper)\n",
        "sub = pd.DataFrame({\n",
        "    'segment_id': ss['segment_id'].values,\n",
        "    'time_to_eruption': pred_aligned.astype(np.int64),\n",
        "})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "log(f'Saved submission.csv ({os.path.getsize(\"submission.csv\")} bytes). Head:')\n",
        "print(sub.head())\n",
        "\n",
        "# Save OOF for diagnostics\n",
        "pd.DataFrame({'segment_id': X.index.values, 'y': y.astype(np.int64), 'oof': oof.astype(np.float32)}).to_csv('oof_xgb_v1.csv', index=False)\n",
        "log('Training+pseudo-inference complete.')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:22:44.211410Z] Loaded features: train (3987, 2066), test (444, 2064)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:22:44.241174Z] Feature columns: 2064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:22:44.252316Z] Fold 1/5: train=3189 valid=798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:24:01.871375Z] Fold 1 MAE=3,166,326 | best_iter=3748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:24:01.918948Z] Fold 1 done in 77.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:24:01.919728Z] Fold 2/5: train=3189 valid=798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:25:23.122693Z] Fold 2 MAE=3,246,702 | best_iter=3910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:25:23.171576Z] Fold 2 done in 81.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:25:23.172255Z] Fold 3/5: train=3190 valid=797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:26:38.986211Z] Fold 3 MAE=3,358,654 | best_iter=3584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:26:39.031233Z] Fold 3 done in 75.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:26:39.031868Z] Fold 4/5: train=3190 valid=797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:28:07.575464Z] Fold 4 MAE=3,181,652 | best_iter=4408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:28:07.630099Z] Fold 4 done in 88.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:28:07.630838Z] Fold 5/5: train=3190 valid=797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:29:35.283277Z] Fold 5 MAE=3,223,680 | best_iter=4270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:29:35.336194Z] Fold 5 done in 87.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:29:35.337189Z] OOF MAE=3,235,388; folds: [3166325, 3246702, 3358653, 3181652, 3223680] | elapsed 411.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:29:35.338403Z] Clip upper 0.99 -> 46,962,008: OOF MAE 3,234,676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:29:35.339230Z] Clip upper 0.995 -> 47,715,640: OOF MAE 3,233,765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:29:35.340285Z] Clip upper 0.999 -> 48,356,643: OOF MAE 3,233,824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:29:35.340980Z] Clip upper 1.0 -> 49,046,088: OOF MAE 3,234,111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:29:35.341354Z] Chosen clip upper=47,715,640 (OOF 3,233,765)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:29:35.344878Z] Saved pred_test_xgb_raw.csv for blending.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:29:35.346140Z] Saved submission.csv (8613 bytes). Head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289          30333056\n1   508758258          15063424\n2  1566132188          33993968\n3  1891418251          21747250\n4  1968343855           7629103\n[2025-09-27T03:29:35.351334Z] Training+pseudo-inference complete.\n"
          ]
        }
      ]
    },
    {
      "id": "659166a7-5a9d-474b-b474-34061ccb3360",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# XGB dual-model (MAE + log1p) 5-fold GroupKFold, blend and submit\n",
        "import os, time, sys, subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "# Load features v2\n",
        "train_path = 'features_train_v1.csv'\n",
        "test_path = 'features_test_v1.csv'\n",
        "train_feats = pd.read_csv(train_path).set_index('segment_id')\n",
        "test_feats = pd.read_csv(test_path).set_index('segment_id')\n",
        "y = train_feats['time_to_eruption'].astype(np.float32).values\n",
        "groups = train_feats['series_id'].values\n",
        "feature_cols = [c for c in train_feats.columns if c not in ('time_to_eruption','series_id')]\n",
        "X = train_feats[feature_cols].astype(np.float32).copy()\n",
        "X_test = test_feats.reindex(columns=feature_cols).astype(np.float32).copy().fillna(0.0)\n",
        "DM_test = xgb.DMatrix(X_test.values, feature_names=feature_cols)\n",
        "log(f'Features loaded: X={X.shape}, X_test={X_test.shape}')\n",
        "\n",
        "n_splits = 5\n",
        "gkf = GroupKFold(n_splits=n_splits)\n",
        "\n",
        "# Model A: MAE objective (direct)\n",
        "params_mae = {\n",
        "    'tree_method': 'gpu_hist',\n",
        "    'predictor': 'gpu_predictor',\n",
        "    'objective': 'reg:absoluteerror',\n",
        "    'max_depth': 6,\n",
        "    'eta': 0.03,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.6,\n",
        "    'min_child_weight': 12,\n",
        "    'alpha': 0.1,\n",
        "    'lambda': 3.0,\n",
        "    'gamma': 0.1,\n",
        "    'eval_metric': 'mae',\n",
        "    'seed': 42,\n",
        "    'verbosity': 0,\n",
        "}\n",
        "\n",
        "oof_mae_direct = np.zeros(len(X), dtype=np.float32)\n",
        "test_mae_direct = []\n",
        "fold_maes_direct = []\n",
        "t0_all = time.time()\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups), 1):\n",
        "    DM_tr = xgb.DMatrix(X.iloc[tr_idx].values, label=y[tr_idx], feature_names=feature_cols)\n",
        "    DM_va = xgb.DMatrix(X.iloc[va_idx].values, label=y[va_idx], feature_names=feature_cols)\n",
        "    bst = xgb.train(params_mae, DM_tr, num_boost_round=10000, evals=[(DM_va,'valid')], early_stopping_rounds=300, verbose_eval=False)\n",
        "    best_iter = getattr(bst, 'best_iteration', None) or bst.num_boosted_rounds\n",
        "    pred_va = bst.predict(DM_va, iteration_range=(0, int(best_iter))).astype(np.float32)\n",
        "    oof_mae_direct[va_idx] = pred_va\n",
        "    fold_mae = mean_absolute_error(y[va_idx], pred_va)\n",
        "    fold_maes_direct.append(fold_mae)\n",
        "    log(f'[MAE] Fold {fold} MAE={fold_mae:,.0f} | best_iter={best_iter}')\n",
        "    pred_te = bst.predict(DM_test, iteration_range=(0, int(best_iter))).astype(np.float32)\n",
        "    test_mae_direct.append(pred_te)\n",
        "log(f'[MAE] OOF MAE={mean_absolute_error(y, oof_mae_direct):,.0f}; folds={[(i+1,int(m)) for i,m in enumerate(fold_maes_direct)]} | {time.time()-t0_all:.1f}s')\n",
        "\n",
        "# Model B: log1p target with RMSE objective\n",
        "y_log = np.log1p(y.astype(np.float64)).astype(np.float32)\n",
        "params_log = params_mae.copy()\n",
        "params_log.update({'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'seed': 43})\n",
        "oof_log = np.zeros(len(X), dtype=np.float32)\n",
        "test_log = []\n",
        "fold_rmses = []\n",
        "t0_all = time.time()\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y_log, groups=groups), 1):\n",
        "    DM_tr = xgb.DMatrix(X.iloc[tr_idx].values, label=y_log[tr_idx], feature_names=feature_cols)\n",
        "    DM_va = xgb.DMatrix(X.iloc[va_idx].values, label=y_log[va_idx], feature_names=feature_cols)\n",
        "    bst = xgb.train(params_log, DM_tr, num_boost_round=10000, evals=[(DM_va,'valid')], early_stopping_rounds=300, verbose_eval=False)\n",
        "    best_iter = getattr(bst, 'best_iteration', None) or bst.num_boosted_rounds\n",
        "    pred_va_log = bst.predict(DM_va, iteration_range=(0, int(best_iter))).astype(np.float32)\n",
        "    oof_log[va_idx] = pred_va_log\n",
        "    # report MAE on original scale for reference\n",
        "    pred_va = np.expm1(pred_va_log).astype(np.float32)\n",
        "    mae_val = mean_absolute_error(y[va_idx], pred_va)\n",
        "    fold_rmses.append(mae_val)\n",
        "    log(f'[LOG] Fold {fold} MAE(orig)={mae_val:,.0f} | best_iter={best_iter}')\n",
        "    pred_te = bst.predict(DM_test, iteration_range=(0, int(best_iter))).astype(np.float32)\n",
        "    test_log.append(pred_te)\n",
        "oof_log_orig = np.expm1(oof_log).astype(np.float32)\n",
        "log(f'[LOG] OOF MAE on orig scale={mean_absolute_error(y, oof_log_orig):,.0f}; folds={[(i+1,int(m)) for i,m in enumerate(fold_rmses)]} | {time.time()-t0_all:.1f}s')\n",
        "\n",
        "# Blend OOF and test predictions\n",
        "test_mae_direct = np.mean(np.stack(test_mae_direct, axis=0), axis=0).astype(np.float32)\n",
        "test_log_orig = np.expm1(np.mean(np.stack(test_log, axis=0), axis=0)).astype(np.float32)\n",
        "w_log = 0.35  # 65% direct MAE model, 35% log1p model\n",
        "oof_blend = (1.0 - w_log) * oof_mae_direct + w_log * oof_log_orig\n",
        "oof_mae_blend = mean_absolute_error(y, oof_blend)\n",
        "log(f'[BLEND] OOF MAE blend={oof_mae_blend:,.0f} (direct={mean_absolute_error(y, oof_mae_direct):,.0f}, log1p={mean_absolute_error(y, oof_log_orig):,.0f})')\n",
        "test_blend = (1.0 - w_log) * test_mae_direct + w_log * test_log_orig\n",
        "\n",
        "# Clipping sweep using blended OOF\n",
        "quantiles = [0.99, 0.995, 0.999, 1.0]\n",
        "best_upper = None\n",
        "best_mae = oof_mae_blend\n",
        "for q in quantiles:\n",
        "    upper = float(np.quantile(y, q)) if q < 1.0 else float(np.max(y))\n",
        "    mae = mean_absolute_error(y, np.clip(oof_blend, 0.0, upper))\n",
        "    log(f'[BLEND] Clip upper {q} -> {upper:,.0f}: OOF MAE {mae:,.0f}')\n",
        "    if mae < best_mae:\n",
        "        best_mae = mae\n",
        "        best_upper = upper\n",
        "if best_upper is None:\n",
        "    best_upper = float(np.max(y))\n",
        "log(f'[BLEND] Chosen upper={best_upper:,.0f} (OOF {best_mae:,.0f})')\n",
        "\n",
        "# Build submission\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "pred_series = pd.Series(test_blend, index=X_test.index.values)\n",
        "pred_aligned = pred_series.reindex(ss['segment_id'].values).fillna(pred_series.median()).values\n",
        "pred_aligned = np.clip(pred_aligned, 0.0, best_upper)\n",
        "sub = pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': pred_aligned.astype(np.int64)})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "pd.DataFrame({'segment_id': X.index.values, 'y': y.astype(np.int64), 'oof_blend': oof_blend.astype(np.float32)}).to_csv('oof_xgb_blend.csv', index=False)\n",
        "log(f'Saved submission.csv ({os.path.getsize(\"submission.csv\")} bytes). Head:')\n",
        "print(sub.head())\n",
        "log('Dual-model blend complete.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:47:14.284848Z] Features loaded: X=(3987, 2064), X_test=(444, 2064)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:50:14.175820Z] [MAE] Fold 1 MAE=3,420,550 | best_iter=9999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:53:14.332124Z] [MAE] Fold 2 MAE=3,666,292 | best_iter=9991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:56:14.361260Z] [MAE] Fold 3 MAE=3,772,811 | best_iter=9985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T02:59:15.489098Z] [MAE] Fold 4 MAE=3,639,105 | best_iter=9998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:02:15.717516Z] [MAE] Fold 5 MAE=3,564,985 | best_iter=9997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:02:15.840011Z] [MAE] OOF MAE=3,612,714; folds=[(1, 3420549), (2, 3666292), (3, 3772811), (4, 3639105), (5, 3564985)] | 901.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:02:36.751845Z] [LOG] Fold 1 MAE(orig)=3,948,449 | best_iter=2220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:03:07.227654Z] [LOG] Fold 2 MAE(orig)=4,195,308 | best_iter=3603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:03:34.723769Z] [LOG] Fold 3 MAE(orig)=4,087,214 | best_iter=3198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:03:54.248148Z] [LOG] Fold 4 MAE(orig)=4,002,513 | best_iter=2322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:04:17.350544Z] [LOG] Fold 5 MAE(orig)=4,044,058 | best_iter=2963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:04:17.368160Z] [LOG] OOF MAE on orig scale=4,055,516; folds=[(1, 3948449), (2, 4195307), (3, 4087213), (4, 4002512), (5, 4044057)] | 121.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:04:17.369736Z] [BLEND] OOF MAE blend=3,634,965 (direct=3,612,714, log1p=4,055,516)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:04:17.370786Z] [BLEND] Clip upper 0.99 -> 46,962,008: OOF MAE 3,634,257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:04:17.371567Z] [BLEND] Clip upper 0.995 -> 47,715,640: OOF MAE 3,633,760\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:04:17.372254Z] [BLEND] Clip upper 0.999 -> 48,356,643: OOF MAE 3,633,702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:04:17.372851Z] [BLEND] Clip upper 1.0 -> 49,046,088: OOF MAE 3,634,047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:04:17.373172Z] [BLEND] Chosen upper=48,356,643 (OOF 3,633,702)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:04:17.380445Z] Saved submission.csv (8606 bytes). Head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289          30214176\n1   508758258          11675458\n2  1566132188          30444312\n3  1891418251          22872668\n4  1968343855           5564844\n[2025-09-27T03:04:17.381466Z] Dual-model blend complete.\n"
          ]
        }
      ]
    },
    {
      "id": "1bed7bc1-9c0f-40ae-aa5f-0830f73f27b6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LightGBM (MAE) 5-fold GroupKFold on v2 features, predict and submit\n",
        "import os, sys, subprocess, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'lightgbm==4.6.0'], check=True)\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Load features v2\n",
        "train_path = 'features_train_v1.csv'\n",
        "test_path = 'features_test_v1.csv'\n",
        "train_feats = pd.read_csv(train_path).set_index('segment_id')\n",
        "test_feats = pd.read_csv(test_path).set_index('segment_id')\n",
        "y = train_feats['time_to_eruption'].astype(np.float32).values\n",
        "groups = train_feats['series_id'].values\n",
        "feature_cols = [c for c in train_feats.columns if c not in ('time_to_eruption','series_id')]\n",
        "X = train_feats[feature_cols].astype(np.float32).copy()\n",
        "X_test = test_feats.reindex(columns=feature_cols).astype(np.float32).copy()\n",
        "X_test = X_test.fillna(0.0)\n",
        "log(f'LGB data: X={X.shape}, X_test={X_test.shape}, feats={len(feature_cols)}')\n",
        "\n",
        "params = {\n",
        "    'objective': 'mae',\n",
        "    'metric': 'mae',\n",
        "    'learning_rate': 0.03,\n",
        "    'num_leaves': 31,\n",
        "    'max_depth': 8,\n",
        "    'feature_fraction': 0.6,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'min_data_in_leaf': 40,\n",
        "    'lambda_l1': 0.1,\n",
        "    'lambda_l2': 3.0,\n",
        "    'force_col_wise': True,\n",
        "    'verbose': -1,\n",
        "    'num_threads': 0,\n",
        "    'seed': 1234,\n",
        "}\n",
        "\n",
        "n_splits = 5\n",
        "gkf = GroupKFold(n_splits=n_splits)\n",
        "oof = np.zeros(len(X), dtype=np.float32)\n",
        "test_preds = []\n",
        "fold_maes = []\n",
        "t0 = time.time()\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups), 1):\n",
        "    X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
        "    X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
        "    dtrain = lgb.Dataset(X_tr, label=y_tr, free_raw_data=False)\n",
        "    dvalid = lgb.Dataset(X_va, label=y_va, reference=dtrain, free_raw_data=False)\n",
        "    log(f'LGB Fold {fold}/{n_splits}: train={len(tr_idx)} valid={len(va_idx)}')\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=20000,\n",
        "        valid_sets=[dvalid],\n",
        "        valid_names=['valid'],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n",
        "    )\n",
        "    va_pred = model.predict(X_va, num_iteration=model.best_iteration).astype(np.float32)\n",
        "    oof[va_idx] = va_pred\n",
        "    mae = mean_absolute_error(y_va, va_pred)\n",
        "    fold_maes.append(mae)\n",
        "    log(f'LGB Fold {fold} MAE={mae:,.0f} | best_iter={model.best_iteration}')\n",
        "    tp = model.predict(X_test, num_iteration=model.best_iteration).astype(np.float32)\n",
        "    test_preds.append(tp)\n",
        "\n",
        "oof_mae = mean_absolute_error(y, oof)\n",
        "log(f'LGB OOF MAE={oof_mae:,.0f}; folds={[(i+1,int(m)) for i,m in enumerate(fold_maes)]} | elapsed {time.time()-t0:.1f}s')\n",
        "\n",
        "# Average test preds and clip based on OOF sweep\n",
        "test_mean = np.mean(np.stack(test_preds, axis=0), axis=0).astype(np.float32)\n",
        "best_upper = float(np.max(y))\n",
        "best_mae = oof_mae\n",
        "for q in [0.99, 0.995, 0.999, 1.0]:\n",
        "    upper = float(np.quantile(y, q)) if q < 1.0 else float(np.max(y))\n",
        "    mae = mean_absolute_error(y, np.clip(oof, 0.0, upper))\n",
        "    log(f'LGB clip {q}: {upper:,.0f} -> OOF {mae:,.0f}')\n",
        "    if mae < best_mae:\n",
        "        best_mae = mae\n",
        "        best_upper = upper\n",
        "log(f'LGB chosen upper={best_upper:,.0f} (OOF {best_mae:,.0f})')\n",
        "\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "pred_series = pd.Series(test_mean, index=X_test.index.values)\n",
        "pred_aligned = pred_series.reindex(ss['segment_id'].values).fillna(pred_series.median()).values\n",
        "pred_aligned = np.clip(pred_aligned, 0.0, best_upper)\n",
        "sub = pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': pred_aligned.astype(np.int64)})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "pd.DataFrame({'segment_id': X.index.values, 'y': y.astype(np.int64), 'oof_lgb': oof.astype(np.float32)}).to_csv('oof_lgb.csv', index=False)\n",
        "log(f'Saved submission.csv ({os.path.getsize(\"submission.csv\")} bytes). Head:')\n",
        "print(sub.head())\n",
        "log('LGB training complete; ready to submit or blend with XGB if needed.')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:05:33.233143Z] LGB data: X=(3987, 2064), X_test=(444, 2064), feats=2064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:05:33.243138Z] LGB Fold 1/5: train=3189 valid=798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:08:38.910964Z] LGB Fold 1 MAE=3,471,504 | best_iter=19988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:08:39.001075Z] LGB Fold 2/5: train=3189 valid=798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:11:51.931349Z] LGB Fold 2 MAE=3,667,655 | best_iter=20000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:11:52.015434Z] LGB Fold 3/5: train=3190 valid=797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:14:52.429707Z] LGB Fold 3 MAE=3,714,156 | best_iter=18950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:14:52.514292Z] LGB Fold 4/5: train=3190 valid=797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:17:54.052613Z] LGB Fold 4 MAE=3,591,186 | best_iter=19964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:17:54.137811Z] LGB Fold 5/5: train=3190 valid=797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:20:59.048819Z] LGB Fold 5 MAE=3,604,946 | best_iter=19976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:20:59.130081Z] LGB OOF MAE=3,609,869; folds=[(1, 3471503), (2, 3667655), (3, 3714156), (4, 3591186), (5, 3604945)] | elapsed 925.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:20:59.131516Z] LGB clip 0.99: 46,962,008 -> OOF 3,610,533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:20:59.132325Z] LGB clip 0.995: 47,715,640 -> OOF 3,609,608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:20:59.133104Z] LGB clip 0.999: 48,356,643 -> OOF 3,609,537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:20:59.133759Z] LGB clip 1.0: 49,046,088 -> OOF 3,609,601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:20:59.134175Z] LGB chosen upper=48,356,643 (OOF 3,609,537)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:20:59.141485Z] Saved submission.csv (8622 bytes). Head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289          30272764\n1   508758258          14239392\n2  1566132188          31933286\n3  1891418251          24881624\n4  1968343855           7650647\n[2025-09-27T03:20:59.142639Z] LGB training complete; ready to submit or blend with XGB if needed.\n"
          ]
        }
      ]
    },
    {
      "id": "c8843fad-a26e-4260-9205-46b66565f493",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Preserve current LGB submission as preds file\n",
        "import pandas as pd, shutil, os\n",
        "src = 'submission.csv'\n",
        "dst = 'pred_test_lgb.csv'\n",
        "if os.path.exists(src):\n",
        "    df = pd.read_csv(src)\n",
        "    # Ensure correct columns and order\n",
        "    df = df[['segment_id','time_to_eruption']].copy()\n",
        "    df.to_csv(dst, index=False)\n",
        "    print(f'Saved {dst} with shape {df.shape}')\n",
        "else:\n",
        "    print('submission.csv not found; nothing to preserve.')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pred_test_lgb.csv with shape (444, 2)\n"
          ]
        }
      ]
    },
    {
      "id": "149340df-22e6-4bac-b822-0a38cb903a37",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blend XGB (v2) and LGB predictions using OOF-optimized weights; clip via OOF; write submission.csv\n",
        "import numpy as np, pandas as pd, os\n",
        "from datetime import datetime\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "# Load OOFs\n",
        "oof_xgb = pd.read_csv('oof_xgb_v1.csv')  # columns: segment_id, y, oof\n",
        "oof_lgb = pd.read_csv('oof_lgb.csv')     # columns: segment_id, y, oof_lgb\n",
        "oof = oof_xgb.merge(oof_lgb, on=['segment_id'], how='inner', suffixes=('_xgb','_lgb'))\n",
        "\n",
        "# Handle duplicate 'y' columns after merge (expect y_xgb and y_lgb)\n",
        "if 'y_xgb' in oof.columns and 'y_lgb' in oof.columns:\n",
        "    if not np.allclose(oof['y_xgb'].astype(np.float64).values, oof['y_lgb'].astype(np.float64).values):\n",
        "        log('Warning: y_xgb and y_lgb differ after merge; proceeding with y_xgb')\n",
        "    y = oof['y_xgb'].astype(np.float64).values\n",
        "elif 'y' in oof.columns:\n",
        "    y = oof['y'].astype(np.float64).values\n",
        "else:\n",
        "    raise KeyError(f\"Target column y not found after merging OOF files; cols={list(oof.columns)}\")\n",
        "\n",
        "px = oof['oof'].values.astype(np.float64)\n",
        "pl = oof['oof_lgb'].values.astype(np.float64)\n",
        "log(f'Merged OOF: {oof.shape}, y range [{y.min()},{y.max()}]')\n",
        "\n",
        "# Grid search weight w for LGB in blend: p = (1-w)*px + w*pl minimizing MAE\n",
        "weights = np.linspace(0.0, 1.0, 21)\n",
        "def mae(a,b):\n",
        "    return float(np.mean(np.abs(a-b)))\n",
        "best_w, best_mae = None, 1e18\n",
        "for w in weights:\n",
        "    p = (1.0 - w) * px + w * pl\n",
        "    m = mae(y, p)\n",
        "    log(f'w={w:.2f} -> OOF MAE {m:,.0f}')\n",
        "    if m < best_mae:\n",
        "        best_mae, best_w = m, w\n",
        "log(f'Chosen weight w_LGB={best_w:.2f} -> OOF MAE {best_mae:,.0f}')\n",
        "\n",
        "# Choose clipping upper bound based on blended OOF\n",
        "p_blend = (1.0 - best_w) * px + best_w * pl\n",
        "best_upper, best_clip_mae = None, best_mae\n",
        "for q in [0.99, 0.995, 0.999, 1.0]:\n",
        "    upper = float(np.quantile(y, q)) if q < 1.0 else float(np.max(y))\n",
        "    m = mae(y, np.clip(p_blend, 0.0, upper))\n",
        "    log(f'Clip upper {q} -> {upper:,.0f}: OOF MAE {m:,.0f}')\n",
        "    if m < best_clip_mae:\n",
        "        best_clip_mae, best_upper = m, upper\n",
        "if best_upper is None:\n",
        "    best_upper = float(np.max(y))\n",
        "log(f'Chosen clip upper={best_upper:,.0f} (OOF {best_clip_mae:,.0f})')\n",
        "\n",
        "# Load test predictions\n",
        "pred_xgb = pd.read_csv('pred_test_xgb_raw.csv')   # aligned to sample order, raw (unclipped) float32\n",
        "pred_lgb = pd.read_csv('pred_test_lgb.csv')       # aligned to sample order, int (from submission), but we can treat as floats\n",
        "\n",
        "# Align to sample_submission order explicitly\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "px_test = pred_xgb.set_index('segment_id').reindex(ss['segment_id'].values)['time_to_eruption'].astype(np.float64).values\n",
        "pl_test = pred_lgb.set_index('segment_id').reindex(ss['segment_id'].values)['time_to_eruption'].astype(np.float64).values\n",
        "\n",
        "# Blend and clip\n",
        "p_test = (1.0 - best_w) * px_test + best_w * pl_test\n",
        "p_test = np.clip(p_test, 0.0, best_upper)\n",
        "\n",
        "# Write submission\n",
        "sub = pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': p_test.astype(np.int64)})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "log(f'Blended submission saved ({os.path.getsize(\"submission.csv\")} bytes). Head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.839455Z] Merged OOF: (3987, 5), y range [6250.0,49046088.0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.840036Z] w=0.00 -> OOF MAE 3,235,388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.840330Z] w=0.05 -> OOF MAE 3,238,633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.840598Z] w=0.10 -> OOF MAE 3,243,832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.840855Z] w=0.15 -> OOF MAE 3,250,681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.841103Z] w=0.20 -> OOF MAE 3,259,396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.841332Z] w=0.25 -> OOF MAE 3,269,461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.841585Z] w=0.30 -> OOF MAE 3,281,221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.841858Z] w=0.35 -> OOF MAE 3,294,968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.842078Z] w=0.40 -> OOF MAE 3,310,771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.842325Z] w=0.45 -> OOF MAE 3,328,371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.842569Z] w=0.50 -> OOF MAE 3,347,208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.842868Z] w=0.55 -> OOF MAE 3,367,667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.843156Z] w=0.60 -> OOF MAE 3,389,620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.843383Z] w=0.65 -> OOF MAE 3,413,142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.843639Z] w=0.70 -> OOF MAE 3,437,734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.843921Z] w=0.75 -> OOF MAE 3,463,636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.844195Z] w=0.80 -> OOF MAE 3,490,705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.844404Z] w=0.85 -> OOF MAE 3,518,808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.844654Z] w=0.90 -> OOF MAE 3,547,991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.844919Z] w=0.95 -> OOF MAE 3,578,254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.845136Z] w=1.00 -> OOF MAE 3,609,869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.845358Z] Chosen weight w_LGB=0.00 -> OOF MAE 3,235,388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.846060Z] Clip upper 0.99 -> 46,962,008: OOF MAE 3,234,677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.846543Z] Clip upper 0.995 -> 47,715,640: OOF MAE 3,233,765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.847004Z] Clip upper 0.999 -> 48,356,643: OOF MAE 3,233,824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.847232Z] Clip upper 1.0 -> 49,046,088: OOF MAE 3,234,111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.847487Z] Chosen clip upper=47,715,640 (OOF 3,233,765)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:32:00.851844Z] Blended submission saved (8613 bytes). Head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289          30333056\n1   508758258          15063424\n2  1566132188          33993970\n3  1891418251          21747250\n4  1968343855           7629103\n"
          ]
        }
      ]
    },
    {
      "id": "89d8fa08-f6d8-4ac2-8795-9f9ec57ca188",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# XGBoost 3-seed bagging (same CV/params), averaged test preds, OOF-based clip, save submission + raw preds\n",
        "import os, time, subprocess, sys, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "# Load features\n",
        "train_path = 'features_train_v1.csv'\n",
        "test_path = 'features_test_v1.csv'\n",
        "assert os.path.exists(train_path) and os.path.exists(test_path)\n",
        "train_feats = pd.read_csv(train_path).set_index('segment_id')\n",
        "test_feats = pd.read_csv(test_path).set_index('segment_id')\n",
        "y = train_feats['time_to_eruption'].astype(np.float32).values\n",
        "groups = train_feats['series_id'].values\n",
        "feature_cols = [c for c in train_feats.columns if c not in ('time_to_eruption','series_id')]\n",
        "X = train_feats[feature_cols].astype(np.float32).copy()\n",
        "X_test = test_feats.reindex(columns=feature_cols).astype(np.float32).copy().fillna(0.0)\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "log(f'Data loaded: X={X.shape}, X_test={X_test.shape}, feats={len(feature_cols)}')\n",
        "\n",
        "seeds = [42, 123, 456]\n",
        "n_splits = 5\n",
        "gkf = GroupKFold(n_splits=n_splits)\n",
        "\n",
        "# Base params; vary seed only\n",
        "base_params = {\n",
        "    'tree_method': 'gpu_hist',\n",
        "    'predictor': 'gpu_predictor',\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 6,\n",
        "    'eta': 0.03,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.6,\n",
        "    'min_child_weight': 12,\n",
        "    'alpha': 0.1,\n",
        "    'lambda': 3.0,\n",
        "    'gamma': 0.1,\n",
        "    'eval_metric': 'mae',\n",
        "    'verbosity': 0,\n",
        "}\n",
        "\n",
        "all_oof = []\n",
        "all_test_preds = []\n",
        "start_all = time.time()\n",
        "DM_test = xgb.DMatrix(X_test.values, feature_names=feature_cols)\n",
        "\n",
        "for si, seed in enumerate(seeds, 1):\n",
        "    params = dict(base_params)\n",
        "    params['seed'] = int(seed)\n",
        "    oof = np.zeros(len(X), dtype=np.float32)\n",
        "    test_preds_folds = []\n",
        "    fold_maes = []\n",
        "    log(f'[Seed {seed}] Training with GroupKFold={n_splits}')\n",
        "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups), 1):\n",
        "        t0 = time.time()\n",
        "        DM_tr = xgb.DMatrix(X.iloc[tr_idx].values, label=y[tr_idx], feature_names=feature_cols)\n",
        "        DM_va = xgb.DMatrix(X.iloc[va_idx].values, label=y[va_idx], feature_names=feature_cols)\n",
        "        bst = xgb.train(params, DM_tr, num_boost_round=10000, evals=[(DM_va,'valid')], early_stopping_rounds=300, verbose_eval=False)\n",
        "        best_iter = getattr(bst, 'best_iteration', None) or bst.num_boosted_rounds\n",
        "        va_pred = bst.predict(DM_va, iteration_range=(0, int(best_iter))).astype(np.float32)\n",
        "        oof[va_idx] = va_pred\n",
        "        mae = mean_absolute_error(y[va_idx], va_pred)\n",
        "        fold_maes.append(mae)\n",
        "        tp = bst.predict(DM_test, iteration_range=(0, int(best_iter))).astype(np.float32)\n",
        "        test_preds_folds.append(tp)\n",
        "        log(f'[Seed {seed}] Fold {fold}/{n_splits} MAE={mae:,.0f} | best_iter={best_iter} | {time.time()-t0:.1f}s')\n",
        "    oof_mae = mean_absolute_error(y, oof)\n",
        "    log(f'[Seed {seed}] OOF MAE={oof_mae:,.0f}; folds={[(i+1,int(m)) for i,m in enumerate(fold_maes)]}')\n",
        "    all_oof.append(oof.astype(np.float32))\n",
        "    all_test_preds.append(np.mean(test_preds_folds, axis=0).astype(np.float32))\n",
        "\n",
        "log(f'All seeds done in {time.time()-start_all:.1f}s')\n",
        "\n",
        "# Average OOF and test preds across seeds\n",
        "oof_mean = np.mean(np.stack(all_oof, axis=0), axis=0).astype(np.float32)\n",
        "test_mean = np.mean(np.stack(all_test_preds, axis=0), axis=0).astype(np.float32)\n",
        "oof_mae_mean = mean_absolute_error(y, oof_mean)\n",
        "log(f'[3-seed] OOF MAE (mean across seeds) = {oof_mae_mean:,.0f}')\n",
        "\n",
        "# Clip selection via OOF sweep\n",
        "best_upper = float(np.max(y))\n",
        "best_mae = oof_mae_mean\n",
        "for q in [0.99, 0.995, 0.999, 1.0]:\n",
        "    upper = float(np.quantile(y, q)) if q < 1.0 else float(np.max(y))\n",
        "    mae_clip = mean_absolute_error(y, np.clip(oof_mean, 0.0, upper))\n",
        "    log(f'[3-seed] Clip {q}: {upper:,.0f} -> OOF {mae_clip:,.0f}')\n",
        "    if mae_clip < best_mae:\n",
        "        best_mae = mae_clip\n",
        "        best_upper = upper\n",
        "log(f'[3-seed] Chosen upper={best_upper:,.0f} (OOF {best_mae:,.0f})')\n",
        "\n",
        "# Align to sample order, save raw and submission\n",
        "pred_series_raw = pd.Series(test_mean, index=X_test.index.values)\n",
        "pred_aligned_raw = pred_series_raw.reindex(ss['segment_id'].values).fillna(pred_series_raw.median()).values.astype(np.float32)\n",
        "pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': pred_aligned_raw}).to_csv('pred_test_xgb_3seed_raw.csv', index=False)\n",
        "log('Saved pred_test_xgb_3seed_raw.csv')\n",
        "\n",
        "pred_clipped = np.clip(pred_aligned_raw, 0.0, best_upper).astype(np.int64)\n",
        "sub = pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': pred_clipped})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "pd.DataFrame({'segment_id': X.index.values, 'y': y.astype(np.int64), 'oof_mean': oof_mean}).to_csv('oof_xgb_3seed.csv', index=False)\n",
        "log(f'[3-seed] Saved submission.csv ({os.path.getsize(\"submission.csv\")} bytes). Head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:36:36.240732Z] Data loaded: X=(3987, 2064), X_test=(444, 2064), feats=2064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:36:36.248324Z] [Seed 42] Training with GroupKFold=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:37:48.221769Z] [Seed 42] Fold 1/5 MAE=3,166,326 | best_iter=3748 | 72.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:39:03.576041Z] [Seed 42] Fold 2/5 MAE=3,246,702 | best_iter=3910 | 75.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:40:13.947053Z] [Seed 42] Fold 3/5 MAE=3,358,654 | best_iter=3584 | 70.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:41:36.022978Z] [Seed 42] Fold 4/5 MAE=3,181,652 | best_iter=4408 | 82.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:42:57.316814Z] [Seed 42] Fold 5/5 MAE=3,223,680 | best_iter=4270 | 81.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:42:57.317751Z] [Seed 42] OOF MAE=3,235,388; folds=[(1, 3166325), (2, 3246702), (3, 3358653), (4, 3181652), (5, 3223680)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:42:57.318169Z] [Seed 123] Training with GroupKFold=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:44:01.282771Z] [Seed 123] Fold 1/5 MAE=3,169,532 | best_iter=3384 | 64.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:45:06.321271Z] [Seed 123] Fold 2/5 MAE=3,292,624 | best_iter=3406 | 65.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:46:25.173599Z] [Seed 123] Fold 3/5 MAE=3,313,426 | best_iter=4080 | 78.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:47:44.670866Z] [Seed 123] Fold 4/5 MAE=3,215,607 | best_iter=4161 | 79.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:48:56.832409Z] [Seed 123] Fold 5/5 MAE=3,289,002 | best_iter=3752 | 72.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:48:56.833356Z] [Seed 123] OOF MAE=3,256,026; folds=[(1, 3169532), (2, 3292623), (3, 3313425), (4, 3215607), (5, 3289002)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:48:56.833823Z] [Seed 456] Training with GroupKFold=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:50:10.356266Z] [Seed 456] Fold 1/5 MAE=3,148,158 | best_iter=3900 | 73.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:51:26.120670Z] [Seed 456] Fold 2/5 MAE=3,297,580 | best_iter=3996 | 75.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:52:27.846598Z] [Seed 456] Fold 3/5 MAE=3,367,945 | best_iter=3119 | 61.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:53:17.663478Z] [Seed 456] Fold 4/5 MAE=3,134,891 | best_iter=2476 | 49.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:54:31.277938Z] [Seed 456] Fold 5/5 MAE=3,252,836 | best_iter=3788 | 73.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:54:31.278897Z] [Seed 456] OOF MAE=3,240,273; folds=[(1, 3148157), (2, 3297579), (3, 3367945), (4, 3134891), (5, 3252835)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:54:31.279430Z] All seeds done in 1075.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:54:31.280256Z] [3-seed] OOF MAE (mean across seeds) = 3,192,504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:54:31.281435Z] [3-seed] Clip 0.99: 46,962,008 -> OOF 3,192,076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:54:31.282102Z] [3-seed] Clip 0.995: 47,715,640 -> OOF 3,191,561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:54:31.282835Z] [3-seed] Clip 0.999: 48,356,643 -> OOF 3,191,449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:54:31.283364Z] [3-seed] Clip 1.0: 49,046,088 -> OOF 3,191,652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:54:31.283726Z] [3-seed] Chosen upper=48,356,643 (OOF 3,191,449)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:54:31.286882Z] Saved pred_test_xgb_3seed_raw.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:54:31.291913Z] [3-seed] Saved submission.csv (8616 bytes). Head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289          30329878\n1   508758258          14477632\n2  1566132188          34124112\n3  1891418251          22477562\n4  1968343855           7643206\n"
          ]
        }
      ]
    },
    {
      "id": "2c22a2f5-404b-4182-a183-5cd0282dab10",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LightGBM re-run (CPU) to save RAW test preds as float32 (pred_test_lgb_raw.csv) without overwriting submission\n",
        "import os, sys, subprocess, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'lightgbm==4.6.0'], check=True)\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Load features v2\n",
        "train_path = 'features_train_v1.csv'\n",
        "test_path = 'features_test_v1.csv'\n",
        "assert os.path.exists(train_path) and os.path.exists(test_path)\n",
        "train_feats = pd.read_csv(train_path).set_index('segment_id')\n",
        "test_feats = pd.read_csv(test_path).set_index('segment_id')\n",
        "y = train_feats['time_to_eruption'].astype(np.float32).values\n",
        "groups = train_feats['series_id'].values\n",
        "feature_cols = [c for c in train_feats.columns if c not in ('time_to_eruption','series_id')]\n",
        "X = train_feats[feature_cols].astype(np.float32).copy()\n",
        "X_test = test_feats.reindex(columns=feature_cols).astype(np.float32).copy().fillna(0.0)\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "log(f'LGB re-run: X={X.shape}, X_test={X_test.shape}, feats={len(feature_cols)}')\n",
        "\n",
        "params = {\n",
        "    'objective': 'mae',\n",
        "    'metric': 'mae',\n",
        "    'learning_rate': 0.03,\n",
        "    'num_leaves': 31,\n",
        "    'max_depth': 8,\n",
        "    'feature_fraction': 0.6,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'min_data_in_leaf': 40,\n",
        "    'lambda_l1': 0.1,\n",
        "    'lambda_l2': 3.0,\n",
        "    'force_col_wise': True,\n",
        "    'verbose': -1,\n",
        "    'num_threads': 0,\n",
        "    'seed': 1234,\n",
        "}\n",
        "\n",
        "n_splits = 5\n",
        "gkf = GroupKFold(n_splits=n_splits)\n",
        "test_preds = []\n",
        "oof = np.zeros(len(X), dtype=np.float32)\n",
        "fold_maes = []\n",
        "t0 = time.time()\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups), 1):\n",
        "    dtrain = lgb.Dataset(X.iloc[tr_idx], label=y[tr_idx], free_raw_data=False)\n",
        "    dvalid = lgb.Dataset(X.iloc[va_idx], label=y[va_idx], reference=dtrain, free_raw_data=False)\n",
        "    log(f'[LGB RAW] Fold {fold}/{n_splits}: train={len(tr_idx)} valid={len(va_idx)}')\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=20000,\n",
        "        valid_sets=[dvalid],\n",
        "        valid_names=['valid'],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)],\n",
        "    )\n",
        "    va_pred = model.predict(X.iloc[va_idx], num_iteration=model.best_iteration).astype(np.float32)\n",
        "    oof[va_idx] = va_pred\n",
        "    mae = mean_absolute_error(y[va_idx], va_pred)\n",
        "    fold_maes.append(mae)\n",
        "    log(f'[LGB RAW] Fold {fold} MAE={mae:,.0f} | best_iter={model.best_iteration}')\n",
        "    tp = model.predict(X_test, num_iteration=model.best_iteration).astype(np.float32)\n",
        "    test_preds.append(tp)\n",
        "\n",
        "oof_mae = mean_absolute_error(y, oof)\n",
        "log(f'[LGB RAW] OOF MAE={oof_mae:,.0f}; folds={[(i+1,int(m)) for i,m in enumerate(fold_maes)]} | elapsed {time.time()-t0:.1f}s')\n",
        "\n",
        "# Average raw test preds and save without clipping/casting\n",
        "test_mean = np.mean(np.stack(test_preds, axis=0), axis=0).astype(np.float32)\n",
        "pred_series = pd.Series(test_mean, index=X_test.index.values)\n",
        "pred_aligned = pred_series.reindex(ss['segment_id'].values).values.astype(np.float32)\n",
        "out_path = 'pred_test_lgb_raw.csv'\n",
        "pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': pred_aligned}).to_csv(out_path, index=False)\n",
        "log(f'Saved {out_path} (raw float) for blending. Head:')\n",
        "print(pd.read_csv(out_path).head())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:54:44.386390Z] LGB re-run: X=(3987, 2064), X_test=(444, 2064), feats=2064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:54:44.399329Z] [LGB RAW] Fold 1/5: train=3189 valid=798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:57:54.755627Z] [LGB RAW] Fold 1 MAE=3,471,504 | best_iter=19988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T03:57:54.849898Z] [LGB RAW] Fold 2/5: train=3189 valid=798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:01:03.014599Z] [LGB RAW] Fold 2 MAE=3,667,655 | best_iter=20000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:01:03.110507Z] [LGB RAW] Fold 3/5: train=3190 valid=797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:04:02.308809Z] [LGB RAW] Fold 3 MAE=3,714,156 | best_iter=18950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:04:02.395807Z] [LGB RAW] Fold 4/5: train=3190 valid=797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:07:05.287647Z] [LGB RAW] Fold 4 MAE=3,591,186 | best_iter=19964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:07:05.381025Z] [LGB RAW] Fold 5/5: train=3190 valid=797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:10:11.773150Z] [LGB RAW] Fold 5 MAE=3,604,946 | best_iter=19976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:10:11.851994Z] [LGB RAW] OOF MAE=3,609,869; folds=[(1, 3471503), (2, 3667655), (3, 3714156), (4, 3591186), (5, 3604945)] | elapsed 927.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:10:11.854960Z] Saved pred_test_lgb_raw.csv (raw float) for blending. Head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289        30272764.0\n1   508758258        14239392.0\n2  1566132188        31933286.0\n3  1891418251        24881624.0\n4  1968343855         7650647.0\n"
          ]
        }
      ]
    },
    {
      "id": "9911849d-b440-4110-a15f-c99c23f897d7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Small-weight RAW blend: XGB (prefer 3-seed) + LGB (raw) with w in {0.10, 0.15, 0.20}; OOF-driven clip; write submission.csv\n",
        "import os, numpy as np, pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "# Load OOFs for weight selection (restricted small weights, but include 0.0 baseline)\n",
        "oof_xgb = pd.read_csv('oof_xgb_v1.csv')  # segment_id, y, oof\n",
        "oof_lgb = pd.read_csv('oof_lgb.csv')     # segment_id, y, oof_lgb\n",
        "oof = oof_xgb.merge(oof_lgb, on='segment_id', how='inner', suffixes=('_xgb','_lgb'))\n",
        "if 'y_xgb' in oof.columns and 'y_lgb' in oof.columns:\n",
        "    y_for_w = oof['y_xgb'].astype(np.float64).values\n",
        "else:\n",
        "    y_for_w = oof['y'].astype(np.float64).values\n",
        "px = oof['oof'].astype(np.float64).values\n",
        "pl = oof['oof_lgb'].astype(np.float64).values\n",
        "log(f'Merged OOF for weight selection: {oof.shape}')\n",
        "\n",
        "cand_weights = [0.0, 0.10, 0.15, 0.20]\n",
        "def mae(a,b): return float(np.mean(np.abs(a-b)))\n",
        "best_w, best_mae_w = None, 1e18\n",
        "for w in cand_weights:\n",
        "    p = (1.0 - w) * px + w * pl\n",
        "    m = mae(y_for_w, p)\n",
        "    log(f'w={w:.2f} -> OOF MAE {m:,.0f}')\n",
        "    if m < best_mae_w:\n",
        "        best_mae_w, best_w = m, w\n",
        "log(f'Chosen small-weight w_LGB={best_w:.2f} (OOF {best_mae_w:,.0f})')\n",
        "\n",
        "# Prefer clipping upper bound from 3-seed XGB OOF if available\n",
        "best_upper = None\n",
        "if os.path.exists('oof_xgb_3seed.csv'):\n",
        "    o3 = pd.read_csv('oof_xgb_3seed.csv')  # segment_id, y, oof_mean\n",
        "    y3 = o3['y'].astype(np.float64).values\n",
        "    p3 = o3['oof_mean'].astype(np.float64).values\n",
        "    best_upper_3s, best_mae_3s = None, mae(y3, p3)\n",
        "    for q in [0.99, 0.995, 0.999, 1.0]:\n",
        "        upper = float(np.quantile(y3, q)) if q < 1.0 else float(np.max(y3))\n",
        "        m = mae(y3, np.clip(p3, 0.0, upper))\n",
        "        log(f'[3-seed OOF] Clip {q}: {upper:,.0f} -> OOF {m:,.0f}')\n",
        "        if (best_upper_3s is None) or (m < best_mae_3s):\n",
        "            best_mae_3s, best_upper_3s = m, upper\n",
        "    best_upper = float(best_upper_3s)\n",
        "    log(f'Chosen clip upper from 3-seed OOF = {best_upper:,.0f}')\n",
        "else:\n",
        "    # Fallback: use current merged OOF blend for clipping\n",
        "    p_blend = (1.0 - best_w) * px + best_w * pl\n",
        "    best_clip = best_mae_w\n",
        "    for q in [0.99, 0.995, 0.999, 1.0]:\n",
        "        upper = float(np.quantile(y_for_w, q)) if q < 1.0 else float(np.max(y_for_w))\n",
        "        m = mae(y_for_w, np.clip(p_blend, 0.0, upper))\n",
        "        log(f'[fallback] Clip {q}: {upper:,.0f} -> OOF {m:,.0f}')\n",
        "        if (best_upper is None) or (m < best_clip):\n",
        "            best_clip, best_upper = m, upper\n",
        "    log(f'Chosen clip upper (fallback)={best_upper:,.0f}')\n",
        "\n",
        "# Load RAW test predictions\n",
        "xgb_path = 'pred_test_xgb_3seed_raw.csv' if os.path.exists('pred_test_xgb_3seed_raw.csv') else 'pred_test_xgb_raw.csv'\n",
        "lgb_path = 'pred_test_lgb_raw.csv'\n",
        "assert os.path.exists(xgb_path), f'Missing {xgb_path}'\n",
        "assert os.path.exists(lgb_path), f'Missing {lgb_path} (run LGB RAW cell first)'\n",
        "px_test_df = pd.read_csv(xgb_path)\n",
        "pl_test_df = pd.read_csv(lgb_path)\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "px_test = px_test_df.set_index('segment_id').reindex(ss['segment_id'].values)['time_to_eruption'].astype(np.float64).values\n",
        "pl_test = pl_test_df.set_index('segment_id').reindex(ss['segment_id'].values)['time_to_eruption'].astype(np.float64).values\n",
        "\n",
        "# Blend and clip\n",
        "p_test = (1.0 - best_w) * px_test + best_w * pl_test\n",
        "p_test = np.clip(p_test, 0.0, best_upper).astype(np.int64)\n",
        "sub = pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': p_test})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "log(f'Saved submission.csv using {os.path.basename(xgb_path)} + pred_test_lgb_raw.csv, w_LGB={best_w:.2f}, upper={best_upper:,.0f}. Head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:11:31.699444Z] Merged OOF for weight selection: (3987, 5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:11:31.700021Z] w=0.00 -> OOF MAE 3,235,388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:11:31.700573Z] w=0.10 -> OOF MAE 3,243,832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:11:31.700922Z] w=0.15 -> OOF MAE 3,250,681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:11:31.701278Z] w=0.20 -> OOF MAE 3,259,396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:11:31.701610Z] Chosen small-weight w_LGB=0.00 (OOF 3,235,388)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:11:31.703965Z] [3-seed OOF] Clip 0.99: 46,962,008 -> OOF 3,192,076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:11:31.704631Z] [3-seed OOF] Clip 0.995: 47,715,640 -> OOF 3,191,561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:11:31.705247Z] [3-seed OOF] Clip 0.999: 48,356,643 -> OOF 3,191,449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:11:31.705620Z] [3-seed OOF] Clip 1.0: 49,046,088 -> OOF 3,191,652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:11:31.705904Z] Chosen clip upper from 3-seed OOF = 48,356,643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:11:31.709726Z] Saved submission.csv using pred_test_xgb_3seed_raw.csv + pred_test_lgb_raw.csv, w_LGB=0.00, upper=48,356,643. Head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289          30329878\n1   508758258          14477632\n2  1566132188          34124110\n3  1891418251          22477562\n4  1968343855           7643206\n"
          ]
        }
      ]
    },
    {
      "id": "d30fbe57-660c-4975-a91a-04d5088fc429",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Manual variant: force small LGB weight and chosen clip to probe LB; writes submission.csv\n",
        "import os, numpy as np, pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "# Config: try pure 3-seed XGB (w=0.0) and no clipping (q=1.0)\n",
        "forced_w = 0.00\n",
        "forced_q = 1.0\n",
        "\n",
        "xgb_path = 'pred_test_xgb_3seed_raw.csv' if os.path.exists('pred_test_xgb_3seed_raw.csv') else 'pred_test_xgb_raw.csv'\n",
        "lgb_path = 'pred_test_lgb_raw.csv'\n",
        "assert os.path.exists(xgb_path) and os.path.exists(lgb_path)\n",
        "\n",
        "# Determine clip upper from 3-seed OOF if available for the chosen quantile (q=1.0 -> max(y))\n",
        "if os.path.exists('oof_xgb_3seed.csv'):\n",
        "    o3 = pd.read_csv('oof_xgb_3seed.csv')\n",
        "    y3 = o3['y'].astype(np.float64).values\n",
        "    if forced_q < 1.0:\n",
        "        upper = float(np.quantile(y3, forced_q))\n",
        "    else:\n",
        "        upper = float(np.max(y3))\n",
        "else:\n",
        "    y_all = pd.read_csv('oof_xgb_v1.csv')['y'].astype(np.float64).values\n",
        "    upper = float(np.quantile(y_all, forced_q)) if forced_q < 1.0 else float(np.max(y_all))\n",
        "\n",
        "px_df = pd.read_csv(xgb_path).set_index('segment_id')\n",
        "pl_df = pd.read_csv(lgb_path).set_index('segment_id')\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "px = px_df.reindex(ss['segment_id'].values)['time_to_eruption'].astype(np.float64).values\n",
        "pl = pl_df.reindex(ss['segment_id'].values)['time_to_eruption'].astype(np.float64).values\n",
        "\n",
        "p = (1.0 - forced_w) * px + forced_w * pl\n",
        "p = np.clip(p, 0.0, upper).astype(np.int64)\n",
        "sub = pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': p})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "log(f'Variant submission saved with w_LGB={forced_w}, clip_q={forced_q} (upper={upper:,.0f}). Head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:22:30.807987Z] Variant submission saved with w_LGB=0.0, clip_q=1.0 (upper=49,046,088). Head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289          30329878\n1   508758258          14477632\n2  1566132188          34124110\n3  1891418251          22477562\n4  1968343855           7643206\n"
          ]
        }
      ]
    },
    {
      "id": "38fc0394-1eaa-4ada-ad47-ed24772b11dc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Robust XGBoost (single-seed) with stronger regularization to fight LB shift; save RAW preds for later blend\n",
        "import os, time, subprocess, sys, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "# Load features\n",
        "train_path = 'features_train_v1.csv'\n",
        "test_path = 'features_test_v1.csv'\n",
        "assert os.path.exists(train_path) and os.path.exists(test_path)\n",
        "train_feats = pd.read_csv(train_path).set_index('segment_id')\n",
        "test_feats = pd.read_csv(test_path).set_index('segment_id')\n",
        "y = train_feats['time_to_eruption'].astype(np.float32).values\n",
        "groups = train_feats['series_id'].values\n",
        "feature_cols = [c for c in train_feats.columns if c not in ('time_to_eruption','series_id')]\n",
        "X = train_feats[feature_cols].astype(np.float32).copy()\n",
        "X_test = test_feats.reindex(columns=feature_cols).astype(np.float32).copy().fillna(0.0)\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "log(f'Robust XGB data: X={X.shape}, X_test={X_test.shape}, feats={len(feature_cols)}')\n",
        "\n",
        "# Robust regularization params (conservative)\n",
        "params = {\n",
        "    'tree_method': 'gpu_hist',\n",
        "    'predictor': 'gpu_predictor',\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 5,\n",
        "    'eta': 0.02,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.5,\n",
        "    'min_child_weight': 25,\n",
        "    'lambda': 12.0,\n",
        "    'alpha': 1.0,\n",
        "    'eval_metric': 'mae',\n",
        "    'seed': 777,\n",
        "    'verbosity': 0,\n",
        "}\n",
        "\n",
        "n_splits = 5\n",
        "gkf = GroupKFold(n_splits=n_splits)\n",
        "oof = np.zeros(len(X), dtype=np.float32)\n",
        "test_preds_folds = []\n",
        "DM_test = xgb.DMatrix(X_test.values, feature_names=feature_cols)\n",
        "t0_all = time.time()\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups), 1):\n",
        "    t0 = time.time()\n",
        "    DM_tr = xgb.DMatrix(X.iloc[tr_idx].values, label=y[tr_idx], feature_names=feature_cols)\n",
        "    DM_va = xgb.DMatrix(X.iloc[va_idx].values, label=y[va_idx], feature_names=feature_cols)\n",
        "    bst = xgb.train(params, DM_tr, num_boost_round=20000, evals=[(DM_va,'valid')], early_stopping_rounds=500, verbose_eval=False)\n",
        "    best_iter = getattr(bst, 'best_iteration', None) or bst.num_boosted_rounds\n",
        "    va_pred = bst.predict(DM_va, iteration_range=(0, int(best_iter))).astype(np.float32)\n",
        "    oof[va_idx] = va_pred\n",
        "    mae = mean_absolute_error(y[va_idx], va_pred)\n",
        "    log(f'[Robust] Fold {fold}/{n_splits} MAE={mae:,.0f} | best_iter={best_iter} | {time.time()-t0:.1f}s')\n",
        "    tp = bst.predict(DM_test, iteration_range=(0, int(best_iter))).astype(np.float32)\n",
        "    test_preds_folds.append(tp)\n",
        "\n",
        "oof_mae = mean_absolute_error(y, oof)\n",
        "log(f'[Robust] OOF MAE={oof_mae:,.0f} | total {time.time()-t0_all:.1f}s')\n",
        "\n",
        "# Choose clip from robust OOF\n",
        "best_upper = float(np.max(y))\n",
        "best_mae_clip = oof_mae\n",
        "for q in [0.99, 0.995, 0.999, 1.0]:\n",
        "    upper = float(np.quantile(y, q)) if q < 1.0 else float(np.max(y))\n",
        "    m = mean_absolute_error(y, np.clip(oof, 0.0, upper))\n",
        "    log(f'[Robust] Clip {q}: {upper:,.0f} -> OOF {m:,.0f}')\n",
        "    if m < best_mae_clip:\n",
        "        best_mae_clip = m\n",
        "        best_upper = upper\n",
        "log(f'[Robust] Chosen upper={best_upper:,.0f} (OOF {best_mae_clip:,.0f})')\n",
        "\n",
        "# Save RAW preds for blending; also save OOF\n",
        "test_mean = np.mean(np.stack(test_preds_folds, axis=0), axis=0).astype(np.float32)\n",
        "pred_series_raw = pd.Series(test_mean, index=X_test.index.values)\n",
        "pred_aligned_raw = pred_series_raw.reindex(ss['segment_id'].values).fillna(pred_series_raw.median()).values.astype(np.float32)\n",
        "pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': pred_aligned_raw}).to_csv('pred_test_xgb_robust_raw.csv', index=False)\n",
        "pd.DataFrame({'segment_id': X.index.values, 'y': y.astype(np.int64), 'oof_robust': oof.astype(np.float32)}).to_csv('oof_xgb_robust.csv', index=False)\n",
        "log('Saved pred_test_xgb_robust_raw.csv and oof_xgb_robust.csv')\n",
        "\n",
        "# Optional: produce a standalone robust submission (clipped) for sanity\n",
        "pred_clipped = np.clip(pred_aligned_raw, 0.0, best_upper).astype(np.int64)\n",
        "sub = pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': pred_clipped})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "log(f'[Robust] Wrote submission.csv ({os.path.getsize(\"submission.csv\")} bytes). Head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:13:42.439186Z] Robust XGB data: X=(3987, 2064), X_test=(444, 2064), feats=2064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:15:27.565045Z] [Robust] Fold 1/5 MAE=3,250,181 | best_iter=9013 | 105.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:17:00.596322Z] [Robust] Fold 2/5 MAE=3,394,504 | best_iter=7672 | 92.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:18:26.903612Z] [Robust] Fold 3/5 MAE=3,349,655 | best_iter=7158 | 86.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:19:59.605831Z] [Robust] Fold 4/5 MAE=3,295,567 | best_iter=7628 | 92.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:22:00.804112Z] [Robust] Fold 5/5 MAE=3,421,705 | best_iter=10304 | 121.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:22:00.913407Z] [Robust] OOF MAE=3,342,313 | total 498.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:22:00.914705Z] [Robust] Clip 0.99: 46,962,008 -> OOF 3,340,892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:22:00.915446Z] [Robust] Clip 0.995: 47,715,640 -> OOF 3,340,312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:22:00.916164Z] [Robust] Clip 0.999: 48,356,643 -> OOF 3,340,761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:22:00.916685Z] [Robust] Clip 1.0: 49,046,088 -> OOF 3,341,257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:22:00.916963Z] [Robust] Chosen upper=47,715,640 (OOF 3,340,312)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:22:00.923632Z] Saved pred_test_xgb_robust_raw.csv and oof_xgb_robust.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:22:00.924783Z] [Robust] Wrote submission.csv (8584 bytes). Head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289          29999788\n1   508758258          13026953\n2  1566132188          33837308\n3  1891418251          23766566\n4  1968343855           5129763\n"
          ]
        }
      ]
    },
    {
      "id": "6f1371cf-8876-4889-a148-97e8a0023a7f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tri-model RAW blend: XGB 3-seed (anchor) + XGB robust + LGB (raw); OOF grid-search weights; OOF-based clip; write submission.csv\n",
        "import os, numpy as np, pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "# Load OOFs\n",
        "o3 = pd.read_csv('oof_xgb_3seed.csv')    # segment_id, y, oof_mean\n",
        "orob = pd.read_csv('oof_xgb_robust.csv')  # segment_id, y, oof_robust\n",
        "olgb = pd.read_csv('oof_lgb.csv')        # segment_id, y, oof_lgb\n",
        "o = o3.merge(orob[['segment_id','oof_robust']], on='segment_id', how='inner').merge(olgb[['segment_id','oof_lgb']], on='segment_id', how='inner')\n",
        "y = o['y'].astype(np.float64).values\n",
        "p3 = o['oof_mean'].astype(np.float64).values\n",
        "pr = o['oof_robust'].astype(np.float64).values\n",
        "pl = o['oof_lgb'].astype(np.float64).values\n",
        "log(f'OOF merged shape: {o.shape}')\n",
        "\n",
        "def mae(a,b): return float(np.mean(np.abs(a-b)))\n",
        "best = {'b':0.0,'c':0.0,'mae':1e18}\n",
        "b_grid = np.linspace(0.0, 0.50, 11)   # robust weight\n",
        "c_grid = np.linspace(0.0, 0.30, 7)    # lgb weight\n",
        "for b in b_grid:\n",
        "    for c in c_grid:\n",
        "        if b + c <= 0.7:\n",
        "            a = 1.0 - b - c\n",
        "            p = a*p3 + b*pr + c*pl\n",
        "            m = mae(y, p)\n",
        "            if m < best['mae']:\n",
        "                best = {'b':float(b), 'c':float(c), 'a':float(a), 'mae':m}\n",
        "log(f\"Chosen weights a(3seed)={best['a']:.2f}, b(robust)={best['b']:.2f}, c(lgb)={best['c']:.2f} -> OOF MAE {best['mae']:,.0f}\")\n",
        "\n",
        "# Clip upper via OOF sweep using blended OOF\n",
        "p_blend_oof = best['a']*p3 + best['b']*pr + best['c']*pl\n",
        "best_upper, best_mae_clip = None, best['mae']\n",
        "for q in [0.99, 0.995, 0.999, 1.0]:\n",
        "    upper = float(np.quantile(y, q)) if q < 1.0 else float(np.max(y))\n",
        "    m = mae(y, np.clip(p_blend_oof, 0.0, upper))\n",
        "    log(f'Clip {q}: {upper:,.0f} -> OOF {m:,.0f}')\n",
        "    if (best_upper is None) or (m < best_mae_clip):\n",
        "        best_upper, best_mae_clip = upper, m\n",
        "log(f'Chosen clip upper={best_upper:,.0f} (OOF {best_mae_clip:,.0f})')\n",
        "\n",
        "# Load RAW test preds\n",
        "px3 = pd.read_csv('pred_test_xgb_3seed_raw.csv').set_index('segment_id')\n",
        "pxr = pd.read_csv('pred_test_xgb_robust_raw.csv').set_index('segment_id')\n",
        "plg = pd.read_csv('pred_test_lgb_raw.csv').set_index('segment_id')\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "x3 = px3.reindex(ss['segment_id'].values)['time_to_eruption'].astype(np.float64).values\n",
        "xr = pxr.reindex(ss['segment_id'].values)['time_to_eruption'].astype(np.float64).values\n",
        "xl = plg.reindex(ss['segment_id'].values)['time_to_eruption'].astype(np.float64).values\n",
        "\n",
        "# Blend and clip\n",
        "ptest = best['a']*x3 + best['b']*xr + best['c']*xl\n",
        "ptest = np.clip(ptest, 0.0, best_upper).astype(np.int64)\n",
        "sub = pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': ptest})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "log(f\"Saved submission.csv (tri-blend). Weights a={best['a']:.2f}, b={best['b']:.2f}, c={best['c']:.2f}; upper={best_upper:,.0f}. Head:\")\n",
        "print(sub.head())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:23:08.838827Z] OOF merged shape: (3987, 5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:23:08.840919Z] Chosen weights a(3seed)=1.00, b(robust)=0.00, c(lgb)=0.00 -> OOF MAE 3,192,504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:23:08.841618Z] Clip 0.99: 46,962,008 -> OOF 3,192,076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:23:08.842157Z] Clip 0.995: 47,715,640 -> OOF 3,191,561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:23:08.842638Z] Clip 0.999: 48,356,643 -> OOF 3,191,449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:23:08.843003Z] Clip 1.0: 49,046,088 -> OOF 3,191,652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:23:08.843261Z] Chosen clip upper=48,356,643 (OOF 3,191,449)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:23:08.847699Z] Saved submission.csv (tri-blend). Weights a=1.00, b=0.00, c=0.00; upper=48,356,643. Head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289          30329878\n1   508758258          14477632\n2  1566132188          34124110\n3  1891418251          22477562\n4  1968343855           7643206\n"
          ]
        }
      ]
    },
    {
      "id": "40e3d019-14a4-4777-a155-8a90a6ea329f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Huber calibration submissions: (1) 3-seed XGB calibrated @0.999 clip; (2) fixed-weight tri-blend calibrated @0.995 clip\n",
        "import os, numpy as np, pandas as pd\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from datetime import datetime\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "def load_test_series(path):\n",
        "    df = pd.read_csv(path).set_index('segment_id')\n",
        "    return df.reindex(ss['segment_id'].values)['time_to_eruption'].astype(np.float64).values\n",
        "\n",
        "def calibrate_and_save(y, oof_pred, test_pred, clip_q, out_path):\n",
        "    # Fit Huber y ~ oof_pred\n",
        "    X = oof_pred.reshape(-1,1)\n",
        "    hub = HuberRegressor(epsilon=1.35, alpha=0.0, fit_intercept=True, max_iter=1000)\n",
        "    hub.fit(X, y.astype(np.float64))\n",
        "    # Apply to test\n",
        "    test_cal = hub.predict(test_pred.reshape(-1,1)).astype(np.float64)\n",
        "    # Clip upper bound from y quantile\n",
        "    if clip_q < 1.0:\n",
        "        upper = float(np.quantile(y, clip_q))\n",
        "    else:\n",
        "        upper = float(np.max(y))\n",
        "    test_cal = np.clip(test_cal, 0.0, upper).astype(np.int64)\n",
        "    sub = pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': test_cal})\n",
        "    sub.to_csv(out_path, index=False)\n",
        "    log(f'Saved {out_path} (clip_q={clip_q}, upper={upper:,.0f}); head:')\n",
        "    print(sub.head())\n",
        "    return out_path\n",
        "\n",
        "# 1) Bias-corrected 3-seed XGB (anchor)\n",
        "o3 = pd.read_csv('oof_xgb_3seed.csv')  # columns: segment_id, y, oof_mean\n",
        "y3 = o3['y'].astype(np.float64).values\n",
        "oof3 = o3['oof_mean'].astype(np.float64).values\n",
        "test3 = load_test_series('pred_test_xgb_3seed_raw.csv')\n",
        "out1 = calibrate_and_save(y3, oof3, test3, clip_q=0.999, out_path='sub_calib_3seed_q0999.csv')\n",
        "\n",
        "# 2) Bias-corrected small-weight tri-blend (fixed weights 0.85 3-seed, 0.10 robust XGB, 0.05 LGB)\n",
        "orob = pd.read_csv('oof_xgb_robust.csv')   # segment_id, y, oof_robust\n",
        "olgb = pd.read_csv('oof_lgb.csv')          # segment_id, y, oof_lgb\n",
        "oo = o3.merge(orob[['segment_id','oof_robust']], on='segment_id').merge(olgb[['segment_id','oof_lgb']], on='segment_id')\n",
        "y_tri = oo['y'].astype(np.float64).values\n",
        "oof_tri = (0.85*oo['oof_mean'].astype(np.float64).values + 0.10*oo['oof_robust'].astype(np.float64).values + 0.05*oo['oof_lgb'].astype(np.float64).values)\n",
        "t3 = load_test_series('pred_test_xgb_3seed_raw.csv')\n",
        "tr = load_test_series('pred_test_xgb_robust_raw.csv')\n",
        "tl = load_test_series('pred_test_lgb_raw.csv')\n",
        "test_tri = (0.85*t3 + 0.10*tr + 0.05*tl).astype(np.float64)\n",
        "out2 = calibrate_and_save(y_tri, oof_tri, test_tri, clip_q=0.995, out_path='sub_calib_tri_q0995.csv')\n",
        "\n",
        "log(f'Calibration files ready: {out1}, {out2}. To submit, copy desired file to submission.csv and submit.')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:28:19.188757Z] Saved sub_calib_3seed_q0999.csv (clip_q=0.999, upper=48,356,643); head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289          30976723\n1   508758258          14786396\n2  1566132188          34851875\n3  1891418251          22956941\n4  1968343855           7806213\n[2025-09-27T04:28:19.200375Z] Saved sub_calib_tri_q0995.csv (clip_q=0.995, upper=47,715,640); head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289          30919273\n1   508758258          14616226\n2  1566132188          34687348\n3  1891418251          23195738\n4  1968343855           7544808\n[2025-09-27T04:28:19.201211Z] Calibration files ready: sub_calib_3seed_q0999.csv, sub_calib_tri_q0995.csv. To submit, copy desired file to submission.csv and submit.\n"
          ]
        }
      ]
    },
    {
      "id": "f86bb97c-ba5b-4e38-af75-52f53fd056f4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set isotonic-calibrated 3-seed submission as final submission.csv; validate shape/order/dtypes\n",
        "import pandas as pd, numpy as np, os\n",
        "from datetime import datetime\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "src = 'sub_iso_3seed_q0999.csv'\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = pd.read_csv(src)\n",
        "\n",
        "# Basic validations\n",
        "assert list(sub.columns) == ['segment_id','time_to_eruption'], 'Columns mismatch'\n",
        "assert len(sub) == len(ss) == 444, 'Row count mismatch'\n",
        "assert np.array_equal(sub['segment_id'].values, ss['segment_id'].values), 'Order/IDs mismatch vs sample_submission'\n",
        "assert sub['time_to_eruption'].dtype == np.int64 or sub['time_to_eruption'].dtype == np.int32, 'time_to_eruption not integer'\n",
        "assert sub['time_to_eruption'].isna().sum() == 0, 'NaNs present'\n",
        "assert (sub['time_to_eruption'] >= 0).all(), 'Negative predictions present'\n",
        "\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "log(f'submission.csv written from {src} ({os.path.getsize(\"submission.csv\")} bytes). Head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:45:55.189122Z] submission.csv written from sub_iso_3seed_q0999.csv (8595 bytes). Head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289          31867039\n1   508758258          12945360\n2  1566132188          37029393\n3  1891418251          22227369\n4  1968343855           6864116\n"
          ]
        }
      ]
    },
    {
      "id": "558fa40d-e262-4b55-b709-18207c99be14",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sample-weighted robust XGB (GPU, MAE) with Huber calibration + 0.995 clip; saves RAW preds and submission\n",
        "import os, time, subprocess, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "# Load features\n",
        "train_path = 'features_train_v1.csv'\n",
        "test_path = 'features_test_v1.csv'\n",
        "assert os.path.exists(train_path) and os.path.exists(test_path), 'Run feature build first.'\n",
        "train_feats = pd.read_csv(train_path).set_index('segment_id')\n",
        "test_feats = pd.read_csv(test_path).set_index('segment_id')\n",
        "y = train_feats['time_to_eruption'].astype(np.float32).values\n",
        "groups = train_feats['series_id'].values\n",
        "feature_cols = [c for c in train_feats.columns if c not in ('time_to_eruption','series_id')]\n",
        "X = train_feats[feature_cols].astype(np.float32).copy()\n",
        "X_test = test_feats.reindex(columns=feature_cols).astype(np.float32).copy().fillna(0.0)\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "log(f'Weighted XGB data: X={X.shape}, X_test={X_test.shape}, feats={len(feature_cols)}')\n",
        "\n",
        "# Sample weights: w = 1 / (1 + y/1e6)\n",
        "w_full = (1.0 / (1.0 + (y.astype(np.float64) / 1e6))).astype(np.float32)\n",
        "\n",
        "# Params per expert advice\n",
        "params = {\n",
        "    'tree_method': 'gpu_hist',\n",
        "    'predictor': 'gpu_predictor',\n",
        "    'objective': 'reg:absoluteerror',\n",
        "    'eval_metric': 'mae',\n",
        "    'max_depth': 5,\n",
        "    'eta': 0.02,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.5,\n",
        "    'min_child_weight': 28,\n",
        "    'lambda': 13.0,\n",
        "    'alpha': 1.0,\n",
        "    'seed': 2025,\n",
        "    'verbosity': 0,\n",
        "}\n",
        "\n",
        "n_splits = 5\n",
        "gkf = GroupKFold(n_splits=n_splits)\n",
        "oof = np.zeros(len(X), dtype=np.float32)\n",
        "test_preds_folds = []\n",
        "DM_test = xgb.DMatrix(X_test.values, feature_names=feature_cols)\n",
        "t0_all = time.time()\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y, groups=groups), 1):\n",
        "    t0 = time.time()\n",
        "    DM_tr = xgb.DMatrix(X.iloc[tr_idx].values, label=y[tr_idx], weight=w_full[tr_idx], feature_names=feature_cols)\n",
        "    DM_va = xgb.DMatrix(X.iloc[va_idx].values, label=y[va_idx], weight=w_full[va_idx], feature_names=feature_cols)\n",
        "    bst = xgb.train(params, DM_tr, num_boost_round=20000, evals=[(DM_va,'valid')], early_stopping_rounds=700, verbose_eval=False)\n",
        "    best_iter = getattr(bst, 'best_iteration', None) or bst.num_boosted_rounds\n",
        "    va_pred = bst.predict(DM_va, iteration_range=(0, int(best_iter))).astype(np.float32)\n",
        "    oof[va_idx] = va_pred\n",
        "    mae = mean_absolute_error(y[va_idx], va_pred)\n",
        "    log(f'[Weighted] Fold {fold}/{n_splits} MAE={mae:,.0f} | best_iter={best_iter} | {time.time()-t0:.1f}s')\n",
        "    tp = bst.predict(DM_test, iteration_range=(0, int(best_iter))).astype(np.float32)\n",
        "    test_preds_folds.append(tp)\n",
        "\n",
        "oof_mae = mean_absolute_error(y, oof)\n",
        "log(f'[Weighted] OOF MAE={oof_mae:,.0f} | total {time.time()-t0_all:.1f}s')\n",
        "\n",
        "# Save OOF for later blending/calibration and RAW test preds\n",
        "pd.DataFrame({'segment_id': X.index.values, 'y': y.astype(np.int64), 'oof_weighted': oof.astype(np.float32)}).to_csv('oof_xgb_weighted.csv', index=False)\n",
        "test_mean = np.mean(np.stack(test_preds_folds, axis=0), axis=0).astype(np.float32)\n",
        "pred_series_raw = pd.Series(test_mean, index=X_test.index.values)\n",
        "pred_aligned_raw = pred_series_raw.reindex(ss['segment_id'].values).fillna(pred_series_raw.median()).values.astype(np.float32)\n",
        "pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': pred_aligned_raw}).to_csv('pred_test_xgb_weighted_raw.csv', index=False)\n",
        "log('Saved oof_xgb_weighted.csv and pred_test_xgb_weighted_raw.csv')\n",
        "\n",
        "# Huber calibration on weighted OOF -> apply to test; clip at 0.995\n",
        "hub = HuberRegressor(epsilon=1.35, alpha=0.0, fit_intercept=True, max_iter=1000)\n",
        "hub.fit(oof.reshape(-1,1), y.astype(np.float64))\n",
        "test_cal = hub.predict(pred_aligned_raw.reshape(-1,1)).astype(np.float64)\n",
        "upper = float(np.quantile(y.astype(np.float64), 0.995))\n",
        "test_cal = np.clip(test_cal, 0.0, upper).astype(np.int64)\n",
        "sub = pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': test_cal})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "log(f'[Weighted Calibrated] Wrote submission.csv with 0.995 clip (upper={upper:,.0f}). Head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:30:11.363694Z] Weighted XGB data: X=(3987, 2064), X_test=(444, 2064), feats=2064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:32:57.103877Z] [Weighted] Fold 1/5 MAE=9,181,826 | best_iter=19956 | 165.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:35:41.167798Z] [Weighted] Fold 2/5 MAE=9,279,052 | best_iter=19954 | 163.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:38:24.974281Z] [Weighted] Fold 3/5 MAE=8,780,537 | best_iter=19999 | 163.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:41:09.158075Z] [Weighted] Fold 4/5 MAE=9,014,827 | best_iter=19998 | 164.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:43:52.950375Z] [Weighted] Fold 5/5 MAE=9,139,727 | best_iter=19999 | 163.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:43:53.107852Z] [Weighted] OOF MAE=9,079,269 | total 821.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:43:53.114646Z] Saved oof_xgb_weighted.csv and pred_test_xgb_weighted_raw.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:43:53.119124Z] [Weighted Calibrated] Wrote submission.csv with 0.995 clip (upper=47,715,640). Head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289          24163019\n1   508758258          12535127\n2  1566132188          30433764\n3  1891418251          26285762\n4  1968343855           7249249\n"
          ]
        }
      ]
    },
    {
      "id": "82289db9-6b43-4d9e-b06e-4eb50b46ab4e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Isotonic calibration submissions: (1) 3-seed XGB @0.999 clip; (2) fixed-weight tri-blend @0.995 clip\n",
        "import numpy as np, pandas as pd, os\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from datetime import datetime\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.utcnow().isoformat()}Z] {msg}\", flush=True)\n",
        "\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "def load_test_series(path):\n",
        "    df = pd.read_csv(path).set_index('segment_id')\n",
        "    return df.reindex(ss['segment_id'].values)['time_to_eruption'].astype(np.float64).values\n",
        "\n",
        "def iso_calibrate_and_save(y, oof_pred, test_pred, clip_q, out_path):\n",
        "    # Fit monotonic isotonic regression: y = f(oof_pred)\n",
        "    x = oof_pred.astype(np.float64)\n",
        "    y = y.astype(np.float64)\n",
        "    iso = IsotonicRegression(out_of_bounds='clip')\n",
        "    iso.fit(x, y)\n",
        "    test_cal = iso.predict(test_pred.astype(np.float64))\n",
        "    if clip_q < 1.0:\n",
        "        upper = float(np.quantile(y, clip_q))\n",
        "    else:\n",
        "        upper = float(np.max(y))\n",
        "    test_cal = np.clip(test_cal, 0.0, upper).astype(np.int64)\n",
        "    sub = pd.DataFrame({'segment_id': ss['segment_id'].values, 'time_to_eruption': test_cal})\n",
        "    sub.to_csv(out_path, index=False)\n",
        "    log(f'Saved {out_path} (clip_q={clip_q}, upper={upper:,.0f}); head:')\n",
        "    print(sub.head())\n",
        "    return out_path\n",
        "\n",
        "# 1) Isotonic-calibrated 3-seed XGB\n",
        "o3 = pd.read_csv('oof_xgb_3seed.csv')  # segment_id,y,oof_mean\n",
        "y3 = o3['y'].astype(np.float64).values\n",
        "oof3 = o3['oof_mean'].astype(np.float64).values\n",
        "test3 = load_test_series('pred_test_xgb_3seed_raw.csv')\n",
        "out1 = iso_calibrate_and_save(y3, oof3, test3, clip_q=0.999, out_path='sub_iso_3seed_q0999.csv')\n",
        "\n",
        "# 2) Isotonic-calibrated fixed-weight tri-blend (0.85/0.10/0.05)\n",
        "orob = pd.read_csv('oof_xgb_robust.csv')\n",
        "olgb = pd.read_csv('oof_lgb.csv')\n",
        "oo = o3.merge(orob[['segment_id','oof_robust']], on='segment_id').merge(olgb[['segment_id','oof_lgb']], on='segment_id')\n",
        "y_tri = oo['y'].astype(np.float64).values\n",
        "oof_tri = (0.85*oo['oof_mean'].astype(np.float64).values + 0.10*oo['oof_robust'].astype(np.float64).values + 0.05*oo['oof_lgb'].astype(np.float64).values)\n",
        "t3 = load_test_series('pred_test_xgb_3seed_raw.csv')\n",
        "tr = load_test_series('pred_test_xgb_robust_raw.csv')\n",
        "tl = load_test_series('pred_test_lgb_raw.csv')\n",
        "test_tri = (0.85*t3 + 0.10*tr + 0.05*tl).astype(np.float64)\n",
        "out2 = iso_calibrate_and_save(y_tri, oof_tri, test_tri, clip_q=0.995, out_path='sub_iso_tri_q0995.csv')\n",
        "\n",
        "log(f'Isotonic calibration files ready: {out1}, {out2}. Copy chosen file to submission.csv then submit.')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-27T04:45:27.660296Z] Saved sub_iso_3seed_q0999.csv (clip_q=0.999, upper=48,356,643); head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289          31867039\n1   508758258          12945360\n2  1566132188          37029393\n3  1891418251          22227369\n4  1968343855           6864116\n[2025-09-27T04:45:27.670162Z] Saved sub_iso_tri_q0995.csv (clip_q=0.995, upper=47,715,640); head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   segment_id  time_to_eruption\n0   951290289          31585738\n1   508758258          13481325\n2  1566132188          35559280\n3  1891418251          21920915\n4  1968343855           6705357\n[2025-09-27T04:45:27.670924Z] Isotonic calibration files ready: sub_iso_3seed_q0999.csv, sub_iso_tri_q0995.csv. Copy chosen file to submission.csv then submit.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}