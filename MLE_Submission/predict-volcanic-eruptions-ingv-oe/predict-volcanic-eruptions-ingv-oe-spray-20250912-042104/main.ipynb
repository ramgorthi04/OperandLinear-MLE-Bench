{
  "cells": [
    {
      "id": "62f2f429-e542-44ca-a9a8-1126670c149c",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# INGV - Volcanic Eruption Prediction: Plan to Medal\n",
        "\n",
        "## Objectives\n",
        "- Build a fast, reliable baseline with strong CV.\n",
        "- Iterate feature engineering (time + frequency) and GPU models (XGBoost/CatBoost).\n",
        "- Ensemble diverse seeds/models once CV is stable.\n",
        "- Submit only after CV aligns with LB.\n",
        "\n",
        "## Data Understanding & Pipeline\n",
        "- Inspect train.csv schema (ID, target column name, any meta).\n",
        "- Load a few train/*.csv segments to confirm sensor columns, sampling rate, length.\n",
        "- Mirror test-time pipeline: per-file features -> model -> predict.\n",
        "- Cache features to disk (parquet) for reuse.\n",
        "\n",
        "## Validation\n",
        "- Avoid leakage: all transforms fit within folds.\n",
        "- Determine grouping (by volcano/station/day if present in train.csv; else robust KFold with file-wise split).\n",
        "- Fix a deterministic CV (e.g., 5 folds GroupKFold if group available; else 5-fold KFold by file).\n",
        "- Track OOF MAE and per-fold times.\n",
        "\n",
        "## Baseline v1\n",
        "- Subsample run: 200 files, 2 folds to smoke-test.\n",
        "- Simple features per channel:\n",
        "  - Global: mean, std, min, max, q01/q05/q25/q50/q75/q95/q99, iqr, skew, kurtosis, zero crossings, RMS.\n",
        "  - Rolling windows (coarse, e.g., 10\u201320 chunks): chunk means/stds and their stats.\n",
        "  - Frequency: FFT bandpowers over log-spaced bands; spectral centroid, roll-off, spectral flatness.\n",
        "- Model: XGBoost regressor with GPU (tree_method='hist', device='cuda').\n",
        "- Early stopping, 2000 rounds max, patience 100.\n",
        "\n",
        "## Feature Set v2\n",
        "- Add robust features:\n",
        "  - Denoised stats (median filtered).\n",
        "  - Autocorr features, dominant period via FFT peak.\n",
        "  - Per-axis + cross-axis correlations if multiple channels.\n",
        "- Try CatBoost GPU as alternative; blend with XGB.\n",
        "\n",
        "## Efficiency\n",
        "- Verify GPU available first.\n",
        "- Parallel feature extraction (joblib) with chunked reads.\n",
        "- Log progress and elapsed times per fold and per 100 files.\n",
        "- Cache train/test features and OOF predictions.\n",
        "\n",
        "## Ensembling\n",
        "- Blend XGB/CatBoost (and seeds). Weight by 1/MAE^p on OOF.\n",
        "\n",
        "## Error Analysis\n",
        "- Inspect OOF residuals by file stats (e.g., signal energy, station/volcano if available).\n",
        "- Iterate on top error buckets.\n",
        "\n",
        "## Deliverables\n",
        "- submission.csv with predictions for test IDs.\n",
        "- Saved features, OOF, model configs.\n",
        "\n",
        "Next:\n",
        "1) Verify GPU and environment; 2) Inspect train.csv and a sample file; 3) Implement feature extractor skeleton; 4) Build CV and baseline XGB."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "4ebb8392-c5a8-484c-8f54-c5828326f906",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup, GPU check, and schema inspection\n",
        "import os, sys, json, time, glob, math, random, gc, warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "print('Python', sys.version)\n",
        "print('CWD:', os.getcwd())\n",
        "\n",
        "# Verify GPU availability (xgboost + optional torch)\n",
        "gpu_info = {}\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    gpu_info['xgboost_version'] = xgb.__version__\n",
        "    print('xgboost', xgb.__version__)\n",
        "except Exception as e:\n",
        "    print('xgboost import failed:', e)\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    gpu_info['torch'] = {\n",
        "        'available': torch.cuda.is_available(),\n",
        "        'count': torch.cuda.device_count(),\n",
        "        'name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n",
        "    }\n",
        "    print('GPU Available (torch):', gpu_info['torch']['available'])\n",
        "    if gpu_info['torch']['available']:\n",
        "        print('GPU Count:', gpu_info['torch']['count'])\n",
        "        print('GPU Name:', gpu_info['torch']['name'])\n",
        "except Exception as e:\n",
        "    print('torch check failed:', e)\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = Path('.')\n",
        "TRAIN_DIR = DATA_DIR / 'train'\n",
        "TEST_DIR = DATA_DIR / 'test'\n",
        "\n",
        "# Inspect train.csv and sample_submission.csv\n",
        "train_meta_path = DATA_DIR / 'train.csv'\n",
        "ss_path = DATA_DIR / 'sample_submission.csv'\n",
        "assert train_meta_path.exists(), 'train.csv not found'\n",
        "assert ss_path.exists(), 'sample_submission.csv not found'\n",
        "\n",
        "train_meta = pd.read_csv(train_meta_path)\n",
        "print('train.csv shape:', train_meta.shape)\n",
        "print('train.csv columns:', list(train_meta.columns))\n",
        "print(train_meta.head(3))\n",
        "\n",
        "ss = pd.read_csv(ss_path)\n",
        "print('sample_submission shape:', ss.shape)\n",
        "print('sample_submission head:')\n",
        "print(ss.head())\n",
        "\n",
        "# Peek a few files to understand sensor columns and length\n",
        "train_files = sorted(glob.glob(str(TRAIN_DIR / '*.csv')))[:5]\n",
        "print('Sample train files:', [Path(f).name for f in train_files])\n",
        "\n",
        "def peek_file(fp, n=5):\n",
        "    df = pd.read_csv(fp, nrows=n)\n",
        "    return df\n",
        "\n",
        "for fp in train_files:\n",
        "    df_head = peek_file(fp, n=5)\n",
        "    print('File:', Path(fp).name, 'shape(head)=', df_head.shape, 'columns=', list(df_head.columns))\n",
        "    break\n",
        "\n",
        "# Read full length of one small file to gauge memory/length (use first file)\n",
        "t0 = time.time()\n",
        "probe_fp = train_files[0] if train_files else None\n",
        "if probe_fp:\n",
        "    df_probe = pd.read_csv(probe_fp)\n",
        "    print('Probe file:', Path(probe_fp).name, 'shape=', df_probe.shape, 'dtypes:', df_probe.dtypes.to_dict())\n",
        "    print('Elapsed to read probe: %.2fs' % (time.time()-t0))\n",
        "    # Basic stats on columns\n",
        "    print('NA fraction per col:', df_probe.isna().mean().to_dict())\n",
        "else:\n",
        "    print('No train files found!')\n",
        "\n",
        "# Utility: define band edges according to sampling rate later after we detect Fs and signal length\n",
        "def describe_signal_layout(df):\n",
        "    cols = list(df.columns)\n",
        "    n_rows = len(df)\n",
        "    return {'n_rows': n_rows, 'cols': cols}\n",
        "\n",
        "if probe_fp:\n",
        "    print('Layout:', describe_signal_layout(df_probe))\n",
        "\n",
        "print('SETUP DONE')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\nCWD: /app/agent_run_states/predict-volcanic-eruptions-ingv-oe-spray-20250912-042104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost 2.1.4\ntorch check failed: No module named 'torch'\ntrain.csv shape: (3987, 2)\ntrain.csv columns: ['segment_id', 'time_to_eruption']\n   segment_id  time_to_eruption\n0  1410693848          20109998\n1   987159268           7202883\n2  1990984540          28138930\nsample_submission shape: (444, 2)\nsample_submission head:\n   segment_id  time_to_eruption\n0   951290289                 0\n1   508758258                 0\n2  1566132188                 0\n3  1891418251                 0\n4  1968343855                 0\nSample train files: ['1000015382.csv', '1000554676.csv', '1000745424.csv', '1001461087.csv', '1001732002.csv']\nFile: 1000015382.csv shape(head)= (5, 10) columns= ['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_10']\nProbe file: 1000015382.csv shape= (60001, 10) dtypes: {'sensor_1': dtype('float64'), 'sensor_2': dtype('float64'), 'sensor_3': dtype('float64'), 'sensor_4': dtype('float64'), 'sensor_5': dtype('float64'), 'sensor_6': dtype('float64'), 'sensor_7': dtype('float64'), 'sensor_8': dtype('float64'), 'sensor_9': dtype('float64'), 'sensor_10': dtype('float64')}\nElapsed to read probe: 0.06s\nNA fraction per col: {'sensor_1': 0.0, 'sensor_2': 0.0025666238896018398, 'sensor_3': 0.0, 'sensor_4': 0.0, 'sensor_5': 0.0, 'sensor_6': 0.0, 'sensor_7': 0.0, 'sensor_8': 0.0, 'sensor_9': 0.0, 'sensor_10': 0.0}\nLayout: {'n_rows': 60001, 'cols': ['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_10']}\nSETUP DONE\n"
          ]
        }
      ]
    },
    {
      "id": "797fcbb1-3542-433b-8309-ec92755cb0bd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Feature extraction utilities and smoke feature build\n",
        "import os, time, glob, math, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# Ensure deps\n",
        "def ensure_imports():\n",
        "    import importlib, subprocess, sys\n",
        "    def pip_install(pkg):\n",
        "        print(f'Installing {pkg} ...');\n",
        "        subprocess.run([sys.executable, '-m', 'pip', 'install', pkg], check=True)\n",
        "    for mod, pkg in [('scipy', 'scipy'), ('joblib', 'joblib'), ('sklearn', 'scikit-learn')]:\n",
        "        try:\n",
        "            importlib.import_module(mod)\n",
        "        except ImportError:\n",
        "            pip_install(pkg)\n",
        "    # re-import after potential install\n",
        "    global signal, welch, RobustScaler, sosfiltfilt, butter, periodogram\n",
        "    from scipy import signal\n",
        "    from scipy.signal import welch, sosfiltfilt, butter, periodogram\n",
        "    from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "ensure_imports()\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "RNG = np.random.default_rng(42)\n",
        "\n",
        "# Sampling rate detection from a probe file (assume constant across files)\n",
        "probe_fp = sorted(glob.glob(str(TRAIN_DIR / '*.csv')))[0]\n",
        "probe_n = len(pd.read_csv(probe_fp))\n",
        "Fs = max(1, round((probe_n - 1) / 600))  # approx samples per second, 600 seconds window\n",
        "print('Detected Fs ~', Fs, 'Hz from n_rows=', probe_n)\n",
        "nyq = Fs / 2.0\n",
        "\n",
        "# Bands (Hz), capped below Nyquist\n",
        "raw_bands = [(0.5,2),(2,4),(4,8),(8,16),(16,32),(32,48)]\n",
        "BANDS = [(lo, min(hi, nyq*0.96)) for (lo,hi) in raw_bands if lo < nyq*0.96]\n",
        "print('Using bands:', BANDS)\n",
        "\n",
        "# Controls\n",
        "USE_ACF = False  # drop autocorr for v1 full run per expert advice\n",
        "\n",
        "def linear_detrend(x: np.ndarray) -> np.ndarray:\n",
        "    return signal.detrend(x, type='linear', overwrite_data=False)\n",
        "\n",
        "def butter_highpass_sos(x: np.ndarray, cutoff=1.0, order=3) -> np.ndarray:\n",
        "    if cutoff >= nyq:\n",
        "        return x\n",
        "    sos = butter(order, Wn=cutoff/nyq, btype='highpass', output='sos')\n",
        "    try:\n",
        "        return sosfiltfilt(sos, x)\n",
        "    except Exception:\n",
        "        return sosfiltfilt(sos, x)\n",
        "\n",
        "def robust_scale_per_series(x: np.ndarray) -> np.ndarray:\n",
        "    med = np.nanmedian(x)\n",
        "    iqr = np.nanpercentile(x, 75) - np.nanpercentile(x, 25)\n",
        "    if iqr == 0 or not np.isfinite(iqr):\n",
        "        return x - med\n",
        "    return (x - med) / iqr\n",
        "\n",
        "def zero_crossing_rate(x: np.ndarray) -> float:\n",
        "    x = np.nan_to_num(x, nan=0.0)\n",
        "    return float(((x[:-1] * x[1:]) < 0).mean())\n",
        "\n",
        "def line_length(x: np.ndarray) -> float:\n",
        "    return float(np.nanmean(np.abs(np.diff(x))))\n",
        "\n",
        "def hjorth_params(x: np.ndarray) -> Tuple[float,float,float]:\n",
        "    x = np.nan_to_num(x, nan=0.0)\n",
        "    var0 = np.var(x)\n",
        "    dx = np.diff(x)\n",
        "    var1 = np.var(dx) if len(dx)>0 else 0.0\n",
        "    ddx = np.diff(dx)\n",
        "    var2 = np.var(ddx) if len(ddx)>0 else 0.0\n",
        "    activity = var0\n",
        "    mobility = math.sqrt(var1/var0) if var0>0 else 0.0\n",
        "    complexity = math.sqrt((var2/var1)) / mobility if (var1>0 and mobility>0) else 0.0\n",
        "    return float(activity), float(mobility), float(complexity)\n",
        "\n",
        "def spectral_features(x: np.ndarray) -> Dict[str, float]:\n",
        "    # Welch PSD\n",
        "    x = np.nan_to_num(x, nan=0.0).astype(np.float32)\n",
        "    try:\n",
        "        f, Pxx = welch(x, fs=Fs, nperseg=min(len(x), 1024), noverlap=512, detrend='constant')\n",
        "    except Exception:\n",
        "        # Fallback: simple periodogram\n",
        "        f, Pxx = periodogram(x, fs=Fs, scaling='density')\n",
        "    Pxx = np.maximum(Pxx, 1e-20)\n",
        "    total_power = np.trapz(Pxx, f)\n",
        "    # bandpowers\n",
        "    feats = {}\n",
        "    for i,(lo,hi) in enumerate(BANDS):\n",
        "        mask = (f>=lo) & (f<hi)\n",
        "        bp = np.trapz(Pxx[mask], f[mask]) if mask.any() else 0.0\n",
        "        feats[f'bandpower_{i}_{lo:.1f}_{hi:.1f}'] = float(bp)\n",
        "    # ratios high/low\n",
        "    if len(BANDS)>=2:\n",
        "        low = feats.get('bandpower_0_0.5_2.0', 0.0)\n",
        "        high = feats.get(f'bandpower_{len(BANDS)-1}_{BANDS[-1][0]:.1f}_{BANDS[-1][1]:.1f}', 0.0)\n",
        "        feats['bp_ratio_high_low'] = float(high/(low+1e-9))\n",
        "    # centroid\n",
        "    centroid = float(np.sum(f*Pxx)/np.sum(Pxx)) if total_power>0 else 0.0\n",
        "    feats['spec_centroid'] = centroid\n",
        "    # roll-off 95%\n",
        "    cumsum = np.cumsum(Pxx) / np.sum(Pxx)\n",
        "    idx95 = np.searchsorted(cumsum, 0.95)\n",
        "    feats['spec_rolloff95'] = float(f[min(idx95, len(f)-1)]) if len(f)>0 else 0.0\n",
        "    # flatness (geometric/arith mean)\n",
        "    geo = float(np.exp(np.mean(np.log(Pxx))))\n",
        "    ari = float(np.mean(Pxx))\n",
        "    feats['spec_flatness'] = float(geo/(ari+1e-12))\n",
        "    # entropy\n",
        "    p = Pxx/np.sum(Pxx)\n",
        "    feats['spec_entropy'] = float(-np.sum(p*np.log(p+1e-12)))\n",
        "    # dominant peak\n",
        "    idx = int(np.argmax(Pxx)) if len(Pxx)>0 else 0\n",
        "    feats['peak_freq'] = float(f[idx]) if len(f)>0 else 0.0\n",
        "    feats['peak_amp'] = float(Pxx[idx]) if len(Pxx)>0 else 0.0\n",
        "    return feats\n",
        "\n",
        "def chunk_features(x: np.ndarray, n_chunks: int = 10) -> Dict[str, float]:\n",
        "    n = len(x)\n",
        "    feats = {}\n",
        "    if n_chunks <= 1 or n < n_chunks:\n",
        "        return feats\n",
        "    idxs = np.linspace(0, n, n_chunks+1, dtype=int)\n",
        "    means, stds, rmss = [], [], []\n",
        "    for i in range(n_chunks):\n",
        "        seg = x[idxs[i]:idxs[i+1]]\n",
        "        if len(seg)==0:\n",
        "            m=0; s=0; r=0\n",
        "        else:\n",
        "            m = float(np.nanmean(seg))\n",
        "            s = float(np.nanstd(seg))\n",
        "            r = float(np.sqrt(np.nanmean(seg**2)))\n",
        "        means.append(m); stds.append(s); rmss.append(r)\n",
        "    means = np.array(means); stds = np.array(stds); rmss = np.array(rmss)\n",
        "    # summarize across chunks\n",
        "    for arr, name in [(means,'mean'), (stds,'std'), (rmss,'rms')]:\n",
        "        feats[f'chunks_{name}_mean'] = float(np.nanmean(arr))\n",
        "        feats[f'chunks_{name}_std'] = float(np.nanstd(arr))\n",
        "        feats[f'chunks_{name}_min'] = float(np.nanmin(arr))\n",
        "        feats[f'chunks_{name}_max'] = float(np.nanmax(arr))\n",
        "        # slope via linear fit\n",
        "        x_idx = np.arange(len(arr))\n",
        "        try:\n",
        "            slope = float(np.polyfit(x_idx, arr, 1)[0])\n",
        "        except Exception:\n",
        "            slope = 0.0\n",
        "        feats[f'chunks_{name}_slope'] = slope\n",
        "        feats[f'chunks_{name}_last_first_delta'] = float(arr[-1] - arr[0])\n",
        "    # energy fraction in last 3 chunks\n",
        "    energy = rmss**2\n",
        "    denom = float(energy.sum()) + 1e-9\n",
        "    feats['chunks_energy_last3_frac'] = float(energy[-3:].sum()/denom) if len(energy)>=3 else 0.0\n",
        "    return feats\n",
        "\n",
        "def global_stats(x: np.ndarray) -> Dict[str, float]:\n",
        "    x = np.array(x, dtype=np.float32)\n",
        "    feats = {}\n",
        "    qs = [1,5,25,50,75,95,99]\n",
        "    qv = np.nanpercentile(x, qs)\n",
        "    feats.update({f'q{q}': float(v) for q,v in zip(qs, qv)})\n",
        "    feats['mean'] = float(np.nanmean(x))\n",
        "    feats['std'] = float(np.nanstd(x))\n",
        "    feats['min'] = float(np.nanmin(x))\n",
        "    feats['max'] = float(np.nanmax(x))\n",
        "    feats['iqr'] = float(np.nanpercentile(x,75) - np.nanpercentile(x,25))\n",
        "    feats['rms'] = float(np.sqrt(np.nanmean(x**2)))\n",
        "    feats['mad'] = float(np.nanmedian(np.abs(x - np.nanmedian(x))))\n",
        "    with np.errstate(all='ignore'):\n",
        "        feats['skew'] = float(pd.Series(x).skew())\n",
        "        feats['kurt'] = float(pd.Series(x).kurt())\n",
        "    feats['zcr'] = zero_crossing_rate(x)\n",
        "    feats['line_len'] = line_length(x)\n",
        "    a,m,c = hjorth_params(x)\n",
        "    feats['hj_activity'] = a; feats['hj_mobility'] = m; feats['hj_complexity'] = c\n",
        "    feats['nan_frac'] = float(np.mean(~np.isfinite(x)))\n",
        "    return feats\n",
        "\n",
        "def compute_features_for_file(fp: str, segment_id: int) -> Dict[str, float]:\n",
        "    t0 = time.time()\n",
        "    df = pd.read_csv(fp, dtype=np.float32)\n",
        "    cols = df.columns.tolist()\n",
        "    # Handle NaNs\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    df = df.fillna(0.0)\n",
        "    # Cross-channel correlations\n",
        "    corr_feats = {}\n",
        "    try:\n",
        "        corr = df.corr().values\n",
        "        # upper triangle without diag\n",
        "        idx = np.triu_indices_from(corr, k=1)\n",
        "        corr_vals = corr[idx]\n",
        "        corr_feats['xcorr_mean'] = float(np.nanmean(corr_vals))\n",
        "        corr_feats['xcorr_std'] = float(np.nanstd(corr_vals))\n",
        "        corr_feats['xcorr_max'] = float(np.nanmax(corr_vals))\n",
        "        corr_feats['xcorr_min'] = float(np.nanmin(corr_vals))\n",
        "    except Exception:\n",
        "        corr_feats['xcorr_mean']=corr_feats['xcorr_std']=corr_feats['xcorr_max']=corr_feats['xcorr_min']=0.0\n",
        "    feats = {**{ 'segment_id': segment_id }, **corr_feats}\n",
        "    # Per-channel features\n",
        "    for col in cols:\n",
        "        x = df[col].values.astype(np.float32)\n",
        "        x = linear_detrend(x)\n",
        "        x = butter_highpass_sos(x, cutoff=1.0, order=3)\n",
        "        # optional robust scale per series (keep unitless features stable)\n",
        "        # x = robust_scale_per_series(x)\n",
        "        g = global_stats(x)\n",
        "        ch = f'{col}'\n",
        "        for k,v in g.items(): feats[f'{ch}_{k}'] = v\n",
        "        spec = spectral_features(x)\n",
        "        for k,v in spec.items(): feats[f'{ch}_{k}'] = v\n",
        "        chf = chunk_features(x, n_chunks=10)\n",
        "        for k,v in chf.items(): feats[f'{ch}_{k}'] = v\n",
        "        if USE_ACF:\n",
        "            acf = autocorr_features(x, max_sec=5.0)\n",
        "            for k,v in acf.items(): feats[f'{ch}_{k}'] = v\n",
        "    # timing\n",
        "    feats['_proc_seconds'] = float(time.time()-t0)\n",
        "    return feats\n",
        "\n",
        "def build_features_for_ids(ids: List[int], split: str, n_jobs: int = 4, max_items: int = None) -> pd.DataFrame:\n",
        "    if max_items is not None:\n",
        "        ids = ids[:max_items]\n",
        "    base_dir = TRAIN_DIR if split=='train' else TEST_DIR\n",
        "    fps = [str(base_dir / f'{i}.csv') for i in ids]\n",
        "    assert all([Path(fp).exists() for fp in fps]), 'Some files missing'\n",
        "    print(f'[{split}] Extracting features for {len(ids)} files with {n_jobs} jobs...')\n",
        "    t0 = time.time()\n",
        "    results = Parallel(n_jobs=n_jobs, prefer='threads')(delayed(compute_features_for_file)(fp, seg_id) for fp, seg_id in zip(fps, ids))\n",
        "    df = pd.DataFrame(results)\n",
        "    print(f'[{split}] Done. Shape={df.shape}. Elapsed {time.time()-t0:.1f}s')\n",
        "    return df\n",
        "\n",
        "# Smoke build: 400 train + all test later\n",
        "train_ids = train_meta['segment_id'].tolist()\n",
        "test_ids = pd.read_csv('sample_submission.csv')['segment_id'].tolist()\n",
        "\n",
        "SMOKE_N = 400\n",
        "train_feats_smoke = build_features_for_ids(train_ids, 'train', n_jobs=6, max_items=SMOKE_N)\n",
        "train_feats_smoke.to_parquet('train_features_smoke.parquet', index=False)\n",
        "print('Saved train_features_smoke.parquet')\n",
        "\n",
        "test_feats_smoke = build_features_for_ids(test_ids, 'test', n_jobs=6, max_items=200)  # limit for speed; full later\n",
        "test_feats_smoke.to_parquet('test_features_smoke.parquet', index=False)\n",
        "print('Saved test_features_smoke.parquet')\n",
        "\n",
        "gc.collect(); print('SMOKE FEATURE EXTRACTION COMPLETE')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Fs ~ 100 Hz from n_rows= 60001\nUsing bands: [(0.5, 2), (2, 4), (4, 8), (8, 16), (16, 32), (32, 48)]\n[train] Extracting features for 400 files with 6 jobs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] Done. Shape=(400, 566). Elapsed 504.1s\nSaved train_features_smoke.parquet\n[test] Extracting features for 200 files with 6 jobs...\n"
          ]
        }
      ]
    },
    {
      "id": "3af1e40e-c216-4abc-97bb-75fd8dd7fcc6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Modeling: XGBoost GPU baseline on smoke features with KFold (temporary) using xgb.train\n",
        "import time, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import xgboost as xgb\n",
        "from pathlib import Path\n",
        "\n",
        "t0 = time.time()\n",
        "train_feats_path = 'train_features_smoke.parquet'\n",
        "assert Path(train_feats_path).exists(), 'train_features_smoke.parquet not found. Build features first.'\n",
        "train_feats = pd.read_parquet(train_feats_path)\n",
        "print('Loaded train features:', train_feats.shape)\n",
        "\n",
        "# Merge target\n",
        "train_df = train_feats.merge(train_meta[['segment_id','time_to_eruption']], on='segment_id', how='left')\n",
        "print('Merged with target:', train_df.shape, 'missing targets:', train_df['time_to_eruption'].isna().sum())\n",
        "\n",
        "# Features/target\n",
        "drop_cols = ['segment_id', 'time_to_eruption']\n",
        "feat_cols = [c for c in train_df.columns if c not in drop_cols]\n",
        "X = train_df[feat_cols].astype(np.float32).values\n",
        "y = train_df['time_to_eruption'].values.astype(np.float32)\n",
        "print('Feature matrix:', X.shape, 'Target shape:', y.shape)\n",
        "\n",
        "# KFold (temporary until we infer proper groups)\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "oof = np.zeros(len(train_df), dtype=np.float32)\n",
        "models = []\n",
        "\n",
        "params = {\n",
        "    'tree_method': 'hist',\n",
        "    'device': 'cuda',\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.04,\n",
        "    'subsample': 0.75,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'min_child_weight': 9,\n",
        "    'reg_alpha': 0.5,\n",
        "    'reg_lambda': 6.0,\n",
        "    'objective': 'reg:absoluteerror',\n",
        "    'eval_metric': 'mae',\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "num_boost_round = 4000\n",
        "esr = 250\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "    t_fold = time.time()\n",
        "    print(f'Fold {fold+1}/{n_splits} - train {len(trn_idx)} val {len(val_idx)}')\n",
        "    X_tr, y_tr = X[trn_idx], y[trn_idx]\n",
        "    X_va, y_va = X[val_idx], y[val_idx]\n",
        "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
        "    dvalid = xgb.DMatrix(X_va, label=y_va)\n",
        "    evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
        "    booster = xgb.train(\n",
        "        params=params,\n",
        "        dtrain=dtrain,\n",
        "        num_boost_round=num_boost_round,\n",
        "        evals=evals,\n",
        "        early_stopping_rounds=esr,\n",
        "        verbose_eval=200\n",
        "    )\n",
        "    # Use best_iteration from early stopping; add +1 since iteration_range is exclusive of end\n",
        "    best_iter = getattr(booster, 'best_iteration', None)\n",
        "    if best_iter is not None:\n",
        "        preds = booster.predict(dvalid, iteration_range=(0, best_iter + 1))\n",
        "    else:\n",
        "        preds = booster.predict(dvalid)\n",
        "    oof[val_idx] = preds.astype(np.float32)\n",
        "    mae = mean_absolute_error(y_va, preds)\n",
        "    print(f'Fold {fold+1} MAE: {mae:,.0f} | best_iter={best_iter} | elapsed {time.time()-t_fold:.1f}s')\n",
        "    models.append(booster)\n",
        "    gc.collect()\n",
        "\n",
        "oof_mae = mean_absolute_error(y, oof)\n",
        "print(f'OOF MAE (KFold, smoke): {oof_mae:,.0f}')\n",
        "print('Total modeling time: %.1fs' % (time.time()-t0))\n",
        "\n",
        "# Save OOF for tracking\n",
        "pd.DataFrame({'segment_id': train_df['segment_id'], 'oof': oof, 'y': y}).to_csv('oof_smoke.csv', index=False)\n",
        "print('Saved oof_smoke.csv')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded train features: (400, 566)\nMerged with target: (400, 567) missing targets: 0\nFeature matrix: (400, 565) Target shape: (400,)\nFold 1/5 - train 320 val 80\n[0]\ttrain-mae:10771263.39687\tvalid-mae:12057508.26250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mae:2241880.19062\tvalid-mae:8217171.87812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mae:1430950.44687\tvalid-mae:8027088.26562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mae:1134374.70391\tvalid-mae:7896189.62812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mae:1016807.18809\tvalid-mae:7878095.45625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mae:950226.04766\tvalid-mae:7877420.52187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain-mae:913312.54004\tvalid-mae:7853254.15937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\ttrain-mae:865172.35469\tvalid-mae:7850800.11562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\ttrain-mae:833669.07871\tvalid-mae:7833513.45937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain-mae:799967.25820\tvalid-mae:7825075.12812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\ttrain-mae:778541.44883\tvalid-mae:7820125.47187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\ttrain-mae:751737.51992\tvalid-mae:7808549.63125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\ttrain-mae:717185.39482\tvalid-mae:7779025.78750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2600]\ttrain-mae:698471.64658\tvalid-mae:7773014.90625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2800]\ttrain-mae:677099.66621\tvalid-mae:7761023.86250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\ttrain-mae:661154.48301\tvalid-mae:7757002.35938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3200]\ttrain-mae:646850.59736\tvalid-mae:7763307.18125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3316]\ttrain-mae:637879.51064\tvalid-mae:7759282.78125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 MAE: 7,752,507 | best_iter=3067 | elapsed 20.9s\nFold 2/5 - train 320 val 80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mae:10884712.80000\tvalid-mae:11557859.46250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mae:1972414.13867\tvalid-mae:7016213.89687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mae:1280609.04238\tvalid-mae:7034757.12500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mae:1056318.05273\tvalid-mae:6975958.63125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mae:974493.12852\tvalid-mae:6976011.10313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mae:891084.98662\tvalid-mae:6967420.00625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1089]\ttrain-mae:872635.85752\tvalid-mae:6967847.63750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 MAE: 6,966,051 | best_iter=840 | elapsed 7.0s\nFold 3/5 - train 320 val 80\n[0]\ttrain-mae:10947865.20000\tvalid-mae:11231243.97500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mae:1872362.40488\tvalid-mae:6685098.28750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mae:1175171.73379\tvalid-mae:6506978.30938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mae:938047.83457\tvalid-mae:6474971.59687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mae:853043.20068\tvalid-mae:6492019.63438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[848]\ttrain-mae:831046.90918\tvalid-mae:6479554.95000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 MAE: 6,474,742 | best_iter=599 | elapsed 5.3s\nFold 4/5 - train 320 val 80\n[0]\ttrain-mae:10933355.78750\tvalid-mae:11484721.80000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mae:1917420.82187\tvalid-mae:8280821.61875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mae:1256486.66992\tvalid-mae:8096406.41875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mae:1014684.13574\tvalid-mae:8079449.51250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[707]\ttrain-mae:944154.42754\tvalid-mae:8080435.73125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 MAE: 8,072,845 | best_iter=457 | elapsed 4.4s\nFold 5/5 - train 320 val 80\n[0]\ttrain-mae:11146637.83437\tvalid-mae:10506216.40000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mae:2016210.62500\tvalid-mae:5453072.30000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mae:1337177.23135\tvalid-mae:5317999.45000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mae:1091180.05674\tvalid-mae:5290912.74375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mae:1002907.26348\tvalid-mae:5281947.68750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mae:921244.12676\tvalid-mae:5268951.65313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain-mae:815355.05488\tvalid-mae:5242442.84375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\ttrain-mae:703308.37461\tvalid-mae:5270824.15313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1426]\ttrain-mae:695759.46719\tvalid-mae:5271523.86250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 MAE: 5,233,041 | best_iter=1177 | elapsed 9.1s\nOOF MAE (KFold, smoke): 6,899,838\nTotal modeling time: 47.2s\nSaved oof_smoke.csv\n"
          ]
        }
      ]
    },
    {
      "id": "df4442b3-c414-49c0-8c0a-7c867bb203c2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# GroupKFold CV sanity-check on smoke features (no leakage)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import xgboost as xgb\n",
        "from pathlib import Path\n",
        "import time, gc\n",
        "\n",
        "t0 = time.time()\n",
        "# Load smoke features and targets\n",
        "train_feats_path = 'train_features_smoke.parquet'\n",
        "assert Path(train_feats_path).exists(), 'Missing train_features_smoke.parquet'\n",
        "train_feats = pd.read_parquet(train_feats_path)\n",
        "df_meta = train_meta[['segment_id','time_to_eruption']].copy()\n",
        "\n",
        "# Build groups: global cumsum of increases in time_to_eruption when sorted by target\n",
        "tmp = df_meta.sort_values('time_to_eruption').reset_index(drop=True)\n",
        "tmp['group'] = (tmp['time_to_eruption'].diff().fillna(1) > 0).cumsum().astype(int)\n",
        "groups_map = tmp.set_index('segment_id')['group']\n",
        "\n",
        "# Merge features with target and groups\n",
        "df = train_feats.merge(df_meta, on='segment_id', how='left')\n",
        "df['group'] = df['segment_id'].map(groups_map)\n",
        "assert df['group'].notna().all(), 'Grouping failed for some rows'\n",
        "print('Data shape:', df.shape, '| unique groups:', df['group'].nunique())\n",
        "\n",
        "drop_cols = ['segment_id','time_to_eruption','group']\n",
        "feat_cols = [c for c in df.columns if c not in drop_cols]\n",
        "X = df[feat_cols].astype(np.float32).values\n",
        "y = df['time_to_eruption'].astype(np.float32).values\n",
        "groups = df['group'].values.astype(int)\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "oof = np.zeros(len(df), dtype=np.float32)\n",
        "models = []\n",
        "\n",
        "params = {\n",
        "    'tree_method': 'gpu_hist',\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.04,\n",
        "    'subsample': 0.75,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'min_child_weight': 9,\n",
        "    'reg_alpha': 0.5,\n",
        "    'reg_lambda': 6.0,\n",
        "    'objective': 'reg:absoluteerror',\n",
        "    'eval_metric': 'mae',\n",
        "    'seed': 42\n",
        "}\n",
        "num_boost_round = 4000\n",
        "esr = 250\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups)):\n",
        "    t_fold = time.time()\n",
        "    print(f'[GroupKFold] Fold {fold+1}/5 | train {len(trn_idx)} val {len(val_idx)} | groups train {len(np.unique(groups[trn_idx]))} val {len(np.unique(groups[val_idx]))}')\n",
        "    dtrain = xgb.DMatrix(X[trn_idx], label=y[trn_idx])\n",
        "    dvalid = xgb.DMatrix(X[val_idx], label=y[val_idx])\n",
        "    booster = xgb.train(\n",
        "        params=params,\n",
        "        dtrain=dtrain,\n",
        "        num_boost_round=num_boost_round,\n",
        "        evals=[(dtrain,'train'),(dvalid,'valid')],\n",
        "        early_stopping_rounds=esr,\n",
        "        verbose_eval=200\n",
        "    )\n",
        "    best_iter = getattr(booster, 'best_iteration', None)\n",
        "    preds = booster.predict(dvalid, iteration_range=(0, best_iter+1)) if best_iter is not None else booster.predict(dvalid)\n",
        "    oof[val_idx] = preds.astype(np.float32)\n",
        "    mae = mean_absolute_error(y[val_idx], preds)\n",
        "    print(f'Fold {fold+1} MAE: {mae:,.0f} | best_iter={best_iter} | elapsed {time.time()-t_fold:.1f}s')\n",
        "    models.append(booster)\n",
        "    gc.collect()\n",
        "\n",
        "oof_mae = mean_absolute_error(y, oof)\n",
        "print(f'OOF MAE (GroupKFold, smoke): {oof_mae:,.0f}')\n",
        "print('Elapsed: %.1fs' % (time.time()-t0))\n",
        "pd.DataFrame({'segment_id': df['segment_id'], 'group': groups, 'oof_groupkfold': oof, 'y': y}).to_csv('oof_smoke_groupkfold.csv', index=False)\n",
        "print('Saved oof_smoke_groupkfold.csv')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (400, 568) | unique groups: 400\n[GroupKFold] Fold 1/5 | train 320 val 80 | groups train 320 val 80\n[0]\ttrain-mae:10972174.19063\tvalid-mae:11153262.85000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mae:2027805.37969\tvalid-mae:6858036.78750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mae:1262405.79746\tvalid-mae:6732352.31250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mae:1005253.85283\tvalid-mae:6657007.12812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mae:898596.05713\tvalid-mae:6618894.47500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mae:823482.88311\tvalid-mae:6594702.48125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain-mae:779602.87744\tvalid-mae:6571923.55312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\ttrain-mae:746336.09297\tvalid-mae:6567170.58750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\ttrain-mae:714133.68193\tvalid-mae:6547479.67500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain-mae:694226.50791\tvalid-mae:6542255.30625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\ttrain-mae:676580.30791\tvalid-mae:6537485.02813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\ttrain-mae:650567.82021\tvalid-mae:6538591.81250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\ttrain-mae:605071.04380\tvalid-mae:6525320.77813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2600]\ttrain-mae:578619.60764\tvalid-mae:6521650.82812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2800]\ttrain-mae:561023.33330\tvalid-mae:6514998.05625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3000]\ttrain-mae:549055.14314\tvalid-mae:6514254.05625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3200]\ttrain-mae:538478.40867\tvalid-mae:6511817.54063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3400]\ttrain-mae:523684.65679\tvalid-mae:6508668.10625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3600]\ttrain-mae:509224.76643\tvalid-mae:6505688.95937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3736]\ttrain-mae:504328.09624\tvalid-mae:6506478.82188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 MAE: 6,503,348 | best_iter=3486 | elapsed 23.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GroupKFold] Fold 2/5 | train 320 val 80 | groups train 320 val 80\n[0]\ttrain-mae:11083110.73438\tvalid-mae:10695187.40000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mae:2053779.98379\tvalid-mae:6419084.64375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mae:1302076.62607\tvalid-mae:6389420.90781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[554]\ttrain-mae:1096443.33652\tvalid-mae:6367755.40156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 MAE: 6,357,028 | best_iter=304 | elapsed 3.5s\n[GroupKFold] Fold 3/5 | train 320 val 80 | groups train 320 val 80\n[0]\ttrain-mae:10921313.98125\tvalid-mae:11129080.80000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mae:2042984.30703\tvalid-mae:6971588.17500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mae:1370740.44961\tvalid-mae:6861166.20937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mae:1160274.64297\tvalid-mae:6846004.06875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mae:1082300.02471\tvalid-mae:6825338.11250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mae:982310.23984\tvalid-mae:6783100.63438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain-mae:924562.00508\tvalid-mae:6766355.49688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\ttrain-mae:868166.41133\tvalid-mae:6731829.68438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\ttrain-mae:827703.55332\tvalid-mae:6724811.99375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1800]\ttrain-mae:778218.85840\tvalid-mae:6717404.62188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2000]\ttrain-mae:744190.74238\tvalid-mae:6704594.52187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2200]\ttrain-mae:717485.32070\tvalid-mae:6693859.89687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2400]\ttrain-mae:692441.93145\tvalid-mae:6694691.33125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2599]\ttrain-mae:672705.53984\tvalid-mae:6697231.53437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 MAE: 6,687,608 | best_iter=2350 | elapsed 16.5s\n[GroupKFold] Fold 4/5 | train 320 val 80 | groups train 320 val 80\n[0]\ttrain-mae:10889311.74687\tvalid-mae:11343677.45000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mae:1980544.47930\tvalid-mae:7788245.80000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mae:1287210.65400\tvalid-mae:7638119.60703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mae:1106964.25186\tvalid-mae:7618815.18984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mae:1005556.73984\tvalid-mae:7600965.63438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain-mae:940986.91777\tvalid-mae:7584368.21641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1200]\ttrain-mae:887864.21230\tvalid-mae:7560587.83906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1400]\ttrain-mae:839791.57227\tvalid-mae:7553723.66953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1600]\ttrain-mae:804906.99717\tvalid-mae:7562101.73906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1733]\ttrain-mae:783116.85225\tvalid-mae:7560125.98203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 MAE: 7,550,699 | best_iter=1483 | elapsed 11.0s\n[GroupKFold] Fold 5/5 | train 320 val 80 | groups train 320 val 80\n[0]\ttrain-mae:11028167.53750\tvalid-mae:11141533.33750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mae:2080156.45000\tvalid-mae:7714866.28906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400]\ttrain-mae:1294914.45137\tvalid-mae:7414221.20312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[600]\ttrain-mae:1034762.07686\tvalid-mae:7379992.76562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[800]\ttrain-mae:925981.86270\tvalid-mae:7353077.61250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[990]\ttrain-mae:884655.40742\tvalid-mae:7349995.57812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 MAE: 7,348,069 | best_iter=741 | elapsed 6.2s\nOOF MAE (GroupKFold, smoke): 6,889,350\nElapsed: 61.5s\nSaved oof_smoke_groupkfold.csv\n"
          ]
        }
      ]
    },
    {
      "id": "a076e458-b61d-4bcf-a829-f80ec6dc2d55",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Full feature extraction (v1) for all train/test with batching + caching\n",
        "import os, time, gc, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Limit thread oversubscription for SciPy/NumPy/BLAS\n",
        "os.environ['OMP_NUM_THREADS'] = os.environ.get('OMP_NUM_THREADS', '1')\n",
        "os.environ['MKL_NUM_THREADS'] = os.environ.get('MKL_NUM_THREADS', '1')\n",
        "os.environ['NUMEXPR_NUM_THREADS'] = os.environ.get('NUMEXPR_NUM_THREADS', '1')\n",
        "\n",
        "train_ids_full = train_meta['segment_id'].tolist()\n",
        "test_ids_full = pd.read_csv('sample_submission.csv')['segment_id'].tolist()\n",
        "\n",
        "train_feat_path = Path('train_features_v1.parquet')\n",
        "test_feat_path = Path('test_features_v1.parquet')\n",
        "\n",
        "parts_dir_train = Path('train_features_v1_parts'); parts_dir_train.mkdir(exist_ok=True)\n",
        "parts_dir_test = Path('test_features_v1_parts'); parts_dir_test.mkdir(exist_ok=True)\n",
        "\n",
        "def extract_in_batches(ids, split: str, out_parts_dir: Path, final_path: Path, batch_size: int = 400, n_jobs: int = 8):\n",
        "    t0 = time.time()\n",
        "    n = len(ids)\n",
        "    n_batches = math.ceil(n / batch_size)\n",
        "    print(f'[{split}] Total {n} ids \u2192 {n_batches} batches of size {batch_size}')\n",
        "    completed_parts = []\n",
        "    for b in range(n_batches):\n",
        "        start = b * batch_size\n",
        "        end = min(n, (b+1) * batch_size)\n",
        "        part_path = out_parts_dir / f'part_{b:03d}.parquet'\n",
        "        if part_path.exists():\n",
        "            print(f'[{split}] Skip batch {b+1}/{n_batches} ({start}:{end}) existing {part_path.name}')\n",
        "            completed_parts.append(part_path)\n",
        "            continue\n",
        "        batch_ids = ids[start:end]\n",
        "        print(f'[{split}] Batch {b+1}/{n_batches} ({start}:{end}) extracting...')\n",
        "        t_batch = time.time()\n",
        "        df_part = build_features_for_ids(batch_ids, split=split, n_jobs=n_jobs, max_items=None)\n",
        "        df_part.to_parquet(part_path, index=False)\n",
        "        completed_parts.append(part_path)\n",
        "        print(f'[{split}] Saved {part_path.name} | shape={df_part.shape} | elapsed {time.time()-t_batch:.1f}s | total {time.time()-t0:.1f}s')\n",
        "        del df_part\n",
        "        gc.collect()\n",
        "    # Concatenate parts into final parquet\n",
        "    print(f'[{split}] Concatenating {len(completed_parts)} parts into {final_path} ...')\n",
        "    dfs = [pd.read_parquet(p) for p in sorted(completed_parts)]\n",
        "    df_all = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "    df_all.to_parquet(final_path, index=False)\n",
        "    print(f'[{split}] Saved {final_path} | shape={df_all.shape} | total elapsed {time.time()-t0:.1f}s')\n",
        "    del dfs, df_all\n",
        "    gc.collect()\n",
        "\n",
        "start_all = time.time()\n",
        "# 1) Train features first (to enable modeling ASAP)\n",
        "if not train_feat_path.exists():\n",
        "    print('Building full train features v1 in batches ...')\n",
        "    extract_in_batches(train_ids_full, split='train', out_parts_dir=parts_dir_train, final_path=train_feat_path, batch_size=400, n_jobs=8)\n",
        "else:\n",
        "    print('Full train features already exist:', train_feat_path)\n",
        "\n",
        "# 2) Test features next (also batched)\n",
        "if not test_feat_path.exists():\n",
        "    print('Building full test features v1 in batches ...')\n",
        "    extract_in_batches(test_ids_full, split='test', out_parts_dir=parts_dir_test, final_path=test_feat_path, batch_size=222, n_jobs=8)\n",
        "else:\n",
        "    print('Full test features already exist:', test_feat_path)\n",
        "\n",
        "print('Full feature extraction v1 (batched) complete. Elapsed: %.1f s' % (time.time()-start_all))\n",
        "gc.collect()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building full train features v1 in batches ...\n[train] Total 3987 ids \u2192 10 batches of size 400\n[train] Batch 1/10 (0:400) extracting...\n[train] Extracting features for 400 files with 8 jobs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] Done. Shape=(400, 566). Elapsed 496.5s\n[train] Saved part_000.parquet | shape=(400, 566) | elapsed 496.6s | total 496.6s\n[train] Batch 2/10 (400:800) extracting...\n[train] Extracting features for 400 files with 8 jobs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] Done. Shape=(400, 566). Elapsed 496.2s\n[train] Saved part_001.parquet | shape=(400, 566) | elapsed 496.3s | total 993.0s\n[train] Batch 3/10 (800:1200) extracting...\n[train] Extracting features for 400 files with 8 jobs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] Done. Shape=(400, 566). Elapsed 494.7s\n[train] Saved part_002.parquet | shape=(400, 566) | elapsed 494.8s | total 1487.9s\n[train] Batch 4/10 (1200:1600) extracting...\n[train] Extracting features for 400 files with 8 jobs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] Done. Shape=(400, 566). Elapsed 493.4s\n[train] Saved part_003.parquet | shape=(400, 566) | elapsed 493.5s | total 1981.5s\n[train] Batch 5/10 (1600:2000) extracting...\n[train] Extracting features for 400 files with 8 jobs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] Done. Shape=(400, 566). Elapsed 499.2s\n[train] Saved part_004.parquet | shape=(400, 566) | elapsed 499.3s | total 2480.9s\n[train] Batch 6/10 (2000:2400) extracting...\n[train] Extracting features for 400 files with 8 jobs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] Done. Shape=(400, 566). Elapsed 499.7s\n[train] Saved part_005.parquet | shape=(400, 566) | elapsed 499.8s | total 2980.8s\n[train] Batch 7/10 (2400:2800) extracting...\n[train] Extracting features for 400 files with 8 jobs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] Done. Shape=(400, 566). Elapsed 494.0s\n[train] Saved part_006.parquet | shape=(400, 566) | elapsed 494.1s | total 3475.0s\n[train] Batch 8/10 (2800:3200) extracting...\n[train] Extracting features for 400 files with 8 jobs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] Done. Shape=(400, 566). Elapsed 497.0s\n[train] Saved part_007.parquet | shape=(400, 566) | elapsed 497.1s | total 3972.2s\n[train] Batch 9/10 (3200:3600) extracting...\n[train] Extracting features for 400 files with 8 jobs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] Done. Shape=(400, 566). Elapsed 490.8s\n[train] Saved part_008.parquet | shape=(400, 566) | elapsed 490.9s | total 4463.2s\n[train] Batch 10/10 (3600:3987) extracting...\n[train] Extracting features for 387 files with 8 jobs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] Done. Shape=(387, 566). Elapsed 476.5s\n[train] Saved part_009.parquet | shape=(387, 566) | elapsed 476.6s | total 4939.8s\n[train] Concatenating 10 parts into train_features_v1.parquet ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] Saved train_features_v1.parquet | shape=(3987, 566) | total elapsed 4940.5s\nBuilding full test features v1 in batches ...\n[test] Total 444 ids \u2192 2 batches of size 222\n[test] Batch 1/2 (0:222) extracting...\n[test] Extracting features for 222 files with 8 jobs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test] Done. Shape=(222, 566). Elapsed 271.7s\n[test] Saved part_000.parquet | shape=(222, 566) | elapsed 271.8s | total 271.8s\n[test] Batch 2/2 (222:444) extracting...\n[test] Extracting features for 222 files with 8 jobs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test] Done. Shape=(222, 566). Elapsed 273.4s\n[test] Saved part_001.parquet | shape=(222, 566) | elapsed 273.5s | total 545.3s\n[test] Concatenating 2 parts into test_features_v1.parquet ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test] Saved test_features_v1.parquet | shape=(444, 566) | total elapsed 545.6s\nFull feature extraction v1 (batched) complete. Elapsed: 5486.3 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "fd3c765b-6a95-4ba9-bf24-da71dd4c3a6e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Full training with GroupKFold on v1 features + test inference and submission\n",
        "import os, time, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import xgboost as xgb\n",
        "\n",
        "t0 = time.time()\n",
        "train_feat_path = Path('train_features_v1.parquet')\n",
        "test_feat_path = Path('test_features_v1.parquet')\n",
        "if not train_feat_path.exists():\n",
        "    print('train_features_v1.parquet not found yet. Run the feature extraction cell first.');\n",
        "elif not test_feat_path.exists():\n",
        "    print('test_features_v1.parquet not found yet. Build test features, then re-run this cell.');\n",
        "else:\n",
        "    print('Loading full features...')\n",
        "    train_feats = pd.read_parquet(train_feat_path)\n",
        "    test_feats = pd.read_parquet(test_feat_path)\n",
        "    print('Train feats:', train_feats.shape, 'Test feats:', test_feats.shape)\n",
        "    # Merge target\n",
        "    df_meta = train_meta[['segment_id','time_to_eruption']].copy()\n",
        "    df = train_feats.merge(df_meta, on='segment_id', how='left')\n",
        "    # Build groups (global cumsum of increases when sorted by target)\n",
        "    tmp = df_meta.sort_values('time_to_eruption').reset_index(drop=True)\n",
        "    tmp['group'] = (tmp['time_to_eruption'].diff().fillna(1) > 0).cumsum().astype(int)\n",
        "    groups_map = tmp.set_index('segment_id')['group']\n",
        "    df['group'] = df['segment_id'].map(groups_map).astype(int)\n",
        "    print('Unique groups:', df['group'].nunique())\n",
        "\n",
        "    # Prepare matrices\n",
        "    drop_cols = ['segment_id','time_to_eruption','group']\n",
        "    feat_cols = [c for c in df.columns if c not in drop_cols]\n",
        "    X = df[feat_cols].astype(np.float32).values\n",
        "    y = df['time_to_eruption'].astype(np.float32).values\n",
        "    groups = df['group'].values.astype(int)\n",
        "    X_test = test_feats[feat_cols].astype(np.float32).values\n",
        "\n",
        "    # XGBoost params\n",
        "    params = {\n",
        "        'tree_method': 'gpu_hist',\n",
        "        'max_depth': 8,\n",
        "        'learning_rate': 0.04,\n",
        "        'subsample': 0.75,\n",
        "        'colsample_bytree': 0.7,\n",
        "        'min_child_weight': 9,\n",
        "        'reg_alpha': 0.5,\n",
        "        'reg_lambda': 6.0,\n",
        "        'objective': 'reg:absoluteerror',\n",
        "        'eval_metric': 'mae',\n",
        "        'seed': 42\n",
        "    }\n",
        "    num_boost_round = 5000\n",
        "    esr = 250\n",
        "\n",
        "    gkf = GroupKFold(n_splits=5)\n",
        "    oof = np.zeros(len(df), dtype=np.float32)\n",
        "    test_preds_folds = []\n",
        "\n",
        "    for fold, (trn_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups)):\n",
        "        t_fold = time.time()\n",
        "        print(f'[FULL] Fold {fold+1}/5 | train {len(trn_idx)} val {len(val_idx)}')\n",
        "        dtrain = xgb.DMatrix(X[trn_idx], label=y[trn_idx])\n",
        "        dvalid = xgb.DMatrix(X[val_idx], label=y[val_idx])\n",
        "        booster = xgb.train(\n",
        "            params=params,\n",
        "            dtrain=dtrain,\n",
        "            num_boost_round=num_boost_round,\n",
        "            evals=[(dtrain,'train'),(dvalid,'valid')],\n",
        "            early_stopping_rounds=esr,\n",
        "            verbose_eval=200\n",
        "        )\n",
        "        best_iter = getattr(booster, 'best_iteration', None)\n",
        "        preds_val = booster.predict(dvalid, iteration_range=(0, best_iter+1)) if best_iter is not None else booster.predict(dvalid)\n",
        "        oof[val_idx] = preds_val.astype(np.float32)\n",
        "        mae = mean_absolute_error(y[val_idx], preds_val)\n",
        "        print(f'Fold {fold+1} MAE: {mae:,.0f} | best_iter={best_iter} | elapsed {time.time()-t_fold:.1f}s')\n",
        "        # Test prediction for this fold\n",
        "        dtest = xgb.DMatrix(X_test)\n",
        "        preds_test = booster.predict(dtest, iteration_range=(0, best_iter+1)) if best_iter is not None else booster.predict(dtest)\n",
        "        test_preds_folds.append(preds_test.astype(np.float32))\n",
        "        gc.collect()\n",
        "\n",
        "    oof_mae = mean_absolute_error(y, oof)\n",
        "    print(f'OOF MAE (GroupKFold, full): {oof_mae:,.0f}')\n",
        "    pd.DataFrame({'segment_id': df['segment_id'], 'oof_full': oof, 'y': y}).to_csv('oof_full_xgb.csv', index=False)\n",
        "    # Average test preds across folds\n",
        "    test_pred = np.mean(np.vstack(test_preds_folds), axis=0)\n",
        "    # Clip predictions\n",
        "    y_max = float(df['time_to_eruption'].max())\n",
        "    test_pred = np.clip(test_pred, 0.0, y_max)\n",
        "\n",
        "    # Build submission\n",
        "    ss = pd.read_csv('sample_submission.csv')\n",
        "    sub = ss[['segment_id']].copy()\n",
        "    # Align predictions by row order of test_feats\n",
        "    # Ensure ordering matches\n",
        "    if 'segment_id' in test_feats.columns:\n",
        "        # Our X_test used test_feats[feat_cols], which excludes segment_id; predictions align with test_feats row order\n",
        "        sub = test_feats[['segment_id']].merge(sub, on='segment_id', how='right')\n",
        "        sub['time_to_eruption'] = test_pred\n",
        "        sub = sub[['segment_id','time_to_eruption']]\n",
        "    else:\n",
        "        # Fallback: assume same order as ss\n",
        "        sub['time_to_eruption'] = test_pred\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv')\n",
        "    print('Total training+inference elapsed: %.1fs' % (time.time()-t0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading full features...\nTrain feats: (3987, 566) Test feats: (444, 566)\nUnique groups: 3986\n[FULL] Fold 1/5 | train 3189 val 798\n[0]\ttrain-mae:11283527.24208\tvalid-mae:11296145.01504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200]\ttrain-mae:1624283.76376\tvalid-mae:3441694.37798\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}