{
  "cells": [
    {
      "id": "b21b020d-21f5-4eb6-9939-b80aa378e20b",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan to Medal: Detecting Insults in Social Commentary\n",
        "\n",
        "Objectives:\n",
        "- Build strong TF-IDF + linear model baseline with robust CV.\n",
        "- Optimize for ROC-AUC using stratified k-folds.\n",
        "- Iterate quickly; log progress; avoid long stalls.\n",
        "\n",
        "Workflow:\n",
        "1) Data load + quick EDA\n",
        "   - Inspect columns, sizes, nulls, label balance.\n",
        "   - Text field likely `Comment`; target `Insult`.\n",
        "\n",
        "2) Validation design\n",
        "   - StratifiedKFold(n_splits=5, shuffle=True, random_state=42).\n",
        "   - Use cross_val_predict or manual loop with consistent logging.\n",
        "\n",
        "3) Baseline model\n",
        "   - TfidfVectorizer:\n",
        "     - word ngrams (1,2) + char ngrams (3,5) union via FeatureUnion or hstack.\n",
        "     - min_df tuned (e.g., 2), max_features large (e.g., 200k total).\n",
        "     - sublinear_tf=True, lowercase=True, strip accents='unicode'.\n",
        "     - stop_words='english' for word-level; none for char-level.\n",
        "   - Model: LogisticRegression(saga, l2, C tuned), or LinearSVC with CalibratedClassifierCV, or SGDClassifier(log).\n",
        "   - Start with LogisticRegression (fast, probabilistic); fallback SGD if memory/time issues.\n",
        "\n",
        "4) Preprocessing\n",
        "   - Normalize URLs, mentions, numbers.\n",
        "   - Keep punctuation; char-ngrams are robust to misspellings/slurs.\n",
        "\n",
        "5) Hyperparameter tuning (targeted)\n",
        "   - Grid/Random small: C in [0.5, 1, 2, 4]; min_df in [1, 2, 3]; word n-grams (1,2)/(1,3); char (3,5)/(3,6).\n",
        "   - Use CV AUC to select.\n",
        "\n",
        "6) Ensembling\n",
        "   - Blend word-only and char-only logistic models (weighted average).\n",
        "   - Optionally add LinearSVC calibrated; simple average.\n",
        "\n",
        "7) Train on full data with best config; predict test; save submission.csv.\n",
        "\n",
        "8) Logging and efficiency\n",
        "   - Print fold indices, shapes, elapsed time per fold.\n",
        "   - Use sparse matrices; avoid dense conversions.\n",
        "\n",
        "Medal Strategy:\n",
        "- This competition historically favors char+word TF-IDF + linear models. Aim for \u22650.80 AUC quickly, then push with tuning/ensembling to \u22650.82.\n",
        "\n",
        "Open Questions for Experts:\n",
        "- Best char n-gram range and min_df for this dataset?\n",
        "- Preference between LogisticRegression vs Calibrated LinearSVC vs SGD for top AUC here?\n",
        "- Any specific pre-processing (e.g., aggressive lowercasing, de-emojify) that boosts AUC notably?\n",
        "- Is ensembling two/three linear models worth it on this dataset?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "d851a3f4-3546-49e2-94f8-a4b5ba7f5e1b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Imports, data load, and quick EDA\n",
        "import os, sys, time, math, gc, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "import re\n",
        "\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "train_path = 'train.csv'\n",
        "test_path = 'test.csv'\n",
        "assert os.path.exists(train_path) and os.path.exists(test_path), 'Missing train/test files'\n",
        "\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "\n",
        "print('Train shape:', train.shape)\n",
        "print('Test shape:', test.shape)\n",
        "print('Train columns:', train.columns.tolist())\n",
        "print('Test columns:', test.columns.tolist())\n",
        "\n",
        "# Infer key columns\n",
        "target_col = 'Insult' if 'Insult' in train.columns else [c for c in train.columns if c.lower()=='insult'][0]\n",
        "text_col_candidates = [c for c in train.columns if c.lower() in ['comment','comment_text','text']]\n",
        "assert len(text_col_candidates) >= 1, f'Could not find text column candidates in {train.columns.tolist()}'\n",
        "text_col = text_col_candidates[0]\n",
        "id_col_candidates = [c for c in train.columns if c.lower() in ['id','id_str','comment_id']]\n",
        "test_id_col_candidates = [c for c in test.columns if c.lower() in ['id','id_str','comment_id']]\n",
        "id_col = id_col_candidates[0] if len(id_col_candidates)>0 else 'id'\n",
        "if id_col not in train.columns:\n",
        "    # create a synthetic id for train to keep consistent shape; test must have an id or we create one\n",
        "    train[id_col] = np.arange(len(train))\n",
        "if id_col not in test.columns:\n",
        "    test[id_col] = np.arange(len(test))\n",
        "\n",
        "print('Using columns -> id:', id_col, '| text:', text_col, '| target:', target_col)\n",
        "print('Target distribution:')\n",
        "print(train[target_col].value_counts(normalize=True))\n",
        "print('Nulls in text:', train[text_col].isna().sum(), ' / ', len(train))\n",
        "\n",
        "# Basic preprocessing utilities\n",
        "url_re = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "user_re = re.compile(r'@\\w+')\n",
        "num_re = re.compile(r'\\b\\d+[\\d,\\.]*\\b')\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    s = url_re.sub(' <url> ', s)\n",
        "    s = user_re.sub(' <user> ', s)\n",
        "    s = num_re.sub(' <num> ', s)\n",
        "    return s\n",
        "\n",
        "# Create meta features (computed before lowercasing; stored separately) if needed later\n",
        "def compute_meta_features(series: pd.Series) -> pd.DataFrame:\n",
        "    texts = series.fillna('')\n",
        "    lens_char = texts.str.len().astype(np.float32)\n",
        "    lens_word = texts.str.split().map(len).astype(np.float32)\n",
        "    caps = texts.map(lambda x: sum(1 for ch in x if ch.isupper())).astype(np.float32)\n",
        "    frac_caps = (caps / (lens_char.clip(lower=1))).astype(np.float32)\n",
        "    excl = texts.str.count('!').astype(np.float32)\n",
        "    ques = texts.str.count('\\?').astype(np.float32)\n",
        "    return pd.DataFrame({'len_char': lens_char, 'len_word': lens_word, 'frac_caps': frac_caps, 'excl': excl, 'ques': ques})\n",
        "\n",
        "# Apply normalization to text fields\n",
        "train_text = train[text_col].fillna('').map(normalize_text)\n",
        "test_text = test[text_col].fillna('').map(normalize_text)\n",
        "\n",
        "y = train[target_col].values.astype(int)\n",
        "train_ids = train[id_col].values\n",
        "test_ids = test[id_col].values\n",
        "\n",
        "print('Sample normalized text:')\n",
        "print(train_text.head(3).tolist())\n",
        "print('Setup complete.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (3947, 3)\nTest shape: (2647, 2)\nTrain columns: ['Insult', 'Date', 'Comment']\nTest columns: ['Date', 'Comment']\nUsing columns -> id: id | text: Comment | target: Insult\nTarget distribution:\nInsult\n0    0.734229\n1    0.265771\nName: proportion, dtype: float64\nNulls in text: 0  /  3947\nSample normalized text:\n['\"You fuck your dad.\"', '\"i really don\\'t understand your point.\\\\xa0 It seems that you are mixing apples and oranges.\"', '\"A\\\\\\\\xc2\\\\\\\\xa0majority of Canadians can and has been wrong before now and will be again.\\\\\\\\n\\\\\\\\nUnless you\\'re supportive of the idea that nothing is full proof or perfect so you take your chances and if we should inadvertently kill your son or daughter then them\\'s the breaks and we can always regard you as collateral damage like in wartime - and sorry, but\\\\\\\\xc2\\\\\\\\xa0the cheques in the mail. \"']\nSetup complete.\n"
          ]
        }
      ]
    },
    {
      "id": "0c1a2d6d-b885-4828-a281-faa68038f239",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5-fold CV for word and char TF-IDF + LogisticRegression; blend OOF; train full and create submission\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "seed = 42\n",
        "n_splits = 5\n",
        "\n",
        "def build_word_pipeline():\n",
        "    return Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=2, lowercase=True, strip_accents='unicode', sublinear_tf=True, max_features=100_000)),\n",
        "        ('lr', LogisticRegression(solver='saga', penalty='l2', C=2.0, max_iter=2000, n_jobs=-1, random_state=seed))\n",
        "    ])\n",
        "\n",
        "def build_char_pipeline():\n",
        "    return Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(analyzer='char', ngram_range=(3,5), min_df=2, lowercase=True, strip_accents='unicode', sublinear_tf=True, max_features=200_000)),\n",
        "        ('lr', LogisticRegression(solver='saga', penalty='l2', C=2.0, max_iter=2000, n_jobs=-1, random_state=seed))\n",
        "    ])\n",
        "\n",
        "def get_oof_predictions(texts, y, pipeline_builder, name):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    oof = np.zeros(len(texts), dtype=np.float32)\n",
        "    fold_aucs = []\n",
        "    start_all = time.time()\n",
        "    for fold, (trn_idx, val_idx) in enumerate(skf.split(texts, y), 1):\n",
        "        t0 = time.time()\n",
        "        X_tr = texts.iloc[trn_idx]\n",
        "        y_tr = y[trn_idx]\n",
        "        X_va = texts.iloc[val_idx]\n",
        "        model = pipeline_builder()\n",
        "        model.fit(X_tr, y_tr)\n",
        "        preds = model.predict_proba(X_va)[:,1]\n",
        "        oof[val_idx] = preds\n",
        "        auc = roc_auc_score(y[val_idx], preds)\n",
        "        fold_aucs.append(auc)\n",
        "        print(f'[{name}] Fold {fold}/{n_splits} AUC: {auc:.5f} | trn {len(trn_idx)} va {len(val_idx)} | elapsed {time.time()-t0:.2f}s', flush=True)\n",
        "    total_auc = roc_auc_score(y, oof)\n",
        "    print(f'[{name}] OOF AUC: {total_auc:.5f} | mean {np.mean(fold_aucs):.5f} +- {np.std(fold_aucs):.5f} | total {time.time()-start_all:.2f}s', flush=True)\n",
        "    return oof, fold_aucs\n",
        "\n",
        "# Compute OOF for word and char\n",
        "oof_word, aucs_word = get_oof_predictions(train_text, y, build_word_pipeline, 'WORD')\n",
        "oof_char, aucs_char = get_oof_predictions(train_text, y, build_char_pipeline, 'CHAR')\n",
        "\n",
        "# Search best blend weight\n",
        "weights = np.linspace(0.0, 1.0, 101)\n",
        "best_w, best_auc = 0.0, -1.0\n",
        "for w in weights:\n",
        "    blend = w * oof_char + (1.0 - w) * oof_word\n",
        "    auc = roc_auc_score(y, blend)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_w = auc, w\n",
        "print(f'Best blend weight (char weight): {best_w:.2f} | OOF AUC: {best_auc:.5f}', flush=True)\n",
        "\n",
        "# Fit full models and predict test\n",
        "print('Training full models on all data...', flush=True)\n",
        "model_word_full = build_word_pipeline()\n",
        "model_char_full = build_char_pipeline()\n",
        "t0 = time.time()\n",
        "model_word_full.fit(train_text, y)\n",
        "print(f'Word model trained in {time.time()-t0:.2f}s', flush=True)\n",
        "t1 = time.time()\n",
        "model_char_full.fit(train_text, y)\n",
        "print(f'Char model trained in {time.time()-t1:.2f}s', flush=True)\n",
        "\n",
        "pred_word_test = model_word_full.predict_proba(test_text)[:,1]\n",
        "pred_char_test = model_char_full.predict_proba(test_text)[:,1]\n",
        "pred_blend_test = best_w * pred_char_test + (1.0 - best_w) * pred_word_test\n",
        "\n",
        "# Create submission\n",
        "sub = pd.DataFrame({\n",
        "    'id': test_ids,\n",
        "    'Insult': pred_blend_test\n",
        "})\n",
        "sub_path = 'submission.csv'\n",
        "sub.to_csv(sub_path, index=False)\n",
        "print('Saved submission to', sub_path, 'with shape', sub.shape, 'and head:')\n",
        "print(sub.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WORD] Fold 1/5 AUC: 0.88871 | trn 3157 va 790 | elapsed 0.50s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WORD] Fold 2/5 AUC: 0.88661 | trn 3157 va 790 | elapsed 0.42s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WORD] Fold 3/5 AUC: 0.89176 | trn 3158 va 789 | elapsed 0.36s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WORD] Fold 4/5 AUC: 0.88418 | trn 3158 va 789 | elapsed 0.50s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WORD] Fold 5/5 AUC: 0.88155 | trn 3158 va 789 | elapsed 0.47s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WORD] OOF AUC: 0.88621 | mean 0.88656 +- 0.00353 | total 2.27s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CHAR] Fold 1/5 AUC: 0.90625 | trn 3157 va 790 | elapsed 4.33s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CHAR] Fold 2/5 AUC: 0.90645 | trn 3157 va 790 | elapsed 3.52s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CHAR] Fold 3/5 AUC: 0.90751 | trn 3158 va 789 | elapsed 3.17s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CHAR] Fold 4/5 AUC: 0.90651 | trn 3158 va 789 | elapsed 4.30s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CHAR] Fold 5/5 AUC: 0.89818 | trn 3158 va 789 | elapsed 4.31s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CHAR] OOF AUC: 0.90491 | mean 0.90498 +- 0.00343 | total 19.64s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best blend weight (char weight): 0.86 | OOF AUC: 0.90546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training full models on all data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word model trained in 0.44s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char model trained in 4.28s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission to submission.csv with shape (2647, 2) and head:\n   id    Insult\n0   0  0.040317\n1   1  0.122544\n2   2  0.718967\n3   3  0.190829\n4   4  0.108523\n"
          ]
        }
      ]
    },
    {
      "id": "61d279b9-956f-4e4f-830c-13965d3e2560",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Re-save submission with correct ID column (Date) expected by autograder\n",
        "import pandas as pd\n",
        "\n",
        "test_df = pd.read_csv('test.csv')\n",
        "assert 'Date' in test_df.columns, 'Expected Date column in test.csv'\n",
        "\n",
        "# Use existing predictions in memory if available; otherwise, rebuild quickly\n",
        "try:\n",
        "    pred_word_test, pred_char_test, best_w\n",
        "except NameError:\n",
        "    # Fallback: rebuild models and predictions (should be quick on this dataset)\n",
        "    model_word_full = build_word_pipeline()\n",
        "    model_char_full = build_char_pipeline()\n",
        "    model_word_full.fit(train_text, y)\n",
        "    model_char_full.fit(train_text, y)\n",
        "    pred_word_test = model_word_full.predict_proba(test_text)[:,1]\n",
        "    pred_char_test = model_char_full.predict_proba(test_text)[:,1]\n",
        "    # Default to char weight from earlier advice if best_w not present\n",
        "    best_w = 0.65\n",
        "\n",
        "pred_blend_test = best_w * pred_char_test + (1.0 - best_w) * pred_word_test\n",
        "\n",
        "sub_fix = pd.DataFrame({\n",
        "    'Date': test_df['Date'],\n",
        "    'Insult': pred_blend_test\n",
        "})\n",
        "sub_fix.to_csv('submission.csv', index=False)\n",
        "print('Resaved submission.csv with columns:', sub_fix.columns.tolist(), 'and shape:', sub_fix.shape)\n",
        "print(sub_fix.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resaved submission.csv with columns: ['Date', 'Insult'] and shape: (2647, 2)\n              Date    Insult\n0              NaN  0.040317\n1  20120618222256Z  0.122544\n2  20120618213617Z  0.718967\n3              NaN  0.190829\n4  20120620003825Z  0.108523\n"
          ]
        }
      ]
    },
    {
      "id": "2035f8db-9b89-4d3f-ae09-ed5f9b557fe5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Re-save submission with columns matching sample_submission_null.csv: ['Insult','Date','Comment']\n",
        "import pandas as pd\n",
        "test_df = pd.read_csv('test.csv')\n",
        "assert all(c in test_df.columns for c in ['Date','Comment'])\n",
        "\n",
        "try:\n",
        "    pred_blend_test\n",
        "except NameError:\n",
        "    # Fallback safety: rebuild predictions\n",
        "    model_word_full = build_word_pipeline()\n",
        "    model_char_full = build_char_pipeline()\n",
        "    model_word_full.fit(train_text, y)\n",
        "    model_char_full.fit(train_text, y)\n",
        "    pred_word_test = model_word_full.predict_proba(test_text)[:,1]\n",
        "    pred_char_test = model_char_full.predict_proba(test_text)[:,1]\n",
        "    best_w = 0.65 if 'best_w' not in globals() else best_w\n",
        "    pred_blend_test = best_w * pred_char_test + (1.0 - best_w) * pred_word_test\n",
        "\n",
        "sub3 = pd.DataFrame({\n",
        "    'Insult': pred_blend_test,\n",
        "    'Date': test_df['Date'],\n",
        "    'Comment': test_df['Comment']\n",
        "})\n",
        "sub3.to_csv('submission.csv', index=False)\n",
        "print('submission.csv columns:', sub3.columns.tolist(), 'shape:', sub3.shape)\n",
        "print(sub3.head(3))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv columns: ['Insult', 'Date', 'Comment'] shape: (2647, 3)\n     Insult             Date  \\\n0  0.040317              NaN   \n1  0.122544  20120618222256Z   \n2  0.718967  20120618213617Z   \n\n                                                                                                                                                                                                   Comment  \n0  \"THE DRUDGE REPORT\\\\n\\\\n\\\\n\\\\nYou won't see this story on foxfag forum because they suck bIacks and gay 0bama all the way to the crack.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nOn Tuesday Rep. Darrell Issa, chairma...  \n1                        \"@ian21\\xa0\"Roger Clemens is the fucking man, and never did any fucking steroids because he is fucking awesome. Did you all misremember Roger's incredibleness?\" - Roger Clemens\"  \n2                                                                                          \"Agree with Alan you are an extremest idiot.   You are not American, we native americans are, you are an alien\"  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}