{
  "cells": [
    {
      "id": "3b848319-1ea1-4863-934b-011014077b73",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan to Medal: Detecting Insults in Social Commentary\n",
        "\n",
        "Goal: Optimize AUC-ROC with robust text modeling under time limits.\n",
        "\n",
        "Milestones:\n",
        "- Data loading & sanity checks (schema, target balance)\n",
        "- Baseline: TF-IDF (word + char) + Logistic Regression with 5-fold Stratified CV\n",
        "- Iterate: tune n-grams, min_df, C, optional class weights\n",
        "- Add simple preprocessing (lowercase, URL/user/number normalization)\n",
        "- Error analysis: top coefficients, misclassified examples\n",
        "- Finalize and generate submission.csv\n",
        "\n",
        "Checkpoints for Expert Review:\n",
        "- After plan (this cell)\n",
        "- After initial EDA\n",
        "- After baseline CV results\n",
        "- After tuning trials (if needed)\n",
        "- Before final submission\n",
        "\n",
        "Time Management:\n",
        "- Baseline within ~15 minutes\n",
        "- Iterative tuning in parallel with light EDA\n",
        "- Log timing per fold; interrupt if slow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "92cae90f-9fbb-4b45-9745-2e71caf037da",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, time, re, html, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "start_time = time.time()\n",
        "print('Working directory:', os.getcwd())\n",
        "\n",
        "# Load files\n",
        "train_path = 'train.csv'\n",
        "test_path = 'test.csv'\n",
        "sample_sub_path = 'sample_submission_null.csv'\n",
        "\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "sample_sub = pd.read_csv(sample_sub_path)\n",
        "\n",
        "print('\\nLoaded files:')\n",
        "print('train.shape:', train.shape)\n",
        "print('test.shape:', test.shape)\n",
        "print('sample_submission.shape:', sample_sub.shape)\n",
        "\n",
        "print('\\nColumns:')\n",
        "print('train.columns:', list(train.columns))\n",
        "print('test.columns:', list(test.columns))\n",
        "print('sample_submission.columns:', list(sample_sub.columns))\n",
        "\n",
        "# Identify target and text columns heuristically\n",
        "target_col = None\n",
        "for c in train.columns:\n",
        "    if c.lower() == 'insult':\n",
        "        target_col = c\n",
        "        break\n",
        "if target_col is None:\n",
        "    raise ValueError('Target column Insult not found in train.csv')\n",
        "\n",
        "text_col_candidates = [c for c in train.columns if c.lower() in ('comment','comment_text','text','content','quotation')]\n",
        "if not text_col_candidates:\n",
        "    # fallback: choose the first object column\n",
        "    obj_cols = [c for c in train.columns if train[c].dtype == 'object']\n",
        "    if not obj_cols:\n",
        "        raise ValueError('No object/text column found in train.csv')\n",
        "    text_col = obj_cols[0]\n",
        "else:\n",
        "    text_col = text_col_candidates[0]\n",
        "\n",
        "print(f\"\\nDetected target_col='{target_col}', text_col='{text_col}'\")\n",
        "\n",
        "# Identify submission id columns from sample submission (everything except the Insult column)\n",
        "insult_pred_col = None\n",
        "for c in sample_sub.columns:\n",
        "    if c.lower() == 'insult':\n",
        "        insult_pred_col = c\n",
        "        break\n",
        "if insult_pred_col is None:\n",
        "    raise ValueError('Could not find Insult column in sample_submission_null.csv')\n",
        "submission_id_cols = [c for c in sample_sub.columns if c != insult_pred_col]\n",
        "missing_in_test = [c for c in submission_id_cols if c not in test.columns]\n",
        "if missing_in_test:\n",
        "    raise ValueError(f'Submission ID columns {missing_in_test} not present in test.csv')\n",
        "print(f\"Submission ID columns: {submission_id_cols}, submission target col='{insult_pred_col}'\")\n",
        "\n",
        "# Basic sanitation\n",
        "for df, name in [(train,'train'),(test,'test')]:\n",
        "    if text_col not in df.columns:\n",
        "        raise ValueError(f\"Text column '{text_col}' not in {name} columns\")\n",
        "    df[text_col] = df[text_col].astype(str).fillna('')\n",
        "\n",
        "# Target checks\n",
        "y = train[target_col].values\n",
        "print('\\nTarget distribution:')\n",
        "print(pd.Series(y).value_counts(normalize=False).rename('count'))\n",
        "print(pd.Series(y).value_counts(normalize=True).rename('ratio'))\n",
        "\n",
        "# Duplicates within train\n",
        "dup_train = train.duplicated(subset=[text_col]).sum()\n",
        "print(f\"\\nDuplicate comments within train: {dup_train}\")\n",
        "\n",
        "# Overlap between train and test comments (exact match)\n",
        "train_text_set = set(train[text_col].values.tolist())\n",
        "test_text_set = set(test[text_col].astype(str).values.tolist())\n",
        "overlap = len(train_text_set & test_text_set)\n",
        "print(f\"Exact text overlap train<->test: {overlap}\")\n",
        "\n",
        "# Show a few rows\n",
        "print('\\nSample train rows:')\n",
        "print(train[[text_col, target_col]].head(3))\n",
        "print('\\nSample test rows (ID cols + text):')\n",
        "cols_to_show = submission_id_cols + [text_col]\n",
        "print(test[cols_to_show].head(3))\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\nData load & checks done in {elapsed:.2f}s\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working directory: /app/agent_run_states/detecting-insults-in-social-commentary-spray-20250910-034902\n\nLoaded files:\ntrain.shape: (3947, 3)\ntest.shape: (2647, 2)\nsample_submission.shape: (2647, 3)\n\nColumns:\ntrain.columns: ['Insult', 'Date', 'Comment']\ntest.columns: ['Date', 'Comment']\nsample_submission.columns: ['Insult', 'Date', 'Comment']\n\nDetected target_col='Insult', text_col='Comment'\nSubmission ID columns: ['Date', 'Comment'], submission target col='Insult'\n\nTarget distribution:\n0    2898\n1    1049\nName: count, dtype: int64\n0    0.734229\n1    0.265771\nName: ratio, dtype: float64\n\nDuplicate comments within train: 12\nExact text overlap train<->test: 0\n\nSample train rows:\n                                             Comment  Insult\n0                               \"You fuck your dad.\"       1\n1  \"i really don't understand your point.\\xa0 It ...       0\n2  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...       0\n\nSample test rows (ID cols + text):\n              Date                                            Comment  \\\n0              NaN  \"THE DRUDGE REPORT\\\\n\\\\n\\\\n\\\\nYou won't see th...   \n1  20120618222256Z  \"@ian21\\xa0\"Roger Clemens is the fucking man, ...   \n2  20120618213617Z  \"Agree with Alan you are an extremest idiot.  ...   \n\n                                             Comment  \n0  \"THE DRUDGE REPORT\\\\n\\\\n\\\\n\\\\nYou won't see th...  \n1  \"@ian21\\xa0\"Roger Clemens is the fucking man, ...  \n2  \"Agree with Alan you are an extremest idiot.  ...  \n\nData load & checks done in 0.04s\n"
          ]
        }
      ]
    },
    {
      "id": "b251696c-31dd-4539-aaa8-90c806be4a93",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time, re, html, gc\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "folds = 5\n",
        "random_state = 42\n",
        "\n",
        "# Preprocess function: minimal normalization\n",
        "url_re = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "user_re = re.compile(r'@\\w+')\n",
        "num_re = re.compile(r'\\d+')\n",
        "def normalize_text(series):\n",
        "    # series is iterable of strings\n",
        "    out = []\n",
        "    for s in series:\n",
        "        s = html.unescape(s)\n",
        "        s = url_re.sub(' _URL_ ', s)\n",
        "        s = user_re.sub(' _USER_ ', s)\n",
        "        s = num_re.sub(' _NUM_ ', s)\n",
        "        out.append(s)\n",
        "    return out\n",
        "\n",
        "print('\\n=== Baseline TF-IDF + Logistic Regression (word+char) ===')\n",
        "word_vect = TfidfVectorizer(ngram_range=(1,2), min_df=2, sublinear_tf=True, lowercase=True, analyzer='word')\n",
        "char_vect = TfidfVectorizer(ngram_range=(3,5), min_df=2, sublinear_tf=True, lowercase=True, analyzer='char')\n",
        "\n",
        "features = FeatureUnion([\n",
        "    ('word', word_vect),\n",
        "    ('char', char_vect),\n",
        "], n_jobs=1)\n",
        "\n",
        "logreg = LogisticRegression(C=3.0, max_iter=2000, solver='saga', n_jobs=-1, class_weight='balanced', verbose=0, random_state=random_state)\n",
        "\n",
        "# We'll handle normalization outside the vectorizers to ensure consistency\n",
        "X_text = normalize_text(train[text_col].tolist())\n",
        "y = train[target_col].values.astype(int)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=random_state)\n",
        "oof = np.zeros(len(train), dtype=float)\n",
        "fold_times = []\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "    t0 = time.time()\n",
        "    print(f'Fold {fold}/{folds}: train={len(trn_idx)} val={len(val_idx)}')\n",
        "    X_trn = [X_text[i] for i in trn_idx]\n",
        "    X_val = [X_text[i] for i in val_idx]\n",
        "    y_trn, y_val = y[trn_idx], y[val_idx]\n",
        "\n",
        "    pipe = Pipeline([\n",
        "        ('feats', features),\n",
        "        ('clf', logreg)\n",
        "    ])\n",
        "    pipe.fit(X_trn, y_trn)\n",
        "    val_pred = pipe.predict_proba(X_val)[:, 1]\n",
        "    oof[val_idx] = val_pred\n",
        "    auc = roc_auc_score(y_val, val_pred)\n",
        "    dt = time.time() - t0\n",
        "    fold_times.append(dt)\n",
        "    print(f'  Fold {fold} AUC: {auc:.5f} | time: {dt:.2f}s', flush=True)\n",
        "    gc.collect()\n",
        "\n",
        "oof_auc = roc_auc_score(y, oof)\n",
        "print(f'OOF AUC: {oof_auc:.5f}')\n",
        "print('Fold times (s):', [round(t,2) for t in fold_times], ' | total:', round(sum(fold_times),2))\n",
        "\n",
        "# Train on full data\n",
        "print('\\nTraining final model on full training data...')\n",
        "t0 = time.time()\n",
        "final_pipe = Pipeline([\n",
        "    ('feats', features),\n",
        "    ('clf', logreg)\n",
        "])\n",
        "final_pipe.fit(X_text, y)\n",
        "print(f'Final training done in {time.time()-t0:.2f}s')\n",
        "\n",
        "# Predict on test\n",
        "X_test_text = normalize_text(test[text_col].astype(str).tolist())\n",
        "test_pred = final_pipe.predict_proba(X_test_text)[:, 1]\n",
        "\n",
        "# Build submission: columns follow sample_submission with Insult first, then ID columns\n",
        "sub = pd.DataFrame({insult_pred_col: test_pred})\n",
        "# Append identifier columns from test in the same order as sample submission (excluding Insult)\n",
        "for c in submission_id_cols:\n",
        "    sub[c] = test[c]\n",
        "sub = sub[[insult_pred_col] + submission_id_cols]\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape:', sub.shape)\n",
        "print(sub.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== Baseline TF-IDF + Logistic Regression (word+char) ===\nFold 1/5: train=3157 val=790\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}