{
  "cells": [
    {
      "id": "9ed5c273-e66d-4e96-a432-522574c52f74",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Normalization Challenge (English) - Medal Plan and Experiment Log\n",
        "\n",
        "## Goal\n",
        "- WIN A MEDAL (>= Bronze, target Silver+). Metric: Accuracy.\n",
        "\n",
        "## High-level Plan\n",
        "1. Data understanding & EDA\n",
        "   - Load train/test and sample submission.\n",
        "   - Inspect schema, sizes, and example rows.\n",
        "2. Baselines\n",
        "   - Simple frequency/lookup baseline: memorize most frequent normalized form for each token/phrase class.\n",
        "   - Identity for <self>/sil classes if present.\n",
        "3. Modeling\n",
        "   - Per-class models:\n",
        "     - Rule-based/regex for numbers, dates, times, currencies, measurements.\n",
        "     - Backoff to CatBoost/LightGBM per class on engineered features if needed.\n",
        "   - Sequence labeling or pointer-generator style fallback if data format supports it.\n",
        "4. CV Strategy\n",
        "   - Robust split preserving distribution by semi-random or group if provided (e.g., sentence id).\n",
        "   - Early small iterations; log per-class accuracy.\n",
        "5. Inference & Submission\n",
        "   - Generate predictions on test.\n",
        "   - Save to `submission.csv` in required format.\n",
        "\n",
        "## Experiment Log\n",
        "- [T0] Setup & EDA: load zipped csvs, inspect columns.\n",
        "- [Next] Build trivial identity/majority baseline and evaluate via CV if possible.\n",
        "- [Then] Add per-class rules for NUM, DATE, TIME, MONEY, MEASURE, etc.\n",
        "- [Iterate] Error analysis -> add rules -> re-evaluate.\n",
        "\n",
        "## Notes\n",
        "- Always log progress with timestamps and fold indices.\n",
        "- Keep runs short; interrupt long ones and iterate.\n",
        "- Ask expert review after plan, EDA, baseline, and if score stalls."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "75c815fe-9126-439c-bc6f-8cc9fb4cbe69",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, time, json, gc\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def log(msg):\n",
        "    ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    print(f\"[{ts}] {msg}\", flush=True)\n",
        "\n",
        "log(\"Listing repository files and sizes...\")\n",
        "for f in sorted(os.listdir('.')):\n",
        "    try:\n",
        "        sz = os.path.getsize(f)\n",
        "    except Exception:\n",
        "        sz = -1\n",
        "    print(f\" - {f:30s} {sz/1e6:8.2f} MB\")\n",
        "\n",
        "log(\"Reading heads of CSVs (from zip) to inspect schema...\")\n",
        "def read_head(path, n=5):\n",
        "    try:\n",
        "        df = pd.read_csv(path, nrows=n)\n",
        "        log(f\"Loaded {path}: shape={df.shape}; columns={list(df.columns)}\")\n",
        "        print(df.head(3))\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        log(f\"Failed to read {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "train_head = read_head('en_train.csv.zip', n=5)\n",
        "test_head = read_head('en_test_2.csv.zip', n=5)\n",
        "sub_head = read_head('en_sample_submission_2.csv.zip', n=5)\n",
        "\n",
        "log(\"Quick class distribution snapshot from a small sample (if class column exists)...\")\n",
        "try:\n",
        "    sample_rows = 200000\n",
        "    df_sample = pd.read_csv('en_train.csv.zip', nrows=sample_rows)\n",
        "    if 'class' in df_sample.columns:\n",
        "        print(df_sample['class'].value_counts().head(20))\n",
        "    else:\n",
        "        print(\"No 'class' column in sample.\")\n",
        "    del df_sample; gc.collect()\n",
        "except Exception as e:\n",
        "    log(f\"Skipping class distribution sample: {e}\")\n",
        "\n",
        "log(\"Done initial EDA.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-08 20:55:55] Listing repository files and sizes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - agent_metadata                     0.00 MB\n - description.md                     0.00 MB\n - docker_run.log                     0.02 MB\n - en_sample_submission_2.csv.zip    17.45 MB\n - en_test_2.csv.zip                 15.46 MB\n - en_train.csv.zip                 291.83 MB\n - main.ipynb                         0.00 MB\n - requirements.txt                   0.00 MB\n - task.txt                           0.00 MB\n[2025-09-08 20:55:55] Reading heads of CSVs (from zip) to inspect schema...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-08 20:55:55] Loaded en_train.csv.zip: shape=(5, 5); columns=['sentence_id', 'token_id', 'class', 'before', 'after']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sentence_id  token_id  class         before          after\n0            0         0  PLAIN  Brillantaisia  Brillantaisia\n1            0         1  PLAIN             is             is\n2            0         2  PLAIN              a              a\n[2025-09-08 20:55:55] Loaded en_test_2.csv.zip: shape=(5, 3); columns=['sentence_id', 'token_id', 'before']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sentence_id  token_id    before\n0            0         0     Rocky\n1            0         1  Mountain\n2            0         2      News\n[2025-09-08 20:55:55] Loaded en_sample_submission_2.csv.zip: shape=(5, 2); columns=['id', 'after']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    id     after\n0  0_0     Rocky\n1  0_1  Mountain\n2  0_2      News\n[2025-09-08 20:55:55] Quick class distribution snapshot from a small sample (if class column exists)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class\nPLAIN         146914\nPUNCT          38555\nDATE            6060\nLETTERS         3047\nCARDINAL        2618\nVERBATIM        1645\nMEASURE          300\nORDINAL          233\nDECIMAL          210\nMONEY             98\nDIGIT             95\nELECTRONIC        90\nTELEPHONE         74\nTIME              29\nFRACTION          26\nADDRESS            6\nName: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-08 20:55:55] Done initial EDA.\n"
          ]
        }
      ]
    },
    {
      "id": "e061d508-b589-4ff6-9a67-e2cb56368b8a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def log(msg):\n",
        "    ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    print(f\"[{ts}] {msg}\", flush=True)\n",
        "\n",
        "log(\"Loading test set...\")\n",
        "test_path = 'en_test_2.csv.zip'\n",
        "df_test = pd.read_csv(test_path)\n",
        "log(f\"Test shape: {df_test.shape}; cols={list(df_test.columns)}\")\n",
        "\n",
        "log(\"Building identity baseline predictions (after = before)...\")\n",
        "pred = df_test[['sentence_id','token_id','before']].copy()\n",
        "pred['id'] = pred['sentence_id'].astype(str) + '_' + pred['token_id'].astype(str)\n",
        "pred.rename(columns={'before':'after'}, inplace=True)\n",
        "\n",
        "sub = pred[['id','after']].copy()\n",
        "out_path = 'submission.csv' \n",
        "sub.to_csv(out_path, index=False)\n",
        "log(f\"Wrote {out_path} with shape {sub.shape}\")\n",
        "print(sub.head(10))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-08 20:59:01] Loading test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-08 20:59:01] Test shape: (993465, 3); cols=['sentence_id', 'token_id', 'before']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-08 20:59:01] Building identity baseline predictions (after = before)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-08 20:59:03] Wrote submission.csv with shape (993465, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    id      after\n0  0_0      Rocky\n1  0_1   Mountain\n2  0_2       News\n3  0_3          .\n4  1_0          \"\n5  1_1       U.S.\n6  1_2  Decennial\n7  1_3     Census\n8  1_4          \"\n9  1_5          .\n"
          ]
        }
      ]
    },
    {
      "id": "9883ca94-5f82-42ac-a88c-d6bbe5953ec9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from datetime import datetime\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\", flush=True)\n",
        "\n",
        "train_path = 'en_train.csv.zip'\n",
        "log('Loading full train...')\n",
        "df_train = pd.read_csv(train_path)\n",
        "log(f\"Train shape: {df_train.shape}; cols={list(df_train.columns)}\")\n",
        "\n",
        "def build_majority_map(df):\n",
        "    # For each 'before', choose most frequent 'after'\n",
        "    counts = df.groupby(['before','after']).size().reset_index(name='cnt')\n",
        "    idx = counts.groupby('before')['cnt'].idxmax()\n",
        "    best = counts.loc[idx, ['before','after','cnt']]\n",
        "    mapping = dict(zip(best['before'], best['after']))\n",
        "    return mapping, best['cnt'].sum(), len(mapping)\n",
        "\n",
        "def infer_with_map(series_before, mapping):\n",
        "    # Vectorized map with fallback to identity\n",
        "    return series_before.map(mapping).fillna(series_before)\n",
        "\n",
        "log('5-fold GroupKFold CV by sentence_id...')\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = df_train['sentence_id'].values\n",
        "fold_acc = []\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(df_train, groups=groups), 1):\n",
        "    t0 = datetime.now()\n",
        "    dtr = df_train.iloc[tr_idx]\n",
        "    dva = df_train.iloc[va_idx]\n",
        "    mapping, total_cnt, uniq = build_majority_map(dtr[['before','after']])\n",
        "    dva_pred = infer_with_map(dva['before'], mapping)\n",
        "    acc = (dva_pred.values == dva['after'].values).mean()\n",
        "    fold_acc.append(acc)\n",
        "    dt = (datetime.now()-t0).total_seconds()\n",
        "    log(f\"Fold {fold}: acc={acc:.6f}, uniq_before={uniq}, build_cnt_sum={total_cnt}, time={dt:.2f}s\")\n",
        "log(f\"CV mean acc: {np.mean(fold_acc):.6f} \u00b1 {np.std(fold_acc):.6f}\")\n",
        "\n",
        "# Fit on full train and generate test predictions\n",
        "log('Fitting majority map on full train...')\n",
        "major_map, total_cnt, uniq = build_majority_map(df_train[['before','after']])\n",
        "log(f\"Major map size={uniq}, total_cnt={total_cnt}\")\n",
        "\n",
        "log('Loading test and predicting...')\n",
        "df_test = pd.read_csv('en_test_2.csv.zip')\n",
        "pred_after = infer_with_map(df_test['before'], major_map)\n",
        "sub = pd.DataFrame({\n",
        "    'id': df_test['sentence_id'].astype(str) + '_' + df_test['token_id'].astype(str),\n",
        "    'after': pred_after\n",
        "})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "log(f\"Wrote submission.csv with shape {sub.shape}\")\n",
        "print(sub.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-09-08 20:59:57] Loading full train...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}