{
  "cells": [
    {
      "id": "f7fc07e2-cb90-495a-b5c7-5b63cc73d383",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Text Normalization Challenge - English Language\n",
        "# Plan:\n",
        "# - Inspect provided files and sample submission format.\n",
        "# - Load train/test from zipped CSVs using pandas.\n",
        "# - EDA: show columns, sizes, and a few samples.\n",
        "# - Build a frequency-based mapper: for each (class, before) choose the most frequent 'after'.\n",
        "# - Apply mapper to test; default backoff: after = before when unseen.\n",
        "# - Save predictions to submission.csv in required format.\n",
        "\n",
        "import os, zipfile, pandas as pd, numpy as np\n",
        "pd.set_option('display.max_columns', 100)\n",
        "print('Files in CWD:', os.listdir())\n",
        "\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Peek sample submission structure\n",
        "ss_zip = 'en_sample_submission_2.csv.zip'\n",
        "with ZipFile(ss_zip) as z:\n",
        "    ss_name = z.namelist()[0]\n",
        "    with z.open(ss_name) as f:\n",
        "        sample_sub = pd.read_csv(f)\n",
        "print('Sample submission shape:', sample_sub.shape)\n",
        "print(sample_sub.head())\n",
        "\n",
        "# Peek train/test columns\n",
        "def read_head(zip_path, n=5):\n",
        "    with ZipFile(zip_path) as z:\n",
        "        name = z.namelist()[0]\n",
        "        with z.open(name) as f:\n",
        "            return pd.read_csv(f, nrows=n)\n",
        "\n",
        "train_head = read_head('en_train.csv.zip', n=10)\n",
        "test_head = read_head('en_test_2.csv.zip', n=10)\n",
        "print('Train head:')\n",
        "print(train_head)\n",
        "print('Test head:')\n",
        "print(test_head)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in CWD: ['main.ipynb', 'agent_metadata', 'en_test_2.csv.zip', 'en_sample_submission_2.csv.zip', 'requirements.txt', 'task.txt', 'en_train.csv.zip', 'docker_run.log', 'description.md']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample submission shape: (993465, 2)\n    id     after\n0  0_0     Rocky\n1  0_1  Mountain\n2  0_2      News\n3  0_3         .\n4  1_0         \"\nTrain head:\n   sentence_id  token_id  class         before          after\n0            0         0  PLAIN  Brillantaisia  Brillantaisia\n1            0         1  PLAIN             is             is\n2            0         2  PLAIN              a              a\n3            0         3  PLAIN          genus          genus\n4            0         4  PLAIN             of             of\n5            0         5  PLAIN          plant          plant\n6            0         6  PLAIN             in             in\n7            0         7  PLAIN         family         family\n8            0         8  PLAIN    Acanthaceae    Acanthaceae\n9            0         9  PUNCT              .              .\nTest head:\n   sentence_id  token_id     before\n0            0         0      Rocky\n1            0         1   Mountain\n2            0         2       News\n3            0         3          .\n4            1         0          \"\n5            1         1       U.S.\n6            1         2  Decennial\n7            1         3     Census\n8            1         4          \"\n9            1         5          .\n"
          ]
        }
      ]
    },
    {
      "id": "f89402b5-969e-4089-8cc3-f3395a51e0c3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "from collections import Counter, defaultdict\n",
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "t0 = time.time()\n",
        "print('Building memorizer from train...')\n",
        "cnt = Counter()\n",
        "rows_processed = 0\n",
        "with ZipFile('en_train.csv.zip') as z:\n",
        "    name = z.namelist()[0]\n",
        "    for i, ch in enumerate(pd.read_csv(z.open(name), usecols=['before','after'], dtype='string', chunksize=2_000_000)):\n",
        "        # Drop rows with missing before/after to avoid pd.NA comparison issues\n",
        "        ch = ch.dropna(subset=['before','after'])\n",
        "        cnt.update(zip(ch['before'].tolist(), ch['after'].tolist()))\n",
        "        rows_processed += len(ch)\n",
        "        if i % 1 == 0:\n",
        "            print(f'  Chunk {i}, cumulative rows: {rows_processed:,}, unique pairs: {len(cnt):,}, elapsed: {time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "print('Consolidating most frequent after per before...')\n",
        "by_before = defaultdict(list)\n",
        "for (b,a), c in cnt.items():\n",
        "    by_before[b].append((c, a))\n",
        "most = {b: max(v)[1] for b, v in by_before.items()}\n",
        "# Unchanged-majority set (safe after NA drop) for potential rule gating later\n",
        "unchanged = set()\n",
        "for b, v in by_before.items():\n",
        "    total = 0\n",
        "    top_c, top_a = 0, None\n",
        "    for c, a in v:\n",
        "        total += c\n",
        "        if c > top_c:\n",
        "            top_c, top_a = c, a\n",
        "    if total > 0 and top_a == b and (top_c / total) >= 0.9:\n",
        "        unchanged.add(b)\n",
        "print(f'Memorizer size: {len(most):,} befores. Unchanged-majority size: {len(unchanged):,}. Elapsed: {time.time()-t0:.1f}s')\n",
        "\n",
        "print('Loading test and generating baseline predictions...')\n",
        "with ZipFile('en_test_2.csv.zip') as zt:\n",
        "    name_t = zt.namelist()[0]\n",
        "    test = pd.read_csv(zt.open(name_t), dtype={'sentence_id':np.int32,'token_id':np.int32,'before':'string'})\n",
        "print('Test shape:', test.shape)\n",
        "pred = test['before'].map(most).fillna(test['before'])\n",
        "sub = pd.DataFrame({\n",
        "    'id': test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str),\n",
        "    'after': pred\n",
        "})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "changed = (sub['after'] != test['before']).mean()\n",
        "covered = test['before'].isin(most.keys()).mean()\n",
        "print(f'Saved submission.csv. Covered by memorizer: {covered:.3%}. Changed fraction: {changed:.3%}. Total time: {time.time()-t0:.1f}s')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building memorizer from train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chunk 0, cumulative rows: 1,999,973, unique pairs: 175,079, elapsed: 2.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Chunk 1, cumulative rows: 3,999,946, unique pairs: 274,252, elapsed: 5.8s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}