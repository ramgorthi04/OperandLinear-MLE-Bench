{
  "cells": [
    {
      "id": "c61c78fa-95c8-408a-ba5a-2aba17fab333",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Normalization (English) - Medal Plan and Experiment Log\n",
        "\n",
        "## Goal\n",
        "- Win a medal (>= bronze) on Kaggle's Text Normalization Challenge - English.\n",
        "\n",
        "## High-level Plan\n",
        "1. Data understanding: load train/test/sample_submission, inspect schema and target.\n",
        "2. Baselines:\n",
        "   - Identity baseline (predict `before` -> `after` as same or majority mapping).\n",
        "   - Memorization: freq-based mapping from `before` (optionally conditioned on class/context) to most-common `after`.\n",
        "3. Modeling:\n",
        "   - Rule-based/regex normalizers for categories: numbers, dates, times, money, measures, ordinals, telephone, electronic, etc.\n",
        "   - Hybrid: backoff to memorized mapping; else identity if class says `PLAIN`/`PUNCT`.\n",
        "   - Optional ML: character-level seq2seq or edit-distance based transducer (if time).\n",
        "4. CV strategy: time-safe split by sequence/utterance id; metric is accuracy over tokens; replicate Kaggle eval locally.\n",
        "5. Iterate with error analysis by class; prioritize high-volume error buckets.\n",
        "\n",
        "## Experiment Log\n",
        "- [T0] Setup, load data, schema inspection.\n",
        "- [T1] Build identity and memorize baselines; local eval.\n",
        "- [T2] Add deterministic rules for common semiotic classes; iterate.\n",
        "- [T3] Full train fit and inference; create submission.csv; refine until medal range.\n",
        "\n",
        "We will request expert reviews at major milestones (plan, EDA, baseline, modeling, and if score stalls)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "67ebe90b-f690-475a-bba5-716adf6f0f9a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T0: Setup, load data, schema inspection\n",
        "import os, sys, gc, time, json, math, textwrap, zipfile, psutil\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "pd.set_option('display.width', 200)\n",
        "\n",
        "DATA_TRAIN = 'en_train.csv.zip'\n",
        "DATA_TEST = 'en_test_2.csv.zip'\n",
        "DATA_SUB = 'en_sample_submission_2.csv.zip'\n",
        "\n",
        "def mem():\n",
        "    p = psutil.Process(os.getpid())\n",
        "    return f\"RAM used: {p.memory_info().rss/1e9:.2f} GB\"\n",
        "\n",
        "t0 = time.time()\n",
        "print('CWD:', os.getcwd())\n",
        "print('Files:', sorted(os.listdir()))\n",
        "print(mem())\n",
        "\n",
        "def read_csv_zip(path):\n",
        "    # pandas can read zip directly\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "print('\\nLoading train...'); t = time.time()\n",
        "train = read_csv_zip(DATA_TRAIN)\n",
        "print('Train shape:', train.shape, '| elapsed:', f\"{time.time()-t:.2f}s\")\n",
        "print(mem())\n",
        "\n",
        "print('\\nTrain columns:', list(train.columns))\n",
        "print('\\nTrain head:')\n",
        "display(train.head(10))\n",
        "\n",
        "# Optional quick peeks if present\n",
        "for col in ['class', 'semiotic_class']:\n",
        "    if col in train.columns:\n",
        "        vc = train[col].value_counts().head(20)\n",
        "        print(f\"\\nTop {col} counts:\\n\", vc)\n",
        "\n",
        "for col in ['before', 'after']:\n",
        "    if col in train.columns:\n",
        "        nulls = train[col].isna().sum()\n",
        "        print(f\"Nulls in {col}:\", nulls)\n",
        "\n",
        "print('\\nLoading test...'); t = time.time()\n",
        "test = read_csv_zip(DATA_TEST)\n",
        "print('Test shape:', test.shape, '| elapsed:', f\"{time.time()-t:.2f}s\")\n",
        "print('Test columns:', list(test.columns))\n",
        "\n",
        "print('\\nLoading sample submission...'); t = time.time()\n",
        "sub = read_csv_zip(DATA_SUB)\n",
        "print('Sample submission shape:', sub.shape, '| elapsed:', f\"{time.time()-t:.2f}s\")\n",
        "print('Sample submission columns:', list(sub.columns))\n",
        "\n",
        "print('\\nDtypes (train):')\n",
        "print(train.dtypes)\n",
        "\n",
        "print('\\nBasic checks complete. Total elapsed:', f\"{time.time()-t0:.2f}s\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CWD: /app/agent_run_states/text-normalization-challenge-english-language-spray-20250909-022048\nFiles: ['agent_metadata', 'description.md', 'docker_run.log', 'en_sample_submission_2.csv.zip', 'en_test_2.csv.zip', 'en_train.csv.zip', 'main.ipynb', 'requirements.txt', 'task.txt']\nRAM used: 0.17 GB\n\nLoading train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (8924976, 5) | elapsed: 6.33s\nRAM used: 1.13 GB\n\nTrain columns: ['sentence_id', 'token_id', 'class', 'before', 'after']\n\nTrain head:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "   sentence_id  token_id  class         before          after\n0            0         0  PLAIN  Brillantaisia  Brillantaisia\n1            0         1  PLAIN             is             is\n2            0         2  PLAIN              a              a\n3            0         3  PLAIN          genus          genus\n4            0         4  PLAIN             of             of\n5            0         5  PLAIN          plant          plant\n6            0         6  PLAIN             in             in\n7            0         7  PLAIN         family         family\n8            0         8  PLAIN    Acanthaceae    Acanthaceae\n9            0         9  PUNCT              .              .",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>token_id</th>\n      <th>class</th>\n      <th>before</th>\n      <th>after</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>PLAIN</td>\n      <td>Brillantaisia</td>\n      <td>Brillantaisia</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>PLAIN</td>\n      <td>is</td>\n      <td>is</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2</td>\n      <td>PLAIN</td>\n      <td>a</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>3</td>\n      <td>PLAIN</td>\n      <td>genus</td>\n      <td>genus</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>4</td>\n      <td>PLAIN</td>\n      <td>of</td>\n      <td>of</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>5</td>\n      <td>PLAIN</td>\n      <td>plant</td>\n      <td>plant</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>6</td>\n      <td>PLAIN</td>\n      <td>in</td>\n      <td>in</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>7</td>\n      <td>PLAIN</td>\n      <td>family</td>\n      <td>family</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>8</td>\n      <td>PLAIN</td>\n      <td>Acanthaceae</td>\n      <td>Acanthaceae</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>9</td>\n      <td>PUNCT</td>\n      <td>.</td>\n      <td>.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nTop class counts:\n class\nPLAIN         6617274\nPUNCT         1692369\nDATE           232281\nLETTERS        137499\nCARDINAL       120265\nVERBATIM        70160\nMEASURE         13285\nORDINAL         11386\nDECIMAL          8828\nMONEY            5510\nDIGIT            4929\nELECTRONIC       4690\nTELEPHONE        3623\nTIME             1318\nFRACTION         1086\nADDRESS           473\nName: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nulls in before: 144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nulls in after: 140\n\nLoading test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test shape: (993465, 3) | elapsed: 0.31s\nTest columns: ['sentence_id', 'token_id', 'before']\n\nLoading sample submission...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample submission shape: (993465, 2) | elapsed: 0.60s\nSample submission columns: ['id', 'after']\n\nDtypes (train):\nsentence_id     int64\ntoken_id        int64\nclass          object\nbefore         object\nafter          object\ndtype: object\n\nBasic checks complete. Total elapsed: 8.57s\n"
          ]
        }
      ]
    },
    {
      "id": "0d0a4adc-0016-47f6-a918-43fcd4dc3e0a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T1: Baselines, mapping, and first submission\n",
        "import time\n",
        "t = time.time()\n",
        "\n",
        "# 1) Verify submission ID format against sample\n",
        "print('Verifying submission id format...')\n",
        "sub_head = sub.head(3).copy()\n",
        "tmp_ids = (test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)).head(3).tolist()\n",
        "print('Sample sub head ids:', sub_head['id'].tolist())\n",
        "print('Constructed head ids:', tmp_ids)\n",
        "id_format_ok = True\n",
        "for a,b in zip(sub_head['id'].tolist(), tmp_ids):\n",
        "    if a != b:\n",
        "        id_format_ok = False\n",
        "        break\n",
        "print('ID format matches sample:', id_format_ok)\n",
        "\n",
        "# 2) Handle nulls: drop only for training map building; keep test but fill for safety\n",
        "n_before_null = train['before'].isna().sum()\n",
        "n_after_null = train['after'].isna().sum()\n",
        "print(f'Nulls -> train.before: {n_before_null}, train.after: {n_after_null}')\n",
        "train_clean = train.dropna(subset=['before','after']).copy()\n",
        "print('Train after dropna:', train_clean.shape)\n",
        "\n",
        "# 3) Identity baseline on train\n",
        "identity_acc = (train_clean['before'] == train_clean['after']).mean()\n",
        "print(f'Identity baseline accuracy (train): {identity_acc:.6f}')\n",
        "\n",
        "# 4) Memorization map: before -> most frequent after\n",
        "print('Building primary memorization map...')\n",
        "t_map = time.time()\n",
        "# Using value_counts idxmax for speed and determinism\n",
        "mode_after = train_clean.groupby('before', sort=False)['after'].agg(lambda s: s.value_counts().idxmax())\n",
        "memo_map = mode_after.to_dict()\n",
        "print(f'Map size: {len(memo_map):,} | elapsed: {time.time()-t_map:.2f}s')\n",
        "\n",
        "# 5) Lowercase backoff map\n",
        "print('Building lowercase backoff map...')\n",
        "t_low = time.time()\n",
        "train_clean['_before_lower'] = train_clean['before'].str.lower()\n",
        "mode_after_lower = train_clean.groupby('_before_lower', sort=False)['after'].agg(lambda s: s.value_counts().idxmax())\n",
        "memo_map_lower = mode_after_lower.to_dict()\n",
        "print(f'Lower map size: {len(memo_map_lower):,} | elapsed: {time.time()-t_low:.2f}s')\n",
        "train_clean.drop(columns=['_before_lower'], inplace=True)\n",
        "\n",
        "# 6) Inference on test: primary -> lowercase -> identity\n",
        "print('Inferring on test...')\n",
        "test_inf = test.copy()\n",
        "test_inf['before'] = test_inf['before'].fillna('')\n",
        "pred = test_inf['before'].map(memo_map)\n",
        "mask_na = pred.isna()\n",
        "if mask_na.any():\n",
        "    pred2 = test_inf.loc[mask_na, 'before'].str.lower().map(memo_map_lower)\n",
        "    pred.loc[mask_na] = pred2\n",
        "mask_na2 = pred.isna()\n",
        "if mask_na2.any():\n",
        "    pred.loc[mask_na2] = test_inf.loc[mask_na2, 'before']\n",
        "\n",
        "# 7) Build submission in the same order as sample\n",
        "test_ids = test_inf['sentence_id'].astype(str) + '_' + test_inf['token_id'].astype(str)\n",
        "submission = pd.DataFrame({'id': test_ids, 'after': pred.astype(str)})\n",
        "print('Submission shape:', submission.shape)\n",
        "\n",
        "# Ensure row count matches sample and first few ids align\n",
        "assert submission.shape[0] == sub.shape[0], f'Row count mismatch: {submission.shape[0]} vs sample {sub.shape[0]}'\n",
        "print('First 3 ids (ours):', submission['id'].head(3).tolist())\n",
        "print('First 3 ids (sample):', sub['id'].head(3).tolist())\n",
        "\n",
        "# 8) Save\n",
        "out_path = 'submission.csv'\n",
        "submission.to_csv(out_path, index=False)\n",
        "print('Saved to', out_path)\n",
        "print('T1 total elapsed:', f'{time.time()-t:.2f}s')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying submission id format...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample sub head ids: ['0_0', '0_1', '0_2']\nConstructed head ids: ['0_0', '0_1', '0_2']\nID format matches sample: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nulls -> train.before: 144, train.after: 140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train after dropna: (8924832, 5)\nIdentity baseline accuracy (train): 0.933502\nBuilding primary memorization map...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map size: 455,220 | elapsed: 87.14s\nBuilding lowercase backoff map...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lower map size: 404,763 | elapsed: 79.28s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inferring on test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission shape: (993465, 2)\nFirst 3 ids (ours): ['0_0', '0_1', '0_2']\nFirst 3 ids (sample): ['0_0', '0_1', '0_2']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to submission.csv\nT1 total elapsed: 173.55s\n"
          ]
        }
      ]
    },
    {
      "id": "d3571382-5c42-48fd-a168-824d611da768",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T1b: Quick local CV with GroupShuffleSplit by sentence_id for memo baseline\n",
        "import time\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "t0 = time.time()\n",
        "print('Starting GroupShuffleSplit CV (80/20) ...')\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "groups = train['sentence_id'].values\n",
        "train_idx, val_idx = next(gss.split(train, groups=groups))\n",
        "print('Train idx size:', len(train_idx), 'Val idx size:', len(val_idx))\n",
        "\n",
        "trn = train.iloc[train_idx].dropna(subset=['before','after']).copy()\n",
        "val = train.iloc[val_idx].copy()\n",
        "print('Train (clean) shape:', trn.shape, 'Val shape:', val.shape)\n",
        "\n",
        "t = time.time()\n",
        "print('Building maps on train split ...')\n",
        "mode_after_cv = trn.groupby('before', sort=False)['after'].agg(lambda s: s.value_counts().idxmax())\n",
        "memo_map_cv = mode_after_cv.to_dict()\n",
        "trn['_before_lower'] = trn['before'].str.lower()\n",
        "mode_after_lower_cv = trn.groupby('_before_lower', sort=False)['after'].agg(lambda s: s.value_counts().idxmax())\n",
        "memo_map_lower_cv = mode_after_lower_cv.to_dict()\n",
        "print('Maps built | sizes:', len(memo_map_cv), len(memo_map_lower_cv), '| elapsed:', f'{time.time()-t:.2f}s')\n",
        "trn.drop(columns=['_before_lower'], inplace=True)\n",
        "\n",
        "print('Predicting on validation ...')\n",
        "val_bef = val['before'].fillna('')\n",
        "pred_val = val_bef.map(memo_map_cv)\n",
        "na1 = pred_val.isna()\n",
        "if na1.any():\n",
        "    pred_val.loc[na1] = val_bef.loc[na1].str.lower().map(memo_map_lower_cv)\n",
        "na2 = pred_val.isna()\n",
        "if na2.any():\n",
        "    pred_val.loc[na2] = val_bef.loc[na2]\n",
        "\n",
        "acc = (pred_val.values == val['after'].values).mean()\n",
        "print(f'CV accuracy (memo + lower backoff): {acc:.6f}')\n",
        "\n",
        "# Error analysis by class (top 20)\n",
        "err_mask = pred_val.values != val['after'].values\n",
        "if 'class' in val.columns:\n",
        "    err_classes = val.loc[err_mask, 'class'].value_counts().head(20)\n",
        "    print('\\nTop error classes (val):')\n",
        "    print(err_classes)\n",
        "else:\n",
        "    print('Class column not available in validation set.')\n",
        "\n",
        "print('T1b total elapsed:', f'{time.time()-t0:.2f}s')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GroupShuffleSplit CV (80/20) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train idx size: 7136041 Val idx size: 1788935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (clean) shape: (7135924, 5) Val shape: (1788935, 5)\nBuilding maps on train split ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maps built | sizes: 395355 351290 | elapsed: 144.49s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting on validation ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV accuracy (memo + lower backoff): 0.986828\n\nTop error classes (val):\nclass\nPLAIN         8352\nDATE          4871\nLETTERS       3447\nCARDINAL      1908\nMEASURE       1120\nELECTRONIC     689\nTELEPHONE      647\nMONEY          608\nDECIMAL        577\nDIGIT          527\nORDINAL        324\nTIME           191\nFRACTION       144\nVERBATIM       119\nADDRESS         40\nName: count, dtype: int64\nT1b total elapsed: 149.98s\n"
          ]
        }
      ]
    },
    {
      "id": "eb4fab8a-f7d8-45a6-a953-17f1580ce6e0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T2: Class-aware memoization + class-guarded deterministic rules, CV and inference\n",
        "import re, time\n",
        "\n",
        "t0 = time.time()\n",
        "print('Building class-aware maps from full train ...')\n",
        "tr = train.dropna(subset=['before','after','class']).copy()\n",
        "\n",
        "# before -> most frequent class\n",
        "cls_map = tr.groupby('before', sort=False)['class'].agg(lambda s: s.value_counts().idxmax()).to_dict()\n",
        "cls_map_lower = tr.assign(_bl=tr['before'].str.lower()).groupby('_bl', sort=False)['class'].agg(lambda s: s.value_counts().idxmax()).to_dict()\n",
        "\n",
        "# (before, class) -> most frequent after\n",
        "pair_map = tr.groupby(['before','class'], sort=False)['after'].agg(lambda s: s.value_counts().idxmax()).to_dict()\n",
        "tr['_bl'] = tr['before'].str.lower()\n",
        "pair_map_lower = tr.groupby(['_bl','class'], sort=False)['after'].agg(lambda s: s.value_counts().idxmax()).to_dict()\n",
        "tr.drop(columns=['_bl'], inplace=True)\n",
        "print('Map sizes | cls:', len(cls_map), 'pair:', len(pair_map))\n",
        "\n",
        "# Reuse simple memo maps from T1 if available; otherwise build quickly\n",
        "if 'memo_map' not in globals():\n",
        "    memo_map = tr.groupby('before', sort=False)['after'].agg(lambda s: s.value_counts().idxmax()).to_dict()\n",
        "if 'memo_map_lower' not in globals():\n",
        "    memo_map_lower = tr.assign(_bl=tr['before'].str.lower()).groupby('_bl', sort=False)['after'].agg(lambda s: s.value_counts().idxmax()).to_dict()\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "digit_words = {'0':'zero','1':'one','2':'two','3':'three','4':'four','5':'five','6':'six','7':'seven','8':'eight','9':'nine'}\n",
        "ones = ['zero','one','two','three','four','five','six','seven','eight','nine']\n",
        "teens = ['ten','eleven','twelve','thirteen','fourteen','fifteen','sixteen','seventeen','eighteen','nineteen']\n",
        "tens_words = ['', '', 'twenty','thirty','forty','fifty','sixty','seventy','eighty','ninety']\n",
        "\n",
        "def two_digit(n):\n",
        "    if n < 10: return ones[n]\n",
        "    if n < 20: return teens[n-10]\n",
        "    t, r = divmod(n, 10)\n",
        "    return tens_words[t] + ('' if r==0 else ' ' + ones[r])\n",
        "\n",
        "def three_digit(n):\n",
        "    h, r = divmod(n, 100)\n",
        "    if h == 0: return two_digit(r)\n",
        "    if r == 0: return ones[h] + ' hundred'\n",
        "    return ones[h] + ' hundred ' + two_digit(r)\n",
        "\n",
        "def int_to_words(num):\n",
        "    # supports up to billions comfortably\n",
        "    num = int(num)\n",
        "    if num == 0: return 'zero'\n",
        "    parts = []\n",
        "    billions, rem = divmod(num, 10**9)\n",
        "    millions, rem = divmod(rem, 10**6)\n",
        "    thousands, rem = divmod(rem, 1000)\n",
        "    if billions: parts.append(three_digit(billions) + ' billion')\n",
        "    if millions: parts.append(three_digit(millions) + ' million')\n",
        "    if thousands: parts.append(three_digit(thousands) + ' thousand')\n",
        "    if rem: parts.append(three_digit(rem))\n",
        "    return ' '.join(parts)\n",
        "\n",
        "def ordinal_word(n):\n",
        "    n = int(n)\n",
        "    special = {1:'first',2:'second',3:'third',4:'fourth',5:'fifth',6:'sixth',7:'seventh',8:'eighth',9:'ninth',10:'tenth',11:'eleventh',12:'twelfth',13:'thirteenth'}\n",
        "    if n in special: return special[n]\n",
        "    if n < 20: return two_digit(n) + 'th'\n",
        "    t, r = divmod(n, 10)\n",
        "    base = tens_words[t]\n",
        "    if r == 0: return base + 'ieth' if t in [2,3,4,5,8,9] else base + 'th'\n",
        "    return base + ' ' + special.get(r, ones[r] + 'th')\n",
        "\n",
        "def normalize_decimal(tok):\n",
        "    # 3.14 -> three point one four; 0.05 -> zero point zero five; .5 -> zero point five\n",
        "    if tok.count('.') != 1: return None\n",
        "    a,b = tok.split('.')\n",
        "    a = a.replace(',', '')\n",
        "    if a == '': a = '0'\n",
        "    if not re.fullmatch(r'-?\\d+', a) or not re.fullmatch(r'\\d+', b):\n",
        "        return None\n",
        "    sign = 'minus ' if a.startswith('-') else ''\n",
        "    if a.startswith('-'): a = a[1:]\n",
        "    left = int_to_words(int(a))\n",
        "    right = ' '.join(digit_words[ch] for ch in b)\n",
        "    return (sign + left + ' point ' + right).strip()\n",
        "\n",
        "month_names = {str(i).zfill(2): m for i,m in enumerate(['','january','february','march','april','may','june','july','august','september','october','november','december']) if i}\n",
        "month_names.update({str(i): m for i,m in enumerate(['','january','february','march','april','may','june','july','august','september','october','november','december']) if i})\n",
        "\n",
        "def year_to_words(y):\n",
        "    y = int(y)\n",
        "    if y < 1000: return int_to_words(y)\n",
        "    if 1900 <= y <= 1999:\n",
        "        h, t = divmod(y, 100)\n",
        "        return int_to_words(h) + ' ' + two_digit(t)\n",
        "    if 2000 <= y <= 2009:\n",
        "        return 'two thousand' if y == 2000 else 'two thousand ' + int_to_words(y-2000)\n",
        "    if 2010 <= y <= 2099:\n",
        "        return 'two thousand ' + int_to_words(y-2000)\n",
        "    return int_to_words(y)\n",
        "\n",
        "def normalize_date(tok):\n",
        "    # YYYY-MM-DD / MM/DD/YYYY / DD-MM-YYYY / M/D/YY / YYYY.MM.DD\n",
        "    if re.fullmatch(r'\\d{4}-\\d{1,2}-\\d{1,2}', tok):\n",
        "        y,m,d = tok.split('-')\n",
        "        mname = month_names.get(m.zfill(2))\n",
        "        if not mname: return None\n",
        "        return f\"{mname} {ordinal_word(int(d))} {year_to_words(y)}\"\n",
        "    if re.fullmatch(r'\\d{1,2}/\\d{1,2}/\\d{4}', tok):\n",
        "        m,d,y = tok.split('/')\n",
        "        mname = month_names.get(m.zfill(2))\n",
        "        if not mname: return None\n",
        "        return f\"{mname} {ordinal_word(int(d))} {year_to_words(y)}\"\n",
        "    if re.fullmatch(r'\\d{1,2}-\\d{1,2}-\\d{4}', tok):\n",
        "        d,m,y = tok.split('-')\n",
        "        mname = month_names.get(m.zfill(2))\n",
        "        if not mname: return None\n",
        "        return f\"{mname} {ordinal_word(int(d))} {year_to_words(y)}\"\n",
        "    if re.fullmatch(r'\\d{1,2}/\\d{1,2}/\\d{2}', tok):\n",
        "        m,d,yy = tok.split('/')\n",
        "        y = '20' + yy if int(yy) <= 30 else '19' + yy\n",
        "        mname = month_names.get(m.zfill(2))\n",
        "        if not mname: return None\n",
        "        return f\"{mname} {ordinal_word(int(d))} {year_to_words(y)}\"\n",
        "    if re.fullmatch(r'\\d{4}\\.\\d{1,2}\\.\\d{1,2}', tok):\n",
        "        y,m,d = tok.split('.')\n",
        "        mname = month_names.get(m.zfill(2))\n",
        "        if not mname: return None\n",
        "        return f\"{mname} {ordinal_word(int(d))} {year_to_words(y)}\"\n",
        "    return None\n",
        "\n",
        "def normalize_letters(tok):\n",
        "    # Avoid altering very short/common capitalized PLAIN tokens\n",
        "    if tok in {'I','A','AM','PM','US','U.S.','U.S','UK','U.K.','UK.'}:\n",
        "        return None\n",
        "    if re.fullmatch(r'[A-Z]{2,}', tok):\n",
        "        return ' '.join(list(tok.lower()))\n",
        "    if re.fullmatch(r'([A-Z]\\.){2,}[A-Z]?\\.?', tok):\n",
        "        return ' '.join(ch.lower() for ch in tok if ch.isalpha())\n",
        "    if 'AT&T' in tok:\n",
        "        return 'a t and t'\n",
        "    return None\n",
        "\n",
        "def normalize_digit(tok):\n",
        "    if not re.fullmatch(r'\\d+', tok): return None\n",
        "    return ' '.join(digit_words[ch] for ch in tok)\n",
        "\n",
        "def normalize_cardinal(tok):\n",
        "    s = tok.replace(',', '')\n",
        "    if not re.fullmatch(r'-?\\d+', s): return None\n",
        "    sign = 'minus ' if s.startswith('-') else ''\n",
        "    if s.startswith('-'): s = s[1:]\n",
        "    return (sign + int_to_words(int(s))).strip()\n",
        "\n",
        "def normalize_ordinal(tok):\n",
        "    m = re.fullmatch(r'(\\d+)(st|nd|rd|th)', tok)\n",
        "    if not m: return None\n",
        "    return ordinal_word(m.group(1))\n",
        "\n",
        "def normalize_fraction(tok):\n",
        "    m = re.fullmatch(r'(\\d+)/(\\d+)', tok)\n",
        "    if not m: return None\n",
        "    a,b = int(m.group(1)), int(m.group(2))\n",
        "    num = int_to_words(a)\n",
        "    den = ordinal_word(b)\n",
        "    if a == 1:\n",
        "        spec = {2:'half',4:'quarter'}\n",
        "        den_word = spec.get(b, den)\n",
        "        return den_word\n",
        "    else:\n",
        "        if den.endswith('f'): den = den[:-1] + 'ves'\n",
        "        elif den.endswith('y'): den = den[:-1] + 'ies'\n",
        "        else: den = den + 's'\n",
        "        return f\"{num} {den}\"\n",
        "\n",
        "unit_map = {\n",
        "    'ft': ('foot','feet'), 'in': ('inch','inches'), 'yd': ('yard','yards'), 'mi': ('mile','miles'),\n",
        "    'lb': ('pound','pounds'), 'lbs': ('pound','pounds'), 'oz': ('ounce','ounces'),\n",
        "    'kg': ('kilogram','kilograms'), 'g': ('gram','grams'), 'mg': ('milligram','milligrams'),\n",
        "    'km': ('kilometer','kilometers'), 'm': ('meter','meters'), 'cm': ('centimeter','centimeters'), 'mm': ('millimeter','millimeters'),\n",
        "    'l': ('liter','liters'), 'ml': ('milliliter','milliliters'),\n",
        "    'mph': ('miles per hour','miles per hour'), 'km/h': ('kilometers per hour','kilometers per hour'), 'kph': ('kilometers per hour','kilometers per hour'),\n",
        "    '%': ('percent','percent'), '\\u00b0c': ('degrees celsius','degrees celsius'), '\\u00b0f': ('degrees fahrenheit','degrees fahrenheit')\n",
        "}\n",
        "\n",
        "def normalize_measure(tok):\n",
        "    t = tok.lower()\n",
        "    m = re.fullmatch(r'(-?[\\d,]+(?:\\.\\d+)?)[\\s]*([a-z\\u00b0/%]+(?:/[a-z]+)?)', t)\n",
        "    if not m:\n",
        "        m = re.fullmatch(r'(-?[\\d,]+)([a-z\\u00b0/%]+)', t)\n",
        "    if not m: return None\n",
        "    num_s, unit = m.group(1), m.group(2)\n",
        "    unit = unit.strip()\n",
        "    singular_plural = unit_map.get(unit)\n",
        "    if not singular_plural:\n",
        "        return None\n",
        "    if '.' in num_s:\n",
        "        spoken_num = normalize_decimal(num_s)\n",
        "    else:\n",
        "        spoken_num = normalize_cardinal(num_s)\n",
        "    if spoken_num is None: return None\n",
        "    try:\n",
        "        val = float(num_s.replace(',', ''))\n",
        "    except:\n",
        "        val = None; \n",
        "    unit_word = singular_plural[0] if val == 1 else singular_plural[1]\n",
        "    return f\"{spoken_num} {unit_word}\"\n",
        "\n",
        "def normalize_money(tok):\n",
        "    m = re.fullmatch(r'([$\\u00a3\\u20ac])\\s*(\\d*[\\d,]*)(?:\\.(\\d{1,2}))?', tok)\n",
        "    if not m: return None\n",
        "    sym, a, c = m.groups()\n",
        "    a = (a or '').replace(',', '')\n",
        "    cur = {'$':'dollar','\\u00a3':'pound','\\u20ac':'euro'}[sym]\n",
        "    out = []\n",
        "    have_dollars = bool(a) and int(a) > 0\n",
        "    cents_val = None\n",
        "    if c is not None:\n",
        "        cents_val = int(c.ljust(2,'0'))\n",
        "    if have_dollars:\n",
        "        out.append(int_to_words(int(a)) + (' ' + cur + ('s' if int(a)!=1 else '')))\n",
        "    if cents_val is not None and cents_val > 0:\n",
        "        out.append(int_to_words(cents_val) + (' cent' + ('s' if cents_val!=1 else '')))\n",
        "    if not out and (cents_val is not None and cents_val > 0):\n",
        "        return int_to_words(cents_val) + (' cent' + ('s' if cents_val!=1 else ''))\n",
        "    if not out:\n",
        "        return None\n",
        "    return ' and '.join(out) if len(out) == 2 else out[0]\n",
        "\n",
        "def normalize_time(tok):\n",
        "    m = re.fullmatch(r'(\\d{1,2}):(\\d{2})', tok)\n",
        "    if not m: return None\n",
        "    hh = int(m.group(1)); mm = int(m.group(2))\n",
        "    hour = int_to_words(hh)\n",
        "    if mm == 0: return hour + \" o'clock\"\n",
        "    if 1 <= mm <= 9:\n",
        "        return hour + ' oh ' + int_to_words(mm)\n",
        "    return hour + ' ' + int_to_words(mm)\n",
        "\n",
        "def normalize_telephone(tok):\n",
        "    t = tok\n",
        "    digits = re.sub(r'\\D', '', t)\n",
        "    if not digits: return None\n",
        "    spoken = ' '.join(digit_words[ch] for ch in digits)\n",
        "    return spoken\n",
        "\n",
        "def normalize_electronic(tok):\n",
        "    t = tok.strip()\n",
        "    # emails: require at least one dot after '@'\n",
        "    if '@' in t:\n",
        "        at_idx = t.find('@')\n",
        "        if '.' in t[at_idx+1:]:\n",
        "            parts = re.split(r'(@|\\.)', t)\n",
        "            spoken = []\n",
        "            for p in parts:\n",
        "                if p == '@': spoken.append('at')\n",
        "                elif p == '.': spoken.append('dot')\n",
        "                else: spoken.append(p)\n",
        "            return ' '.join(spoken).replace('  ', ' ').strip()\n",
        "    # urls/domains: require slash with letters/digits or www. prefix\n",
        "    if t.lower().startswith('www.') or re.search(r'/[A-Za-z0-9]', t):\n",
        "        rep = t.replace('.', ' dot ').replace('/', ' slash ').replace('-', ' dash ')\n",
        "        rep = re.sub(r'\\bwww\\b', 'w w w', rep)\n",
        "        return ' '.join(rep.split())\n",
        "    return None\n",
        "\n",
        "# Lightweight detectors (used only if class not in pair maps)\n",
        "pat_decimal = re.compile(r'^-?[\\d,]+\\.\\d+$')\n",
        "pat_cardinal = re.compile(r'^-?[\\d,]+$')\n",
        "pat_ordinal = re.compile(r'^\\d+(st|nd|rd|th)$')\n",
        "pat_fraction = re.compile(r'^\\d+/\\d+$')\n",
        "pat_date1 = re.compile(r'^\\d{4}-\\d{1,2}-\\d{1,2}$')\n",
        "pat_date2 = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{4}$')\n",
        "pat_date3 = re.compile(r'^\\d{1,2}-\\d{1,2}-\\d{4}$')\n",
        "pat_date4 = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2}$')\n",
        "pat_date5 = re.compile(r'^\\d{4}\\.\\d{1,2}\\.\\d{1,2}$')\n",
        "pat_letters = re.compile(r'^[A-Z]{2,}$')\n",
        "pat_phone = re.compile(r'^[+\\d][\\d\\s().-]{5,}$')\n",
        "pat_money = re.compile(r'^[$\\u00a3\\u20ac]')\n",
        "pat_time = re.compile(r'^\\d{1,2}:\\d{2}$')\n",
        "pat_measure = re.compile(r'^-?[\\d,]+(?:\\.\\d+)?\\s?[a-zA-Z\\u00b0/%]+')\n",
        "\n",
        "def infer_class(tok, prior=None):\n",
        "    if prior: return prior\n",
        "    # High-certainty symbols first\n",
        "    if pat_money.search(tok): return 'MONEY'\n",
        "    if pat_time.fullmatch(tok): return 'TIME'\n",
        "    if pat_fraction.fullmatch(tok): return 'FRACTION'\n",
        "    if pat_decimal.fullmatch(tok) or tok.startswith('.') and re.fullmatch(r'\\.\\d+', tok): return 'DECIMAL'\n",
        "    if pat_ordinal.fullmatch(tok): return 'ORDINAL'\n",
        "    if pat_cardinal.fullmatch(tok): return 'CARDINAL'\n",
        "    # Dates\n",
        "    if pat_date1.fullmatch(tok) or pat_date2.fullmatch(tok) or pat_date3.fullmatch(tok) or pat_date4.fullmatch(tok) or pat_date5.fullmatch(tok): return 'DATE'\n",
        "    # LETTERS guarded by prior elsewhere; still detect pattern\n",
        "    if pat_letters.fullmatch(tok): return 'LETTERS'\n",
        "    # Telephone\n",
        "    if pat_phone.fullmatch(tok): return 'TELEPHONE'\n",
        "    # Electronic tightened\n",
        "    t = tok.strip()\n",
        "    if ('@' in t and '.' in t[t.find('@')+1:]) or t.lower().startswith('www.') or re.search(r'/[A-Za-z0-9]', t):\n",
        "        return 'ELECTRONIC'\n",
        "    if pat_measure.match(tok.lower()): return 'MEASURE'\n",
        "    return None\n",
        "\n",
        "def apply_rules(tok, cls):\n",
        "    if cls == 'DATE':\n",
        "        return normalize_date(tok)\n",
        "    if cls == 'LETTERS':\n",
        "        return normalize_letters(tok)\n",
        "    if cls == 'DECIMAL':\n",
        "        return normalize_decimal(tok)\n",
        "    if cls == 'CARDINAL':\n",
        "        return normalize_cardinal(tok)\n",
        "    if cls == 'ORDINAL':\n",
        "        return normalize_ordinal(tok)\n",
        "    if cls == 'DIGIT':\n",
        "        return normalize_digit(tok)\n",
        "    if cls == 'MEASURE':\n",
        "        return normalize_measure(tok)\n",
        "    if cls == 'TELEPHONE':\n",
        "        return normalize_telephone(tok)\n",
        "    if cls == 'ELECTRONIC':\n",
        "        return normalize_electronic(tok)\n",
        "    if cls == 'MONEY':\n",
        "        return normalize_money(tok)\n",
        "    if cls == 'TIME':\n",
        "        return normalize_time(tok)\n",
        "    if cls == 'FRACTION':\n",
        "        return normalize_fraction(tok)\n",
        "    return None\n",
        "\n",
        "def cascade_predict_tokens(df_before_series):\n",
        "    bef = df_before_series.fillna('')\n",
        "    # Predict prior class for all tokens\n",
        "    prior = bef.map(cls_map)\n",
        "    need_lower = prior.isna()\n",
        "    if need_lower.any():\n",
        "        prior.loc[need_lower] = bef.loc[need_lower].str.lower().map(cls_map_lower)\n",
        "\n",
        "    # Stage 1: class-aware exact memo via pair maps; fallback to token memo\n",
        "    pred = pd.Series(index=bef.index, dtype=object)\n",
        "    # exact (before, prior) where prior is known\n",
        "    has_prior = prior.notna()\n",
        "    if has_prior.any():\n",
        "        idxp = prior.index[has_prior]\n",
        "        keys = list(zip(bef.loc[idxp].tolist(), prior.loc[idxp].tolist()))\n",
        "        pred.loc[idxp] = [pair_map.get(k) for k in keys]\n",
        "        # lower pair for remaining with prior\n",
        "        rem = pred.loc[idxp].isna()\n",
        "        if rem.any():\n",
        "            idxp2 = idxp[rem.values]\n",
        "            keys2 = list(zip(bef.loc[idxp2].str.lower().tolist(), prior.loc[idxp2].tolist()))\n",
        "            pred.loc[idxp2] = [pair_map_lower.get(k) for k in keys2]\n",
        "    # fill remaining with simple memo\n",
        "    miss = pred.isna()\n",
        "    if miss.any():\n",
        "        pred.loc[miss] = bef.loc[miss].map(memo_map)\n",
        "\n",
        "    # Stage 2: class-guarded rules (skip for predicted PLAIN/PUNCT)\n",
        "    miss2 = pred.isna()\n",
        "    if miss2.any():\n",
        "        idx2 = pred.index[miss2]\n",
        "        toks = bef.loc[idx2]\n",
        "        prs = prior.loc[idx2]\n",
        "        outs = []\n",
        "        for tk, pr in zip(toks.tolist(), prs.tolist()):\n",
        "            if pr in ('PLAIN','PUNCT'):\n",
        "                outs.append(None)\n",
        "            else:\n",
        "                ic = infer_class(tk, pr)\n",
        "                outs.append(apply_rules(tk, ic) if ic not in ('PLAIN','PUNCT', None) else None)\n",
        "        pred.loc[idx2] = outs\n",
        "\n",
        "    # Stage 3: lowercase memo\n",
        "    miss3 = pred.isna()\n",
        "    if miss3.any():\n",
        "        pred.loc[miss3] = bef.loc[miss3].str.lower().map(memo_map_lower)\n",
        "\n",
        "    # Stage 4: identity (and safeguard for predicted PLAIN/PUNCT to avoid unwanted changes)\n",
        "    miss4 = pred.isna()\n",
        "    if miss4.any():\n",
        "        pred.loc[miss4] = bef.loc[miss4]\n",
        "    if prior.notna().any():\n",
        "        mask_plain = prior.isin(['PLAIN','PUNCT']) & (pred != bef)\n",
        "        if mask_plain.any():\n",
        "            pred.loc[mask_plain] = bef.loc[mask_plain]\n",
        "    return pred\n",
        "\n",
        "# ----- Local CV using same cascade -----\n",
        "print('Running local CV (single split) with cascade ...')\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=123)\n",
        "groups = train['sentence_id'].values\n",
        "tr_idx, va_idx = next(gss.split(train, groups=groups))\n",
        "va = train.iloc[va_idx].copy()\n",
        "pred_va = cascade_predict_tokens(va['before'])\n",
        "cv_acc = (pred_va.values == va['after'].values).mean()\n",
        "print(f'Cascade CV accuracy: {cv_acc:.6f}')\n",
        "err_mask = pred_va.values != va['after'].values\n",
        "print('Errors remaining:', int(err_mask.sum()))\n",
        "if 'class' in va.columns:\n",
        "    print('Top error classes after rules:')\n",
        "    print(va.loc[err_mask, 'class'].value_counts().head(15))\n",
        "\n",
        "# ----- Inference on test with cascade -----\n",
        "print('Inferring on test with cascade ...')\n",
        "pred_test = cascade_predict_tokens(test['before'])\n",
        "test_ids = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)\n",
        "submission2 = pd.DataFrame({'id': test_ids, 'after': pred_test.astype(str)})\n",
        "submission2.to_csv('submission.csv', index=False)\n",
        "print('Wrote improved submission.csv')\n",
        "print('T2 total elapsed:', f'{time.time()-t0:.2f}s')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "id": "9842d514-b29f-4295-9e6c-1f4ec8a7aba8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T2b: Full-train guarded cascade with confidence-gated maps (mirrors leak-free CV logic) to generate submission\n",
        "import time, re\n",
        "t0 = time.time()\n",
        "\n",
        "SAFE_RULE_CLASSES = {'CARDINAL','DECIMAL','ORDINAL','DIGIT','FRACTION','MONEY','MEASURE','TIME','DATE'}\n",
        "GUARDED_RULE_CLASSES = {'LETTERS','ELECTRONIC','TELEPHONE','VERBATIM'}\n",
        "\n",
        "def is_word_like(tok: str) -> bool:\n",
        "    if not isinstance(tok, str):\n",
        "        return False\n",
        "    has_lower = any(c.isalpha() and c.islower() for c in tok)\n",
        "    has_upper = any(c.isalpha() and c.isupper() for c in tok)\n",
        "    return bool(tok.istitle() or (has_lower and has_upper) or has_lower)\n",
        "\n",
        "def count_digits(tok: str) -> int:\n",
        "    return sum(ch.isdigit() for ch in tok) if isinstance(tok, str) else 0\n",
        "\n",
        "# Strong LETTERS heuristic\n",
        "LETTERS_EXCEPT = {'I','A','AM','PM','US','U.S.','U.S','UK','U.K.','UK.','EU','U.N.','UN','TV','OK','OK.','AI','ML','CEO'}\n",
        "def is_strong_letters(tok: str) -> bool:\n",
        "    if tok in LETTERS_EXCEPT: return False\n",
        "    if re.fullmatch(r'[A-Z]{3,}', tok): return True\n",
        "    if re.fullmatch(r'([A-Z]\\.){3,}[A-Z]?\\.?', tok): return True\n",
        "    if re.fullmatch(r'[A-Z]+-[A-Z]+', tok): return True\n",
        "    return False\n",
        "\n",
        "# Safety: robust number/ordinal helpers mirroring Cell 6\n",
        "SCALE_WORDS = ['', 'thousand', 'million', 'billion', 'trillion', 'quadrillion', 'quintillion']\n",
        "ONES_WORDS = ['zero','one','two','three','four','five','six','seven','eight','nine']\n",
        "TEENS_WORDS = ['ten','eleven','twelve','thirteen','fourteen','fifteen','sixteen','seventeen','eighteen','nineteen']\n",
        "TENS_WORDS = ['', '', 'twenty','thirty','forty','fifty','sixty','seventy','eighty','ninety']\n",
        "\n",
        "def _two_digit_safe(n:int) -> str:\n",
        "    if n < 10: return ONES_WORDS[n]\n",
        "    if n < 20: return TEENS_WORDS[n-10]\n",
        "    t, r = divmod(n, 10)\n",
        "    return TENS_WORDS[t] + ('' if r==0 else ' ' + ONES_WORDS[r])\n",
        "\n",
        "def _three_digit_safe(n:int) -> str:\n",
        "    h, r = divmod(n, 100)\n",
        "    if h == 0: return _two_digit_safe(r) if r else ''\n",
        "    if r == 0: return ONES_WORDS[h] + ' hundred'\n",
        "    return ONES_WORDS[h] + ' hundred ' + _two_digit_safe(r)\n",
        "\n",
        "def int_to_words_safe(num_in) -> str:\n",
        "    s = str(num_in).replace(',', '')\n",
        "    sign = ''\n",
        "    if s.startswith('-'): sign, s = 'minus ', s[1:]\n",
        "    if not s.isdigit():\n",
        "        try:\n",
        "            s = str(int(float(s)))\n",
        "        except:\n",
        "            return ''\n",
        "    n = int(s)\n",
        "    if n == 0: return 'zero'\n",
        "    parts = []\n",
        "    idx = 0\n",
        "    while n > 0 and idx < len(SCALE_WORDS):\n",
        "        n, chunk = divmod(n, 1000)\n",
        "        if chunk:\n",
        "            words = _three_digit_safe(chunk)\n",
        "            if SCALE_WORDS[idx]:\n",
        "                parts.append(words + ' ' + SCALE_WORDS[idx])\n",
        "            else:\n",
        "                parts.append(words)\n",
        "        idx += 1\n",
        "    return (sign + ' '.join(reversed([p for p in parts if p]))).strip()\n",
        "\n",
        "def ordinal_word_safe(n):\n",
        "    try:\n",
        "        n = int(n)\n",
        "    except:\n",
        "        return None\n",
        "    try:\n",
        "        return _ordinalize_words(int_to_words(n))\n",
        "    except NameError:\n",
        "        words = int_to_words_safe(n)\n",
        "        ws = words.split()\n",
        "        if not ws: return words\n",
        "        last = ws[-1]\n",
        "        if last.endswith('y'):\n",
        "            ws[-1] = last[:-1] + 'ieth'\n",
        "        elif last in ('one','two','three','five','eight','nine','twelve'):\n",
        "            repl = {'one':'first','two':'second','three':'third','five':'fifth','eight':'eighth','nine':'ninth','twelve':'twelfth'}\n",
        "            ws[-1] = repl[last]\n",
        "        else:\n",
        "            ws[-1] = last + 'th'\n",
        "        return ' '.join(ws)\n",
        "\n",
        "def normalize_fraction_safe(tok):\n",
        "    m = re.fullmatch(r'(\\d+)/(\\d+)', tok or '')\n",
        "    if not m: return None\n",
        "    a,b = int(m.group(1)), int(m.group(2))\n",
        "    num = int_to_words_safe(a)\n",
        "    den = ordinal_word_safe(b)\n",
        "    if a == 1:\n",
        "        spec = {2:'half',4:'quarter'}\n",
        "        den_word = spec.get(b, den)\n",
        "        return den_word\n",
        "    else:\n",
        "        if den.endswith('f'): den = den[:-1] + 'ves'\n",
        "        elif den.endswith('y'): den = den[:-1] + 'ies'\n",
        "        else: den = den + 's'\n",
        "        return f\"{num} {den}\"\n",
        "\n",
        "def normalize_cardinal_safe(tok):\n",
        "    s = str(tok).replace(',', '')\n",
        "    sign = 'minus ' if s.startswith('-') else ''\n",
        "    if s.startswith('-'): s = s[1:]\n",
        "    if not re.fullmatch(r'\\d+', s):\n",
        "        return None\n",
        "    return (sign + int_to_words_safe(int(s))).strip()\n",
        "\n",
        "def normalize_decimal_safe(tok):\n",
        "    if tok.count('.') != 1: return None\n",
        "    a,b = tok.split('.')\n",
        "    a = a.replace(',', '')\n",
        "    if a == '': a = '0'\n",
        "    if not re.fullmatch(r'-?\\d+', a) or not re.fullmatch(r'\\d+', b):\n",
        "        return None\n",
        "    sign = 'minus ' if a.startswith('-') else ''\n",
        "    if a.startswith('-'): a = a[1:]\n",
        "    left = int_to_words_safe(int(a))\n",
        "    right = ' '.join({'0':'zero','1':'one','2':'two','3':'three','4':'four','5':'five','6':'six','7':'seven','8':'eight','9':'nine'}[ch] for ch in b)\n",
        "    return (sign + left + ' point ' + right).strip()\n",
        "\n",
        "def apply_rules_safe(tok, cls):\n",
        "    if cls == 'FRACTION':\n",
        "        return normalize_fraction_safe(tok)\n",
        "    if cls == 'CARDINAL':\n",
        "        return normalize_cardinal_safe(tok)\n",
        "    if cls == 'ORDINAL':\n",
        "        m = re.fullmatch(r'(\\d+)(st|nd|rd|th)', tok or '')\n",
        "        return ordinal_word_safe(m.group(1)) if m else None\n",
        "    if cls == 'DECIMAL':\n",
        "        return normalize_decimal_safe(tok)\n",
        "    return apply_rules(tok, cls)\n",
        "\n",
        "def build_maps(df):\n",
        "    d = df.dropna(subset=['before','after','class']).copy()\n",
        "    # cls maps with confidence\n",
        "    grp_cls = d.groupby(['before','class']).size().rename('cnt').reset_index()\n",
        "    tot_cls = grp_cls.groupby('before')['cnt'].sum().rename('tot')\n",
        "    top_cls = grp_cls.sort_values(['before','cnt'], ascending=[True, False]).drop_duplicates('before')\n",
        "    top_cls = top_cls.merge(tot_cls, on='before', how='left')\n",
        "    top_cls['conf'] = top_cls['cnt'] / top_cls['tot']\n",
        "    cls_map_loc = dict(zip(top_cls['before'], top_cls['class']))\n",
        "    cls_conf_map_loc = dict(zip(top_cls['before'], top_cls['conf']))\n",
        "    d['_bl'] = d['before'].str.lower()\n",
        "    grp_cls_l = d.groupby(['_bl','class']).size().rename('cnt').reset_index()\n",
        "    tot_cls_l = grp_cls_l.groupby('_bl')['cnt'].sum().rename('tot')\n",
        "    top_cls_l = grp_cls_l.sort_values(['_bl','cnt'], ascending=[True, False]).drop_duplicates('_bl')\n",
        "    top_cls_l = top_cls_l.merge(tot_cls_l, on='_bl', how='left')\n",
        "    top_cls_l['conf'] = top_cls_l['cnt'] / top_cls_l['tot']\n",
        "    cls_map_lower_loc = dict(zip(top_cls_l['_bl'], top_cls_l['class']))\n",
        "    cls_conf_map_lower_loc = dict(zip(top_cls_l['_bl'], top_cls_l['conf']))\n",
        "\n",
        "    # (before, class) -> after\n",
        "    pair_map_loc = d.groupby(['before','class'], sort=False)['after'].agg(lambda s: s.value_counts().idxmax()).to_dict()\n",
        "    pair_map_lower_loc = d.groupby(['_bl','class'], sort=False)['after'].agg(lambda s: s.value_counts().idxmax()).to_dict()\n",
        "\n",
        "    # memo maps with confidence\n",
        "    grp_memo = d.groupby(['before','after']).size().rename('cnt').reset_index()\n",
        "    tot_memo = grp_memo.groupby('before')['cnt'].sum().rename('tot')\n",
        "    top_memo = grp_memo.sort_values(['before','cnt'], ascending=[True, False]).drop_duplicates('before')\n",
        "    top_memo = top_memo.merge(tot_memo, on='before', how='left')\n",
        "    top_memo['conf'] = top_memo['cnt'] / top_memo['tot']\n",
        "    memo_map_loc = dict(zip(top_memo['before'], top_memo['after']))\n",
        "    memo_conf_loc = dict(zip(top_memo['before'], top_memo['conf']))\n",
        "\n",
        "    grp_memo_l = d.groupby(['_bl','after']).size().rename('cnt').reset_index()\n",
        "    tot_memo_l = grp_memo_l.groupby('_bl')['cnt'].sum().rename('tot')\n",
        "    top_memo_l = grp_memo_l.sort_values(['_bl','cnt'], ascending=[True, False]).drop_duplicates('_bl')\n",
        "    top_memo_l = top_memo_l.merge(tot_memo_l, on='_bl', how='left')\n",
        "    top_memo_l['conf'] = top_memo_l['cnt'] / top_memo_l['tot']\n",
        "    memo_map_lower_loc = dict(zip(top_memo_l['_bl'], top_memo_l['after']))\n",
        "    memo_conf_lower_loc = dict(zip(top_memo_l['_bl'], top_memo_l['conf']))\n",
        "\n",
        "    return (cls_map_loc, cls_map_lower_loc, pair_map_loc, pair_map_lower_loc, memo_map_loc, memo_map_lower_loc,\n",
        "            cls_conf_map_loc, cls_conf_map_lower_loc, memo_conf_loc, memo_conf_lower_loc)\n",
        "\n",
        "def cascade_predict_tokens_with_maps(bef_series, maps, memo_conf_thr=0.70, cls_conf_thr=0.75):\n",
        "    # Strategic order: pair_map -> guarded rules -> memo -> lower_memo -> identity -> final veto\n",
        "    (cls_map_loc, cls_map_lower_loc, pair_map_loc, pair_map_lower_loc, memo_map_loc, memo_map_lower_loc,\n",
        "     cls_conf_map_loc, cls_conf_map_lower_loc, memo_conf_loc, memo_conf_lower_loc) = maps\n",
        "    bef = bef_series.fillna('')\n",
        "\n",
        "    # Prior class and confidence\n",
        "    prior = bef.map(cls_map_loc)\n",
        "    prior_conf = bef.map(cls_conf_map_loc)\n",
        "    need_lower = prior.isna()\n",
        "    if need_lower.any():\n",
        "        bef_l = bef.loc[need_lower].str.lower()\n",
        "        prior.loc[need_lower] = bef_l.map(cls_map_lower_loc)\n",
        "        prior_conf.loc[need_lower] = bef_l.map(cls_conf_map_lower_loc)\n",
        "\n",
        "    # Stage 1: pair maps (gate by prior_conf >= 0.85)\n",
        "    pred = pd.Series(index=bef.index, dtype=object)\n",
        "    has_prior = prior.notna()\n",
        "    if has_prior.any():\n",
        "        conf_ok = prior_conf.fillna(0.0) >= 0.85\n",
        "        idxp = prior.index[has_prior & conf_ok]\n",
        "        if len(idxp) > 0:\n",
        "            keys = list(zip(bef.loc[idxp].tolist(), prior.loc[idxp].tolist()))\n",
        "            pred.loc[idxp] = [pair_map_loc.get(k) for k in keys]\n",
        "            rem = pred.loc[idxp].isna()\n",
        "            if rem.any():\n",
        "                idxp2 = idxp[rem.values]\n",
        "                keys2 = list(zip(bef.loc[idxp2].str.lower().tolist(), prior.loc[idxp2].tolist()))\n",
        "                pred.loc[idxp2] = [pair_map_lower_loc.get(k) for k in keys2]\n",
        "\n",
        "    # Stage 2: guarded rules with SAFE passthrough and relaxed LETTERS\n",
        "    miss2 = pred.isna()\n",
        "    if miss2.any():\n",
        "        idx2 = pred.index[miss2]\n",
        "        toks = bef.loc[idx2].tolist()\n",
        "        prs = prior.loc[idx2].tolist()\n",
        "        prc = prior_conf.loc[idx2].fillna(0.0).tolist()\n",
        "        outs = []\n",
        "        for tk, pr, pc in zip(toks, prs, prc):\n",
        "            ic = infer_class(tk)  # infer independently\n",
        "            # If prior is PLAIN/PUNCT, only skip for non-SAFE inferred classes\n",
        "            if pr in ('PLAIN','PUNCT') and (ic not in SAFE_RULE_CLASSES):\n",
        "                outs.append(None); continue\n",
        "            # Special allowance: strong LETTERS acronyms (non word-like) may fire regardless of prior/pc\n",
        "            special_letters = (ic == 'LETTERS' and is_strong_letters(tk) and not is_word_like(tk))\n",
        "            # Guarded classes gating (unless special_letters)\n",
        "            if ic in GUARDED_RULE_CLASSES and not special_letters:\n",
        "                # word-like veto for guarded\n",
        "                if is_word_like(tk):\n",
        "                    outs.append(None); continue\n",
        "                # confidence gating for guarded\n",
        "                if pc < cls_conf_thr:\n",
        "                    if ic == 'ELECTRONIC':\n",
        "                        t = tk.strip()\n",
        "                        cond_email = (t.count('@') == 1 and '.' in t[t.find('@')+1:])\n",
        "                        cond_web = t.lower().startswith('www.') or re.search(r'/[A-Za-z0-9]', t)\n",
        "                        if not (cond_email or cond_web):\n",
        "                            outs.append(None); continue\n",
        "                    if ic == 'TELEPHONE' and count_digits(tk) < 7:\n",
        "                        outs.append(None); continue\n",
        "            # apply rules\n",
        "            if ic in SAFE_RULE_CLASSES:\n",
        "                outs.append(apply_rules_safe(tk, ic))\n",
        "            elif special_letters:\n",
        "                outs.append(apply_rules_safe(tk, ic))\n",
        "            elif ic in GUARDED_RULE_CLASSES and pr == ic:\n",
        "                outs.append(apply_rules_safe(tk, ic))\n",
        "            else:\n",
        "                outs.append(None)\n",
        "        pred.loc[idx2] = outs\n",
        "\n",
        "    # Stage 3: memo with tighter, class-aware gating\n",
        "    miss3 = pred.isna()\n",
        "    if miss3.any():\n",
        "        idx3 = pred.index[miss3]\n",
        "        outs3 = []\n",
        "        for i in idx3:\n",
        "            tk = bef.loc[i]\n",
        "            pr = prior.loc[i]\n",
        "            pc = float(prior_conf.loc[i]) if prior_conf.loc[i] == prior_conf.loc[i] else 0.0\n",
        "            if pr in ('PLAIN','PUNCT'):\n",
        "                outs3.append(None); continue\n",
        "            cand = memo_map_loc.get(tk)\n",
        "            conf = float(memo_conf_loc.get(tk, 0.0))\n",
        "            thr = 0.95 if pr in GUARDED_RULE_CLASSES else memo_conf_thr\n",
        "            if pr in GUARDED_RULE_CLASSES and is_word_like(tk):\n",
        "                outs3.append(None); continue\n",
        "            if cand is not None and conf >= thr and pc >= cls_conf_thr:\n",
        "                outs3.append(cand)\n",
        "            else:\n",
        "                outs3.append(None)\n",
        "        pred.loc[idx3] = outs3\n",
        "\n",
        "    # Stage 4: lowercase memo with tighter, class-aware gating\n",
        "    miss4 = pred.isna()\n",
        "    if miss4.any():\n",
        "        idx4 = pred.index[miss4]\n",
        "        outs4 = []\n",
        "        for i in idx4:\n",
        "            tk = bef.loc[i]\n",
        "            pr = prior.loc[i]\n",
        "            pc = float(prior_conf.loc[i]) if prior_conf.loc[i] == prior_conf.loc[i] else 0.0\n",
        "            if pr in ('PLAIN','PUNCT'):\n",
        "                outs4.append(None); continue\n",
        "            tkl = str(tk).lower()\n",
        "            cand2 = memo_map_lower_loc.get(tkl)\n",
        "            conf2 = float(memo_conf_lower_loc.get(tkl, 0.0))\n",
        "            thr2 = 0.96 if pr in GUARDED_RULE_CLASSES else memo_conf_thr\n",
        "            if pr in GUARDED_RULE_CLASSES and is_word_like(tk):\n",
        "                outs4.append(None); continue\n",
        "            if cand2 is not None and conf2 >= thr2 and pc >= cls_conf_thr:\n",
        "                outs4.append(cand2)\n",
        "            else:\n",
        "                outs4.append(None)\n",
        "        pred.loc[idx4] = outs4\n",
        "\n",
        "    # Stage 5: identity\n",
        "    miss5 = pred.isna()\n",
        "    if miss5.any():\n",
        "        pred.loc[miss5] = bef.loc[miss5]\n",
        "\n",
        "    # Stage 6: final vetoes - revert casing-only changes on word-like tokens\n",
        "    try:\n",
        "        pred_l = pred.fillna('').astype(str).str.lower()\n",
        "        bef_l = bef.fillna('').astype(str).str.lower()\n",
        "        casing_only = (pred_l == bef_l) & (pred != bef) & bef.apply(is_word_like)\n",
        "        if casing_only.any():\n",
        "            pred.loc[casing_only] = bef.loc[casing_only]\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return pred\n",
        "\n",
        "print('Building full-train maps with confidence ...')\n",
        "tm = time.time()\n",
        "maps_full = build_maps(train)\n",
        "print('Maps built in', f'{time.time()-tm:.2f}s')\n",
        "\n",
        "print('Inferring on test with guarded, confidence-gated cascade ...')\n",
        "tp = time.time()\n",
        "pred_test = cascade_predict_tokens_with_maps(test['before'], maps_full, memo_conf_thr=0.70, cls_conf_thr=0.75)\n",
        "print('Test inference elapsed:', f'{time.time()-tp:.2f}s')\n",
        "\n",
        "test_ids = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)\n",
        "submission_v1 = pd.DataFrame({'id': test_ids, 'after': pred_test.astype(str)})\n",
        "submission_v1.to_csv('submission.csv', index=False)\n",
        "print('Wrote submission.csv (guarded + confidence-gated, full-train maps)')\n",
        "print('T2b elapsed:', f'{time.time()-t0:.2f}s')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building full-train maps with confidence ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maps built in 206.68s\nInferring on test with guarded, confidence-gated cascade ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test inference elapsed: 6.89s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv (guarded + confidence-gated, full-train maps)\nT2b elapsed: 215.11s\n"
          ]
        }
      ]
    },
    {
      "id": "77e88148-6862-4e99-92ca-c525656e20f0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T3: Leak-free CV (train-split maps) + confidence gating + strict guarding + final veto\n",
        "import time, re\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "SAFE_RULE_CLASSES = {'CARDINAL','DECIMAL','ORDINAL','DIGIT','FRACTION','MONEY','MEASURE','TIME','DATE'}\n",
        "GUARDED_RULE_CLASSES = {'LETTERS','ELECTRONIC','TELEPHONE','VERBATIM'}\n",
        "\n",
        "def is_word_like(tok: str) -> bool:\n",
        "    if not isinstance(tok, str):\n",
        "        return False\n",
        "    has_lower = any(c.isalpha() and c.islower() for c in tok)\n",
        "    has_upper = any(c.isalpha() and c.isupper() for c in tok)\n",
        "    return bool(tok.istitle() or (has_lower and has_upper) or has_lower)\n",
        "\n",
        "def count_digits(tok: str) -> int:\n",
        "    return sum(ch.isdigit() for ch in tok) if isinstance(tok, str) else 0\n",
        "\n",
        "# Adjust LETTERS output to lowercase spaced letters\n",
        "def normalize_letters(tok):\n",
        "    if re.fullmatch(r'[A-Z]{2,}', tok):\n",
        "        return ' '.join(list(tok.lower()))\n",
        "    if re.fullmatch(r'([A-Z]\\.){2,}[A-Z]?\\.?', tok):\n",
        "        return ' '.join(ch.lower() for ch in tok if ch.isalpha())\n",
        "    if 'AT&T' in tok:\n",
        "        return 'A T and T'.lower()\n",
        "    return None\n",
        "\n",
        "# Strong LETTERS heuristic\n",
        "LETTERS_EXCEPT = {'I','A','AM','PM','US','U.S.','U.S','UK','U.K.','UK.','EU','U.N.','UN','TV','OK','OK.','AI','ML'}\n",
        "def is_strong_letters(tok):\n",
        "    if tok in LETTERS_EXCEPT: return False\n",
        "    if re.fullmatch(r'[A-Z]{3,}', tok): return True\n",
        "    if re.fullmatch(r'([A-Z]\\.){3,}[A-Z]?\\.?', tok): return True\n",
        "    return False\n",
        "\n",
        "# month name helpers for context and date normalization reused from support cell 9\n",
        "months_full = ['january','february','march','april','may','june','july','august','september','october','november','december']\n",
        "months_abbr = ['jan','feb','mar','apr','may','jun','jul','aug','sep','sept','oct','nov','dec','jan.','feb.','mar.','apr.','jun.','jul.','aug.','sep.','sept.','oct.','nov.','dec.']\n",
        "month_set = set(months_full + months_abbr)\n",
        "\n",
        "# Safety: robust number-to-words (handles large numbers by 3-digit groups) and safe ordinal/fraction\n",
        "SCALE_WORDS = ['', 'thousand', 'million', 'billion', 'trillion', 'quadrillion', 'quintillion']\n",
        "ONES_WORDS = ['zero','one','two','three','four','five','six','seven','eight','nine']\n",
        "TEENS_WORDS = ['ten','eleven','twelve','thirteen','fourteen','fifteen','sixteen','seventeen','eighteen','nineteen']\n",
        "TENS_WORDS = ['', '', 'twenty','thirty','forty','fifty','sixty','seventy','eighty','ninety']\n",
        "\n",
        "def _two_digit_safe(n:int) -> str:\n",
        "    if n < 10: return ONES_WORDS[n]\n",
        "    if n < 20: return TEENS_WORDS[n-10]\n",
        "    t, r = divmod(n, 10)\n",
        "    return TENS_WORDS[t] + ('' if r==0 else ' ' + ONES_WORDS[r])\n",
        "\n",
        "def _three_digit_safe(n:int) -> str:\n",
        "    h, r = divmod(n, 100)\n",
        "    if h == 0: return _two_digit_safe(r) if r else ''\n",
        "    if r == 0: return ONES_WORDS[h] + ' hundred'\n",
        "    return ONES_WORDS[h] + ' hundred ' + _two_digit_safe(r)\n",
        "\n",
        "def int_to_words_safe(num_in) -> str:\n",
        "    s = str(num_in).replace(',', '')\n",
        "    sign = ''\n",
        "    if s.startswith('-'): sign, s = 'minus ', s[1:]\n",
        "    if not s.isdigit():\n",
        "        try:\n",
        "            s = str(int(float(s)))\n",
        "        except:\n",
        "            return ''\n",
        "    n = int(s)\n",
        "    if n == 0: return 'zero'\n",
        "    parts = []\n",
        "    idx = 0\n",
        "    while n > 0 and idx < len(SCALE_WORDS):\n",
        "        n, chunk = divmod(n, 1000)\n",
        "        if chunk:\n",
        "            words = _three_digit_safe(chunk)\n",
        "            if SCALE_WORDS[idx]:\n",
        "                parts.append(words + ' ' + SCALE_WORDS[idx])\n",
        "            else:\n",
        "                parts.append(words)\n",
        "        idx += 1\n",
        "    return (sign + ' '.join(reversed([p for p in parts if p]))).strip()\n",
        "\n",
        "# Safety wrappers to avoid ordinal_word IndexError from older defs\n",
        "def ordinal_word_safe(n):\n",
        "    try:\n",
        "        n = int(n)\n",
        "    except:\n",
        "        return None\n",
        "    try:\n",
        "        return _ordinalize_words(int_to_words(n))\n",
        "    except NameError:\n",
        "        words = int_to_words_safe(n)\n",
        "        ws = words.split()\n",
        "        if not ws:\n",
        "            return words\n",
        "        last = ws[-1]\n",
        "        if last.endswith('y'):\n",
        "            ws[-1] = last[:-1] + 'ieth'\n",
        "        elif last in ('one','two','three','five','eight','nine','twelve'):\n",
        "            repl = {'one':'first','two':'second','three':'third','five':'fifth','eight':'eighth','nine':'ninth','twelve':'twelfth'}\n",
        "            ws[-1] = repl[last]\n",
        "        else:\n",
        "            ws[-1] = last + 'th'\n",
        "        return ' '.join(ws)\n",
        "\n",
        "def normalize_fraction_safe(tok):\n",
        "    m = re.fullmatch(r'(\\d+)/(\\d+)', tok or '')\n",
        "    if not m: return None\n",
        "    a,b = int(m.group(1)), int(m.group(2))\n",
        "    num = int_to_words_safe(a)\n",
        "    den = ordinal_word_safe(b)\n",
        "    if a == 1:\n",
        "        spec = {2:'half',4:'quarter'}\n",
        "        den_word = spec.get(b, den)\n",
        "        return den_word\n",
        "    else:\n",
        "        if den.endswith('f'): den = den[:-1] + 'ves'\n",
        "        elif den.endswith('y'): den = den[:-1] + 'ies'\n",
        "        else: den = den + 's'\n",
        "        return f\"{num} {den}\"\n",
        "\n",
        "def normalize_cardinal_safe(tok):\n",
        "    s = str(tok).replace(',', '')\n",
        "    sign = 'minus ' if s.startswith('-') else ''\n",
        "    if s.startswith('-'): s = s[1:]\n",
        "    if not re.fullmatch(r'\\d+', s):\n",
        "        return None\n",
        "    return (sign + int_to_words_safe(int(s))).strip()\n",
        "\n",
        "def normalize_decimal_safe(tok):\n",
        "    if tok.count('.') != 1: return None\n",
        "    a,b = tok.split('.')\n",
        "    a = a.replace(',', '')\n",
        "    if a == '': a = '0'\n",
        "    if not re.fullmatch(r'-?\\d+', a) or not re.fullmatch(r'\\d+', b):\n",
        "        return None\n",
        "    sign = 'minus ' if a.startswith('-') else ''\n",
        "    if a.startswith('-'): a = a[1:]\n",
        "    left = int_to_words_safe(int(a))\n",
        "    right = ' '.join({'0':'zero','1':'one','2':'two','3':'three','4':'four','5':'five','6':'six','7':'seven','8':'eight','9':'nine'}[ch] for ch in b)\n",
        "    return (sign + left + ' point ' + right).strip()\n",
        "\n",
        "def apply_rules_safe(tok, cls):\n",
        "    if cls == 'FRACTION':\n",
        "        return normalize_fraction_safe(tok)\n",
        "    if cls == 'CARDINAL':\n",
        "        return normalize_cardinal_safe(tok)\n",
        "    if cls == 'ORDINAL':\n",
        "        m = re.fullmatch(r'(\\d+)(st|nd|rd|th)', tok or '')\n",
        "        return ordinal_word_safe(m.group(1)) if m else None\n",
        "    if cls == 'DECIMAL':\n",
        "        return normalize_decimal_safe(tok)\n",
        "    # fall back to global apply_rules for other classes (from support cell 9)\n",
        "    return apply_rules(tok, cls)\n",
        "\n",
        "def build_maps(df):\n",
        "    # returns maps + confidence for cls and memo variants\n",
        "    d = df.dropna(subset=['before','after','class']).copy()\n",
        "    # cls maps with confidence\n",
        "    grp_cls = d.groupby(['before','class']).size().rename('cnt').reset_index()\n",
        "    tot_cls = grp_cls.groupby('before')['cnt'].sum().rename('tot')\n",
        "    top_cls = grp_cls.sort_values(['before','cnt'], ascending=[True, False]).drop_duplicates('before')\n",
        "    top_cls = top_cls.merge(tot_cls, on='before', how='left')\n",
        "    top_cls['conf'] = top_cls['cnt'] / top_cls['tot']\n",
        "    cls_map_loc = dict(zip(top_cls['before'], top_cls['class']))\n",
        "    cls_conf_map_loc = dict(zip(top_cls['before'], top_cls['conf']))\n",
        "    d['_bl'] = d['before'].str.lower()\n",
        "    grp_cls_l = d.groupby(['_bl','class']).size().rename('cnt').reset_index()\n",
        "    tot_cls_l = grp_cls_l.groupby('_bl')['cnt'].sum().rename('tot')\n",
        "    top_cls_l = grp_cls_l.sort_values(['_bl','cnt'], ascending=[True, False]).drop_duplicates('_bl')\n",
        "    top_cls_l = top_cls_l.merge(tot_cls_l, on='_bl', how='left')\n",
        "    top_cls_l['conf'] = top_cls_l['cnt'] / top_cls_l['tot']\n",
        "    cls_map_lower_loc = dict(zip(top_cls_l['_bl'], top_cls_l['class']))\n",
        "    cls_conf_map_lower_loc = dict(zip(top_cls_l['_bl'], top_cls_l['conf']))\n",
        "\n",
        "    # (before, class) -> after (no confidence used here)\n",
        "    pair_map_loc = d.groupby(['before','class'], sort=False)['after'].agg(lambda s: s.value_counts().idxmax()).to_dict()\n",
        "    pair_map_lower_loc = d.groupby(['_bl','class'], sort=False)['after'].agg(lambda s: s.value_counts().idxmax()).to_dict()\n",
        "\n",
        "    # memo maps with confidence\n",
        "    grp_memo = d.groupby(['before','after']).size().rename('cnt').reset_index()\n",
        "    tot_memo = grp_memo.groupby('before')['cnt'].sum().rename('tot')\n",
        "    top_memo = grp_memo.sort_values(['before','cnt'], ascending=[True, False]).drop_duplicates('before')\n",
        "    top_memo = top_memo.merge(tot_memo, on='before', how='left')\n",
        "    top_memo['conf'] = top_memo['cnt'] / top_memo['tot']\n",
        "    memo_map_loc = dict(zip(top_memo['before'], top_memo['after']))\n",
        "    memo_conf_loc = dict(zip(top_memo['before'], top_memo['conf']))\n",
        "\n",
        "    grp_memo_l = d.groupby(['_bl','after']).size().rename('cnt').reset_index()\n",
        "    tot_memo_l = grp_memo_l.groupby('_bl')['cnt'].sum().rename('tot')\n",
        "    top_memo_l = grp_memo_l.sort_values(['_bl','cnt'], ascending=[True, False]).drop_duplicates('_bl')\n",
        "    top_memo_l = top_memo_l.merge(tot_memo_l, on='_bl', how='left')\n",
        "    top_memo_l['conf'] = top_memo_l['cnt'] / top_memo_l['tot']\n",
        "    memo_map_lower_loc = dict(zip(top_memo_l['_bl'], top_memo_l['after']))\n",
        "    memo_conf_lower_loc = dict(zip(top_memo_l['_bl'], top_memo_l['conf']))\n",
        "\n",
        "    return (cls_map_loc, cls_map_lower_loc, pair_map_loc, pair_map_lower_loc, memo_map_loc, memo_map_lower_loc,\n",
        "            cls_conf_map_loc, cls_conf_map_lower_loc, memo_conf_loc, memo_conf_lower_loc)\n",
        "\n",
        "def cascade_predict_tokens_with_maps(bef_series, maps, memo_conf_thr=0.70, cls_conf_thr=0.75):\n",
        "    # Strategic order: pair_map -> guarded rules -> memo -> lower_memo -> identity -> final veto\n",
        "    (cls_map_loc, cls_map_lower_loc, pair_map_loc, pair_map_lower_loc, memo_map_loc, memo_map_lower_loc,\n",
        "     cls_conf_map_loc, cls_conf_map_lower_loc, memo_conf_loc, memo_conf_lower_loc) = maps\n",
        "    bef = bef_series.fillna('')\n",
        "\n",
        "    # Prior class and confidence\n",
        "    prior = bef.map(cls_map_loc)\n",
        "    prior_conf = bef.map(cls_conf_map_loc)\n",
        "    need_lower = prior.isna()\n",
        "    if need_lower.any():\n",
        "        bef_l = bef.loc[need_lower].str.lower()\n",
        "        prior.loc[need_lower] = bef_l.map(cls_map_lower_loc)\n",
        "        prior_conf.loc[need_lower] = bef_l.map(cls_conf_map_lower_loc)\n",
        "\n",
        "    # Stage 1: pair maps (gate by prior_conf >= 0.85)\n",
        "    pred = pd.Series(index=bef.index, dtype=object)\n",
        "    has_prior = prior.notna()\n",
        "    if has_prior.any():\n",
        "        conf_ok = prior_conf.fillna(0.0) >= 0.85\n",
        "        idxp = prior.index[has_prior & conf_ok]\n",
        "        if len(idxp) > 0:\n",
        "            keys = list(zip(bef.loc[idxp].tolist(), prior.loc[idxp].tolist()))\n",
        "            pred.loc[idxp] = [pair_map_loc.get(k) for k in keys]\n",
        "            rem = pred.loc[idxp].isna()\n",
        "            if rem.any():\n",
        "                idxp2 = idxp[rem.values]\n",
        "                keys2 = list(zip(bef.loc[idxp2].str.lower().tolist(), prior.loc[idxp2].tolist()))\n",
        "                pred.loc[idxp2] = [pair_map_lower_loc.get(k) for k in keys2]\n",
        "\n",
        "    # Stage 2: guarded rules with SAFE passthrough and relaxed LETTERS\n",
        "    miss2 = pred.isna()\n",
        "    if miss2.any():\n",
        "        idx2 = pred.index[miss2]\n",
        "        toks = bef.loc[idx2].tolist()\n",
        "        prs = prior.loc[idx2].tolist()\n",
        "        prc = prior_conf.loc[idx2].fillna(0.0).tolist()\n",
        "        outs = []\n",
        "        for tk, pr, pc in zip(toks, prs, prc):\n",
        "            ic = infer_class(tk)  # infer independently\n",
        "            # If prior is PLAIN/PUNCT, only skip for non-SAFE inferred classes\n",
        "            if pr in ('PLAIN','PUNCT') and (ic not in SAFE_RULE_CLASSES):\n",
        "                outs.append(None); continue\n",
        "            # Special allowance: strong LETTERS acronyms (non word-like) may fire regardless of prior/pc\n",
        "            special_letters = (ic == 'LETTERS' and is_strong_letters(tk) and not is_word_like(tk))\n",
        "            # Guarded classes gating (unless special_letters)\n",
        "            if ic in GUARDED_RULE_CLASSES and not special_letters:\n",
        "                # word-like veto for guarded\n",
        "                if is_word_like(tk):\n",
        "                    outs.append(None); continue\n",
        "                # confidence gating for guarded\n",
        "                if pc < cls_conf_thr:\n",
        "                    if ic == 'ELECTRONIC':\n",
        "                        t = tk.strip()\n",
        "                        cond_email = (t.count('@') == 1 and '.' in t[t.find('@')+1:])\n",
        "                        cond_web = t.lower().startswith('www.') or re.search(r'/[A-Za-z0-9]', t)\n",
        "                        if not (cond_email or cond_web):\n",
        "                            outs.append(None); continue\n",
        "                    if ic == 'TELEPHONE' and count_digits(tk) < 7:\n",
        "                        outs.append(None); continue\n",
        "            # apply rules\n",
        "            if ic in SAFE_RULE_CLASSES:\n",
        "                outs.append(apply_rules_safe(tk, ic))\n",
        "            elif special_letters:\n",
        "                outs.append(apply_rules_safe(tk, ic))\n",
        "            elif ic in GUARDED_RULE_CLASSES and pr == ic:\n",
        "                outs.append(apply_rules_safe(tk, ic))\n",
        "            else:\n",
        "                outs.append(None)\n",
        "        pred.loc[idx2] = outs\n",
        "\n",
        "    # Stage 3: memo with tighter, class-aware gating\n",
        "    miss3 = pred.isna()\n",
        "    if miss3.any():\n",
        "        idx3 = pred.index[miss3]\n",
        "        outs3 = []\n",
        "        for i in idx3:\n",
        "            tk = bef.loc[i]\n",
        "            pr = prior.loc[i]\n",
        "            pc = float(prior_conf.loc[i]) if prior_conf.loc[i] == prior_conf.loc[i] else 0.0\n",
        "            if pr in ('PLAIN','PUNCT'):\n",
        "                outs3.append(None); continue\n",
        "            cand = memo_map_loc.get(tk)\n",
        "            conf = float(memo_conf_loc.get(tk, 0.0))\n",
        "            # dynamic threshold: stricter for guarded priors\n",
        "            thr = 0.95 if pr in GUARDED_RULE_CLASSES else memo_conf_thr\n",
        "            # extra veto for word-like tokens under guarded prior\n",
        "            if pr in GUARDED_RULE_CLASSES and is_word_like(tk):\n",
        "                outs3.append(None); continue\n",
        "            if cand is not None and conf >= thr and pc >= cls_conf_thr:\n",
        "                outs3.append(cand)\n",
        "            else:\n",
        "                outs3.append(None)\n",
        "        pred.loc[idx3] = outs3\n",
        "\n",
        "    # Stage 4: lowercase memo with tighter, class-aware gating\n",
        "    miss4 = pred.isna()\n",
        "    if miss4.any():\n",
        "        idx4 = pred.index[miss4]\n",
        "        outs4 = []\n",
        "        for i in idx4:\n",
        "            tk = bef.loc[i]\n",
        "            pr = prior.loc[i]\n",
        "            pc = float(prior_conf.loc[i]) if prior_conf.loc[i] == prior_conf.loc[i] else 0.0\n",
        "            if pr in ('PLAIN','PUNCT'):\n",
        "                outs4.append(None); continue\n",
        "            tkl = str(tk).lower()\n",
        "            cand2 = memo_map_lower_loc.get(tkl)\n",
        "            conf2 = float(memo_conf_lower_loc.get(tkl, 0.0))\n",
        "            thr2 = 0.96 if pr in GUARDED_RULE_CLASSES else memo_conf_thr\n",
        "            if pr in GUARDED_RULE_CLASSES and is_word_like(tk):\n",
        "                outs4.append(None); continue\n",
        "            if cand2 is not None and conf2 >= thr2 and pc >= cls_conf_thr:\n",
        "                outs4.append(cand2)\n",
        "            else:\n",
        "                outs4.append(None)\n",
        "        pred.loc[idx4] = outs4\n",
        "\n",
        "    # Stage 5: identity\n",
        "    miss5 = pred.isna()\n",
        "    if miss5.any():\n",
        "        pred.loc[miss5] = bef.loc[miss5]\n",
        "\n",
        "    # Stage 6: final vetoes\n",
        "    # Revert casing-only changes on word-like tokens\n",
        "    try:\n",
        "        pred_l = pred.fillna('').astype(str).str.lower()\n",
        "        bef_l = bef.fillna('').astype(str).str.lower()\n",
        "        casing_only = (pred_l == bef_l) & (pred != bef) & bef.apply(is_word_like)\n",
        "        if casing_only.any():\n",
        "            pred.loc[casing_only] = bef.loc[casing_only]\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return pred\n",
        "\n",
        "# Context post-processing disabled (confirmed to slightly hurt CV)\n",
        "\n",
        "print('Leak-free CV: building maps on train split only ...')\n",
        "t0 = time.time()\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=2025)\n",
        "groups = train['sentence_id'].values\n",
        "tr_idx, va_idx = next(gss.split(train, groups=groups))\n",
        "trn = train.iloc[tr_idx].copy()\n",
        "val = train.iloc[va_idx].copy()\n",
        "print('Split sizes:', trn.shape, val.shape)\n",
        "tm = time.time()\n",
        "maps = build_maps(trn)\n",
        "print('Maps built (leak-free) in', f\"{time.time()-tm:.2f}s\")\n",
        "print('Predicting validation with guarded cascade + confidence gating ...')\n",
        "tp = time.time()\n",
        "pred_val = cascade_predict_tokens_with_maps(val['before'], maps, memo_conf_thr=0.70, cls_conf_thr=0.75)\n",
        "print('Val prediction elapsed:', f\"{time.time()-tp:.2f}s\")\n",
        "acc = (pred_val.values == val['after'].values).mean()\n",
        "print(f'Leak-free CV accuracy (guarded+conf): {acc:.6f}')\n",
        "err_mask = pred_val.values != val['after'].values\n",
        "print('Errors remaining:', int(err_mask.sum()))\n",
        "print('Top error classes:')\n",
        "print(val.loc[err_mask, 'class'].value_counts().head(15))\n",
        "print('T3 elapsed:', f\"{time.time()-t0:.2f}s\")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leak-free CV: building maps on train split only ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split sizes: (7140066, 5) (1784910, 5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maps built (leak-free) in 178.96s\nPredicting validation with guarded cascade + confidence gating ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val prediction elapsed: 14.00s\nLeak-free CV accuracy (guarded+conf): 0.991772\nErrors remaining: 14687\nTop error classes:\nclass\nDATE          4962\nPLAIN         3463\nLETTERS       1780\nCARDINAL       824\nTELEPHONE      685\nELECTRONIC     652\nDIGIT          635\nMEASURE        603\nORDINAL        260\nDECIMAL        234\nMONEY          213\nVERBATIM       186\nTIME           106\nADDRESS         53\nFRACTION        31\nName: count, dtype: int64\nT3 elapsed: 195.00s\n"
          ]
        }
      ]
    },
    {
      "id": "2d3fa0f6-efa6-4cc2-b2dc-781df9285494",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T4: Context-aware post-processing (month + day ordinals, year in dates) and new submission\n",
        "import time, re\n",
        "t0 = time.time()\n",
        "\n",
        "months_full = ['january','february','march','april','may','june','july','august','september','october','november','december']\n",
        "months_abbr = ['jan','feb','mar','apr','may','jun','jul','aug','sep','sept','oct','nov','dec']\n",
        "month_set = set(months_full + months_abbr)\n",
        "\n",
        "def is_int_str(s):\n",
        "    return re.fullmatch(r'\\d+', s) is not None\n",
        "\n",
        "def context_postprocess(df_tokens, pred_series):\n",
        "    # df_tokens must have columns: sentence_id, token_id, before\n",
        "    pred = pred_series.copy()\n",
        "    df = df_tokens[['sentence_id','token_id','before']].copy()\n",
        "    df['pred'] = pred.values\n",
        "    df['before_l'] = df['before'].fillna('').astype(str)\n",
        "    df['before_l'] = df['before_l']\n",
        "    # Build previous tokens within sentence\n",
        "    df = df.sort_values(['sentence_id','token_id'])\n",
        "    df['prev_before'] = df.groupby('sentence_id')['before_l'].shift(1)\n",
        "    df['prev2_before'] = df.groupby('sentence_id')['before_l'].shift(2)\n",
        "    df['prev_pred'] = df.groupby('sentence_id')['pred'].shift(1)\n",
        "\n",
        "    # Rule 1: Day-of-month ordinals when preceded by a month name/abbr\n",
        "    # If current token is integer 1..31 and previous token is a month token\n",
        "    def day_to_ordinal_safe(x):\n",
        "        try:\n",
        "            v = int(x)\n",
        "            if 1 <= v <= 31:\n",
        "                return ordinal_word(v)\n",
        "        except:\n",
        "            return None\n",
        "        return None\n",
        "\n",
        "    mask_day = df['before_l'].str.fullmatch(r'\\d{1,2}')\n",
        "    prev_is_month = df['prev_before'].str.lower().isin(month_set)\n",
        "    m1 = mask_day & prev_is_month\n",
        "    if m1.any():\n",
        "        df.loc[m1, 'pred'] = df.loc[m1, 'before_l'].apply(day_to_ordinal_safe)\n",
        "\n",
        "    # Rule 2: Year words when part of date context: if token is 4-digit year and prev token is month or prev2 is month with prev comma\n",
        "    def is_year_token(s):\n",
        "        if re.fullmatch(r'\\d{4}', s):\n",
        "            y = int(s)\n",
        "            return 1000 <= y <= 2099\n",
        "        return False\n",
        "\n",
        "    mask_year = df['before_l'].apply(is_year_token)\n",
        "    prev_is_comma = df['prev_before'] == ','\n",
        "    prev_is_month2 = df['prev2_before'].str.lower().isin(month_set)\n",
        "    prev_is_month1 = df['prev_before'].str.lower().isin(month_set)\n",
        "    m2 = mask_year & (prev_is_month1 | (prev_is_comma & prev_is_month2))\n",
        "    if m2.any():\n",
        "        df.loc[m2, 'pred'] = df.loc[m2, 'before_l'].apply(year_to_words)\n",
        "\n",
        "    # Return updated prediction aligned to original index order\n",
        "    df = df.sort_index()\n",
        "    return df['pred']\n",
        "\n",
        "print('Generating base predictions with cascade v1 ...')\n",
        "pred_test_base = cascade_predict_tokens_v1(test['before'])\n",
        "print('Applying context post-processing ...')\n",
        "pred_test_ctx = context_postprocess(test[['sentence_id','token_id','before']], pred_test_base)\n",
        "\n",
        "test_ids = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)\n",
        "sub_ctx = pd.DataFrame({'id': test_ids, 'after': pred_test_ctx.astype(str)})\n",
        "sub_ctx.to_csv('submission.csv', index=False)\n",
        "print('Context-enhanced submission.csv written')\n",
        "print('T4 elapsed:', f'{time.time()-t0:.2f}s')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating base predictions with cascade v1 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying context post-processing ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context-enhanced submission.csv written\nT4 elapsed: 6.50s\n"
          ]
        }
      ]
    },
    {
      "id": "72450f90-abcc-452b-9156-bc5a52506828",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T5: Add context-aware memo (prev/before/next) to reduce PLAIN errors; infer on test and save submission\n",
        "import time\n",
        "t0 = time.time()\n",
        "print('Building context-aware maps from full train ...')\n",
        "tr_ctx = train.dropna(subset=['before','after']).copy()\n",
        "tr_ctx = tr_ctx.sort_values(['sentence_id','token_id'])\n",
        "tr_ctx['prev_before'] = tr_ctx.groupby('sentence_id')['before'].shift(1).fillna('')\n",
        "tr_ctx['next_before'] = tr_ctx.groupby('sentence_id')['before'].shift(-1).fillna('')\n",
        "\n",
        "# (prev, before) -> after\n",
        "prev_map = tr_ctx.groupby(['prev_before','before'], sort=False)['after'].agg(lambda s: s.value_counts().idxmax()).to_dict()\n",
        "# (before, next) -> after\n",
        "next_map = tr_ctx.groupby(['before','next_before'], sort=False)['after'].agg(lambda s: s.value_counts().idxmax()).to_dict()\n",
        "# lowercase backoffs\n",
        "tr_ctx['_prev_l'] = tr_ctx['prev_before'].str.lower()\n",
        "tr_ctx['_bef_l'] = tr_ctx['before'].str.lower()\n",
        "tr_ctx['_next_l'] = tr_ctx['next_before'].str.lower()\n",
        "prev_map_lower = tr_ctx.groupby(['_prev_l','_bef_l'], sort=False)['after'].agg(lambda s: s.value_counts().idxmax()).to_dict()\n",
        "next_map_lower = tr_ctx.groupby(['_bef_l','_next_l'], sort=False)['after'].agg(lambda s: s.value_counts().idxmax()).to_dict()\n",
        "print('Context maps sizes | prev:', len(prev_map), 'next:', len(next_map))\n",
        "\n",
        "def cascade_predict_tokens_v2(df_tokens):\n",
        "    # df_tokens must have sentence_id, token_id, before\n",
        "    df = df_tokens[['sentence_id','token_id','before']].copy()\n",
        "    df = df.sort_values(['sentence_id','token_id'])\n",
        "    bef = df['before'].fillna('')\n",
        "    # Stage 1: exact memo\n",
        "    pred = bef.map(memo_map)\n",
        "    miss = pred.isna()\n",
        "    if miss.any():\n",
        "        idx = pred.index[miss]\n",
        "        sub_df = df.loc[idx].copy()\n",
        "        sub_df['prev_before'] = sub_df.groupby('sentence_id')['before'].shift(1).fillna('')\n",
        "        sub_df['next_before'] = sub_df.groupby('sentence_id')['before'].shift(-1).fillna('')\n",
        "        # prev map exact\n",
        "        keys_prev = list(zip(sub_df['prev_before'].tolist(), sub_df['before'].tolist()))\n",
        "        vals_prev = [prev_map.get(k) for k in keys_prev]\n",
        "        pred.loc[idx] = vals_prev\n",
        "        # fill remaining via next map exact\n",
        "        miss2 = pred.loc[idx].isna()\n",
        "        if miss2.any():\n",
        "            idx2 = idx[miss2.values]\n",
        "            sub2 = df.loc[idx2]\n",
        "            sub2_prev = sub2.groupby('sentence_id')['before'].shift(1).fillna('')\n",
        "            sub2_next = sub2.groupby('sentence_id')['before'].shift(-1).fillna('')\n",
        "            keys_next = list(zip(sub2['before'].tolist(), sub2_next.tolist()))\n",
        "            pred.loc[idx2] = [next_map.get(k) for k in keys_next]\n",
        "        # lowercase backoffs for context\n",
        "        miss3 = pred.loc[idx].isna()\n",
        "        if miss3.any():\n",
        "            idx3 = idx[miss3.values]\n",
        "            sub3 = df.loc[idx3]\n",
        "            prev_l = sub3.groupby('sentence_id')['before'].shift(1).fillna('').str.lower()\n",
        "            bef_l = sub3['before'].str.lower()\n",
        "            next_l = sub3.groupby('sentence_id')['before'].shift(-1).fillna('').str.lower()\n",
        "            keys_prev_l = list(zip(prev_l.tolist(), bef_l.tolist()))\n",
        "            vals_prev_l = [prev_map_lower.get(k) for k in keys_prev_l]\n",
        "            pred.loc[idx3] = vals_prev_l\n",
        "            miss4 = pred.loc[idx3].isna()\n",
        "            if miss4.any():\n",
        "                idx4 = idx3[miss4.values]\n",
        "                sub4 = df.loc[idx4]\n",
        "                bef4_l = sub4['before'].str.lower()\n",
        "                next4_l = sub4.groupby('sentence_id')['before'].shift(-1).fillna('').str.lower()\n",
        "                keys_next_l = list(zip(bef4_l.tolist(), next4_l.tolist()))\n",
        "                pred.loc[idx4] = [next_map_lower.get(k) for k in keys_next_l]\n",
        "    # Stage 2: class-aware pair/rules (reuse earlier infer/rules with prior from cls_map) for remaining\n",
        "    missA = pred.isna()\n",
        "    if missA.any():\n",
        "        miss_idx = missA[missA].index\n",
        "        sub_bef = bef.loc[miss_idx]\n",
        "        prior = sub_bef.map(cls_map)\n",
        "        need_lower = prior.isna()\n",
        "        if need_lower.any():\n",
        "            prior.loc[need_lower] = sub_bef.loc[need_lower].str.lower().map(cls_map_lower)\n",
        "        # (before, class) maps\n",
        "        key_exact = list(zip(sub_bef.tolist(), prior.tolist()))\n",
        "        vals = [pair_map.get(k) for k in key_exact]\n",
        "        out2 = pd.Series(vals, index=miss_idx, dtype=object)\n",
        "        rem = out2.isna()\n",
        "        if rem.any():\n",
        "            idxr = out2.index[rem]\n",
        "            keys2 = list(zip(sub_bef.loc[idxr].str.lower().tolist(), prior.loc[idxr].tolist()))\n",
        "            out2.loc[idxr] = [pair_map_lower.get(k) for k in keys2]\n",
        "        rem2 = out2.isna()\n",
        "        if rem2.any():\n",
        "            idx2 = out2.index[rem2]\n",
        "            toks = sub_bef.loc[idx2].tolist()\n",
        "            prs = prior.loc[idx2].tolist()\n",
        "            finals = []\n",
        "            for tk, pr in zip(toks, prs):\n",
        "                ic = infer_class(tk, pr)\n",
        "                finals.append(apply_rules(tk, ic) if ic not in ('PLAIN','PUNCT', None) else None)\n",
        "            out2.loc[idx2] = finals\n",
        "        pred.loc[miss_idx] = out2.values\n",
        "    # Stage 3: lowercase memo\n",
        "    missB = pred.isna()\n",
        "    if missB.any():\n",
        "        pred.loc[missB] = bef.loc[missB].str.lower().map(memo_map_lower)\n",
        "    # Stage 4: identity\n",
        "    missC = pred.isna()\n",
        "    if missC.any():\n",
        "        pred.loc[missC] = bef.loc[missC]\n",
        "    return pred\n",
        "\n",
        "print('Inferring on test with context-aware cascade v2 ...')\n",
        "pred_test_v2 = cascade_predict_tokens_v2(test[['sentence_id','token_id','before']])\n",
        "test_ids = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)\n",
        "submission_ctx = pd.DataFrame({'id': test_ids, 'after': pred_test_v2.astype(str)})\n",
        "submission_ctx.to_csv('submission.csv', index=False)\n",
        "print('Wrote submission.csv with context-aware memo v2')\n",
        "print('T5 elapsed:', f'{time.time()-t0:.2f}s')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "id": "2f9e52d0-c77f-4263-8be7-1faef78fd1c9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T3-support: Lightweight rule and detector definitions (no heavy map building)\n",
        "import re\n",
        "\n",
        "digit_words = {'0':'zero','1':'one','2':'two','3':'three','4':'four','5':'five','6':'six','7':'seven','8':'eight','9':'nine'}\n",
        "ones = ['zero','one','two','three','four','five','six','seven','eight','nine']\n",
        "teens = ['ten','eleven','twelve','thirteen','fourteen','fifteen','sixteen','seventeen','eighteen','nineteen']\n",
        "tens_words = ['', '', 'twenty','thirty','forty','fifty','sixty','seventy','eighty','ninety']\n",
        "\n",
        "def two_digit(n):\n",
        "    if n < 10: return ones[n]\n",
        "    if n < 20: return teens[n-10]\n",
        "    t, r = divmod(n, 10)\n",
        "    return tens_words[t] + ('' if r==0 else ' ' + ones[r])\n",
        "\n",
        "def three_digit(n):\n",
        "    h, r = divmod(n, 100)\n",
        "    if h == 0: return two_digit(r)\n",
        "    if r == 0: return ones[h] + ' hundred'\n",
        "    return ones[h] + ' hundred ' + two_digit(r)\n",
        "\n",
        "def int_to_words(num):\n",
        "    num = int(num)\n",
        "    if num == 0: return 'zero'\n",
        "    parts = []\n",
        "    billions, rem = divmod(num, 10**9)\n",
        "    millions, rem = divmod(rem, 10**6)\n",
        "    thousands, rem = divmod(rem, 1000)\n",
        "    if billions: parts.append(three_digit(billions) + ' billion')\n",
        "    if millions: parts.append(three_digit(millions) + ' million')\n",
        "    if thousands: parts.append(three_digit(thousands) + ' thousand')\n",
        "    if rem: parts.append(three_digit(rem))\n",
        "    return ' '.join(parts)\n",
        "\n",
        "def _ordinalize_words(words: str) -> str:\n",
        "    irregular = {\n",
        "        'one':'first','two':'second','three':'third','four':'fourth','five':'fifth','six':'sixth','seven':'seventh','eight':'eighth','nine':'ninth',\n",
        "        'ten':'tenth','eleven':'eleventh','twelve':'twelfth','thirteen':'thirteenth','fourteen':'fourteenth','fifteen':'fifteenth','sixteen':'sixteenth','seventeen':'seventeenth','eighteen':'eighteenth','nineteen':'nineteenth',\n",
        "        'twenty':'twentieth','thirty':'thirtieth','forty':'fortieth','fifty':'fiftieth','sixty':'sixtieth','seventy':'seventieth','eighty':'eightieth','ninety':'ninetieth'\n",
        "    }\n",
        "    scale = {'hundred','thousand','million','billion'}\n",
        "    ws = words.split()\n",
        "    if not ws:\n",
        "        return words\n",
        "    if ws[-1] in scale:\n",
        "        ws[-1] = ws[-1] + 'th'\n",
        "        return ' '.join(ws)\n",
        "    last = ws[-1]\n",
        "    if last in irregular:\n",
        "        ws[-1] = irregular[last]\n",
        "    elif last.endswith('y'):\n",
        "        ws[-1] = last[:-1] + 'ieth'\n",
        "    else:\n",
        "        ws[-1] = last + 'th'\n",
        "    return ' '.join(ws)\n",
        "\n",
        "def ordinal_word(n):\n",
        "    n = int(n)\n",
        "    # Robust ordinalization for any positive integer\n",
        "    if n <= 0:\n",
        "        return int_to_words(n)  # fallback\n",
        "    return _ordinalize_words(int_to_words(n))\n",
        "\n",
        "def normalize_decimal(tok):\n",
        "    if tok.count('.') != 1: return None\n",
        "    a,b = tok.split('.')\n",
        "    a = a.replace(',', '')\n",
        "    if a == '': a = '0'\n",
        "    if not re.fullmatch(r'-?\\d+', a) or not re.fullmatch(r'\\d+', b):\n",
        "        return None\n",
        "    sign = 'minus ' if a.startswith('-') else ''\n",
        "    if a.startswith('-'): a = a[1:]\n",
        "    left = int_to_words(int(a))\n",
        "    right = ' '.join(digit_words[ch] for ch in b)\n",
        "    return (sign + left + ' point ' + right).strip()\n",
        "\n",
        "month_names = {str(i).zfill(2): m for i,m in enumerate(['','january','february','march','april','may','june','july','august','september','october','november','december']) if i}\n",
        "month_names.update({str(i): m for i,m in enumerate(['','january','february','march','april','may','june','july','august','september','october','november','december']) if i})\n",
        "\n",
        "# Month name handling (full and abbreviations, with optional dot)\n",
        "MONTH_CANON = ['january','february','march','april','may','june','july','august','september','october','november','december']\n",
        "MONTH_ABBR = {'jan':'january','feb':'february','mar':'march','apr':'april','may':'may','jun':'june','jul':'july','aug':'august','sep':'september','sept':'september','oct':'october','nov':'november','dec':'december'}\n",
        "WEEKDAY = ['monday','tuesday','wednesday','thursday','friday','saturday','sunday']\n",
        "MONTH_ALT_PATTERN = r'(?:jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec|january|february|march|april|june|july|august|september|october|november|december)\\.?'\n",
        "\n",
        "def canon_month(s):\n",
        "    s0 = s.strip().lower().rstrip('.')\n",
        "    if s0 in MONTH_CANON: return s0\n",
        "    if s0 in MONTH_ABBR: return MONTH_ABBR[s0]\n",
        "    return None\n",
        "\n",
        "def year_to_words(y):\n",
        "    y = int(y)\n",
        "    if y < 1000: return int_to_words(y)\n",
        "    if 1900 <= y <= 1999:\n",
        "        h, t = divmod(y, 100)\n",
        "        return int_to_words(h) + ' ' + two_digit(t)\n",
        "    if 2000 <= y <= 2009:\n",
        "        return 'two thousand' if y == 2000 else 'two thousand ' + int_to_words(y-2000)\n",
        "    if 2010 <= y <= 2099:\n",
        "        return 'two thousand ' + int_to_words(y-2000)\n",
        "    return int_to_words(y)\n",
        "\n",
        "def decade_to_words(y):\n",
        "    y = int(y)\n",
        "    century = y // 100\n",
        "    decade = (y % 100) // 10 * 10\n",
        "    if 1900 <= y <= 1990:\n",
        "        return int_to_words(century) + ' ' + two_digit(decade).replace('y', 'ies')\n",
        "    if 2000 <= y <= 2090:\n",
        "        if decade == 0:\n",
        "            return 'two thousands'\n",
        "        return 'two thousand ' + two_digit(decade).replace('y', 'ies')\n",
        "    return int_to_words(y)\n",
        "\n",
        "def _unambiguous_mdy(d1, d2, y):\n",
        "    try:\n",
        "        a = int(d1); b = int(d2)\n",
        "        y_int = int(y)\n",
        "    except:\n",
        "        return None\n",
        "    if len(y) == 2:\n",
        "        y4 = int('20'+y) if int(y) <= 30 else int('19'+y)\n",
        "    else:\n",
        "        y4 = y_int\n",
        "    if a <= 12 and b <= 12:\n",
        "        return None\n",
        "    if a <= 12 and 1 <= b <= 31:\n",
        "        return (a, b, y4)\n",
        "    if b <= 12 and 1 <= a <= 31:\n",
        "        return (b, a, y4)\n",
        "    return None\n",
        "\n",
        "def normalize_date(tok):\n",
        "    # YYYY-MM-DD\n",
        "    if re.fullmatch(r'\\d{4}-\\d{1,2}-\\d{1,2}', tok):\n",
        "        y,m,d = tok.split('-')\n",
        "        mname = month_names.get(m.zfill(2))\n",
        "        if not mname: return None\n",
        "        return f\"{mname} {ordinal_word(int(d))} {year_to_words(y)}\"\n",
        "    # YYYY/MM/DD\n",
        "    if re.fullmatch(r'\\d{4}/\\d{1,2}/\\d{1,2}', tok):\n",
        "        y,m,d = tok.split('/')\n",
        "        mname = month_names.get(m.zfill(2))\n",
        "        if not mname: return None\n",
        "        return f\"{mname} {ordinal_word(int(d))} {year_to_words(y)}\"\n",
        "    # DD-MM-YYYY or MM-DD-YYYY (unambiguous guard)\n",
        "    if re.fullmatch(r'\\d{1,2}-\\d{1,2}-\\d{4}', tok):\n",
        "        a,b,y = tok.split('-')\n",
        "        if int(a) <= 12 and int(b) <= 12:\n",
        "            return None\n",
        "        if int(a) <= 12:\n",
        "            m,d = a,b\n",
        "        else:\n",
        "            d,m = a,b\n",
        "        mname = month_names.get(m.zfill(2))\n",
        "        if not mname: return None\n",
        "        return f\"{mname} {ordinal_word(int(d))} {year_to_words(y)}\"\n",
        "    # M/D/YY or M/D/YYYY with ambiguity guard\n",
        "    if re.fullmatch(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', tok):\n",
        "        a,b,y = tok.split('/')\n",
        "        parsed = _unambiguous_mdy(a,b,y)\n",
        "        if not parsed: return None\n",
        "        m,d,y4 = parsed\n",
        "        mname = month_names.get(str(m).zfill(2))\n",
        "        if not mname: return None\n",
        "        return f\"{mname} {ordinal_word(int(d))} {year_to_words(y4)}\"\n",
        "    # YYYY.MM.DD or DD.MM.YYYY\n",
        "    if re.fullmatch(r'\\d{4}\\.\\d{1,2}\\.\\d{1,2}', tok):\n",
        "        y,m,d = tok.split('.')\n",
        "        mname = month_names.get(m.zfill(2))\n",
        "        if not mname: return None\n",
        "        return f\"{mname} {ordinal_word(int(d))} {year_to_words(y)}\"\n",
        "    if re.fullmatch(r'\\d{1,2}\\.\\d{1,2}\\.\\d{2,4}', tok):\n",
        "        a,b,y = tok.split('.')\n",
        "        parsed = _unambiguous_mdy(a,b,y)\n",
        "        if not parsed: return None\n",
        "        m,d,y4 = parsed\n",
        "        mname = month_names.get(str(m).zfill(2))\n",
        "        if not mname: return None\n",
        "        return f\"{mname} {ordinal_word(int(d))} {year_to_words(y4)}\"\n",
        "    # Month name patterns\n",
        "    s = tok.strip()\n",
        "    m = re.fullmatch(r'(?:(?:Mon|Tue|Tues|Wed|Thu|Thur|Thurs|Fri|Sat|Sun|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday),?\\s+)?(' + MONTH_ALT_PATTERN + r')\\s+(\\d{1,2})(?:,)?(?:\\s+(\\d{4}))?$', s, re.IGNORECASE)\n",
        "    if m:\n",
        "        mon_raw, d, y = m.group(1), m.group(2), m.group(3)\n",
        "        mon = canon_month(mon_raw)\n",
        "        if not mon: return None\n",
        "        out = f\"{mon} {ordinal_word(int(d))}\"\n",
        "        if y: out += ' ' + year_to_words(y)\n",
        "        return out\n",
        "    m = re.fullmatch(r'^(\\d{1,2})\\s+(' + MONTH_ALT_PATTERN + r')(?:,)?(?:\\s+(\\d{4}))?$', s, re.IGNORECASE)\n",
        "    if m:\n",
        "        d, mon_raw, y = m.group(1), m.group(2), m.group(3)\n",
        "        mon = canon_month(mon_raw)\n",
        "        if not mon: return None\n",
        "        out = f\"{mon} {ordinal_word(int(d))}\"\n",
        "        if y: out += ' ' + year_to_words(y)\n",
        "        return out\n",
        "    m = re.fullmatch(r'(\\d{4})s', tok)\n",
        "    if m:\n",
        "        y = int(m.group(1))\n",
        "        base = decade_to_words(y)\n",
        "        return base\n",
        "    return None\n",
        "\n",
        "LETTERS_EXCEPT = {'I','A','AM','PM','US','U.S.','U.S','UK','U.K.','UK.','EU','U.N.','UN','TV','OK','OK.','AI','ML','CEO'}\n",
        "\n",
        "def normalize_letters(tok):\n",
        "    if tok in LETTERS_EXCEPT:\n",
        "        return None\n",
        "    if re.fullmatch(r'[A-Z]{3,}', tok):\n",
        "        return ' '.join(list(tok.lower()))\n",
        "    if re.fullmatch(r'([A-Z]\\.){3,}[A-Z]?\\.?', tok):\n",
        "        return ' '.join(ch.lower() for ch in tok if ch.isalpha())\n",
        "    if re.fullmatch(r'[A-Z]+-[A-Z]+', tok):\n",
        "        letters = tok.replace('-', '')\n",
        "        return ' '.join(list(letters.lower()))\n",
        "    if 'AT&T' in tok:\n",
        "        return 'a t and t'\n",
        "    return None\n",
        "\n",
        "def normalize_digit(tok):\n",
        "    if not re.fullmatch(r'\\d+', tok): return None\n",
        "    return ' '.join(digit_words[ch] for ch in tok)\n",
        "\n",
        "def normalize_cardinal(tok):\n",
        "    s = tok.replace(',', '')\n",
        "    if not re.fullmatch(r'-?\\d+', s): return None\n",
        "    sign = 'minus ' if s.startswith('-') else ''\n",
        "    if s.startswith('-'): s = s[1:]\n",
        "    return (sign + int_to_words(int(s))).strip()\n",
        "\n",
        "def normalize_ordinal(tok):\n",
        "    m = re.fullmatch(r'(\\d+)(st|nd|rd|th)', tok)\n",
        "    if not m: return None\n",
        "    return ordinal_word(m.group(1))\n",
        "\n",
        "def normalize_fraction(tok):\n",
        "    m = re.fullmatch(r'(\\d+)/(\\d+)', tok)\n",
        "    if not m: return None\n",
        "    a,b = int(m.group(1)), int(m.group(2))\n",
        "    num = int_to_words(a)\n",
        "    den = ordinal_word(b)\n",
        "    if a == 1:\n",
        "        spec = {2:'half',4:'quarter'}\n",
        "        den_word = spec.get(b, den)\n",
        "        return den_word\n",
        "    else:\n",
        "        if den.endswith('f'): den = den[:-1] + 'ves'\n",
        "        elif den.endswith('y'): den = den[:-1] + 'ies'\n",
        "        else: den = den + 's'\n",
        "        return f\"{num} {den}\"\n",
        "\n",
        "unit_map = {\n",
        "    'ft': ('foot','feet'), 'in': ('inch','inches'), 'yd': ('yard','yards'), 'mi': ('mile','miles'),\n",
        "    'lb': ('pound','pounds'), 'lbs': ('pound','pounds'), 'oz': ('ounce','ounces'),\n",
        "    'kg': ('kilogram','kilograms'), 'g': ('gram','grams'), 'mg': ('milligram','milligrams'),\n",
        "    'km': ('kilometer','kilometers'), 'm': ('meter','meters'), 'cm': ('centimeter','centimeters'), 'mm': ('millimeter','millimeters'),\n",
        "    'l': ('liter','liters'), 'ml': ('milliliter','milliliters'),\n",
        "    'mph': ('miles per hour','miles per hour'), 'km/h': ('kilometers per hour','kilometers per hour'), 'kph': ('kilometers per hour','kilometers per hour'),\n",
        "    '%': ('percent','percent'), '\\u00b0c': ('degrees celsius','degrees celsius'), '\\u00b0f': ('degrees fahrenheit','degrees fahrenheit')\n",
        "}\n",
        "\n",
        "def normalize_measure(tok):\n",
        "    t = tok.lower()\n",
        "    m = re.fullmatch(r'(-?[\\d,]+(?:\\.\\d+)?)[\\s]*([a-z\\u00b0/%]+(?:/[a-z]+)?)', t)\n",
        "    if not m:\n",
        "        m = re.fullmatch(r'(-?[\\d,]+)([a-z\\u00b0/%]+)', t)\n",
        "    if not m: return None\n",
        "    num_s, unit = m.group(1), m.group(2)\n",
        "    unit = unit.strip()\n",
        "    singular_plural = unit_map.get(unit)\n",
        "    if not singular_plural:\n",
        "        return None\n",
        "    if '.' in num_s:\n",
        "        spoken_num = normalize_decimal(num_s)\n",
        "    else:\n",
        "        spoken_num = normalize_cardinal(num_s)\n",
        "    if spoken_num is None: return None\n",
        "    try:\n",
        "        val = float(num_s.replace(',', ''))\n",
        "    except:\n",
        "        val = None\n",
        "    unit_word = singular_plural[0] if val == 1 else singular_plural[1]\n",
        "    return f\"{spoken_num} {unit_word}\"\n",
        "\n",
        "def normalize_money(tok):\n",
        "    m = re.fullmatch(r'([$\\u00a3\\u20ac])\\s*(\\d*[\\d,]*)(?:\\.(\\d{1,2}))?', tok)\n",
        "    if not m: return None\n",
        "    sym, a, c = m.groups()\n",
        "    a = (a or '').replace(',', '')\n",
        "    cur = {'$':'dollar','\\u00a3':'pound','\\u20ac':'euro'}[sym]\n",
        "    out = []\n",
        "    have_dollars = bool(a) and int(a) > 0\n",
        "    cents_val = None\n",
        "    if c is not None:\n",
        "        cents_val = int(c.ljust(2,'0'))\n",
        "    if have_dollars:\n",
        "        out.append(int_to_words(int(a)) + (' ' + cur + ('s' if int(a)!=1 else '')))\n",
        "    if cents_val is not None and cents_val > 0:\n",
        "        out.append(int_to_words(cents_val) + (' cent' + ('s' if cents_val!=1 else '')))\n",
        "    if not out and (cents_val is not None and cents_val > 0):\n",
        "        return int_to_words(cents_val) + (' cent' + ('s' if cents_val!=1 else ''))\n",
        "    if not out:\n",
        "        return None\n",
        "    return ' and '.join(out) if len(out) == 2 else out[0]\n",
        "\n",
        "def normalize_time(tok):\n",
        "    m = re.fullmatch(r'(\\d{1,2}):(\\d{2})', tok)\n",
        "    if not m: return None\n",
        "    hh = int(m.group(1)); mm = int(m.group(2))\n",
        "    hour = int_to_words(hh)\n",
        "    if mm == 0: return hour + \" o'clock\"\n",
        "    if 1 <= mm <= 9:\n",
        "        return hour + ' oh ' + int_to_words(mm)\n",
        "    return hour + ' ' + int_to_words(mm)\n",
        "\n",
        "def normalize_telephone(tok):\n",
        "    t = tok\n",
        "    digits = re.sub(r'\\D', '', t)\n",
        "    if len(digits) < 7: return None\n",
        "    spoken = ' '.join(digit_words[ch] for ch in digits)\n",
        "    return spoken\n",
        "\n",
        "def normalize_electronic(tok):\n",
        "    t = tok.strip()\n",
        "    # emails: require exactly one '@' and at least one dot after '@'\n",
        "    if t.count('@') == 1:\n",
        "        at_idx = t.find('@')\n",
        "        if '.' in t[at_idx+1:]:\n",
        "            parts = re.split(r'(@|\\.)', t)\n",
        "            spoken = []\n",
        "            for p in parts:\n",
        "                if p == '@': spoken.append('at')\n",
        "                elif p == '.': spoken.append('dot')\n",
        "                else: spoken.append(p)\n",
        "            return ' '.join(spoken).replace('  ', ' ').strip()\n",
        "    # urls/domains: require www. prefix or a slash with alnum after\n",
        "    if t.lower().startswith('www.') or re.search(r'/[A-Za-z0-9]', t):\n",
        "        rep = t.replace('.', ' dot ').replace('/', ' slash ').replace('-', ' dash ')\n",
        "        rep = re.sub(r'\\bwww\\b', 'w w w', rep)\n",
        "        return ' '.join(rep.split())\n",
        "    return None\n",
        "\n",
        "# Detectors (tightened LETTERS and DATE ambiguity guard, add month-name dates)\n",
        "pat_decimal = re.compile(r'^-?[\\d,]+\\.\\d+$')\n",
        "pat_cardinal = re.compile(r'^-?[\\d,]+$')\n",
        "pat_ordinal = re.compile(r'^\\d+(st|nd|rd|th)$')\n",
        "pat_fraction = re.compile(r'^\\d+/\\d+$')\n",
        "pat_date_y_m_d_dash = re.compile(r'^\\d{4}-\\d{1,2}-\\d{1,2}$')\n",
        "pat_date_y_m_d_slash = re.compile(r'^\\d{4}/\\d{1,2}/\\d{1,2}$')\n",
        "pat_date_two_sep = re.compile(r'^(\\d{1,2})[/-](\\d{1,2})[/-](\\d{2,4})$')\n",
        "pat_date_dot_y_m_d = re.compile(r'^\\d{4}\\.\\d{1,2}\\.\\d{1,2}$')\n",
        "pat_date_dot_two_sep = re.compile(r'^(\\d{1,2})\\.(\\d{1,2})\\.(\\d{2,4})$')\n",
        "pat_decade = re.compile(r'^\\d{4}s$')\n",
        "pat_letters_strong = re.compile(r'^(?:[A-Z]{3,}|(?:[A-Z]\\.){3,}[A-Z]?\\.?|[A-Z]+-[A-Z]+)$')\n",
        "pat_phone = re.compile(r'^[+\\d][\\d\\s().-]{6,}$')  # at least ~7 digits overall\n",
        "pat_money = re.compile(r'^[$\\u00a3\\u20ac]')\n",
        "pat_time = re.compile(r'^\\d{1,2}:\\d{2}$')\n",
        "pat_measure = re.compile(r'^-?[\\d,]+(?:\\.\\d+)?\\s?[a-zA-Z\\u00b0/%]+')\n",
        "# Month-name date detectors\n",
        "pat_month_name_1 = re.compile(r'(?:(?:Mon|Tue|Tues|Wed|Thu|Thur|Thurs|Fri|Sat|Sun|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday),?\\s+)?(' + MONTH_ALT_PATTERN + r')\\s+\\d{1,2}(?:,)?(?:\\s+\\d{4})?$', re.IGNORECASE)\n",
        "pat_month_name_2 = re.compile(r'^\\d{1,2}\\s+(' + MONTH_ALT_PATTERN + r')(?:,)?(?:\\s+\\d{4})?$', re.IGNORECASE)\n",
        "\n",
        "def infer_class(tok, prior=None):\n",
        "    if prior: return prior\n",
        "    if pat_money.search(tok): return 'MONEY'\n",
        "    if pat_time.fullmatch(tok): return 'TIME'\n",
        "    if pat_fraction.fullmatch(tok): return 'FRACTION'\n",
        "    if pat_decimal.fullmatch(tok) or tok.startswith('.') and re.fullmatch(r'\\.\\d+', tok): return 'DECIMAL'\n",
        "    if pat_ordinal.fullmatch(tok): return 'ORDINAL'\n",
        "    if pat_cardinal.fullmatch(tok): return 'CARDINAL'\n",
        "    # DATE: unambiguous numeric forms or decades\n",
        "    if pat_date_y_m_d_dash.fullmatch(tok) or pat_date_y_m_d_slash.fullmatch(tok) or pat_date_dot_y_m_d.fullmatch(tok):\n",
        "        return 'DATE'\n",
        "    m = pat_date_two_sep.fullmatch(tok)\n",
        "    if m:\n",
        "        a,b,y = m.groups()\n",
        "        if _unambiguous_mdy(a,b,y):\n",
        "            return 'DATE'\n",
        "    m = pat_date_dot_two_sep.fullmatch(tok)\n",
        "    if m:\n",
        "        a,b,y = m.groups()\n",
        "        if _unambiguous_mdy(a,b,y):\n",
        "            return 'DATE'\n",
        "    if pat_decade.fullmatch(tok):\n",
        "        return 'DATE'\n",
        "    # Month-name dates\n",
        "    if pat_month_name_1.fullmatch(tok) or pat_month_name_2.fullmatch(tok):\n",
        "        return 'DATE'\n",
        "    # LETTERS strong only, with blacklist exceptions\n",
        "    if tok in LETTERS_EXCEPT:\n",
        "        pass\n",
        "    else:\n",
        "        if pat_letters_strong.fullmatch(tok):\n",
        "            return 'LETTERS'\n",
        "    # Telephone (require at least ~7 digits total)\n",
        "    if pat_phone.fullmatch(tok):\n",
        "        return 'TELEPHONE'\n",
        "    # Electronic tightened\n",
        "    t = tok.strip()\n",
        "    if (t.count('@') == 1 and '.' in t[t.find('@')+1:]) or t.lower().startswith('www.') or re.search(r'/[A-Za-z0-9]', t):\n",
        "        return 'ELECTRONIC'\n",
        "    if pat_measure.match(tok.lower()): return 'MEASURE'\n",
        "    return None\n",
        "\n",
        "def apply_rules(tok, cls):\n",
        "    if cls == 'DATE': return normalize_date(tok)\n",
        "    if cls == 'LETTERS': return normalize_letters(tok)\n",
        "    if cls == 'DECIMAL': return normalize_decimal(tok)\n",
        "    if cls == 'CARDINAL': return normalize_cardinal(tok)\n",
        "    if cls == 'ORDINAL': return normalize_ordinal(tok)\n",
        "    if cls == 'DIGIT': return normalize_digit(tok)\n",
        "    if cls == 'MEASURE': return normalize_measure(tok)\n",
        "    if cls == 'TELEPHONE': return normalize_telephone(tok)\n",
        "    if cls == 'ELECTRONIC': return normalize_electronic(tok)\n",
        "    if cls == 'MONEY': return normalize_money(tok)\n",
        "    if cls == 'TIME': return normalize_time(tok)\n",
        "    if cls == 'FRACTION': return normalize_fraction(tok)\n",
        "    return None"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "id": "fa742ba9-d093-49dc-b9e2-0582524ed909",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# T6: Targeted error analysis for DATE / LETTERS / CARDINAL on leak-free CV\n",
        "import re, time\n",
        "t0 = time.time()\n",
        "assert 'val' in globals() and 'pred_val' in globals(), 'Run Cell 6 (CV) first to populate val/pred_val'\n",
        "\n",
        "df_err = val.copy()\n",
        "df_err = df_err.assign(pred=pred_val.values)\n",
        "mask_err = df_err['pred'] != df_err['after']\n",
        "df_err = df_err.loc[mask_err, ['sentence_id','token_id','class','before','after','pred']].copy()\n",
        "df_err = df_err.sort_values(['sentence_id','token_id'])\n",
        "df_err['prev_before'] = df_err.groupby('sentence_id')['before'].shift(1)\n",
        "df_err['next_before'] = df_err.groupby('sentence_id')['before'].shift(-1)\n",
        "df_err['prev2_before'] = df_err.groupby('sentence_id')['before'].shift(2)\n",
        "\n",
        "print('Error counts (top 10):')\n",
        "print(df_err['class'].value_counts().head(10))\n",
        "\n",
        "def preview(cls_name, n=10):\n",
        "    print(f\"\\n--- {cls_name} sample errors ---\")\n",
        "    display(df_err[df_err['class']==cls_name].head(n))\n",
        "\n",
        "preview('DATE', 12)\n",
        "preview('LETTERS', 12)\n",
        "preview('CARDINAL', 12)\n",
        "\n",
        "# DATE diagnostics\n",
        "date_err = df_err[df_err['class']=='DATE'].copy()\n",
        "date_err['before_s'] = date_err['before'].astype(str)\n",
        "pat_slash = re.compile(r'^\\d{1,4}/\\d{1,2}/\\d{1,4}$')\n",
        "pat_dash = re.compile(r'^\\d{1,4}-\\d{1,2}-\\d{1,4}$')\n",
        "pat_dot  = re.compile(r'^\\d{1,4}\\.\\d{1,2}\\.\\d{1,4}$')\n",
        "pat_decade = re.compile(r'^\\d{4}s$')\n",
        "def bucket_date(s):\n",
        "    if pat_decade.fullmatch(s): return 'decade'\n",
        "    if pat_slash.fullmatch(s): return 'slash'\n",
        "    if pat_dash.fullmatch(s): return 'dash'\n",
        "    if pat_dot.fullmatch(s): return 'dot'\n",
        "    return 'other'\n",
        "date_err['bucket'] = date_err['before_s'].apply(bucket_date)\n",
        "print('\\nDATE error buckets:')\n",
        "print(date_err['bucket'].value_counts())\n",
        "\n",
        "# Ambiguous M/D/Y vs D/M/Y check (both parts <=12)\n",
        "def ambiguous_mdy(s):\n",
        "    m = re.fullmatch(r'(?:(\\d{1,2})[/-](\\d{1,2})[/-](\\d{2,4}))', s or '')\n",
        "    if not m: return False\n",
        "    a,b,y = m.groups()\n",
        "    try:\n",
        "        a=int(a); b=int(b)\n",
        "    except:\n",
        "        return False\n",
        "    return a<=12 and b<=12\n",
        "date_err['ambig_mdy'] = date_err['before_s'].apply(ambiguous_mdy)\n",
        "print('\\nDATE ambiguous M/D/Y count:', int(date_err['ambig_mdy'].sum()))\n",
        "\n",
        "# Month context around 4-digit years in errors\n",
        "month_tokens = set(['january','february','march','april','may','june','july','august','september','october','november','december',\n",
        "                    'jan','feb','mar','apr','jun','jul','aug','sep','sept','oct','nov','dec','jan.','feb.','mar.','apr.','jun.','jul.','aug.','sep.','sept.','oct.','nov.','dec.'])\n",
        "def is_year4(s):\n",
        "    return bool(re.fullmatch(r'\\d{4}', str(s) or ''))\n",
        "yr_err = date_err[date_err['before'].apply(is_year4)].copy()\n",
        "yr_err['prev_is_month'] = yr_err['prev_before'].str.lower().isin(month_tokens)\n",
        "yr_err['prev2_is_month'] = yr_err['prev2_before'].str.lower().isin(month_tokens)\n",
        "print('\\nDATE 4-digit year errors with month context:')\n",
        "print('prev month:', int(yr_err['prev_is_month'].sum()), '| prev2 month:', int(yr_err['prev2_is_month'].sum()))\n",
        "\n",
        "# LETTERS diagnostics\n",
        "let_err = df_err[df_err['class']=='LETTERS'].copy()\n",
        "def letters_bucket(s):\n",
        "    s2 = str(s or '')\n",
        "    if re.fullmatch(r'[A-Z]{3,}', s2): return 'ALLCAPS>=3'\n",
        "    if re.fullmatch(r'([A-Z]\\.){3,}[A-Z]?\\.?', s2): return 'DOTTED>=3'\n",
        "    if re.fullmatch(r'[A-Z]+-[A-Z]+', s2): return 'HYPHEN_CAPS'\n",
        "    if re.fullmatch(r'[A-Za-z]+', s2) and any(c.islower() for c in s2): return 'wordlike_mixed'\n",
        "    return 'other'\n",
        "let_err['bucket'] = let_err['before'].apply(letters_bucket)\n",
        "print('\\nLETTERS error buckets:')\n",
        "print(let_err['bucket'].value_counts())\n",
        "\n",
        "# CARDINAL diagnostics\n",
        "card_err = df_err[df_err['class']=='CARDINAL'].copy()\n",
        "def is_four_digit_yearish(s):\n",
        "    if not re.fullmatch(r'\\d{4}', str(s) or ''): return False\n",
        "    v = int(s)\n",
        "    return 1000 <= v <= 2099\n",
        "card_err['four_digit_yearish'] = card_err['before'].apply(is_four_digit_yearish)\n",
        "print('\\nCARDINAL 4-digit yearish errors:', int(card_err['four_digit_yearish'].sum()))\n",
        "card_yr = card_err[card_err['four_digit_yearish']].copy()\n",
        "card_yr['prev_is_month'] = card_yr['prev_before'].str.lower().isin(month_tokens)\n",
        "card_yr['prev2_is_month'] = card_yr['prev2_before'].str.lower().isin(month_tokens)\n",
        "print('...with month context -> prev:', int(card_yr['prev_is_month'].sum()), 'prev2:', int(card_yr['prev2_is_month'].sum()))\n",
        "\n",
        "print('\\nT6 elapsed:', f\"{time.time()-t0:.2f}s\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error counts (top 10):\nclass\nPLAIN         6606\nDATE          4921\nLETTERS       3546\nCARDINAL      1889\nMEASURE       1127\nTELEPHONE      682\nELECTRONIC     652\nMONEY          621\nDECIMAL        577\nDIGIT          527\nName: count, dtype: int64\n\n--- DATE sample errors ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "       sentence_id  token_id class              before                                            after                pred prev_before next_before prev2_before\n454             34         1  DATE     7 December 2015           the seventh of december twenty fifteen     7 December 2015         NaN         NaN          NaN\n1342           106         0  DATE    January 30, 1987          january thirtieth nineteen eighty seven    January 30, 1987         NaN         NaN          NaN\n5289           427         5  DATE   February 14, 1999         february fourteenth nineteen ninety nine   February 14, 1999         NaN         NaN          NaN\n5996           485         1  DATE        Feb. 6, 2008                february sixth two thousand eight        Feb. 6, 2008         NaN         NaN          NaN\n10755          881         5  DATE          2006-05-23         the twenty third of may two thousand six          2006-05-23         NaN         NaN          NaN\n13272         1094         1  DATE  Thursday, April 19                        thursday april nineteenth  Thursday, April 19         NaN         NaN          NaN\n16084         1310         5  DATE       March 3, 1884                 march third eighteen eighty four       March 3, 1884         NaN         NaN          NaN\n19544         1568         6  DATE     August 31, 1945          august thirty first nineteen forty five     August 31, 1945          dr         NaN          NaN\n21325         1713         1  DATE      April 14, 1999            april fourteenth nineteen ninety nine      April 14, 1999         NaN         NaN          NaN\n25300         2014         0  DATE   December 26, 1993      december twenty sixth nineteen ninety three   December 26, 1993         NaN         NaN          NaN\n29424         2336         4  DATE      March 23, 1952            march twenty third nineteen fifty two      March 23, 1952         NaN         NaN          NaN\n33154         2629         0  DATE    19 December 1924  the nineteenth of december nineteen twenty four    19 December 1924         NaN         NaN          NaN",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>token_id</th>\n      <th>class</th>\n      <th>before</th>\n      <th>after</th>\n      <th>pred</th>\n      <th>prev_before</th>\n      <th>next_before</th>\n      <th>prev2_before</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>454</th>\n      <td>34</td>\n      <td>1</td>\n      <td>DATE</td>\n      <td>7 December 2015</td>\n      <td>the seventh of december twenty fifteen</td>\n      <td>7 December 2015</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1342</th>\n      <td>106</td>\n      <td>0</td>\n      <td>DATE</td>\n      <td>January 30, 1987</td>\n      <td>january thirtieth nineteen eighty seven</td>\n      <td>January 30, 1987</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5289</th>\n      <td>427</td>\n      <td>5</td>\n      <td>DATE</td>\n      <td>February 14, 1999</td>\n      <td>february fourteenth nineteen ninety nine</td>\n      <td>February 14, 1999</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5996</th>\n      <td>485</td>\n      <td>1</td>\n      <td>DATE</td>\n      <td>Feb. 6, 2008</td>\n      <td>february sixth two thousand eight</td>\n      <td>Feb. 6, 2008</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10755</th>\n      <td>881</td>\n      <td>5</td>\n      <td>DATE</td>\n      <td>2006-05-23</td>\n      <td>the twenty third of may two thousand six</td>\n      <td>2006-05-23</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13272</th>\n      <td>1094</td>\n      <td>1</td>\n      <td>DATE</td>\n      <td>Thursday, April 19</td>\n      <td>thursday april nineteenth</td>\n      <td>Thursday, April 19</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16084</th>\n      <td>1310</td>\n      <td>5</td>\n      <td>DATE</td>\n      <td>March 3, 1884</td>\n      <td>march third eighteen eighty four</td>\n      <td>March 3, 1884</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>19544</th>\n      <td>1568</td>\n      <td>6</td>\n      <td>DATE</td>\n      <td>August 31, 1945</td>\n      <td>august thirty first nineteen forty five</td>\n      <td>August 31, 1945</td>\n      <td>dr</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>21325</th>\n      <td>1713</td>\n      <td>1</td>\n      <td>DATE</td>\n      <td>April 14, 1999</td>\n      <td>april fourteenth nineteen ninety nine</td>\n      <td>April 14, 1999</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25300</th>\n      <td>2014</td>\n      <td>0</td>\n      <td>DATE</td>\n      <td>December 26, 1993</td>\n      <td>december twenty sixth nineteen ninety three</td>\n      <td>December 26, 1993</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>29424</th>\n      <td>2336</td>\n      <td>4</td>\n      <td>DATE</td>\n      <td>March 23, 1952</td>\n      <td>march twenty third nineteen fifty two</td>\n      <td>March 23, 1952</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>33154</th>\n      <td>2629</td>\n      <td>0</td>\n      <td>DATE</td>\n      <td>19 December 1924</td>\n      <td>the nineteenth of december nineteen twenty four</td>\n      <td>19 December 1924</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n--- LETTERS sample errors ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "       sentence_id  token_id    class   before          after     pred prev_before next_before prev2_before\n740             54        12  LETTERS      Ukh          u k h      Ukh         NaN         NaN          NaN\n2715           219         0  LETTERS    CSIPL      c s i p l    CSIPL         NaN         MUV          NaN\n2725           219        10  LETTERS      MUV          m u v      MUV       CSIPL         NaN          NaN\n7125           582         1  LETTERS     SSNP        s s n p     SSNP         NaN   organised          NaN\n7817           635         0  LETTERS     FISD        f i s d     FISD         NaN         NaN          NaN\n11049          908         4  LETTERS    RCACC      r c a c c    RCACC        2968         NaN          NaN\n14108         1147        13  LETTERS     Tarw        t a r w     Tarw         NaN         NaN          NaN\n16185         1317         6  LETTERS  NYCLU's    n y c l u's  NYCLU's         NaN         NaN          NaN\n16271         1321        21  LETTERS     WDTA        w d t a     WDTA         NaN         NaN          NaN\n16392         1332         3  LETTERS  HURINET  h u r i n e t  HURINET         NaN         NaN          NaN\n17477         1416         6  LETTERS      ACA          a c a      ACA         NaN         NaN          NaN\n18992         1527         3  LETTERS    SWNTs      s w n t's    SWNTs         NaN         NaN          NaN",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>token_id</th>\n      <th>class</th>\n      <th>before</th>\n      <th>after</th>\n      <th>pred</th>\n      <th>prev_before</th>\n      <th>next_before</th>\n      <th>prev2_before</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>740</th>\n      <td>54</td>\n      <td>12</td>\n      <td>LETTERS</td>\n      <td>Ukh</td>\n      <td>u k h</td>\n      <td>Ukh</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2715</th>\n      <td>219</td>\n      <td>0</td>\n      <td>LETTERS</td>\n      <td>CSIPL</td>\n      <td>c s i p l</td>\n      <td>CSIPL</td>\n      <td>NaN</td>\n      <td>MUV</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2725</th>\n      <td>219</td>\n      <td>10</td>\n      <td>LETTERS</td>\n      <td>MUV</td>\n      <td>m u v</td>\n      <td>MUV</td>\n      <td>CSIPL</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7125</th>\n      <td>582</td>\n      <td>1</td>\n      <td>LETTERS</td>\n      <td>SSNP</td>\n      <td>s s n p</td>\n      <td>SSNP</td>\n      <td>NaN</td>\n      <td>organised</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7817</th>\n      <td>635</td>\n      <td>0</td>\n      <td>LETTERS</td>\n      <td>FISD</td>\n      <td>f i s d</td>\n      <td>FISD</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11049</th>\n      <td>908</td>\n      <td>4</td>\n      <td>LETTERS</td>\n      <td>RCACC</td>\n      <td>r c a c c</td>\n      <td>RCACC</td>\n      <td>2968</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14108</th>\n      <td>1147</td>\n      <td>13</td>\n      <td>LETTERS</td>\n      <td>Tarw</td>\n      <td>t a r w</td>\n      <td>Tarw</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16185</th>\n      <td>1317</td>\n      <td>6</td>\n      <td>LETTERS</td>\n      <td>NYCLU's</td>\n      <td>n y c l u's</td>\n      <td>NYCLU's</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16271</th>\n      <td>1321</td>\n      <td>21</td>\n      <td>LETTERS</td>\n      <td>WDTA</td>\n      <td>w d t a</td>\n      <td>WDTA</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16392</th>\n      <td>1332</td>\n      <td>3</td>\n      <td>LETTERS</td>\n      <td>HURINET</td>\n      <td>h u r i n e t</td>\n      <td>HURINET</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17477</th>\n      <td>1416</td>\n      <td>6</td>\n      <td>LETTERS</td>\n      <td>ACA</td>\n      <td>a c a</td>\n      <td>ACA</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18992</th>\n      <td>1527</td>\n      <td>3</td>\n      <td>LETTERS</td>\n      <td>SWNTs</td>\n      <td>s w n t's</td>\n      <td>SWNTs</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n--- CARDINAL sample errors ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "       sentence_id  token_id     class     before                                                       after                  pred prev_before next_before prev2_before\n460             35         4  CARDINAL    1065520         one million sixty five thousand five hundred twenty               1065520         NaN         NaN          NaN\n3356           275         2  CARDINAL     20,300                               twenty thousand three hundred                20,300         NaN         NaN          NaN\n7563           617        17  CARDINAL    235,758  two hundred thirty five thousand seven hundred fifty eight               235,758         NaN      24,606          NaN\n7569           617        23  CARDINAL     24,606                        twenty four thousand six hundred six                24,606     235,758         NaN          NaN\n8917           731         2  CARDINAL     11,339                   eleven thousand three hundred thirty nine                11,339         NaN       550.8          NaN\n11048          908         3  CARDINAL       2968                       two thousand nine hundred sixty eight                  2968         NaN       RCACC          NaN\n11681          964        12  CARDINAL     10,044                                     ten thousand forty four                10,044         NaN         NaN          NaN\n12262         1014         4  CARDINAL         00                                                        zero                   o o         NaN         NaN          NaN\n12510         1037        17  CARDINAL     19,506                          nineteen thousand five hundred six                19,506         NaN         NaN          NaN\n16477         1339         7  CARDINAL        78,                                               seventy eight                   78,         NaN         NaN          NaN\n16549         1345         6  CARDINAL  322 U.S.                                     three hundred twenty two             322 U.S.          NaN         NaN          NaN\n17900         1445        13  CARDINAL       1994                       one thousand nine hundred ninety four  nineteen ninety four         NaN         NaN          NaN",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>token_id</th>\n      <th>class</th>\n      <th>before</th>\n      <th>after</th>\n      <th>pred</th>\n      <th>prev_before</th>\n      <th>next_before</th>\n      <th>prev2_before</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>460</th>\n      <td>35</td>\n      <td>4</td>\n      <td>CARDINAL</td>\n      <td>1065520</td>\n      <td>one million sixty five thousand five hundred twenty</td>\n      <td>1065520</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3356</th>\n      <td>275</td>\n      <td>2</td>\n      <td>CARDINAL</td>\n      <td>20,300</td>\n      <td>twenty thousand three hundred</td>\n      <td>20,300</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7563</th>\n      <td>617</td>\n      <td>17</td>\n      <td>CARDINAL</td>\n      <td>235,758</td>\n      <td>two hundred thirty five thousand seven hundred fifty eight</td>\n      <td>235,758</td>\n      <td>NaN</td>\n      <td>24,606</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7569</th>\n      <td>617</td>\n      <td>23</td>\n      <td>CARDINAL</td>\n      <td>24,606</td>\n      <td>twenty four thousand six hundred six</td>\n      <td>24,606</td>\n      <td>235,758</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8917</th>\n      <td>731</td>\n      <td>2</td>\n      <td>CARDINAL</td>\n      <td>11,339</td>\n      <td>eleven thousand three hundred thirty nine</td>\n      <td>11,339</td>\n      <td>NaN</td>\n      <td>550.8</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11048</th>\n      <td>908</td>\n      <td>3</td>\n      <td>CARDINAL</td>\n      <td>2968</td>\n      <td>two thousand nine hundred sixty eight</td>\n      <td>2968</td>\n      <td>NaN</td>\n      <td>RCACC</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11681</th>\n      <td>964</td>\n      <td>12</td>\n      <td>CARDINAL</td>\n      <td>10,044</td>\n      <td>ten thousand forty four</td>\n      <td>10,044</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12262</th>\n      <td>1014</td>\n      <td>4</td>\n      <td>CARDINAL</td>\n      <td>00</td>\n      <td>zero</td>\n      <td>o o</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12510</th>\n      <td>1037</td>\n      <td>17</td>\n      <td>CARDINAL</td>\n      <td>19,506</td>\n      <td>nineteen thousand five hundred six</td>\n      <td>19,506</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16477</th>\n      <td>1339</td>\n      <td>7</td>\n      <td>CARDINAL</td>\n      <td>78,</td>\n      <td>seventy eight</td>\n      <td>78,</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16549</th>\n      <td>1345</td>\n      <td>6</td>\n      <td>CARDINAL</td>\n      <td>322 U.S.</td>\n      <td>three hundred twenty two</td>\n      <td>322 U.S.</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17900</th>\n      <td>1445</td>\n      <td>13</td>\n      <td>CARDINAL</td>\n      <td>1994</td>\n      <td>one thousand nine hundred ninety four</td>\n      <td>nineteen ninety four</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nDATE error buckets:\nbucket\nother     4287\ndash       525\nslash       67\ndot         31\ndecade      11\nName: count, dtype: int64\n\nDATE ambiguous M/D/Y count: 37\n\nDATE 4-digit year errors with month context:\nprev month: 0 | prev2 month: 0\n\nLETTERS error buckets:\nbucket\nALLCAPS>=3        2036\nother              752\nwordlike_mixed     642\nDOTTED>=3          116\nName: count, dtype: int64\n\nCARDINAL 4-digit yearish errors: 405\n...with month context -> prev: 0 prev2: 0\n\nT6 elapsed: 0.77s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}