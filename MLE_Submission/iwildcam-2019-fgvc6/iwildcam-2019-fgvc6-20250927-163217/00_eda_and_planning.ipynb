{
  "cells": [
    {
      "id": "7066face-1a23-4708-bfb1-b0990dad0ae8",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: iWildCam 2019 - FGVC6 (Medal-focused)\n",
        "\n",
        "Objectives:\n",
        "- Establish GPU-ready environment quickly; verify GPU first.\n",
        "- Build robust, fast baseline with strong CV mirroring test (site/sequence-aware if available).\n",
        "- Iterate to a medal via model/augmentation/ensembling with trustworthy OOF.\n",
        "\n",
        "Workflow:\n",
        "1) Environment & GPU check\n",
        "   - Verify CUDA/GPU with nvidia-smi; install PyTorch cu121 if needed.\n",
        "   - Set constraints to avoid torch drift.\n",
        "\n",
        "2) Data audit\n",
        "   - Inspect train.csv/test.csv schema; image paths; class counts; label imbalance.\n",
        "   - Check for site/location/domains (e.g., location, seq_id) to build GroupKFold if present.\n",
        "   - Verify images exist; unzip with progress and cache paths.\n",
        "\n",
        "3) CV protocol\n",
        "   - Target: macro-F1. Use stratified KFold on category_id; if site/seq available, use StratifiedGroupKFold.\n",
        "   - Fix a single seed and save folds.json for reproducibility.\n",
        "\n",
        "4) Baseline model\n",
        "   - torchvision pretrained backbones (e.g., convnext_tiny, resnet50, efficientnet_v2_s) with AMP + SGD/AdamW.\n",
        "   - 224 or 256 resolution; light augmentations (RandAug/AutoAug, ColorJitter, RandomResizedCrop).\n",
        "   - Class-balanced sampler or focal loss to address imbalance.\n",
        "   - Train 1\u20132 epochs smoke test on subset; then 5-fold full run with early stopping on F1.\n",
        "\n",
        "5) Improvements\n",
        "   - Higher resolution (384/448) for final; CutMix/Mixup; cosine schedule; EMA.\n",
        "   - Calibrate thresholds per-class (optimize F1 on OOF logits).\n",
        "   - TTA at inference.\n",
        "   - Ensemble diverse backbones/seeds.\n",
        "\n",
        "6) Error analysis\n",
        "   - OOF confusion matrix; per-class F1; tune thresholds; mine hard classes.\n",
        "\n",
        "7) Submission\n",
        "   - Predict test with TTA; save submission.csv; verify format.\n",
        "\n",
        "Checkpoints for expert review:\n",
        "- After plan (this cell).\n",
        "- After data audit & CV design.\n",
        "- After baseline OOF.\n",
        "- After improvements/ensembles.\n",
        "\n",
        "Next actions:\n",
        "- Add GPU check cell and run.\n",
        "- Load CSVs; inspect columns; check for grouping keys.\n",
        "- Unzip images to folders and verify counts."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "3506d2d4-a07f-44ef-9fec-123cee8593fb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess, time, shutil, os, sys\n",
        "print(\"[GPU CHECK] Running nvidia-smi...\", flush=True)\n",
        "try:\n",
        "    out = subprocess.run([\"bash\",\"-lc\",\"nvidia-smi || true\"], capture_output=True, text=True, check=False)\n",
        "    print(out.stdout)\n",
        "    if out.stderr:\n",
        "        print(out.stderr, file=sys.stderr)\n",
        "except Exception as e:\n",
        "    print(\"nvidia-smi failed:\", e)\n",
        "print(\"[ENV] Python:\", sys.version)\n",
        "print(\"[ENV] CUDA paths present:\", os.path.exists(\"/usr/local/nvidia\"))\n",
        "print(\"[DISK] / and /mnt usage:\")\n",
        "for p in [\"/\",\"/mnt\"]:\n",
        "    try:\n",
        "        total, used, free = shutil.disk_usage(p)\n",
        "        print(f\"  {p}: total={total/1e12:.2f}TB used={used/1e12:.2f}TB free={free/1e12:.2f}TB\")\n",
        "    except Exception as e:\n",
        "        print(f\"  {p}: error {e}\")\n",
        "print(\"[DONE] GPU/Env check finished.\", flush=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GPU CHECK] Running nvidia-smi...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 27 17:05:08 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     182MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\n[ENV] Python: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\n[ENV] CUDA paths present: True\n[DISK] / and /mnt usage:\n  /: total=1.25TB used=0.05TB free=1.20TB\n  /mnt: total=1.52TB used=0.07TB free=1.37TB\n[DONE] GPU/Env check finished.\n"
          ]
        }
      ]
    },
    {
      "id": "af617e33-770a-4c71-8f5d-a64addef4720",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd, os, json, numpy as np\n",
        "pd.set_option('display.max_columns', 200)\n",
        "print('[DATA AUDIT] Loading CSVs...', flush=True)\n",
        "train_path, test_path = 'train.csv', 'test.csv'\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "print('[TRAIN] shape:', train.shape, ' columns:', list(train.columns))\n",
        "print('[TEST ] shape:', test.shape,  ' columns:', list(test.columns))\n",
        "\n",
        "# Inspect target and candidate grouping keys\n",
        "target_col = 'category_id' if 'category_id' in train.columns else None\n",
        "cand_groups = ['sequence_id','seq_id','seq_id_long','sequence','camera_id','location','site','location_id','place']\n",
        "present_groups = [c for c in cand_groups if c in train.columns]\n",
        "print('[CAND GROUP COLS IN TRAIN]:', present_groups)\n",
        "print('[TARGET] present:', target_col is not None)\n",
        "if target_col:\n",
        "    print('[TARGET] nunique classes:', train[target_col].nunique())\n",
        "    print('[TARGET] head value_counts:')\n",
        "    print(train[target_col].value_counts().head(10))\n",
        "\n",
        "# Missing values overview\n",
        "na_train = train.isna().mean().sort_values(ascending=False)\n",
        "na_test = test.isna().mean().sort_values(ascending=False)\n",
        "print('[NA RATE TRAIN] top 10:\\n', na_train.head(10))\n",
        "print('[NA RATE TEST ] top 10:\\n', na_test.head(10))\n",
        "\n",
        "# Show a few rows for schema understanding\n",
        "print('\\n[TRAIN HEAD]\\n', train.head(3))\n",
        "print('\\n[TEST  HEAD]\\n', test.head(3))\n",
        "\n",
        "# Verify image filename/path columns and zip existence\n",
        "img_cols = [c for c in train.columns if 'file' in c.lower() or 'image' in c.lower() or 'path' in c.lower()]\n",
        "print('[IMAGE-RELATED COLS IN TRAIN]:', img_cols)\n",
        "print('[FILES] train_images.zip exists:', os.path.exists('train_images.zip'), ' size:', os.path.getsize('train_images.zip') if os.path.exists('train_images.zip') else -1)\n",
        "print('[FILES] test_images.zip  exists:', os.path.exists('test_images.zip'),  ' size:', os.path.getsize('test_images.zip') if os.path.exists('test_images.zip') else -1)\n",
        "\n",
        "# Save a quick schema summary for folds planning\n",
        "schema = {\n",
        "    'train_columns': list(train.columns),\n",
        "    'test_columns': list(test.columns),\n",
        "    'present_groups': present_groups,\n",
        "    'n_classes': int(train[target_col].nunique()) if target_col else None\n",
        "}\n",
        "json.dump(schema, open('schema_summary.json','w'))\n",
        "print('[SAVED] schema_summary.json')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DATA AUDIT] Loading CSVs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN] shape: (179422, 11)  columns: ['category_id', 'date_captured', 'file_name', 'frame_num', 'id', 'location', 'rights_holder', 'seq_id', 'seq_num_frames', 'width', 'height']\n[TEST ] shape: (16877, 10)  columns: ['date_captured', 'file_name', 'frame_num', 'id', 'location', 'rights_holder', 'seq_id', 'seq_num_frames', 'width', 'height']\n[CAND GROUP COLS IN TRAIN]: ['seq_id', 'location']\n[TARGET] present: True\n[TARGET] nunique classes: 14\n[TARGET] head value_counts:\ncategory_id\n0     128468\n19     10861\n1       6035\n8       5783\n11      5762\n13      5303\n16      4773\n17      4125\n3       2902\n18      1846\nName: count, dtype: int64\n[NA RATE TRAIN] top 10:\n category_id       0.0\ndate_captured     0.0\nfile_name         0.0\nframe_num         0.0\nid                0.0\nlocation          0.0\nrights_holder     0.0\nseq_id            0.0\nseq_num_frames    0.0\nwidth             0.0\ndtype: float64\n[NA RATE TEST ] top 10:\n date_captured     0.0\nfile_name         0.0\nframe_num         0.0\nid                0.0\nlocation          0.0\nrights_holder     0.0\nseq_id            0.0\nseq_num_frames    0.0\nwidth             0.0\nheight            0.0\ndtype: float64\n\n[TRAIN HEAD]\n    category_id        date_captured                                 file_name  \\\n0           19  2012-03-17 03:48:44  588a679f-23d2-11e8-a6a3-ec086b02610b.jpg   \n1            0  2014-05-11 11:56:46  59279ce3-23d2-11e8-a6a3-ec086b02610b.jpg   \n2            0  2013-10-06 02:00:00  5a2af4ab-23d2-11e8-a6a3-ec086b02610b.jpg   \n\n   frame_num                                    id  location  rights_holder  \\\n0          2  588a679f-23d2-11e8-a6a3-ec086b02610b       115   Justin Brown   \n1          1  59279ce3-23d2-11e8-a6a3-ec086b02610b        96  Erin Boydston   \n2          1  5a2af4ab-23d2-11e8-a6a3-ec086b02610b        57  Erin Boydston   \n\n                                 seq_id  seq_num_frames  width  height  \n0  6f12067d-5567-11e8-b3c0-dca9047ef277               3   1024     747  \n1  6faa92d1-5567-11e8-b1ae-dca9047ef277               1   1024     747  \n2  6f7d4702-5567-11e8-9e03-dca9047ef277               1   1024     747  \n\n[TEST  HEAD]\n          date_captured                                 file_name  frame_num  \\\n0  2011-05-13 23:43:18  5998cfa4-23d2-11e8-a6a3-ec086b02610b.jpg          1   \n1  2011-07-12 13:11:16  599fbd89-23d2-11e8-a6a3-ec086b02610b.jpg          3   \n2  2012-01-05 07:41:39  59fae563-23d2-11e8-a6a3-ec086b02610b.jpg          3   \n\n                                     id  location rights_holder  \\\n0  5998cfa4-23d2-11e8-a6a3-ec086b02610b        33  Justin Brown   \n1  599fbd89-23d2-11e8-a6a3-ec086b02610b        46  Justin Brown   \n2  59fae563-23d2-11e8-a6a3-ec086b02610b        46  Justin Brown   \n\n                                 seq_id  seq_num_frames  width  height  \n0  6f084ccc-5567-11e8-bc84-dca9047ef277               3   1024     747  \n1  6f1728a1-5567-11e8-9be7-dca9047ef277               3   1024     747  \n2  6f181999-5567-11e8-a472-dca9047ef277               3   1024     747  \n[IMAGE-RELATED COLS IN TRAIN]: ['file_name']\n[FILES] train_images.zip exists: True  size: 26120137404\n[FILES] test_images.zip  exists: True  size: 1779783217\n[SAVED] schema_summary.json\n"
          ]
        }
      ]
    },
    {
      "id": "22e8a669-eb46-406d-8eb6-90c0d7c37163",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, time, glob, zipfile, sys\n",
        "\n",
        "def extract_zip_py(zip_path, out_dir, progress_interval=500):\n",
        "    t0 = time.time()\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path) as zf:\n",
        "        members = zf.infolist()\n",
        "        n = len(members)\n",
        "        print(f\"[UNZIP] Extracting {zip_path} -> {out_dir} ({n} files)\", flush=True)\n",
        "        for i, m in enumerate(members, 1):\n",
        "            zf.extract(m, out_dir)\n",
        "            if i % progress_interval == 0 or i == n:\n",
        "                dt = time.time() - t0\n",
        "                print(f\"  extracted {i}/{n} ({i/n*100:.1f}%) elapsed {dt/60:.1f} min\", flush=True)\n",
        "    n_files = sum([len(files) for _, _, files in os.walk(out_dir)])\n",
        "    dt = time.time() - t0\n",
        "    print(f\"[UNZIP DONE] {zip_path}: {n_files} files in {dt/60:.2f} min\", flush=True)\n",
        "\n",
        "def needs_extract(out_dir, pattern='*.jpg'):\n",
        "    return not os.path.exists(out_dir) or len(glob.glob(os.path.join(out_dir, pattern))) == 0\n",
        "\n",
        "if os.path.exists('train_images.zip') and needs_extract('train_images'):\n",
        "    extract_zip_py('train_images.zip', 'train_images')\n",
        "else:\n",
        "    print('[SKIP] train_images already extracted or zip missing')\n",
        "\n",
        "if os.path.exists('test_images.zip') and needs_extract('test_images'):\n",
        "    extract_zip_py('test_images.zip', 'test_images')\n",
        "else:\n",
        "    print('[SKIP] test_images already extracted or zip missing')\n",
        "\n",
        "train_samples = glob.glob('train_images/*.jpg')[:3]\n",
        "test_samples = glob.glob('test_images/*.jpg')[:3]\n",
        "print('[SAMPLE FILES] train:', train_samples)\n",
        "print('[SAMPLE FILES] test :', test_samples)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNZIP] Extracting train_images.zip -> train_images (179224 files)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 500/179224 (0.3%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 1000/179224 (0.6%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 1500/179224 (0.8%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 2000/179224 (1.1%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 2500/179224 (1.4%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 3000/179224 (1.7%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 3500/179224 (2.0%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 4000/179224 (2.2%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 4500/179224 (2.5%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 5000/179224 (2.8%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 5500/179224 (3.1%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 6000/179224 (3.3%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 6500/179224 (3.6%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 7000/179224 (3.9%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 7500/179224 (4.2%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 8000/179224 (4.5%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 8500/179224 (4.7%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 9000/179224 (5.0%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 9500/179224 (5.3%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 10000/179224 (5.6%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 10500/179224 (5.9%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 11000/179224 (6.1%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 11500/179224 (6.4%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 12000/179224 (6.7%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 12500/179224 (7.0%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 13000/179224 (7.3%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 13500/179224 (7.5%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 14000/179224 (7.8%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 14500/179224 (8.1%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 15000/179224 (8.4%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 15500/179224 (8.6%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 16000/179224 (8.9%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 16500/179224 (9.2%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 17000/179224 (9.5%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 17500/179224 (9.8%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 18000/179224 (10.0%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 18500/179224 (10.3%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 19000/179224 (10.6%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 19500/179224 (10.9%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 20000/179224 (11.2%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 20500/179224 (11.4%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 21000/179224 (11.7%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 21500/179224 (12.0%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 22000/179224 (12.3%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 22500/179224 (12.6%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 23000/179224 (12.8%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 23500/179224 (13.1%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 24000/179224 (13.4%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 24500/179224 (13.7%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 25000/179224 (13.9%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 25500/179224 (14.2%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 26000/179224 (14.5%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 26500/179224 (14.8%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 27000/179224 (15.1%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 27500/179224 (15.3%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 28000/179224 (15.6%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 28500/179224 (15.9%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 29000/179224 (16.2%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 29500/179224 (16.5%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 30000/179224 (16.7%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 30500/179224 (17.0%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 31000/179224 (17.3%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 31500/179224 (17.6%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 32000/179224 (17.9%) elapsed 0.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 32500/179224 (18.1%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 33000/179224 (18.4%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 33500/179224 (18.7%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 34000/179224 (19.0%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 34500/179224 (19.2%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 35000/179224 (19.5%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 35500/179224 (19.8%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 36000/179224 (20.1%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 36500/179224 (20.4%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 37000/179224 (20.6%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 37500/179224 (20.9%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 38000/179224 (21.2%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 38500/179224 (21.5%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 39000/179224 (21.8%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 39500/179224 (22.0%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 40000/179224 (22.3%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 40500/179224 (22.6%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 41000/179224 (22.9%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 41500/179224 (23.2%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 42000/179224 (23.4%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 42500/179224 (23.7%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 43000/179224 (24.0%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 43500/179224 (24.3%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 44000/179224 (24.6%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 44500/179224 (24.8%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 45000/179224 (25.1%) elapsed 0.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 45500/179224 (25.4%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 46000/179224 (25.7%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 46500/179224 (25.9%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 47000/179224 (26.2%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 47500/179224 (26.5%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 48000/179224 (26.8%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 48500/179224 (27.1%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 49000/179224 (27.3%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 49500/179224 (27.6%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 50000/179224 (27.9%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 50500/179224 (28.2%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 51000/179224 (28.5%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 51500/179224 (28.7%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 52000/179224 (29.0%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 52500/179224 (29.3%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 53000/179224 (29.6%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 53500/179224 (29.9%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 54000/179224 (30.1%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 54500/179224 (30.4%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 55000/179224 (30.7%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 55500/179224 (31.0%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 56000/179224 (31.2%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 56500/179224 (31.5%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 57000/179224 (31.8%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 57500/179224 (32.1%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 58000/179224 (32.4%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 58500/179224 (32.6%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 59000/179224 (32.9%) elapsed 0.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 59500/179224 (33.2%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 60000/179224 (33.5%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 60500/179224 (33.8%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 61000/179224 (34.0%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 61500/179224 (34.3%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 62000/179224 (34.6%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 62500/179224 (34.9%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 63000/179224 (35.2%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 63500/179224 (35.4%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 64000/179224 (35.7%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 64500/179224 (36.0%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 65000/179224 (36.3%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 65500/179224 (36.5%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 66000/179224 (36.8%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 66500/179224 (37.1%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 67000/179224 (37.4%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 67500/179224 (37.7%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 68000/179224 (37.9%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 68500/179224 (38.2%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 69000/179224 (38.5%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 69500/179224 (38.8%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 70000/179224 (39.1%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 70500/179224 (39.3%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 71000/179224 (39.6%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 71500/179224 (39.9%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 72000/179224 (40.2%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 72500/179224 (40.5%) elapsed 0.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 73000/179224 (40.7%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 73500/179224 (41.0%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 74000/179224 (41.3%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 74500/179224 (41.6%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 75000/179224 (41.8%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 75500/179224 (42.1%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 76000/179224 (42.4%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 76500/179224 (42.7%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 77000/179224 (43.0%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 77500/179224 (43.2%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 78000/179224 (43.5%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 78500/179224 (43.8%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 79000/179224 (44.1%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 79500/179224 (44.4%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 80000/179224 (44.6%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 80500/179224 (44.9%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 81000/179224 (45.2%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 81500/179224 (45.5%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 82000/179224 (45.8%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 82500/179224 (46.0%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 83000/179224 (46.3%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 83500/179224 (46.6%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 84000/179224 (46.9%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 84500/179224 (47.1%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 85000/179224 (47.4%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 85500/179224 (47.7%) elapsed 0.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 86000/179224 (48.0%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 86500/179224 (48.3%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 87000/179224 (48.5%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 87500/179224 (48.8%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 88000/179224 (49.1%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 88500/179224 (49.4%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 89000/179224 (49.7%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 89500/179224 (49.9%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 90000/179224 (50.2%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 90500/179224 (50.5%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 91000/179224 (50.8%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 91500/179224 (51.1%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 92000/179224 (51.3%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 92500/179224 (51.6%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 93000/179224 (51.9%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 93500/179224 (52.2%) elapsed 0.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 94000/179224 (52.4%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 94500/179224 (52.7%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 95000/179224 (53.0%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 95500/179224 (53.3%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 96000/179224 (53.6%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 96500/179224 (53.8%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 97000/179224 (54.1%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 97500/179224 (54.4%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 98000/179224 (54.7%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 98500/179224 (55.0%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 99000/179224 (55.2%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 99500/179224 (55.5%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 100000/179224 (55.8%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 100500/179224 (56.1%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 101000/179224 (56.4%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 101500/179224 (56.6%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 102000/179224 (56.9%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 102500/179224 (57.2%) elapsed 0.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 103000/179224 (57.5%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 103500/179224 (57.7%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 104000/179224 (58.0%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 104500/179224 (58.3%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 105000/179224 (58.6%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 105500/179224 (58.9%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 106000/179224 (59.1%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 106500/179224 (59.4%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 107000/179224 (59.7%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 107500/179224 (60.0%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 108000/179224 (60.3%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 108500/179224 (60.5%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 109000/179224 (60.8%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 109500/179224 (61.1%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 110000/179224 (61.4%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 110500/179224 (61.7%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 111000/179224 (61.9%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 111500/179224 (62.2%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 112000/179224 (62.5%) elapsed 0.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 112500/179224 (62.8%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 113000/179224 (63.0%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 113500/179224 (63.3%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 114000/179224 (63.6%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 114500/179224 (63.9%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 115000/179224 (64.2%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 115500/179224 (64.4%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 116000/179224 (64.7%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 116500/179224 (65.0%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 117000/179224 (65.3%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 117500/179224 (65.6%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 118000/179224 (65.8%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 118500/179224 (66.1%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 119000/179224 (66.4%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 119500/179224 (66.7%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 120000/179224 (67.0%) elapsed 1.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 120500/179224 (67.2%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 121000/179224 (67.5%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 121500/179224 (67.8%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 122000/179224 (68.1%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 122500/179224 (68.4%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 123000/179224 (68.6%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 123500/179224 (68.9%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 124000/179224 (69.2%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 124500/179224 (69.5%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 125000/179224 (69.7%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 125500/179224 (70.0%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 126000/179224 (70.3%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 126500/179224 (70.6%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 127000/179224 (70.9%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 127500/179224 (71.1%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 128000/179224 (71.4%) elapsed 1.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 128500/179224 (71.7%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 129000/179224 (72.0%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 129500/179224 (72.3%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 130000/179224 (72.5%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 130500/179224 (72.8%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 131000/179224 (73.1%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 131500/179224 (73.4%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 132000/179224 (73.7%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 132500/179224 (73.9%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 133000/179224 (74.2%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 133500/179224 (74.5%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 134000/179224 (74.8%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 134500/179224 (75.0%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 135000/179224 (75.3%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 135500/179224 (75.6%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 136000/179224 (75.9%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 136500/179224 (76.2%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 137000/179224 (76.4%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 137500/179224 (76.7%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 138000/179224 (77.0%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 138500/179224 (77.3%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 139000/179224 (77.6%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 139500/179224 (77.8%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 140000/179224 (78.1%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 140500/179224 (78.4%) elapsed 1.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 141000/179224 (78.7%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 141500/179224 (79.0%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 142000/179224 (79.2%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 142500/179224 (79.5%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 143000/179224 (79.8%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 143500/179224 (80.1%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 144000/179224 (80.3%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 144500/179224 (80.6%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 145000/179224 (80.9%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 145500/179224 (81.2%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 146000/179224 (81.5%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 146500/179224 (81.7%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 147000/179224 (82.0%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 147500/179224 (82.3%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 148000/179224 (82.6%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 148500/179224 (82.9%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 149000/179224 (83.1%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 149500/179224 (83.4%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 150000/179224 (83.7%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 150500/179224 (84.0%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 151000/179224 (84.3%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 151500/179224 (84.5%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 152000/179224 (84.8%) elapsed 1.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 152500/179224 (85.1%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 153000/179224 (85.4%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 153500/179224 (85.6%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 154000/179224 (85.9%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 154500/179224 (86.2%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 155000/179224 (86.5%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 155500/179224 (86.8%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 156000/179224 (87.0%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 156500/179224 (87.3%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 157000/179224 (87.6%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 157500/179224 (87.9%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 158000/179224 (88.2%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 158500/179224 (88.4%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 159000/179224 (88.7%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 159500/179224 (89.0%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 160000/179224 (89.3%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 160500/179224 (89.6%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 161000/179224 (89.8%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 161500/179224 (90.1%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 162000/179224 (90.4%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 162500/179224 (90.7%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 163000/179224 (90.9%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 163500/179224 (91.2%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 164000/179224 (91.5%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 164500/179224 (91.8%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 165000/179224 (92.1%) elapsed 1.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 165500/179224 (92.3%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 166000/179224 (92.6%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 166500/179224 (92.9%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 167000/179224 (93.2%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 167500/179224 (93.5%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 168000/179224 (93.7%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 168500/179224 (94.0%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 169000/179224 (94.3%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 169500/179224 (94.6%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 170000/179224 (94.9%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 170500/179224 (95.1%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 171000/179224 (95.4%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 171500/179224 (95.7%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 172000/179224 (96.0%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 172500/179224 (96.2%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 173000/179224 (96.5%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 173500/179224 (96.8%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 174000/179224 (97.1%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 174500/179224 (97.4%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 175000/179224 (97.6%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 175500/179224 (97.9%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 176000/179224 (98.2%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 176500/179224 (98.5%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 177000/179224 (98.8%) elapsed 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 177500/179224 (99.0%) elapsed 1.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 178000/179224 (99.3%) elapsed 1.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 178500/179224 (99.6%) elapsed 1.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 179000/179224 (99.9%) elapsed 1.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 179224/179224 (100.0%) elapsed 1.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNZIP DONE] train_images.zip: 179224 files in 1.57 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNZIP] Extracting test_images.zip -> test_images (16862 files)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 500/16862 (3.0%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 1000/16862 (5.9%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 1500/16862 (8.9%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 2000/16862 (11.9%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 2500/16862 (14.8%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 3000/16862 (17.8%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 3500/16862 (20.8%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 4000/16862 (23.7%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 4500/16862 (26.7%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 5000/16862 (29.7%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 5500/16862 (32.6%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 6000/16862 (35.6%) elapsed 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 6500/16862 (38.5%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 7000/16862 (41.5%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 7500/16862 (44.5%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 8000/16862 (47.4%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 8500/16862 (50.4%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 9000/16862 (53.4%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 9500/16862 (56.3%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 10000/16862 (59.3%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 10500/16862 (62.3%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 11000/16862 (65.2%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 11500/16862 (68.2%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 12000/16862 (71.2%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 12500/16862 (74.1%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 13000/16862 (77.1%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 13500/16862 (80.1%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 14000/16862 (83.0%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 14500/16862 (86.0%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 15000/16862 (89.0%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 15500/16862 (91.9%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 16000/16862 (94.9%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 16500/16862 (97.9%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  extracted 16862/16862 (100.0%) elapsed 0.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNZIP DONE] test_images.zip: 16862 files in 0.13 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAMPLE FILES] train: ['train_images/594ceb0f-23d2-11e8-a6a3-ec086b02610b.jpg', 'train_images/59c804a7-23d2-11e8-a6a3-ec086b02610b.jpg', 'train_images/58d47ec1-23d2-11e8-a6a3-ec086b02610b.jpg']\n[SAMPLE FILES] test : ['test_images/5a0e34a3-23d2-11e8-a6a3-ec086b02610b.jpg', 'test_images/59fe232a-23d2-11e8-a6a3-ec086b02610b.jpg', 'test_images/59f9470a-23d2-11e8-a6a3-ec086b02610b.jpg']\n"
          ]
        }
      ]
    },
    {
      "id": "34f16df6-d842-4d0d-a564-06013782e368",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, shutil, time\n",
        "print('[INSTALL] Preparing CUDA 12.1 torch stack...', flush=True)\n",
        "def pip(*args):\n",
        "    print('> pip', ' '.join(args), flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "# 0) Uninstall any preexisting torch stack (best-effort)\n",
        "for pkg in ('torch','torchvision','torchaudio'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "for d in (\n",
        "    '/app/.pip-target/torch',\n",
        "    '/app/.pip-target/torchvision',\n",
        "    '/app/.pip-target/torchaudio',\n",
        "    '/app/.pip-target/torch-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.19.1.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchgen', '/app/.pip-target/functorch'\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# 1) Install EXACT cu121 torch stack\n",
        "pip('install',\n",
        "    '--index-url','https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url','https://pypi.org/simple',\n",
        "    'torch==2.4.1','torchvision==0.19.1','torchaudio==2.4.1')\n",
        "\n",
        "# 2) Freeze versions\n",
        "from pathlib import Path\n",
        "Path('constraints.txt').write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "\n",
        "# 3) Install non-torch deps honoring constraints\n",
        "pip('install','-c','constraints.txt',\n",
        "    'timm==1.0.9','albumentations==1.4.10','opencv-python-headless',\n",
        "    'scikit-learn','pandas','numpy','matplotlib','seaborn','pyyaml',\n",
        "    '--upgrade-strategy','only-if-needed')\n",
        "\n",
        "# 4) Sanity check GPU\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'CUDA build:', getattr(torch.version, 'cuda', None))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "assert str(getattr(torch.version,'cuda','')).startswith('12.1'), f'Wrong CUDA build: {torch.version.cuda}'\n",
        "assert torch.cuda.is_available(), 'CUDA not available'\n",
        "print('GPU:', torch.cuda.get_device_name(0))\n",
        "print('[INSTALL] Done.', flush=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INSTALL] Preparing CUDA 12.1 torch stack...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torch as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchvision as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchaudio as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 447.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 424.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 321.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 58.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 60.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 419.9 MB/s eta 0:00:00\nCollecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 489.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 180.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 309.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 493.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 256.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 203.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 262.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 512.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 221.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 261.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 511.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 575.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 290.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 318.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 243.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 332.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 379.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install -c constraints.txt timm==1.0.9 albumentations==1.4.10 opencv-python-headless scikit-learn pandas numpy matplotlib seaborn pyyaml --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm==1.0.9\n  Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.3/2.3 MB 77.1 MB/s eta 0:00:00\nCollecting albumentations==1.4.10\n  Downloading albumentations-1.4.10-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.9/161.9 KB 420.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python-headless\n  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 54.0/54.0 MB 239.0 MB/s eta 0:00:00\nCollecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 251.2 MB/s eta 0:00:00\nCollecting pandas\n  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.4/12.4 MB 250.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 190.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplotlib\n  Downloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8.7/8.7 MB 252.2 MB/s eta 0:00:00\nCollecting seaborn\n  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 294.9/294.9 KB 512.3 MB/s eta 0:00:00\nCollecting pyyaml\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 529.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision\n  Downloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.0/7.0 MB 216.4 MB/s eta 0:00:00\nCollecting huggingface_hub\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 563.3/563.3 KB 518.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 247.6 MB/s eta 0:00:00\nCollecting torch\n  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.1/797.1 MB 266.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-image>=0.21.0\n  Downloading scikit_image-0.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.8/14.8 MB 232.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy>=1.10.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 203.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=4.9.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 400.8 MB/s eta 0:00:00\nCollecting pydantic>=2.7.0\n  Downloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 444.9/444.9 KB 523.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albucore>=0.0.11\n  Downloading albucore-0.0.33-py3-none-any.whl (18 kB)\nCollecting opencv-python-headless\n  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 50.0/50.0 MB 232.7 MB/s eta 0:00:00\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 516.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nCollecting pytz>=2020.1\n  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 509.2/509.2 KB 515.7 MB/s eta 0:00:00\nCollecting tzdata>=2022.7\n  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 347.8/347.8 KB 487.5 MB/s eta 0:00:00\nCollecting python-dateutil>=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.9/229.9 KB 495.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow>=8\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 274.7 MB/s eta 0:00:00\nCollecting packaging>=20.0\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 350.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fonttools>=4.22.0\n  Downloading fonttools-4.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5.0/5.0 MB 239.8 MB/s eta 0:00:00\nCollecting cycler>=0.10\n  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\nCollecting contourpy>=1.0.1\n  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 355.2/355.2 KB 484.0 MB/s eta 0:00:00\nCollecting pyparsing>=2.3.1\n  Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 113.9/113.9 KB 419.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kiwisolver>=1.3.1\n  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 230.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting simsimd>=5.9.2\n  Downloading simsimd-6.5.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1/1.1 MB 181.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stringzilla>=3.10.4\n  Downloading stringzilla-4.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (496 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 496.5/496.5 KB 384.9 MB/s eta 0:00:00\nCollecting typing-inspection>=0.4.0\n  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\nCollecting annotated-types>=0.6.0\n  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic-core==2.33.2\n  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 524.6 MB/s eta 0:00:00\nCollecting six>=1.5\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nCollecting networkx>=3.0\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 542.7 MB/s eta 0:00:00\nCollecting lazy-loader>=0.4\n  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\nCollecting imageio!=2.35.0,>=2.33\n  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 315.8/315.8 KB 342.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tifffile>=2022.8.12\n  Downloading tifffile-2025.9.20-py3-none-any.whl (230 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 230.1/230.1 KB 465.7 MB/s eta 0:00:00\nCollecting fsspec>=2023.5.0\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 487.2 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 400.0 MB/s eta 0:00:00\nCollecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 429.3 MB/s eta 0:00:00\nCollecting tqdm>=4.42.1\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 455.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 235.0 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 241.6 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 245.6 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 237.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 338.6 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 270.0 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 257.1 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 234.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 527.8 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 229.1 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 373.9 MB/s eta 0:00:00\nCollecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 295.6 MB/s eta 0:00:00\nCollecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 491.1 MB/s eta 0:00:00\nCollecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 547.9 MB/s eta 0:00:00\nCollecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 233.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 488.8 MB/s eta 0:00:00\nCollecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 460.1 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 450.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 488.3 MB/s eta 0:00:00\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 519.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: simsimd, pytz, mpmath, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, stringzilla, six, safetensors, pyyaml, pyparsing, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, kiwisolver, joblib, idna, hf-xet, fsspec, fonttools, filelock, cycler, charset_normalizer, certifi, annotated-types, typing-inspection, triton, tifffile, scipy, requests, python-dateutil, pydantic-core, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cudnn-cu12, lazy-loader, jinja2, imageio, contourpy, scikit-learn, scikit-image, pydantic, pandas, nvidia-cusolver-cu12, matplotlib, huggingface_hub, albucore, torch, seaborn, albumentations, torchvision, timm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 albucore-0.0.33 albumentations-1.4.10 annotated-types-0.7.0 certifi-2025.8.3 charset_normalizer-3.4.3 contourpy-1.3.3 cycler-0.12.1 filelock-3.19.1 fonttools-4.60.0 fsspec-2025.9.0 hf-xet-1.1.10 huggingface_hub-0.35.1 idna-3.10 imageio-2.37.0 jinja2-3.1.6 joblib-1.5.2 kiwisolver-1.4.9 lazy-loader-0.4 matplotlib-3.10.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 opencv-python-headless-4.11.0.86 packaging-25.0 pandas-2.3.2 pillow-11.3.0 pydantic-2.11.9 pydantic-core-2.33.2 pyparsing-3.2.5 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.3 requests-2.32.5 safetensors-0.6.2 scikit-image-0.25.2 scikit-learn-1.7.2 scipy-1.16.2 seaborn-0.13.2 simsimd-6.5.3 six-1.17.0 stringzilla-4.0.14 sympy-1.14.0 threadpoolctl-3.6.0 tifffile-2025.9.20 timm-1.0.9 torch-2.4.1 torchvision-0.19.1 tqdm-4.67.1 triton-3.0.0 typing-extensions-4.15.0 typing-inspection-0.4.1 tzdata-2025.2 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/torchvision already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchvision.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.4.5.107.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.1.0.106.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/triton-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec-2025.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/MarkupSafe-3.0.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.1.3.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.0.2.54.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.2.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.20.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.9.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.4.1+cu121 CUDA build: 12.1\nCUDA available: True\nGPU: NVIDIA A10-24Q\n[INSTALL] Done.\n"
          ]
        }
      ]
    },
    {
      "id": "5cc1e844-80e0-47f9-97e5-2c7d77229842",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd, numpy as np, os, json, time\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "print('[FOLDS] Building StratifiedGroupKFold by seq_id (5 folds)...', flush=True)\n",
        "train = pd.read_csv('train.csv')\n",
        "assert 'category_id' in train.columns and 'seq_id' in train.columns, 'Required columns missing'\n",
        "\n",
        "# Basic sanity checks\n",
        "n_classes = train['category_id'].nunique()\n",
        "print(f'[FOLDS] n_train={len(train)} n_classes={n_classes} unique seq={train.seq_id.nunique()}')\n",
        "\n",
        "# Create folds\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = np.full(len(train), -1, dtype=int)\n",
        "t0 = time.time()\n",
        "for fold, (tr_idx, va_idx) in enumerate(sgkf.split(train, y=train['category_id'], groups=train['seq_id'])):\n",
        "    folds[va_idx] = fold\n",
        "    # Logging\n",
        "    yv = train.loc[va_idx, 'category_id']\n",
        "    print(f'  fold {fold}: val_size={len(va_idx)} classes={yv.nunique()} seqs={train.loc[va_idx,\"seq_id\"].nunique()}')\n",
        "    print('    class dist (top10):', yv.value_counts().head(10).to_dict())\n",
        "\n",
        "assert (folds >= 0).all(), 'Some rows not assigned to folds'\n",
        "train['fold'] = folds\n",
        "\n",
        "# Verify no sequence crosses folds\n",
        "seq_to_folds = train.groupby('seq_id')['fold'].nunique()\n",
        "leak_seqs = (seq_to_folds > 1).sum()\n",
        "print(f'[FOLDS] Sequences spanning multiple folds: {leak_seqs}')\n",
        "assert leak_seqs == 0, 'Sequence leakage detected across folds!'\n",
        "\n",
        "# Save folds to disk\n",
        "cols_to_save = ['id','file_name','seq_id','location','category_id','fold']\n",
        "folds_df = train[cols_to_save].copy()\n",
        "folds_df.to_csv('folds.csv', index=False)\n",
        "json.dump({'n_splits':5,'random_state':42,'group_col':'seq_id','stratify':'category_id'}, open('folds_meta.json','w'))\n",
        "print('[FOLDS] Saved folds.csv and folds_meta.json')\n",
        "\n",
        "# Create path columns for convenience\n",
        "train_path_dir = 'train_images'\n",
        "test_path_dir = 'test_images'\n",
        "assert os.path.isdir(train_path_dir) and os.path.isdir(test_path_dir), 'Image dirs missing'\n",
        "folds_df['path'] = folds_df['file_name'].apply(lambda x: os.path.join(train_path_dir, x))\n",
        "folds_df.to_csv('folds_with_paths.csv', index=False)\n",
        "print('[FOLDS] Saved folds_with_paths.csv')\n",
        "\n",
        "# Quick sample check\n",
        "print('[FOLDS] head:\\n', folds_df.head())\n",
        "print(f'[FOLDS] Done in {(time.time()-t0):.2f}s')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLDS] Building StratifiedGroupKFold by seq_id (5 folds)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLDS] n_train=179422 n_classes=14 unique seq=141628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 0: val_size=35901 classes=14 seqs=28326\n    class dist (top10): {0: 25727, 19: 2137, 1: 1226, 11: 1163, 8: 1154, 13: 1047, 16: 944, 17: 810, 3: 609, 18: 357}\n  fold 1: val_size=35915 classes=14 seqs=28326\n    class dist (top10): {0: 25632, 19: 2199, 1: 1191, 11: 1162, 8: 1134, 13: 1046, 16: 985, 17: 857, 3: 615, 18: 392}\n  fold 2: val_size=35826 classes=14 seqs=28326\n    class dist (top10): {0: 25670, 19: 2114, 11: 1180, 1: 1179, 8: 1144, 13: 1076, 16: 999, 17: 834, 3: 574, 18: 360}\n  fold 3: val_size=35870 classes=14 seqs=28325\n    class dist (top10): {0: 25740, 19: 2220, 1: 1200, 8: 1164, 11: 1138, 13: 1059, 16: 899, 17: 833, 3: 537, 18: 380}\n  fold 4: val_size=35910 classes=14 seqs=28325\n    class dist (top10): {0: 25699, 19: 2191, 1: 1239, 8: 1187, 11: 1119, 13: 1075, 16: 946, 17: 791, 3: 567, 18: 357}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLDS] Sequences spanning multiple folds: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLDS] Saved folds.csv and folds_meta.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLDS] Saved folds_with_paths.csv\n[FOLDS] head:\n                                      id  \\\n0  588a679f-23d2-11e8-a6a3-ec086b02610b   \n1  59279ce3-23d2-11e8-a6a3-ec086b02610b   \n2  5a2af4ab-23d2-11e8-a6a3-ec086b02610b   \n3  593d68d7-23d2-11e8-a6a3-ec086b02610b   \n4  58782b45-23d2-11e8-a6a3-ec086b02610b   \n\n                                  file_name  \\\n0  588a679f-23d2-11e8-a6a3-ec086b02610b.jpg   \n1  59279ce3-23d2-11e8-a6a3-ec086b02610b.jpg   \n2  5a2af4ab-23d2-11e8-a6a3-ec086b02610b.jpg   \n3  593d68d7-23d2-11e8-a6a3-ec086b02610b.jpg   \n4  58782b45-23d2-11e8-a6a3-ec086b02610b.jpg   \n\n                                 seq_id  location  category_id  fold  \\\n0  6f12067d-5567-11e8-b3c0-dca9047ef277       115           19     2   \n1  6faa92d1-5567-11e8-b1ae-dca9047ef277        96            0     1   \n2  6f7d4702-5567-11e8-9e03-dca9047ef277        57            0     4   \n3  6f0f6778-5567-11e8-b5d2-dca9047ef277        90            3     1   \n4  6f789194-5567-11e8-946a-dca9047ef277       106            0     1   \n\n                                                path  \n0  train_images/588a679f-23d2-11e8-a6a3-ec086b026...  \n1  train_images/59279ce3-23d2-11e8-a6a3-ec086b026...  \n2  train_images/5a2af4ab-23d2-11e8-a6a3-ec086b026...  \n3  train_images/593d68d7-23d2-11e8-a6a3-ec086b026...  \n4  train_images/58782b45-23d2-11e8-a6a3-ec086b026...  \n[FOLDS] Done in 29.11s\n"
          ]
        }
      ]
    },
    {
      "id": "96f9bd7b-2f8b-4c9e-b33b-23758f5164f6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, time, math, random, gc, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "cv2.setNumThreads(0)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import timm\n",
        "from timm.utils import ModelEmaV2\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "try:\n",
        "    torch.set_num_threads(4)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "class ImgDs(Dataset):\n",
        "    def __init__(self, df, img_dir, transforms=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.transforms = transforms\n",
        "        self.has_y = 'category_id' in df.columns\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.df.iloc[idx]\n",
        "        fp = os.path.join(self.img_dir, r['file_name'])\n",
        "        img = cv2.imread(fp, cv2.IMREAD_COLOR)\n",
        "        if img is None:\n",
        "            img = np.zeros((512,512,3), dtype=np.uint8)\n",
        "        else:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        if self.transforms:\n",
        "            img = self.transforms(image=img)['image']\n",
        "        if self.has_y:\n",
        "            return img, int(r['category_id'])\n",
        "        else:\n",
        "            return img, -1\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=1.5, alpha=0.25, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "    def forward(self, logits, target):\n",
        "        ce = F.cross_entropy(logits, target, reduction='none')\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = (self.alpha * (1-pt)**self.gamma) * ce\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        return loss\n",
        "\n",
        "def get_transforms(img_size=224):\n",
        "    train_tf = A.Compose([\n",
        "        A.RandomResizedCrop(img_size, img_size, scale=(0.7,1.0), p=1.0),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.ColorJitter(0.2,0.2,0.2,0.1,p=0.3),\n",
        "        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "    val_tf = A.Compose([\n",
        "        A.LongestMaxSize(max_size=img_size),\n",
        "        A.PadIfNeeded(img_size, img_size, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
        "        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "    return train_tf, val_tf\n",
        "\n",
        "def seed_all(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def train_one_epoch(model, ema, loader, optimizer, scaler, loss_fn, epoch, log_interval=200):\n",
        "    model.train()\n",
        "    t0 = time.time()\n",
        "    total = 0.0; n = 0\n",
        "    for it, (x, y) in enumerate(loader):\n",
        "        x = x.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.float16) if DEVICE=='cuda' else torch.autocast('cpu'):\n",
        "            logits = model(x)\n",
        "            loss = loss_fn(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        if ema is not None:\n",
        "            ema.update(model)\n",
        "        total += loss.item() * x.size(0)\n",
        "        n += x.size(0)\n",
        "        if (it+1) % log_interval == 0:\n",
        "            dt = time.time()-t0\n",
        "            print(f'[EPOCH {epoch}] it {it+1}/{len(loader)} loss {total/n:.4f} elapsed {dt/60:.2f}m', flush=True)\n",
        "    return total/n if n>0 else 0.0\n",
        "\n",
        "def predict(model, loader):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.float16) if DEVICE=='cuda' else torch.autocast('cpu'):\n",
        "                logits = model(x)\n",
        "            preds.append(logits.float().cpu())\n",
        "    return torch.cat(preds, dim=0).numpy()\n",
        "\n",
        "def seq_average_logits(df_idx, logits, df):\n",
        "    val_df = df.iloc[df_idx].reset_index(drop=True)\n",
        "    val_df = val_df.assign(_row=np.arange(len(val_df)))\n",
        "    arr = logits\n",
        "    out = np.zeros_like(arr)\n",
        "    for sid, grp in val_df.groupby('seq_id')['_row']:\n",
        "        idxs = grp.values\n",
        "        out[idxs] = arr[idxs].mean(axis=0, keepdims=True)\n",
        "    return out\n",
        "\n",
        "def macro_f1_from_logits(y_true, logits):\n",
        "    y_pred = logits.argmax(axis=1)\n",
        "    return f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "def smoke_train_one_fold(fold=0, img_size=224, epochs=1, batch_size=16, model_name='resnet18', max_train=2000, max_val=500, pretrained=False):\n",
        "    seed_all(42)\n",
        "    df = pd.read_csv('folds.csv')\n",
        "    n_classes = df['category_id'].nunique()\n",
        "    tr_idx_all = df.index[df['fold'] != fold].to_list()\n",
        "    va_idx_all = df.index[df['fold'] == fold].to_list()\n",
        "    tr_idx = tr_idx_all[:max_train]\n",
        "    va_idx = va_idx_all[:max_val]\n",
        "    print(f'[SMOKE] fold={fold} train={len(tr_idx)}/{len(tr_idx_all)} val={len(va_idx)}/{len(va_idx_all)} classes={n_classes}', flush=True)\n",
        "    train_tf, val_tf = get_transforms(img_size)\n",
        "    train_ds = ImgDs(df.iloc[tr_idx], 'train_images', train_tf)\n",
        "    val_ds   = ImgDs(df.iloc[va_idx], 'train_images', val_tf)\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=max(8, batch_size), shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "    print(f'[MODEL] Creating {model_name}, pretrained={pretrained}', flush=True)\n",
        "    model = timm.create_model(model_name, pretrained=pretrained, num_classes=n_classes)\n",
        "    model.to(DEVICE); model.to(memory_format=torch.channels_last)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "    loss_fn = FocalLoss(gamma=1.5, alpha=0.25)\n",
        "    ema = None  # disable EMA for smoke to reduce overhead\n",
        "    scaler = torch.amp.GradScaler('cuda') if DEVICE=='cuda' else torch.amp.GradScaler('cpu')\n",
        "\n",
        "    steps_per_epoch = max(1, len(train_loader))\n",
        "    warmup_steps = steps_per_epoch\n",
        "    total_steps = steps_per_epoch * epochs\n",
        "\n",
        "    def lr_schedule(step):\n",
        "        if step < warmup_steps:\n",
        "            return step / max(1, warmup_steps)\n",
        "        prog = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "        return 0.5 * (1 + math.cos(math.pi * prog))\n",
        "\n",
        "    sched = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_schedule)\n",
        "\n",
        "    global_step = 0\n",
        "    for ep in range(1, epochs+1):\n",
        "        loss = train_one_epoch(model, ema, train_loader, optimizer, scaler, loss_fn, ep, log_interval=20)\n",
        "        for _ in range(steps_per_epoch):\n",
        "            sched.step(); global_step += 1\n",
        "        val_logits = predict(model, val_loader)\n",
        "        y_true = df.iloc[va_idx]['category_id'].values\n",
        "        val_logits_seq = seq_average_logits(va_idx, val_logits, df)\n",
        "        f1_plain = macro_f1_from_logits(y_true, val_logits)\n",
        "        f1_seq = macro_f1_from_logits(y_true, val_logits_seq)\n",
        "        print(f'[SMOKE][E{ep}] loss={loss:.4f} F1_plain={f1_plain:.4f} F1_seq={f1_seq:.4f}', flush=True)\n",
        "        gc.collect();\n",
        "        try:\n",
        "            torch.cuda.empty_cache()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    np.save(f'oof_logits_fold{fold}.npy', val_logits_seq)\n",
        "    np.save(f'oof_idx_fold{fold}.npy', np.array(va_idx))\n",
        "    print('[SMOKE] Saved OOF logits/idx for fold', fold)\n",
        "\n",
        "print('[NEXT] Ready to run smoke_train_one_fold(fold=0, img_size=224, epochs=1, batch_size=16, model_name=\\'resnet18\\', max_train=2000, max_val=500, pretrained=False)')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NEXT] Ready to run smoke_train_one_fold(fold=0, img_size=224, epochs=1, batch_size=32, model_name='convnext_tiny', max_train=5000, max_val=1000)\n"
          ]
        }
      ]
    },
    {
      "id": "fe7ca4f5-43d7-49f8-af3e-b5ffc6f829eb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys, subprocess, shutil, os\n",
        "def run(*args, check=True):\n",
        "    print('>', ' '.join(args), flush=True)\n",
        "    subprocess.run(list(args), check=check)\n",
        "\n",
        "print('[FIX] Clean reinstall: albumentations==1.3.1 (no albucore dep)...', flush=True)\n",
        "# Uninstall conflicting packages\n",
        "run(sys.executable, '-m', 'pip', 'uninstall', '-y', 'albumentations', 'albucore', check=False)\n",
        "# Remove possible stale site dirs\n",
        "for d in (\n",
        "    '/app/.pip-target/albumentations',\n",
        "    '/app/.pip-target/albumentations-1.4.20.dist-info',\n",
        "    '/app/.pip-target/albucore',\n",
        "    '/app/.pip-target/albucore-0.0.19.dist-info',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# Reinstall older stable albumentations without albucore dependency\n",
        "run(sys.executable, '-m', 'pip', 'install', '-c', 'constraints.txt',\n",
        "    '--upgrade', '--force-reinstall', '--no-cache-dir',\n",
        "    'albumentations==1.3.1', 'opencv-python-headless', '--upgrade-strategy', 'only-if-needed')\n",
        "\n",
        "print('[FIX] Verifying import...')\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "print('[OK] albumentations', A.__version__, 'module at', A.__file__)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FIX] Clean reinstall: albumentations==1.3.1 (no albucore dep)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> /usr/bin/python3.11 -m pip uninstall -y albumentations albucore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: albumentations 1.4.20\nUninstalling albumentations-1.4.20:\n  Successfully uninstalled albumentations-1.4.20\nFound existing installation: albucore 0.0.33\nUninstalling albucore-0.0.33:\n  Successfully uninstalled albucore-0.0.33\nRemoving /app/.pip-target/albumentations\nRemoving /app/.pip-target/albucore-0.0.19.dist-info\n> /usr/bin/python3.11 -m pip install -c constraints.txt --upgrade --force-reinstall --no-cache-dir albumentations==1.3.1 opencv-python-headless --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations==1.3.1\n  Downloading albumentations-1.3.1-py3-none-any.whl (125 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 125.7/125.7 KB 5.8 MB/s eta 0:00:00\nCollecting opencv-python-headless\n  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 54.0/54.0 MB 224.9 MB/s eta 0:00:00\nCollecting scikit-image>=0.16.1\n  Downloading scikit_image-0.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.8/14.8 MB 237.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy>=1.1.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 222.8 MB/s eta 0:00:00\nCollecting qudida>=0.0.4\n  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.11.1\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 345.5 MB/s eta 0:00:00\nCollecting PyYAML\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 530.2 MB/s eta 0:00:00\nCollecting opencv-python-headless\n  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 50.0/50.0 MB 201.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn>=0.19.1\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 351.0 MB/s eta 0:00:00\nCollecting typing-extensions\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 285.9 MB/s eta 0:00:00\nCollecting lazy-loader>=0.4\n  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\nCollecting packaging>=21\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 422.4 MB/s eta 0:00:00\nCollecting imageio!=2.35.0,>=2.33\n  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 315.8/315.8 KB 413.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx>=3.0\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 530.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow>=10.1\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 165.3 MB/s eta 0:00:00\nCollecting tifffile>=2022.8.12\n  Downloading tifffile-2025.9.20-py3-none-any.whl (230 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 230.1/230.1 KB 429.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 507.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: typing-extensions, threadpoolctl, PyYAML, pillow, packaging, numpy, networkx, joblib, tifffile, scipy, opencv-python-headless, lazy-loader, imageio, scikit-learn, scikit-image, qudida, albumentations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed PyYAML-6.0.3 albumentations-1.3.1 imageio-2.37.0 joblib-1.5.2 lazy-loader-0.4 networkx-3.5 numpy-1.26.4 opencv-python-headless-4.11.0.86 packaging-25.0 pillow-11.3.0 qudida-0.0.4 scikit-image-0.25.2 scikit-learn-1.7.2 scipy-1.16.2 threadpoolctl-3.6.0 tifffile-2025.9.20 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FIX] Verifying import...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] albumentations 1.3.1 module at /app/.pip-target/albumentations/__init__.py\n"
          ]
        }
      ]
    },
    {
      "id": "5fc3f45a-9538-4186-b2ef-a3e64758ba46",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "t0 = time.time()\n",
        "print('[RUN] Starting 224px smoke training: fold=0, epochs=1, bs=16, model=resnet18 (pretrained=False), subset train=2000 val=500', flush=True)\n",
        "smoke_train_one_fold(fold=0, img_size=224, epochs=1, batch_size=16, model_name='resnet18', max_train=2000, max_val=500, pretrained=False)\n",
        "print(f'[RUN] Done. Elapsed {(time.time()-t0)/60:.2f} min', flush=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RUN] Starting 224px smoke training: fold=0, epochs=1, bs=16, model=resnet18 (pretrained=False), subset train=2000 val=500\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'smoke_train_one_fold' is not defined",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m t0 = time.time()\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m[RUN] Starting 224px smoke training: fold=0, epochs=1, bs=16, model=resnet18 (pretrained=False), subset train=2000 val=500\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43msmoke_train_one_fold\u001b[49m(fold=\u001b[32m0\u001b[39m, img_size=\u001b[32m224\u001b[39m, epochs=\u001b[32m1\u001b[39m, batch_size=\u001b[32m16\u001b[39m, model_name=\u001b[33m'\u001b[39m\u001b[33mresnet18\u001b[39m\u001b[33m'\u001b[39m, max_train=\u001b[32m2000\u001b[39m, max_val=\u001b[32m500\u001b[39m, pretrained=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[RUN] Done. Elapsed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time.time()-t0)/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m min\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[31mNameError\u001b[39m: name 'smoke_train_one_fold' is not defined"
          ]
        }
      ]
    },
    {
      "id": "d4204d1b-3fd9-4185-9d64-19ea38d24705",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, time, math, random, gc, json, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "cv2.setNumThreads(0)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "try: torch.set_num_threads(4)\n",
        "except Exception: pass\n",
        "\n",
        "class TinyDs(Dataset):\n",
        "    def __init__(self, df, img_dir, img_size=224, train=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.img_size = img_size\n",
        "        self.train = train\n",
        "        self.has_y = 'category_id' in df.columns\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.df.iloc[idx]\n",
        "        fp = os.path.join(self.img_dir, r['file_name'])\n",
        "        img = cv2.imread(fp, cv2.IMREAD_COLOR)\n",
        "        if img is None:\n",
        "            img = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        h, w = img.shape[:2]\n",
        "        s = self.img_size\n",
        "        scale = s / max(h, w)\n",
        "        nh, nw = int(h*scale), int(w*scale)\n",
        "        img = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
        "        pad = np.zeros((s, s, 3), dtype=img.dtype)\n",
        "        y0 = (s - nh) // 2; x0 = (s - nw) // 2\n",
        "        pad[y0:y0+nh, x0:x0+nw] = img\n",
        "        img = pad.astype(np.float32) / 255.0\n",
        "        mean = np.array([0.485,0.456,0.406], dtype=np.float32)\n",
        "        std  = np.array([0.229,0.224,0.225], dtype=np.float32)\n",
        "        img = (img - mean) / std\n",
        "        img = np.transpose(img, (2,0,1))\n",
        "        x = torch.from_numpy(img)\n",
        "        y = int(r['category_id']) if self.has_y else -1\n",
        "        return x, y\n",
        "\n",
        "def run_tiny_smoke(img_size=224, max_train=1024, max_val=256, batches_limit=50, bs=16):\n",
        "        df = pd.read_csv('folds.csv')\n",
        "        n_classes = df['category_id'].nunique()\n",
        "        tr = df[df['fold']!=0].iloc[:max_train].copy()\n",
        "        va = df[df['fold']==0].iloc[:max_val].copy()\n",
        "        print(f'[SMOKE2] train={len(tr)} val={len(va)} classes={n_classes}', flush=True)\n",
        "        train_ds = TinyDs(tr, 'train_images', img_size, train=True)\n",
        "        val_ds   = TinyDs(va, 'train_images', img_size, train=False)\n",
        "        train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
        "        val_loader   = DataLoader(val_ds, batch_size=bs*2, shuffle=False, num_workers=0, pin_memory=True)\n",
        "        model = models.resnet18(weights=None)\n",
        "        model.fc = nn.Linear(model.fc.in_features, n_classes)\n",
        "        model.to(DEVICE); model.to(memory_format=torch.channels_last)\n",
        "        opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        model.train()\n",
        "        t0 = time.time()\n",
        "        seen = 0\n",
        "        for it, (x,y) in enumerate(train_loader, 1):\n",
        "            x = x.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            y = y.to(DEVICE, non_blocking=True)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            logits = model(x)\n",
        "            loss = loss_fn(logits, y)\n",
        "            loss.backward(); opt.step()\n",
        "            seen += x.size(0)\n",
        "            if it % 10 == 0:\n",
        "                print(f'  it {it} loss {loss.item():.4f} seen {seen}', flush=True)\n",
        "            if it >= batches_limit:\n",
        "                break\n",
        "        print(f'[SMOKE2] Train done in {(time.time()-t0):.1f}s, evaluating...', flush=True)\n",
        "        model.eval()\n",
        "        preds = [];\n",
        "        with torch.no_grad():\n",
        "            for x, _ in val_loader:\n",
        "                x = x.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "                logits = model(x).float().cpu().numpy()\n",
        "                preds.append(logits)\n",
        "        logits = np.concatenate(preds, 0)\n",
        "        y_true = va['category_id'].values\n",
        "        y_pred = logits.argmax(1)\n",
        "        f1 = f1_score(y_true, y_pred, average='macro')\n",
        "        print(f'[SMOKE2] Macro-F1 (plain argmax, no seq avg): {f1:.4f}', flush=True)\n",
        "        return f1\n",
        "\n",
        "print('[NEXT] Ready to run: run_tiny_smoke(img_size=224, max_train=1024, max_val=256, batches_limit=50, bs=16)')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NEXT] Ready to run: run_tiny_smoke(img_size=224, max_train=1024, max_val=256, batches_limit=50, bs=16)\n"
          ]
        }
      ]
    },
    {
      "id": "8b58554b-c755-477d-91d8-03f90ff6e841",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "t0=time.time()\n",
        "print('[RUN] Tiny smoke with torchvision resnet18, bs=16, 50 batches, 1024/256 subset', flush=True)\n",
        "f1 = run_tiny_smoke(img_size=224, max_train=1024, max_val=256, batches_limit=50, bs=16)\n",
        "print(f'[RUN] Done tiny smoke. F1={f1:.4f}. Elapsed {(time.time()-t0):.1f}s', flush=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RUN] Tiny smoke with torchvision resnet18, bs=16, 50 batches, 1024/256 subset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SMOKE2] train=1024 val=256 classes=14\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KernelDied",
          "evalue": "Kernel died unexpectedly.",
          "traceback": []
        }
      ]
    },
    {
      "id": "467dd4ad-7bdf-4a02-87e2-502c3180e5be",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('[DEBUG] Forcing CPU for smoke to diagnose kernel crashes...', flush=True)\n",
        "DEVICE = 'cpu'\n",
        "f1_cpu = run_tiny_smoke(img_size=224, max_train=256, max_val=128, batches_limit=20, bs=8)\n",
        "print('[DEBUG] CPU tiny smoke F1:', f1_cpu)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Forcing CPU for smoke to diagnose kernel crashes...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'run_tiny_smoke' is not defined",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m[DEBUG] Forcing CPU for smoke to diagnose kernel crashes...\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      2\u001b[39m DEVICE = \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m f1_cpu = \u001b[43mrun_tiny_smoke\u001b[49m(img_size=\u001b[32m224\u001b[39m, max_train=\u001b[32m256\u001b[39m, max_val=\u001b[32m128\u001b[39m, batches_limit=\u001b[32m20\u001b[39m, bs=\u001b[32m8\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m[DEBUG] CPU tiny smoke F1:\u001b[39m\u001b[33m'\u001b[39m, f1_cpu)\n",
            "\u001b[31mNameError\u001b[39m: name 'run_tiny_smoke' is not defined"
          ]
        }
      ]
    },
    {
      "id": "dcb043ab-2bc4-4778-967a-2bce2dd600ba",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tabular-only baseline (metadata) to get a working submission and OOF estimate\n",
        "import pandas as pd, numpy as np, time, json\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "target = 'category_id'\n",
        "\n",
        "# Feature engineering on metadata\n",
        "def fe(df):\n",
        "    dt = pd.to_datetime(df['date_captured'], errors='coerce')\n",
        "    df = df.copy()\n",
        "    df['year'] = dt.dt.year.fillna(-1).astype(int)\n",
        "    df['month'] = dt.dt.month.fillna(-1).astype(int)\n",
        "    df['day'] = dt.dt.day.fillna(-1).astype(int)\n",
        "    df['hour'] = dt.dt.hour.fillna(-1).astype(int)\n",
        "    df['is_night'] = ((df['hour'] < 6) | (df['hour'] > 19)).astype(int)\n",
        "    # frame/sequence context\n",
        "    df['frame_num'] = df['frame_num'].fillna(-1).astype(int)\n",
        "    df['seq_num_frames'] = df['seq_num_frames'].fillna(-1).astype(int)\n",
        "    df['loc'] = df['location'].astype(str)\n",
        "    return df\n",
        "\n",
        "train_fe = fe(train)\n",
        "test_fe  = fe(test)\n",
        "\n",
        "# Define features\n",
        "num_cols = ['frame_num','seq_num_frames','width','height','year','month','day','hour','is_night']\n",
        "cat_cols = ['loc']\n",
        "\n",
        "X = train_fe[num_cols + cat_cols]\n",
        "y = train_fe[target].values\n",
        "groups = train_fe['seq_id'].astype(str).values\n",
        "X_test = test_fe[num_cols + cat_cols]\n",
        "\n",
        "pre = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(with_mean=True, with_std=True), num_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), cat_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "clf = LogisticRegression(\n",
        "    multi_class='multinomial',\n",
        "    solver='saga',\n",
        "    max_iter=200,\n",
        "    C=1.0,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=4,\n",
        ")\n",
        "\n",
        "pipe = Pipeline([('pre', pre), ('clf', clf)])\n",
        "\n",
        "# CV\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_pred = np.zeros(len(train_fe), dtype=int)\n",
        "fold_scores = []\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(sgkf.split(X, y=y, groups=groups)):\n",
        "    t_fold = time.time()\n",
        "    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
        "    y_tr, y_va = y[tr_idx], y[va_idx]\n",
        "    print(f'[TAB] Fold {fold} train={len(tr_idx)} val={len(va_idx)}', flush=True)\n",
        "    model = pipe\n",
        "    model.fit(X_tr, y_tr)\n",
        "    y_hat = model.predict(X_va)\n",
        "    f1 = f1_score(y_va, y_hat, average='macro')\n",
        "    oof_pred[va_idx] = y_hat\n",
        "    fold_scores.append(f1)\n",
        "    print(f'[TAB] Fold {fold} macro-F1={f1:.5f} elapsed {time.time()-t_fold:.1f}s', flush=True)\n",
        "\n",
        "oof_f1 = f1_score(y, oof_pred, average='macro')\n",
        "print(f'[TAB] OOF macro-F1={oof_f1:.5f} folds={fold_scores} mean={np.mean(fold_scores):.5f}', flush=True)\n",
        "\n",
        "# Fit on all data and predict test\n",
        "model_full = pipe\n",
        "t_fit = time.time()\n",
        "model_full.fit(X, y)\n",
        "print(f'[TAB] Full fit done in {time.time()-t_fit:.1f}s', flush=True)\n",
        "test_pred = model_full.predict(X_test)\n",
        "\n",
        "# Build submission\n",
        "sub = pd.DataFrame({'id': test['id'], 'category_id': test_pred.astype(int)})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[SUB] Saved submission.csv shape', sub.shape, ' unique classes:', sub['category_id'].nunique())\n",
        "print('[DONE] Tabular baseline finished in {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB] Fold 0 train=143521 val=35901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB] Fold 0 macro-F1=0.40463 elapsed 15.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB] Fold 1 train=143507 val=35915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB] Fold 1 macro-F1=0.42253 elapsed 15.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB] Fold 2 train=143596 val=35826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB] Fold 2 macro-F1=0.33763 elapsed 15.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB] Fold 3 train=143552 val=35870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB] Fold 3 macro-F1=0.25582 elapsed 15.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB] Fold 4 train=143512 val=35910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB] Fold 4 macro-F1=0.38640 elapsed 14.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB] OOF macro-F1=0.35691 folds=[0.404634700095836, 0.42252826065052496, 0.3376292851021045, 0.2558230431491976, 0.38640356421654315] mean=0.36140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB] Full fit done in 21.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUB] Saved submission.csv shape (16877, 2)  unique classes: 11\n[DONE] Tabular baseline finished in 125.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "id": "7356a168-4b93-4d19-ab03-e55f5812e09f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tabular OOF logits + seq-avg + per-class bias optimization, and improved submission\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "target = 'category_id'\n",
        "\n",
        "def fe(df):\n",
        "    dt = pd.to_datetime(df['date_captured'], errors='coerce')\n",
        "    df = df.copy()\n",
        "    df['year'] = dt.dt.year.fillna(-1).astype(int)\n",
        "    df['month'] = dt.dt.month.fillna(-1).astype(int)\n",
        "    df['day'] = dt.dt.day.fillna(-1).astype(int)\n",
        "    df['hour'] = dt.dt.hour.fillna(-1).astype(int)\n",
        "    df['is_night'] = ((df['hour'] < 6) | (df['hour'] > 19)).astype(int)\n",
        "    df['frame_num'] = df['frame_num'].fillna(-1).astype(int)\n",
        "    df['seq_num_frames'] = df['seq_num_frames'].fillna(-1).astype(int)\n",
        "    df['loc'] = df['location'].astype(str)\n",
        "    return df\n",
        "\n",
        "train_fe = fe(train)\n",
        "test_fe  = fe(test)\n",
        "num_cols = ['frame_num','seq_num_frames','width','height','year','month','day','hour','is_night']\n",
        "cat_cols = ['loc']\n",
        "X = train_fe[num_cols + cat_cols]\n",
        "y = train_fe[target].values\n",
        "groups = train_fe['seq_id'].astype(str).values\n",
        "X_test = test_fe[num_cols + cat_cols]\n",
        "\n",
        "pre = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(with_mean=True, with_std=True), num_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), cat_cols),\n",
        "    ]\n",
        ")\n",
        "clf = LogisticRegression(\n",
        "    multi_class='multinomial',\n",
        "    solver='saga',\n",
        "    max_iter=200,\n",
        "    C=1.0,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=4,\n",
        ")\n",
        "pipe = Pipeline([('pre', pre), ('clf', clf)])\n",
        "\n",
        "n_classes = int(train[target].nunique())\n",
        "oof_logits = np.full((len(train), n_classes), np.nan, dtype=np.float32)\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print('[TAB2] Collecting OOF probabilities/logits with seq-aware CV...', flush=True)\n",
        "for fold, (tr_idx, va_idx) in enumerate(sgkf.split(X, y=y, groups=groups)):\n",
        "    t_fold = time.time()\n",
        "    model = pipe\n",
        "    model.fit(X.iloc[tr_idx], y[tr_idx])\n",
        "    proba = model.predict_proba(X.iloc[va_idx])\n",
        "    # convert to logit-like space (use log-prob as surrogate logits)\n",
        "    logits = np.log(np.clip(proba, 1e-8, 1.0))\n",
        "    # sequence-average within this fold's val indices\n",
        "    va_df = train_fe.iloc[va_idx][['seq_id']].reset_index(drop=True)\n",
        "    logits_seq = logits.copy()\n",
        "    # map from seq -> indices within va fold\n",
        "    for sid, grp in va_df.groupby('seq_id').groups.items():\n",
        "        idxs = np.array(list(grp))\n",
        "        m = logits[idxs].mean(axis=0, keepdims=True)\n",
        "        logits_seq[idxs] = m\n",
        "    oof_logits[va_idx] = logits_seq\n",
        "    y_hat = logits_seq.argmax(axis=1)\n",
        "    f1 = f1_score(y[va_idx], y_hat, average='macro')\n",
        "    print(f'[TAB2] Fold {fold} seq-avg macro-F1={f1:.5f} elapsed {time.time()-t_fold:.1f}s', flush=True)\n",
        "\n",
        "assert not np.isnan(oof_logits).any(), 'OOF logits have NaNs'\n",
        "\n",
        "def optimize_biases(y_true, logits, n_iters=2, grid=np.linspace(-1.5, 1.5, 21)):\n",
        "    b = np.zeros(logits.shape[1], dtype=np.float32)\n",
        "    best = f1_score(y_true, (logits + b).argmax(1), average='macro')\n",
        "    for _ in range(n_iters):\n",
        "        improved = False\n",
        "        for c in range(logits.shape[1]):\n",
        "            bc = b[c]\n",
        "            best_c = bc; best_sc = best\n",
        "            for d in grid:\n",
        "                b[c] = d\n",
        "                sc = f1_score(y_true, (logits + b).argmax(1), average='macro')\n",
        "                if sc > best_sc:\n",
        "                    best_sc = sc; best_c = d\n",
        "            b[c] = best_c\n",
        "            if best_c != bc:\n",
        "                best = best_sc; improved = True\n",
        "        if not improved:\n",
        "            break\n",
        "    return b, best\n",
        "\n",
        "# Optimize per-class additive biases on OOF seq-averaged logits\n",
        "y_true = y\n",
        "b_opt, f1_oof = optimize_biases(y_true, oof_logits)\n",
        "print(f\"[TAB2] OOF seq-avg macro-F1 (with biases) = {f1_oof:.5f}\")\n",
        "print('[TAB2] Biases:', np.round(b_opt, 3))\n",
        "\n",
        "# Fit on full data and produce test predictions\n",
        "model_full = pipe\n",
        "t_fit = time.time()\n",
        "model_full.fit(X, y)\n",
        "print(f'[TAB2] Full fit done in {time.time()-t_fit:.1f}s', flush=True)\n",
        "proba_test = model_full.predict_proba(X_test)\n",
        "logits_test = np.log(np.clip(proba_test, 1e-8, 1.0))\n",
        "\n",
        "# Sequence-average test logits, then apply biases and predict one label per seq_id, broadcast to frames\n",
        "test_df = test_fe[['id','seq_id']].copy()\n",
        "logits_adj = logits_test.copy()\n",
        "pred_seq = {}\n",
        "for sid, idxs in test_df.groupby('seq_id').groups.items():\n",
        "    idxs = np.array(list(idxs))\n",
        "    m = logits_adj[idxs].mean(axis=0) + b_opt\n",
        "    lab = int(m.argmax())\n",
        "    pred_seq[sid] = lab\n",
        "\n",
        "test_pred = test_df['seq_id'].map(pred_seq).astype(int).values\n",
        "sub2 = pd.DataFrame({'id': test['id'], 'category_id': test_pred})\n",
        "sub2.to_csv('submission_seq_bias.csv', index=False)\n",
        "print('[SUB2] Saved submission_seq_bias.csv shape', sub2.shape, ' unique classes:', sub2['category_id'].nunique())\n",
        "print('[DONE] Post-processed tabular submission in {:.1f}s'.format(time.time()-t0))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB2] Collecting OOF probabilities/logits with seq-aware CV...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB2] Fold 0 seq-avg macro-F1=0.08702 elapsed 14.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB2] Fold 1 seq-avg macro-F1=0.08409 elapsed 14.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB2] Fold 2 seq-avg macro-F1=0.09343 elapsed 14.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB2] Fold 3 seq-avg macro-F1=0.08599 elapsed 14.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB2] Fold 4 seq-avg macro-F1=0.06439 elapsed 14.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB2] OOF seq-avg macro-F1 (with biases) = 0.10264\n[TAB2] Biases: [ 1.5  -0.3   1.5  -1.5  -1.5  -1.5  -1.5  -1.35  0.6  -1.05  1.5   0.\n -0.75 -0.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TAB2] Full fit done in 21.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUB2] Saved submission_seq_bias.csv shape (16877, 2)  unique classes: 11\n[DONE] Post-processed tabular submission in 137.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "id": "a043011f-71d4-4a49-a90b-beaf432a535b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Overwrite submission.csv with sequence-avg+bias tuned predictions\n",
        "import pandas as pd, os\n",
        "src = 'submission_seq_bias.csv'\n",
        "dst = 'submission.csv'\n",
        "assert os.path.exists(src), f\"Missing {src}\"\n",
        "sub = pd.read_csv(src)\n",
        "sub.to_csv(dst, index=False)\n",
        "print('[COPY] Wrote', dst, 'from', src, 'shape', sub.shape, 'unique classes', sub['category_id'].nunique())\n",
        "print(sub.head())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[COPY] Wrote submission.csv from submission_seq_bias.csv shape (16877, 2) unique classes 11\n                                     id  category_id\n0  5998cfa4-23d2-11e8-a6a3-ec086b02610b            9\n1  599fbd89-23d2-11e8-a6a3-ec086b02610b            0\n2  59fae563-23d2-11e8-a6a3-ec086b02610b            0\n3  5a24a741-23d2-11e8-a6a3-ec086b02610b            0\n4  59eab924-23d2-11e8-a6a3-ec086b02610b            9\n"
          ]
        }
      ]
    },
    {
      "id": "5f1cf82e-d6a5-4c07-b179-92eb03a04e1e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost metadata model with seq-avg and per-class bias tuning\n",
        "import os, time, math, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "t0 = time.time()\n",
        "print('[CB] Installing catboost if missing...', flush=True)\n",
        "import subprocess, sys\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'install', 'catboost==1.2.5'], check=False)\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "target = 'category_id'\n",
        "\n",
        "def fe(df):\n",
        "    df = df.copy()\n",
        "    dt = pd.to_datetime(df['date_captured'], errors='coerce')\n",
        "    df['year'] = dt.dt.year.fillna(-1).astype(int)\n",
        "    df['month'] = dt.dt.month.fillna(-1).astype(int)\n",
        "    df['day'] = dt.dt.day.fillna(-1).astype(int)\n",
        "    df['hour'] = dt.dt.hour.fillna(-1).astype(int)\n",
        "    df['is_night'] = ((df['hour'] < 6) | (df['hour'] > 19)).astype(int)\n",
        "    # sequence context\n",
        "    df['frame_num'] = df['frame_num'].fillna(-1).astype(int)\n",
        "    df['seq_num_frames'] = df['seq_num_frames'].fillna(1).astype(int)\n",
        "    df['frame_ratio'] = (df['frame_num'] / df['seq_num_frames']).clip(0,1)\n",
        "    df['is_first'] = (df['frame_num'] <= 1).astype(int)\n",
        "    df['is_last']  = (df['frame_num'] >= df['seq_num_frames']).astype(int)\n",
        "    # cyclical time\n",
        "    df['hour_sin'] = np.sin(2*np.pi*df['hour']/24.0)\n",
        "    df['hour_cos'] = np.cos(2*np.pi*df['hour']/24.0)\n",
        "    return df\n",
        "\n",
        "train_fe = fe(train)\n",
        "test_fe  = fe(test)\n",
        "\n",
        "num_cols = [\n",
        "    'width','height','year','month','day','hour','is_night',\n",
        "    'frame_num','seq_num_frames','frame_ratio','is_first','is_last','hour_sin','hour_cos'\n",
        "]\n",
        "cat_cols = ['location','rights_holder']\n",
        "all_cols = num_cols + cat_cols\n",
        "\n",
        "X_all = train_fe[all_cols].copy()\n",
        "y_all = train_fe[target].astype(int).values\n",
        "groups = train_fe['seq_id'].astype(str).values\n",
        "X_test = test_fe[all_cols].copy()\n",
        "\n",
        "cat_idx = [all_cols.index(c) for c in cat_cols]\n",
        "n_classes = train[target].nunique()\n",
        "\n",
        "def add_fold_safe_counts(X_tr, X_va, cols=('location','rights_holder')):\n",
        "    X_tr = X_tr.copy(); X_va = X_va.copy()\n",
        "    for c in cols:\n",
        "        cnt = X_tr[c].value_counts()\n",
        "        X_tr[f'cnt_{c}'] = X_tr[c].map(cnt).fillna(1).astype(int)\n",
        "        X_va[f'cnt_{c}'] = X_va[c].map(cnt).fillna(1).astype(int)\n",
        "    return X_tr, X_va\n",
        "\n",
        "print('[CB] 5-fold SGKF by seq_id starting...', flush=True)\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_logits = np.full((len(train_fe), n_classes), np.nan, dtype=np.float32)\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(sgkf.split(X_all, y=y_all, groups=groups)):\n",
        "    t_fold = time.time()\n",
        "    X_tr = X_all.iloc[tr_idx].copy(); X_va = X_all.iloc[va_idx].copy()\n",
        "    y_tr = y_all[tr_idx]; y_va = y_all[va_idx]\n",
        "    # fold-safe counts\n",
        "    X_tr, X_va = add_fold_safe_counts(X_tr, X_va)\n",
        "    # build Pools\n",
        "    cat_features_idx = cat_idx + [X_tr.columns.get_loc('cnt_location'), X_tr.columns.get_loc('cnt_rights_holder')]\n",
        "    train_pool = Pool(X_tr, label=y_tr, cat_features=cat_features_idx)\n",
        "    valid_pool = Pool(X_va, label=y_va, cat_features=cat_features_idx)\n",
        "    model = CatBoostClassifier(\n",
        "        loss_function='MultiClass',\n",
        "        eval_metric='MultiClass',\n",
        "        iterations=2000,\n",
        "        depth=8,\n",
        "        learning_rate=0.05,\n",
        "        l2_leaf_reg=6,\n",
        "        auto_class_weights='Balanced',\n",
        "        early_stopping_rounds=100,\n",
        "        random_seed=42,\n",
        "        task_type='CPU',\n",
        "        verbose=False\n",
        "    )\n",
        "    model.fit(train_pool, eval_set=valid_pool, use_best_model=True, verbose=False)\n",
        "    proba = model.predict_proba(valid_pool)\n",
        "    logits = np.log(np.clip(np.asarray(proba), 1e-8, 1.0))\n",
        "    # seq-average within val fold\n",
        "    va_df = train_fe.iloc[va_idx][['seq_id']].reset_index(drop=True)\n",
        "    logits_seq = logits.copy()\n",
        "    for sid, grp in va_df.groupby('seq_id').groups.items():\n",
        "        idxs = np.array(list(grp))\n",
        "        m = logits[idxs].mean(axis=0, keepdims=True)\n",
        "        logits_seq[idxs] = m\n",
        "    oof_logits[va_idx] = logits_seq\n",
        "    f1 = f1_score(y_va, logits_seq.argmax(1), average='macro')\n",
        "    print(f\"[CB] Fold {fold} seq-avg macro-F1={f1:.5f} elapsed {time.time()-t_fold:.1f}s\", flush=True)\n",
        "\n",
        "assert not np.isnan(oof_logits).any(), 'NaNs in OOF logits'\n",
        "\n",
        "def optimize_biases(y_true, logits, n_iters=2, grid=np.linspace(-1.5, 1.5, 21)):\n",
        "    b = np.zeros(logits.shape[1], dtype=np.float32)\n",
        "    best = f1_score(y_true, (logits + b).argmax(1), average='macro')\n",
        "    for _ in range(n_iters):\n",
        "        improved = False\n",
        "        for c in range(logits.shape[1]):\n",
        "            bc = b[c]; best_c = bc; best_sc = best\n",
        "            for d in grid:\n",
        "                b[c] = d\n",
        "                sc = f1_score(y_true, (logits + b).argmax(1), average='macro')\n",
        "                if sc > best_sc:\n",
        "                    best_sc = sc; best_c = d\n",
        "            b[c] = best_c\n",
        "            if best_c != bc:\n",
        "                best = best_sc; improved = True\n",
        "        if not improved: break\n",
        "    return b, best\n",
        "\n",
        "b_opt, f1_oof = optimize_biases(y_all, oof_logits)\n",
        "print(f\"[CB] OOF seq-avg macro-F1 (with biases) = {f1_oof:.5f}\", flush=True)\n",
        "print('[CB] Biases:', np.round(b_opt, 3))\n",
        "\n",
        "# Fit on full data with counts (computed on full train for test mapping) and predict test\n",
        "X_full = X_all.copy()\n",
        "for c in ['location','rights_holder']:\n",
        "    cnt = X_full[c].value_counts()\n",
        "    X_full[f'cnt_{c}'] = X_full[c].map(cnt).fillna(1).astype(int)\n",
        "X_test_full = X_test.copy()\n",
        "for c in ['location','rights_holder']:\n",
        "    cnt = X_full[c].value_counts()\n",
        "    X_test_full[f'cnt_{c}'] = X_test_full[c].map(cnt).fillna(1).astype(int)\n",
        "cat_features_idx_full = [X_full.columns.get_loc(c) for c in cat_cols] + [X_full.columns.get_loc('cnt_location'), X_full.columns.get_loc('cnt_rights_holder')]\n",
        "pool_full = Pool(X_full, label=y_all, cat_features=cat_features_idx_full)\n",
        "pool_test = Pool(X_test_full, cat_features=cat_features_idx_full)\n",
        "model_full = CatBoostClassifier(\n",
        "    loss_function='MultiClass',\n",
        "    eval_metric='MultiClass',\n",
        "    iterations=2000,\n",
        "    depth=8,\n",
        "    learning_rate=0.05,\n",
        "    l2_leaf_reg=6,\n",
        "    auto_class_weights='Balanced',\n",
        "    early_stopping_rounds=100,\n",
        "    random_seed=42,\n",
        "    task_type='CPU',\n",
        "    verbose=False\n",
        ")\n",
        "print('[CB] Fitting full model...', flush=True)\n",
        "model_full.fit(pool_full, verbose=False)\n",
        "proba_test = model_full.predict_proba(pool_test)\n",
        "logits_test = np.log(np.clip(np.asarray(proba_test), 1e-8, 1.0))\n",
        "\n",
        "# Sequence-average test logits, apply biases, predict one label per seq_id, broadcast\n",
        "test_df = test_fe[['id','seq_id']].copy()\n",
        "pred_seq = {}\n",
        "for sid, idxs in test_df.groupby('seq_id').groups.items():\n",
        "    idxs = np.array(list(idxs))\n",
        "    m = logits_test[idxs].mean(axis=0) + b_opt\n",
        "    pred_seq[sid] = int(m.argmax())\n",
        "test_pred = test_df['seq_id'].map(pred_seq).astype(int).values\n",
        "sub_cb = pd.DataFrame({'id': test['id'], 'category_id': test_pred})\n",
        "sub_cb.to_csv('submission_cat_seq_bias.csv', index=False)\n",
        "print('[SUB-CB] Saved submission_cat_seq_bias.csv shape', sub_cb.shape, 'unique classes', sub_cb['category_id'].nunique())\n",
        "print('[CB] Done in {:.1f}s'.format(time.time()-t0), flush=True)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB] Installing catboost if missing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost==1.2.5\n  Downloading catboost-1.2.5-cp311-cp311-manylinux2014_x86_64.whl (98.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.2/98.2 MB 140.8 MB/s eta 0:00:00\nCollecting plotly\n  Downloading plotly-6.3.0-py3-none-any.whl (9.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.8/9.8 MB 104.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 126.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas>=0.24\n  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.4/12.4 MB 160.6 MB/s eta 0:00:00\nCollecting six\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nCollecting graphviz\n  Downloading graphviz-0.21-py3-none-any.whl (47 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 47.3/47.3 KB 397.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplotlib\n  Downloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8.7/8.7 MB 288.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.16.0\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 140.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dateutil>=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.9/229.9 KB 478.2 MB/s eta 0:00:00\nCollecting tzdata>=2022.7\n  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 347.8/347.8 KB 395.8 MB/s eta 0:00:00\nCollecting pytz>=2020.1\n  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 509.2/509.2 KB 526.3 MB/s eta 0:00:00\nCollecting cycler>=0.10\n  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow>=8\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 35.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fonttools>=4.22.0\n  Downloading fonttools-4.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5.0/5.0 MB 265.1 MB/s eta 0:00:00\nCollecting contourpy>=1.0.1\n  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 355.2/355.2 KB 492.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kiwisolver>=1.3.1\n  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 465.6 MB/s eta 0:00:00\nCollecting packaging>=20.0\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 396.6 MB/s eta 0:00:00\nCollecting pyparsing>=2.3.1\n  Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 113.9/113.9 KB 453.9 MB/s eta 0:00:00\nCollecting narwhals>=1.15.1\n  Downloading narwhals-2.5.0-py3-none-any.whl (407 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 407.3/407.3 KB 327.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pytz, tzdata, six, pyparsing, pillow, packaging, numpy, narwhals, kiwisolver, graphviz, fonttools, cycler, scipy, python-dateutil, plotly, contourpy, pandas, matplotlib, catboost\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nalbumentations 1.4.10 requires albucore>=0.0.11, which is not installed.\nWARNING: Target directory /app/.pip-target/matplotlib-3.10.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpl_toolkits already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/matplotlib already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pylab.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pandas-2.3.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pandas already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/contourpy-1.3.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/contourpy already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed catboost-1.2.5 contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.0 graphviz-0.21 kiwisolver-1.4.9 matplotlib-3.10.6 narwhals-2.5.0 numpy-1.26.4 packaging-25.0 pandas-2.3.2 pillow-11.3.0 plotly-6.3.0 pyparsing-3.2.5 python-dateutil-2.9.0.post0 pytz-2025.2 scipy-1.16.2 six-1.17.0 tzdata-2025.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/python_dateutil-2.9.0.post0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/dateutil already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy-1.16.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/cycler-0.12.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/cycler already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fonttools-4.60.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fontTools already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/kiwisolver-1.4.9.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/kiwisolver already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging-25.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pyparsing-3.2.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pyparsing already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/six-1.17.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/six.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tzdata-2025.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tzdata already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pytz-2025.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pytz already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB] 5-fold SGKF by seq_id starting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB] Fold 0 seq-avg macro-F1=0.09345 elapsed 379.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB] Fold 1 seq-avg macro-F1=0.09262 elapsed 402.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB] Fold 2 seq-avg macro-F1=0.09276 elapsed 457.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB] Fold 3 seq-avg macro-F1=0.09357 elapsed 368.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB] Fold 4 seq-avg macro-F1=0.09397 elapsed 285.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB] OOF seq-avg macro-F1 (with biases) = 0.10027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB] Biases: [ 1.35 -0.75  0.6  -1.5  -1.5  -1.35 -1.5  -1.05  1.5  -0.15  0.45  1.5\n  1.05 -1.5 ]\n[CB] Fitting full model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUB-CB] Saved submission_cat_seq_bias.csv shape (16877, 2) unique classes 13\n[CB] Done in 2641.5s\n"
          ]
        }
      ]
    },
    {
      "id": "1c9cb19f-cc9a-403a-8db8-13bb1cbb77fa",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Overwrite submission.csv with CatBoost seq-avg+bias tuned predictions\n",
        "import pandas as pd, os\n",
        "src = 'submission_cat_seq_bias.csv'\n",
        "dst = 'submission.csv'\n",
        "assert os.path.exists(src), f\"Missing {src}\"\n",
        "sub = pd.read_csv(src)\n",
        "sub.to_csv(dst, index=False)\n",
        "print('[COPY-CB] Wrote', dst, 'from', src, 'shape', sub.shape, 'unique classes', sub['category_id'].nunique())\n",
        "print(sub.head())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[COPY-CB] Wrote submission.csv from submission_cat_seq_bias.csv shape (16877, 2) unique classes 13\n                                     id  category_id\n0  5998cfa4-23d2-11e8-a6a3-ec086b02610b            0\n1  599fbd89-23d2-11e8-a6a3-ec086b02610b            0\n2  59fae563-23d2-11e8-a6a3-ec086b02610b            8\n3  5a24a741-23d2-11e8-a6a3-ec086b02610b            0\n4  59eab924-23d2-11e8-a6a3-ec086b02610b            0\n"
          ]
        }
      ]
    },
    {
      "id": "38b0b3fd-31a4-4bc3-935a-5e484bb9485c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost with fold-safe priors/TE + optional cheap image features, seq-avg + bias tuning\n",
        "import numpy as np, pandas as pd, time, os\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "target = 'category_id'\n",
        "n_classes = int(train[target].nunique())\n",
        "\n",
        "def fe_base(df):\n",
        "    df = df.copy()\n",
        "    dt = pd.to_datetime(df['date_captured'], errors='coerce')\n",
        "    df['year'] = dt.dt.year.fillna(-1).astype(int)\n",
        "    df['month'] = dt.dt.month.fillna(-1).astype(int)\n",
        "    df['day'] = dt.dt.day.fillna(-1).astype(int)\n",
        "    df['hour'] = dt.dt.hour.fillna(-1).astype(int)\n",
        "    df['doy']  = dt.dt.dayofyear.fillna(1).astype(int)\n",
        "    df['is_night'] = ((df['hour'] < 6) | (df['hour'] > 19)).astype(int)\n",
        "    df['frame_num'] = df['frame_num'].fillna(-1).astype(int)\n",
        "    df['seq_num_frames'] = df['seq_num_frames'].fillna(1).astype(int)\n",
        "    df['frame_ratio'] = (df['frame_num'] / df['seq_num_frames']).clip(0,1)\n",
        "    df['is_first'] = (df['frame_num'] <= 1).astype(int)\n",
        "    df['is_last']  = (df['frame_num'] >= df['seq_num_frames']).astype(int)\n",
        "    # cyclical time\n",
        "    df['hour_sin'] = np.sin(2*np.pi*df['hour']/24.0)\n",
        "    df['hour_cos'] = np.cos(2*np.pi*df['hour']/24.0)\n",
        "    df['doy_sin']  = np.sin(2*np.pi*df['doy']/366.0)\n",
        "    df['doy_cos']  = np.cos(2*np.pi*df['doy']/366.0)\n",
        "    # hour bins for loc x hour interactions\n",
        "    bins = [-1,3,7,11,15,19,23]\n",
        "    labels = [0,1,2,3,4,5]\n",
        "    df['hour_bin'] = pd.cut(df['hour'], bins=bins, labels=labels, include_lowest=True).astype(int)\n",
        "    # aspect ratio\n",
        "    df['aspect'] = (df['width'] / df['height']).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    return df\n",
        "\n",
        "def entropy_from_probs(p):\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    return float(-(p * np.log(p)).sum())\n",
        "\n",
        "def m_estimate_prior(counts, total, pg, m):\n",
        "    return (counts + m * pg) / (total + m)\n",
        "\n",
        "def build_group_priors(train_idx_df, key_col, classes, m):\n",
        "    g = {}\n",
        "    grp = train_idx_df.groupby([key_col, 'category_id']).size().unstack(fill_value=0)\n",
        "    for c in classes:\n",
        "        if c not in grp.columns:\n",
        "            grp[c] = 0\n",
        "    grp = grp[classes]\n",
        "    total_counts = grp.sum(axis=1).astype(int)\n",
        "    pg = train_idx_df['category_id'].value_counts(normalize=True).reindex(classes).fillna(0).values\n",
        "    for key, row in grp.iterrows():\n",
        "        cnts = row.values.astype(float)\n",
        "        n = int(total_counts.loc[key])\n",
        "        p = m_estimate_prior(cnts, n, pg, m)\n",
        "        ent = entropy_from_probs(p)\n",
        "        g[key] = (p, n, ent)\n",
        "    p_global = pg.copy(); ent_global = entropy_from_probs(p_global)\n",
        "    return g, p_global, ent_global\n",
        "\n",
        "def map_group_priors(df_in, key_col, prior_map, p_global, ent_global, prefix, classes):\n",
        "    df = df_in.copy()\n",
        "    probs_mat = np.zeros((len(df), len(classes)), dtype=np.float32)\n",
        "    counts = np.zeros(len(df), dtype=np.int32)\n",
        "    ents = np.zeros(len(df), dtype=np.float32)\n",
        "    key_vals = df[key_col].values\n",
        "    for i, k in enumerate(key_vals):\n",
        "        tpl = prior_map.get(k)\n",
        "        if tpl is None:\n",
        "            probs_mat[i] = p_global; counts[i] = 0; ents[i] = ent_global\n",
        "        else:\n",
        "            p, n, e = tpl; probs_mat[i] = p; counts[i] = n; ents[i] = e\n",
        "    for j, c in enumerate(classes):\n",
        "        df[f'{prefix}_p_{c}'] = probs_mat[:, j]\n",
        "    df[f'{prefix}_count'] = np.log1p(counts)\n",
        "    df[f'{prefix}_entropy'] = ents\n",
        "    return df\n",
        "\n",
        "def build_loc_hour_entropy(train_idx_df, m=300):\n",
        "    key = train_idx_df['location'].astype(str) + '|' + train_idx_df['hour_bin'].astype(str)\n",
        "    grp = train_idx_df.assign(k=key).groupby(['k','category_id']).size().unstack(fill_value=0)\n",
        "    classes = sorted(train_idx_df['category_id'].unique().tolist())\n",
        "    for c in classes:\n",
        "        if c not in grp.columns: grp[c] = 0\n",
        "    grp = grp[classes]\n",
        "    total_counts = grp.sum(axis=1).astype(int)\n",
        "    pg = train_idx_df['category_id'].value_counts(normalize=True).reindex(classes).fillna(0).values\n",
        "    ent_map = {}\n",
        "    for k, row in grp.iterrows():\n",
        "        cnts = row.values.astype(float); n = int(total_counts.loc[k])\n",
        "        p = m_estimate_prior(cnts, n, pg, m)\n",
        "        ent_map[k] = (entropy_from_probs(p), n)\n",
        "    ent_global = entropy_from_probs(pg)\n",
        "    return ent_map, ent_global\n",
        "\n",
        "def map_loc_hour_entropy(df_in, ent_map, ent_global):\n",
        "    df = df_in.copy()\n",
        "    k = df['location'].astype(str) + '|' + df['hour_bin'].astype(str)\n",
        "    ents = np.zeros(len(df), dtype=np.float32)\n",
        "    cnts = np.zeros(len(df), dtype=np.int32)\n",
        "    for i, key in enumerate(k.values):\n",
        "        tpl = ent_map.get(key)\n",
        "        if tpl is None:\n",
        "            ents[i] = ent_global; cnts[i] = 0\n",
        "        else:\n",
        "            e, n = tpl; ents[i] = e; cnts[i] = n\n",
        "    df['loc_hour_entropy'] = ents\n",
        "    df['loc_hour_count'] = np.log1p(cnts)\n",
        "    return df\n",
        "\n",
        "def maybe_merge_img_feats(df_tr, df_te):\n",
        "    img_tr_path = 'img_feats_train.csv'; img_te_path = 'img_feats_test.csv'\n",
        "    if os.path.exists(img_tr_path) and os.path.exists(img_te_path):\n",
        "        fe_tr = pd.read_csv(img_tr_path); fe_te = pd.read_csv(img_te_path)\n",
        "        df_tr = df_tr.merge(fe_tr, on='id', how='left')\n",
        "        df_te = df_te.merge(fe_te, on='id', how='left')\n",
        "        print('[CB-P] Merged image feats:', fe_tr.shape, fe_te.shape, flush=True)\n",
        "    else:\n",
        "        print('[CB-P] Image feats not found; proceeding without.', flush=True)\n",
        "    return df_tr, df_te\n",
        "\n",
        "train_fe = fe_base(train)\n",
        "test_fe  = fe_base(test)\n",
        "classes_all = sorted(train[target].unique().tolist())\n",
        "\n",
        "# Ensure id column exists for merging image features\n",
        "train_fe['id'] = train['id'].astype(str).values\n",
        "test_fe['id']  = test['id'].astype(str).values\n",
        "train_fe, test_fe = maybe_merge_img_feats(train_fe, test_fe)\n",
        "\n",
        "img_num = [c for c in ['laplacian_var','hsv_s_mean','gray_mean','gray_std','hsv_v_mean','file_size_kb'] if c in train_fe.columns]\n",
        "base_num = ['width','height','aspect','year','month','day','hour','doy','is_night','frame_num','seq_num_frames','frame_ratio','is_first','is_last','hour_sin','hour_cos','doy_sin','doy_cos'] + img_num\n",
        "cat_cols = ['rights_holder']\n",
        "\n",
        "X_all = train_fe[['location','rights_holder','hour_bin','id'] + base_num].copy()\n",
        "y_all = train_fe[target].astype(int).values\n",
        "groups = train_fe['seq_id'].astype(str).values\n",
        "X_test_base = test_fe[['location','rights_holder','hour_bin','id'] + base_num].copy()\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_logits = np.full((len(train_fe), n_classes), np.nan, dtype=np.float32)\n",
        "\n",
        "print('[CB-P] 5-fold with fold-safe priors/entropy...', flush=True)\n",
        "for fold, (tr_idx, va_idx) in enumerate(sgkf.split(X_all, y=y_all, groups=groups)):\n",
        "    t_fold = time.time()\n",
        "    # Build priors on train fold\n",
        "    idx_cols = ['location','hour_bin','category_id']\n",
        "    loc_map, loc_pg, loc_entg = build_group_priors(train_fe.iloc[tr_idx][['location','category_id']], 'location', classes_all, m=100)\n",
        "    rh_map,  rh_pg,  rh_entg  = build_group_priors(train_fe.iloc[tr_idx][['rights_holder','category_id']], 'rights_holder', classes_all, m=50)\n",
        "    lxh_map, lxh_entg = build_loc_hour_entropy(train_fe.iloc[tr_idx][idx_cols].copy(), m=300)\n",
        "\n",
        "    X_tr = X_all.iloc[tr_idx].copy()\n",
        "    X_va = X_all.iloc[va_idx].copy()\n",
        "\n",
        "    # Fold-safe imputation for image features: median on train, apply to val\n",
        "    for c in img_num:\n",
        "        med = X_tr[c].median() if np.isfinite(X_tr[c]).any() else 0.0\n",
        "        X_tr[c] = X_tr[c].fillna(med)\n",
        "        X_va[c] = X_va[c].fillna(med)\n",
        "\n",
        "    X_tr = map_group_priors(X_tr, 'location', loc_map, loc_pg, loc_entg, 'loc', classes_all)\n",
        "    X_va = map_group_priors(X_va, 'location', loc_map, loc_pg, loc_entg, 'loc', classes_all)\n",
        "    X_tr = map_group_priors(X_tr, 'rights_holder', rh_map, rh_pg, rh_entg, 'rh', classes_all)\n",
        "    X_va = map_group_priors(X_va, 'rights_holder', rh_map, rh_pg, rh_entg, 'rh', classes_all)\n",
        "    X_tr = map_loc_hour_entropy(X_tr, lxh_map, lxh_entg)\n",
        "    X_va = map_loc_hour_entropy(X_va, lxh_map, lxh_entg)\n",
        "\n",
        "    use_cols = base_num + [\n",
        "        'loc_count','loc_entropy','rh_count','rh_entropy','loc_hour_entropy','loc_hour_count'\n",
        "    ] + [f'loc_p_{c}' for c in classes_all] + [f'rh_p_{c}' for c in classes_all] + cat_cols\n",
        "    X_tr_use = X_tr[use_cols].copy()\n",
        "    X_va_use = X_va[use_cols].copy()\n",
        "    cat_idx = [use_cols.index(c) for c in cat_cols]\n",
        "    train_pool = Pool(X_tr_use, label=y_all[tr_idx], cat_features=cat_idx)\n",
        "    valid_pool = Pool(X_va_use, label=y_all[va_idx], cat_features=cat_idx)\n",
        "    model = CatBoostClassifier(\n",
        "        loss_function='MultiClass',\n",
        "        eval_metric='MultiClass',\n",
        "        iterations=2500,\n",
        "        depth=8,\n",
        "        learning_rate=0.05,\n",
        "        l2_leaf_reg=10,\n",
        "        random_strength=4,\n",
        "        subsample=0.8,\n",
        "        rsm=0.8,\n",
        "        bootstrap_type='Bernoulli',\n",
        "        auto_class_weights='Balanced',\n",
        "        early_stopping_rounds=150,\n",
        "        random_seed=42,\n",
        "        task_type='CPU',\n",
        "        verbose=False\n",
        "    )\n",
        "    model.fit(train_pool, eval_set=valid_pool, use_best_model=True, verbose=False)\n",
        "    proba = model.predict_proba(valid_pool)\n",
        "    logits = np.log(np.clip(np.asarray(proba), 1e-8, 1.0))\n",
        "    # seq-avg within val fold\n",
        "    va_seq = train_fe.iloc[va_idx]['seq_id'].values\n",
        "    logits_seq = logits.copy()\n",
        "    from collections import defaultdict\n",
        "    gmap = defaultdict(list)\n",
        "    for i, sid in enumerate(va_seq):\n",
        "        gmap[sid].append(i)\n",
        "    for idxs in gmap.values():\n",
        "        mlog = logits[idxs].mean(axis=0, keepdims=True)\n",
        "        logits_seq[idxs] = mlog\n",
        "    oof_logits[va_idx] = logits_seq\n",
        "    f1 = f1_score(y_all[va_idx], logits_seq.argmax(1), average='macro')\n",
        "    print(f\"[CB-P] Fold {fold} seq-avg macro-F1={f1:.5f} elapsed {time.time()-t_fold:.1f}s\", flush=True)\n",
        "\n",
        "assert not np.isnan(oof_logits).any(), 'NaNs in OOF logits'\n",
        "\n",
        "def optimize_biases(y_true, logits, n_iters=3, grid=np.linspace(-2.0, 2.0, 41)):\n",
        "    b = np.zeros(logits.shape[1], dtype=np.float32)\n",
        "    best = f1_score(y_true, (logits + b).argmax(1), average='macro')\n",
        "    for _ in range(n_iters):\n",
        "        improved = False\n",
        "        for c in range(logits.shape[1]):\n",
        "            bc = b[c]; best_c = bc; best_sc = best\n",
        "            for d in grid:\n",
        "                b[c] = d\n",
        "                sc = f1_score(y_true, (logits + b).argmax(1), average='macro')\n",
        "                if sc > best_sc:\n",
        "                    best_sc = sc; best_c = d\n",
        "            b[c] = best_c\n",
        "            if best_c != bc:\n",
        "                best = best_sc; improved = True\n",
        "        if not improved: break\n",
        "    return b, best\n",
        "\n",
        "b_opt, f1_oof = optimize_biases(y_all, oof_logits)\n",
        "print(f\"[CB-P] OOF seq-avg macro-F1 (with biases) = {f1_oof:.5f}\", flush=True)\n",
        "print('[CB-P] Biases:', np.round(b_opt, 3))\n",
        "\n",
        "# Fit full model with priors for test mapping\n",
        "print('[CB-P] Fitting full model and predicting test...', flush=True)\n",
        "X_full = X_all.copy()\n",
        "X_test = X_test_base.copy()\n",
        "for c in img_num:\n",
        "    med = X_full[c].median() if np.isfinite(X_full[c]).any() else 0.0\n",
        "    X_full[c] = X_full[c].fillna(med)\n",
        "    X_test[c] = X_test[c].fillna(med)\n",
        "\n",
        "loc_map_full, loc_pg_full, loc_entg_full = build_group_priors(train_fe[['location','category_id']], 'location', classes_all, m=100)\n",
        "rh_map_full,  rh_pg_full,  rh_entg_full  = build_group_priors(train_fe[['rights_holder','category_id']], 'rights_holder', classes_all, m=50)\n",
        "lxh_map_full, lxh_entg_full = build_loc_hour_entropy(train_fe[['location','hour_bin','category_id']].copy(), m=300)\n",
        "X_full = map_group_priors(X_full, 'location', loc_map_full, loc_pg_full, loc_entg_full, 'loc', classes_all)\n",
        "X_full = map_group_priors(X_full, 'rights_holder', rh_map_full,  rh_pg_full,  rh_entg_full,  'rh', classes_all)\n",
        "X_full = map_loc_hour_entropy(X_full, lxh_map_full, lxh_entg_full)\n",
        "X_test = map_group_priors(X_test, 'location', loc_map_full, loc_pg_full, loc_entg_full, 'loc', classes_all)\n",
        "X_test = map_group_priors(X_test, 'rights_holder', rh_map_full,  rh_pg_full,  rh_entg_full,  'rh', classes_all)\n",
        "X_test = map_loc_hour_entropy(X_test, lxh_map_full, lxh_entg_full)\n",
        "use_cols = base_num + ['loc_count','loc_entropy','rh_count','rh_entropy','loc_hour_entropy','loc_hour_count'] + [f'loc_p_{c}' for c in classes_all] + [f'rh_p_{c}' for c in classes_all] + cat_cols\n",
        "cat_idx_full = [use_cols.index(c) for c in cat_cols]\n",
        "pool_full = Pool(X_full[use_cols], label=y_all, cat_features=cat_idx_full)\n",
        "pool_test = Pool(X_test[use_cols], cat_features=cat_idx_full)\n",
        "model_full = CatBoostClassifier(\n",
        "    loss_function='MultiClass',\n",
        "    eval_metric='MultiClass',\n",
        "    iterations=2500,\n",
        "    depth=8,\n",
        "    learning_rate=0.05,\n",
        "    l2_leaf_reg=10,\n",
        "    random_strength=4,\n",
        "    subsample=0.8,\n",
        "    rsm=0.8,\n",
        "    bootstrap_type='Bernoulli',\n",
        "    auto_class_weights='Balanced',\n",
        "    early_stopping_rounds=150,\n",
        "    random_seed=42,\n",
        "    task_type='CPU',\n",
        "    verbose=False\n",
        ")\n",
        "model_full.fit(pool_full, verbose=False)\n",
        "proba_test = model_full.predict_proba(pool_test)\n",
        "logits_test = np.log(np.clip(np.asarray(proba_test), 1e-8, 1.0))\n",
        "\n",
        "# Sequence-average per seq and apply biases\n",
        "test_df = test_fe[['id','seq_id']].copy()\n",
        "from collections import defaultdict\n",
        "seq_map = defaultdict(list)\n",
        "for i, sid in enumerate(test_df['seq_id'].values):\n",
        "    seq_map[sid].append(i)\n",
        "pred_seq = {}\n",
        "for sid, idxs in seq_map.items():\n",
        "    m = logits_test[idxs].mean(axis=0) + b_opt\n",
        "    pred_seq[sid] = int(np.argmax(m))\n",
        "test_pred = test_df['seq_id'].map(pred_seq).astype(int).values\n",
        "# Use test_df['id'] to ensure same length as predictions to avoid mismatches; later mapping cell aligns strictly to test ids\n",
        "sub_cbp = pd.DataFrame({'id': test_df['id'].astype(str).values, 'category_id': test_pred})\n",
        "sub_cbp.to_csv('submission_cb_priors_seq_bias.csv', index=False)\n",
        "print('[SUB-CB-P] Saved submission_cb_priors_seq_bias.csv', sub_cbp.shape, 'unique classes', sub_cbp['category_id'].nunique())\n",
        "print('[CB-P] Done in {:.1f}s'.format(time.time()-t0), flush=True)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB-P] Merged image feats: (179422, 7) (16877, 7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB-P] 5-fold with fold-safe priors/entropy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB-P] Fold 0 seq-avg macro-F1=0.09273 elapsed 114.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB-P] Fold 1 seq-avg macro-F1=0.09286 elapsed 129.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB-P] Fold 2 seq-avg macro-F1=0.09378 elapsed 119.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB-P] Fold 3 seq-avg macro-F1=0.09248 elapsed 126.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB-P] Fold 4 seq-avg macro-F1=0.09351 elapsed 121.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB-P] OOF seq-avg macro-F1 (with biases) = 0.10246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CB-P] Biases: [ 1.3 -0.8  0.4 -2.  -2.  -0.9 -2.  -0.4  2.  -1.9  1.6  2.   0.5  0. ]\n[CB-P] Fitting full model and predicting test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUB-CB-P] Saved submission_cb_priors_seq_bias.csv (16937, 2) unique classes 5\n[CB-P] Done in 899.1s\n"
          ]
        }
      ]
    },
    {
      "id": "288c6385-5a93-4252-9af0-b309fe87c435",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# FIX submission: map class indices to original category_id labels and regenerate submission.csv\n",
        "import numpy as np, pandas as pd\n",
        "from catboost import Pool\n",
        "\n",
        "# Rebuild features exactly as in Cell 15 to get test Pool; reuse model_full and b_opt from memory\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "classes_all = sorted(train['category_id'].unique().tolist())\n",
        "\n",
        "def fe(df):\n",
        "    df = df.copy()\n",
        "    dt = pd.to_datetime(df['date_captured'], errors='coerce')\n",
        "    df['year'] = dt.dt.year.fillna(-1).astype(int)\n",
        "    df['month'] = dt.dt.month.fillna(-1).astype(int)\n",
        "    df['day'] = dt.dt.day.fillna(-1).astype(int)\n",
        "    df['hour'] = dt.dt.hour.fillna(-1).astype(int)\n",
        "    df['is_night'] = ((df['hour'] < 6) | (df['hour'] > 19)).astype(int)\n",
        "    df['frame_num'] = df['frame_num'].fillna(-1).astype(int)\n",
        "    df['seq_num_frames'] = df['seq_num_frames'].fillna(1).astype(int)\n",
        "    df['frame_ratio'] = (df['frame_num'] / df['seq_num_frames']).clip(0,1)\n",
        "    df['is_first'] = (df['frame_num'] <= 1).astype(int)\n",
        "    df['is_last']  = (df['frame_num'] >= df['seq_num_frames']).astype(int)\n",
        "    df['hour_sin'] = np.sin(2*np.pi*df['hour']/24.0)\n",
        "    df['hour_cos'] = np.cos(2*np.pi*df['hour']/24.0)\n",
        "    return df\n",
        "\n",
        "train_fe = fe(train)\n",
        "test_fe  = fe(test)\n",
        "\n",
        "num_cols = [\n",
        "    'width','height','year','month','day','hour','is_night',\n",
        "    'frame_num','seq_num_frames','frame_ratio','is_first','is_last','hour_sin','hour_cos'\n",
        "]\n",
        "cat_cols = ['location','rights_holder']\n",
        "all_cols = num_cols + cat_cols\n",
        "\n",
        "X_full = train_fe[all_cols].copy()\n",
        "y_all = train_fe['category_id'].astype(int).values\n",
        "X_test_full = test_fe[all_cols].copy()\n",
        "\n",
        "# add count features as in Cell 15\n",
        "for c in ['location','rights_holder']:\n",
        "    cnt = X_full[c].value_counts()\n",
        "    X_full[f'cnt_{c}'] = X_full[c].map(cnt).fillna(1).astype(int)\n",
        "for c in ['location','rights_holder']:\n",
        "    cnt = X_full[c].value_counts()\n",
        "    X_test_full[f'cnt_{c}'] = X_test_full[c].map(cnt).fillna(1).astype(int)\n",
        "\n",
        "use_cols = all_cols + ['cnt_location','cnt_rights_holder']\n",
        "cat_features_idx_full = [use_cols.index(c) for c in cat_cols] + [use_cols.index('cnt_location'), use_cols.index('cnt_rights_holder')]\n",
        "\n",
        "# Build Pools for prediction with the same cat feature indices\n",
        "pool_test = Pool(X_test_full[use_cols], cat_features=cat_features_idx_full)\n",
        "\n",
        "# Predict probabilities with trained model_full (from Cell 15)\n",
        "proba_test = model_full.predict_proba(pool_test)\n",
        "logits_test = np.log(np.clip(np.asarray(proba_test), 1e-8, 1.0))\n",
        "\n",
        "# Sequence-average logits and apply learned biases b_opt (from Cell 15), then map argmax indices to original labels\n",
        "from collections import defaultdict\n",
        "seq_map = defaultdict(list)\n",
        "for i, sid in enumerate(test_fe['seq_id'].values):\n",
        "    seq_map[sid].append(i)\n",
        "pred_seq = {}\n",
        "for sid, idxs in seq_map.items():\n",
        "    m = logits_test[idxs].mean(axis=0) + b_opt\n",
        "    idx = int(np.argmax(m))\n",
        "    pred_seq[sid] = classes_all[idx]  # map index -> original category_id label\n",
        "\n",
        "test_pred = test_fe['seq_id'].map(pred_seq).astype(int).values\n",
        "sub_fix = pd.DataFrame({'id': test['id'], 'category_id': test_pred})\n",
        "sub_fix.to_csv('submission.csv', index=False)\n",
        "print('[SUB-FIX] Saved submission.csv (mapped to original labels) shape', sub_fix.shape, 'unique classes', sub_fix['category_id'].nunique())\n",
        "print(sub_fix.head())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUB-FIX] Saved submission.csv (mapped to original labels) shape (16877, 2) unique classes 13\n                                     id  category_id\n0  5998cfa4-23d2-11e8-a6a3-ec086b02610b            0\n1  599fbd89-23d2-11e8-a6a3-ec086b02610b            0\n2  59fae563-23d2-11e8-a6a3-ec086b02610b           14\n3  5a24a741-23d2-11e8-a6a3-ec086b02610b            0\n4  59eab924-23d2-11e8-a6a3-ec086b02610b            0\n"
          ]
        }
      ]
    },
    {
      "id": "93ca69cc-388d-4a15-bdf1-86f8df5c02f8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Validate submission format against sample_submission\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "sub = pd.read_csv('submission.csv')\n",
        "print('[CHK] sample_submission shape:', ss.shape, 'columns:', list(ss.columns))\n",
        "print('[CHK] submission shape:', sub.shape, 'columns:', list(sub.columns))\n",
        "print('[CHK] sample head:', ss.head().to_dict('records')[:3])\n",
        "print('[CHK] sub head:', sub.head().to_dict('records')[:3])\n",
        "# Check column names exact match\n",
        "cols_match = list(ss.columns) == list(sub.columns)\n",
        "print('[CHK] Columns match exactly:', cols_match)\n",
        "# Check id coverage and order\n",
        "ss_ids = ss['id'].astype(str)\n",
        "sub_ids = sub['id'].astype(str)\n",
        "missing = set(ss_ids) - set(sub_ids)\n",
        "extra = set(sub_ids) - set(ss_ids)\n",
        "print('[CHK] missing ids in sub:', len(missing))\n",
        "print('[CHK] extra ids in sub:', len(extra))\n",
        "if (not cols_match) or (len(missing) > 0) or (len(extra) > 0) or (len(ss) != len(sub)):\n",
        "    print('[FIX] Rebuilding submission to match sample_submission id order and columns...')\n",
        "    # If sub has different column names, rename accordingly\n",
        "    sub_renamed = sub.copy()\n",
        "    # Ensure correct columns names\n",
        "    sub_renamed.columns = ['id','category_id']\n",
        "    # Merge to sample order\n",
        "    merged = ss[['id']].merge(sub_renamed, on='id', how='left')\n",
        "    # If any ids missing, fill with mode of train category_id (fallback 0)\n",
        "    if merged['category_id'].isna().any():\n",
        "        try:\n",
        "            tr = pd.read_csv('train.csv')\n",
        "            fallback = int(tr['category_id'].mode().iloc[0])\n",
        "        except Exception:\n",
        "            fallback = 0\n",
        "        merged['category_id'] = merged['category_id'].fillna(fallback).astype(int)\n",
        "    merged.to_csv('submission.csv', index=False)\n",
        "    print('[FIX] Wrote aligned submission.csv shape', merged.shape)\n",
        "else:\n",
        "    # Ensure dtypes\n",
        "    sub['category_id'] = sub['category_id'].astype(int)\n",
        "    # Reorder to sample order just in case\n",
        "    sub_aligned = ss[['id']].merge(sub, on='id', how='left')\n",
        "    sub_aligned.to_csv('submission.csv', index=False)\n",
        "    print('[OK] submission.csv already matches; re-saved aligned order. shape', sub_aligned.shape)\n",
        "print('[DONE] Validation complete.')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CHK] sample_submission shape: (16877, 3) columns: ['Unnamed: 0', 'Id', 'Category']\n[CHK] submission shape: (16877, 2) columns: ['id', 'category_id']\n[CHK] sample head: [{'Unnamed: 0': 0, 'Id': '5998cfa4-23d2-11e8-a6a3-ec086b02610b', 'Category': 0}, {'Unnamed: 0': 4, 'Id': '599fbd89-23d2-11e8-a6a3-ec086b02610b', 'Category': 0}, {'Unnamed: 0': 6, 'Id': '59fae563-23d2-11e8-a6a3-ec086b02610b', 'Category': 0}]\n[CHK] sub head: [{'id': '5998cfa4-23d2-11e8-a6a3-ec086b02610b', 'category_id': 0}, {'id': '599fbd89-23d2-11e8-a6a3-ec086b02610b', 'category_id': 0}, {'id': '59fae563-23d2-11e8-a6a3-ec086b02610b', 'category_id': 14}]\n[CHK] Columns match exactly: False\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'id'",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mKeyError\u001b[39m: 'id'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m[CHK] Columns match exactly:\u001b[39m\u001b[33m'\u001b[39m, cols_match)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Check id coverage and order\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m ss_ids = \u001b[43mss\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m     15\u001b[39m sub_ids = sub[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m     16\u001b[39m missing = \u001b[38;5;28mset\u001b[39m(ss_ids) - \u001b[38;5;28mset\u001b[39m(sub_ids)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
            "\u001b[31mKeyError\u001b[39m: 'id'"
          ]
        }
      ]
    },
    {
      "id": "54ffa3b9-93ec-4d7b-aacc-c8502287f5bf",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Rebuild submission.csv to match competition headers exactly: ['Id','Category'] and sample order\n",
        "import pandas as pd\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "pred = pd.read_csv('submission.csv')  # current predictions with columns ['id','category_id']\n",
        "pred = pred.rename(columns={'id':'Id', 'category_id':'Category'})\n",
        "# Align to sample order and columns; drop any extra columns\n",
        "sub_aligned = ss[['Id']].merge(pred[['Id','Category']], on='Id', how='left')\n",
        "# Fill any missing categories with majority class 0 fallback (safe default)\n",
        "if sub_aligned['Category'].isna().any():\n",
        "    sub_aligned['Category'] = sub_aligned['Category'].fillna(0).astype(int)\n",
        "sub_aligned = sub_aligned[['Id','Category']].copy()\n",
        "sub_aligned.to_csv('submission.csv', index=False)\n",
        "print('[FIX-FMT] Wrote submission.csv with columns', list(sub_aligned.columns), 'shape', sub_aligned.shape)\n",
        "print(sub_aligned.head())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FIX-FMT] Wrote submission.csv with columns ['Id', 'Category'] shape (16937, 2)\n                                     Id  Category\n0  5998cfa4-23d2-11e8-a6a3-ec086b02610b         0\n1  599fbd89-23d2-11e8-a6a3-ec086b02610b         0\n2  59fae563-23d2-11e8-a6a3-ec086b02610b        14\n3  5a24a741-23d2-11e8-a6a3-ec086b02610b         0\n4  59eab924-23d2-11e8-a6a3-ec086b02610b         0\n"
          ]
        }
      ]
    },
    {
      "id": "99082c03-2cf0-4e10-8e8b-c18d60d9a8d0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure submission.csv exactly matches test ids and required headers\n",
        "import pandas as pd\n",
        "test = pd.read_csv('test.csv')\n",
        "pred = pd.read_csv('submission.csv')  # could be with either header style\n",
        "\n",
        "# Normalize column names\n",
        "cols = {c.lower(): c for c in pred.columns}\n",
        "if 'id' in cols and 'category' in cols:\n",
        "    pred = pred.rename(columns={cols['id']:'Id', cols['category']:'Category'})\n",
        "elif 'id' in cols and 'category_id' in cols:\n",
        "    pred = pred.rename(columns={cols['id']:'Id', cols['category_id']:'Category'})\n",
        "elif 'Id' in pred.columns and 'Category' in pred.columns:\n",
        "    pass\n",
        "else:\n",
        "    raise RuntimeError(f'Unexpected submission columns: {list(pred.columns)}')\n",
        "\n",
        "# Align to test ids and order\n",
        "sub = pd.DataFrame({'Id': test['id'].astype(str)})\n",
        "pred['Id'] = pred['Id'].astype(str)\n",
        "sub = sub.merge(pred[['Id','Category']], on='Id', how='left')\n",
        "\n",
        "# Fill any missing with majority class 0 (safe fallback)\n",
        "sub['Category'] = sub['Category'].fillna(0).astype(int)\n",
        "\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[FINAL-SUB] submission.csv shape', sub.shape, 'columns', list(sub.columns), 'nunique Category', sub['Category'].nunique())\n",
        "print(sub.head())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FINAL-SUB] submission.csv shape (17177, 2) columns ['Id', 'Category'] nunique Category 13\n                                     Id  Category\n0  5998cfa4-23d2-11e8-a6a3-ec086b02610b         0\n1  599fbd89-23d2-11e8-a6a3-ec086b02610b         0\n2  59fae563-23d2-11e8-a6a3-ec086b02610b        14\n3  5a24a741-23d2-11e8-a6a3-ec086b02610b         0\n4  59eab924-23d2-11e8-a6a3-ec086b02610b         0\n"
          ]
        }
      ]
    },
    {
      "id": "e923f23a-8e48-49d1-8467-4d89cc1fa930",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Rebuild final submission from CatBoost predictions with proper label mapping and exact length == len(test)\n",
        "import pandas as pd, numpy as np\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "pred_raw = pd.read_csv('submission_cat_seq_bias.csv')  # columns: id, category_id (indices 0..13)\n",
        "\n",
        "# Map class indices -> original category_id labels\n",
        "classes_all = sorted(train['category_id'].unique().tolist())\n",
        "idx2label = {i: lab for i, lab in enumerate(classes_all)}\n",
        "\n",
        "# Ensure one row per test id in predictions\n",
        "pred_raw = pred_raw.drop_duplicates(subset=['id'], keep='first')\n",
        "pred_raw['mapped'] = pred_raw['category_id'].map(idx2label).astype(int)\n",
        "\n",
        "# Align strictly to test ids (no row count change, no duplication)\n",
        "sub = pd.DataFrame({'Id': test['id'].astype(str)})\n",
        "m = test[['id']].merge(pred_raw[['id','mapped']], on='id', how='left')\n",
        "if m['mapped'].isna().any():\n",
        "    # fallback to majority class 0 if any missing\n",
        "    m['mapped'] = m['mapped'].fillna(0).astype(int)\n",
        "sub['Category'] = m['mapped'].astype(int).values\n",
        "\n",
        "# Final validations\n",
        "assert len(sub) == len(test), f'Row count mismatch: {len(sub)} vs {len(test)}'\n",
        "assert set(sub.columns) == {'Id','Category'}\n",
        "\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[FINAL-SUB-MAP] submission.csv shape', sub.shape, 'columns', list(sub.columns), 'nunique Category', sub['Category'].nunique())\n",
        "print(sub.head())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FINAL-SUB-MAP] submission.csv shape (16877, 2) columns ['Id', 'Category'] nunique Category 13\n                                     Id  Category\n0  5998cfa4-23d2-11e8-a6a3-ec086b02610b         0\n1  599fbd89-23d2-11e8-a6a3-ec086b02610b         0\n2  59fae563-23d2-11e8-a6a3-ec086b02610b        14\n3  5a24a741-23d2-11e8-a6a3-ec086b02610b         0\n4  59eab924-23d2-11e8-a6a3-ec086b02610b         0\n"
          ]
        }
      ]
    },
    {
      "id": "9d066747-391c-4e30-a975-37276938e372",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Map advanced CB priors submission (indices) to original labels and build final submission.csv\n",
        "import pandas as pd, numpy as np\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "pred_raw = pd.read_csv('submission_cb_priors_seq_bias.csv')  # columns: id, category_id (indices 0..13)\n",
        "\n",
        "# Map class indices -> original category_id labels\n",
        "classes_all = sorted(train['category_id'].unique().tolist())\n",
        "idx2label = {i: lab for i, lab in enumerate(classes_all)}\n",
        "\n",
        "# Ensure one row per test id\n",
        "pred_raw = pred_raw.drop_duplicates(subset=['id'], keep='first')\n",
        "pred_raw['mapped'] = pred_raw['category_id'].map(idx2label).astype(int)\n",
        "\n",
        "# Align strictly to test ids\n",
        "sub = pd.DataFrame({'Id': test['id'].astype(str)})\n",
        "m = test[['id']].merge(pred_raw[['id','mapped']], on='id', how='left')\n",
        "if m['mapped'].isna().any():\n",
        "    m['mapped'] = m['mapped'].fillna(0).astype(int)\n",
        "sub['Category'] = m['mapped'].astype(int).values\n",
        "\n",
        "assert len(sub) == len(test), f'Row count mismatch: {len(sub)} vs {len(test)}'\n",
        "assert set(sub.columns) == {'Id','Category'}\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[FINAL-SUB-ADV] submission.csv shape', sub.shape, 'columns', list(sub.columns), 'nunique Category', sub['Category'].nunique())\n",
        "print(sub.head())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FINAL-SUB-ADV] submission.csv shape (16877, 2) columns ['Id', 'Category'] nunique Category 5\n                                     Id  Category\n0  5998cfa4-23d2-11e8-a6a3-ec086b02610b         0\n1  599fbd89-23d2-11e8-a6a3-ec086b02610b         0\n2  59fae563-23d2-11e8-a6a3-ec086b02610b         0\n3  5a24a741-23d2-11e8-a6a3-ec086b02610b         0\n4  59eab924-23d2-11e8-a6a3-ec086b02610b         0\n"
          ]
        }
      ]
    },
    {
      "id": "5a93c954-a3d0-4149-b3bd-86f64313c571",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cheap CPU image features (laplacian_var, hsv_s_mean, gray_mean/std, v_mean, file_size_kb)\n",
        "import os, cv2, numpy as np, pandas as pd, time\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "cv2.setNumThreads(0)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "\n",
        "def compute_feats(img_path, max_side=128):\n",
        "    try:\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        if img is None:\n",
        "            return (np.nan, np.nan, np.nan, np.nan, np.nan)\n",
        "        h, w = img.shape[:2]\n",
        "        s = max(h, w)\n",
        "        if s > max_side and s > 0:\n",
        "            scale = max_side / float(s)\n",
        "            nh, nw = max(1, int(h*scale)), max(1, int(w*scale))\n",
        "            img = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_AREA)\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        lap = cv2.Laplacian(gray, cv2.CV_64F)\n",
        "        lap_var = float(lap.var())\n",
        "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "        s_mean = float(hsv[...,1].mean())\n",
        "        v_mean = float(hsv[...,2].mean())\n",
        "        g_mean = float(gray.mean())\n",
        "        g_std  = float(gray.std())\n",
        "        return (lap_var, s_mean, g_mean, g_std, v_mean)\n",
        "    except Exception:\n",
        "        return (np.nan, np.nan, np.nan, np.nan, np.nan)\n",
        "\n",
        "def process_df(df, img_dir):\n",
        "    paths = [os.path.join(img_dir, fn) for fn in df['file_name'].tolist()]\n",
        "    sizes = []\n",
        "    for p in paths:\n",
        "        try:\n",
        "            sizes.append(os.path.getsize(p) / 1024.0)\n",
        "        except Exception:\n",
        "            sizes.append(np.nan)\n",
        "    t0 = time.time()\n",
        "    feats = Parallel(n_jobs=8, prefer='threads', batch_size=64)(delayed(compute_feats)(p) for p in paths)\n",
        "    dt = time.time() - t0\n",
        "    print(f'[IMG-FE] processed {len(paths)} images from {img_dir} in {dt/60:.2f} min')\n",
        "    feats = np.asarray(feats, dtype=np.float32)\n",
        "    out = pd.DataFrame({\n",
        "        'id': df['id'].astype(str).values,\n",
        "        'laplacian_var': feats[:,0],\n",
        "        'hsv_s_mean': feats[:,1],\n",
        "        'gray_mean': feats[:,2],\n",
        "        'gray_std': feats[:,3],\n",
        "        'hsv_v_mean': feats[:,4],\n",
        "        'file_size_kb': np.array(sizes, dtype=np.float32)\n",
        "    })\n",
        "    return out\n",
        "\n",
        "print('[IMG-FE] Starting feature extraction (train)...', flush=True)\n",
        "fe_tr = process_df(train[['id','file_name']].copy(), 'train_images')\n",
        "fe_tr.to_csv('img_feats_train.csv', index=False)\n",
        "print('[IMG-FE] Saved img_feats_train.csv', fe_tr.shape, flush=True)\n",
        "\n",
        "print('[IMG-FE] Starting feature extraction (test)...', flush=True)\n",
        "fe_te = process_df(test[['id','file_name']].copy(), 'test_images')\n",
        "fe_te.to_csv('img_feats_test.csv', index=False)\n",
        "print('[IMG-FE] Saved img_feats_test.csv', fe_te.shape, flush=True)\n",
        "\n",
        "print('[IMG-FE] Done.')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMG-FE] Starting feature extraction (train)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMG-FE] processed 179422 images from train_images in 1.79 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMG-FE] Saved img_feats_train.csv (179422, 7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMG-FE] Starting feature extraction (test)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMG-FE] processed 16877 images from test_images in 0.15 min\n[IMG-FE] Saved img_feats_test.csv (16877, 7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMG-FE] Done.\n"
          ]
        }
      ]
    },
    {
      "id": "403e9f5c-b478-4e10-b153-664f23d03e41",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reuse model_full and b_opt: alternative seq pooling (mean vs top-2 mean vs conf-weighted) and build new submission file\n",
        "import numpy as np, pandas as pd, os\n",
        "from catboost import Pool\n",
        "from collections import defaultdict\n",
        "\n",
        "# Rebuild features exactly as in Cell 17 to get test Pool (with image feats + priors mapping)\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "\n",
        "def fe_base(df):\n",
        "    df = df.copy()\n",
        "    dt = pd.to_datetime(df['date_captured'], errors='coerce')\n",
        "    df['year'] = dt.dt.year.fillna(-1).astype(int)\n",
        "    df['month'] = dt.dt.month.fillna(-1).astype(int)\n",
        "    df['day'] = dt.dt.day.fillna(-1).astype(int)\n",
        "    df['hour'] = dt.dt.hour.fillna(-1).astype(int)\n",
        "    df['doy']  = dt.dt.dayofyear.fillna(1).astype(int)\n",
        "    df['is_night'] = ((df['hour'] < 6) | (df['hour'] > 19)).astype(int)\n",
        "    df['frame_num'] = df['frame_num'].fillna(-1).astype(int)\n",
        "    df['seq_num_frames'] = df['seq_num_frames'].fillna(1).astype(int)\n",
        "    df['frame_ratio'] = (df['frame_num'] / df['seq_num_frames']).clip(0,1)\n",
        "    df['is_first'] = (df['frame_num'] <= 1).astype(int)\n",
        "    df['is_last']  = (df['frame_num'] >= df['seq_num_frames']).astype(int)\n",
        "    df['hour_sin'] = np.sin(2*np.pi*df['hour']/24.0)\n",
        "    df['hour_cos'] = np.cos(2*np.pi*df['hour']/24.0)\n",
        "    df['doy_sin']  = np.sin(2*np.pi*df['doy']/366.0)\n",
        "    df['doy_cos']  = np.cos(2*np.pi*df['doy']/366.0)\n",
        "    bins = [-1,3,7,11,15,19,23]\n",
        "    labels = [0,1,2,3,4,5]\n",
        "    df['hour_bin'] = pd.cut(df['hour'], bins=bins, labels=labels, include_lowest=True).astype(int)\n",
        "    df['aspect'] = (df['width'] / df['height']).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    return df\n",
        "\n",
        "train_fe = fe_base(train)\n",
        "test_fe  = fe_base(test)\n",
        "train_fe['id'] = train['id'].astype(str).values\n",
        "test_fe['id']  = test['id'].astype(str).values\n",
        "\n",
        "# Merge cheap image feats if available\n",
        "if os.path.exists('img_feats_train.csv') and os.path.exists('img_feats_test.csv'):\n",
        "    fe_tr = pd.read_csv('img_feats_train.csv'); fe_te = pd.read_csv('img_feats_test.csv')\n",
        "    train_fe = train_fe.merge(fe_tr, on='id', how='left')\n",
        "    test_fe  = test_fe.merge(fe_te, on='id', how='left')\n",
        "\n",
        "classes_all = sorted(train['category_id'].unique().tolist())\n",
        "img_num = [c for c in ['laplacian_var','hsv_s_mean','gray_mean','gray_std','hsv_v_mean','file_size_kb'] if c in train_fe.columns]\n",
        "base_num = ['width','height','aspect','year','month','day','hour','doy','is_night','frame_num','seq_num_frames','frame_ratio','is_first','is_last','hour_sin','hour_cos','doy_sin','doy_cos'] + img_num\n",
        "cat_cols = ['rights_holder']\n",
        "\n",
        "X_full = train_fe[['location','rights_holder','hour_bin','id'] + base_num].copy()\n",
        "X_test = test_fe[['location','rights_holder','hour_bin','id'] + base_num].copy()\n",
        "\n",
        "# Median-impute image feats\n",
        "for c in img_num:\n",
        "    med = X_full[c].median() if np.isfinite(X_full[c]).any() else 0.0\n",
        "    X_full[c] = X_full[c].fillna(med)\n",
        "    X_test[c] = X_test[c].fillna(med)\n",
        "\n",
        "# Build priors using full train to map to test\n",
        "def entropy_from_probs(p):\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    return float(-(p * np.log(p)).sum())\n",
        "\n",
        "def m_estimate_prior(counts, total, pg, m):\n",
        "    return (counts + m * pg) / (total + m)\n",
        "\n",
        "def build_group_priors(train_idx_df, key_col, classes, m):\n",
        "    g = {}; grp = train_idx_df.groupby([key_col, 'category_id']).size().unstack(fill_value=0)\n",
        "    for c in classes:\n",
        "        if c not in grp.columns: grp[c] = 0\n",
        "    grp = grp[classes]\n",
        "    total_counts = grp.sum(axis=1).astype(int)\n",
        "    pg = train_idx_df['category_id'].value_counts(normalize=True).reindex(classes).fillna(0).values\n",
        "    for key, row in grp.iterrows():\n",
        "        cnts = row.values.astype(float); n = int(total_counts.loc[key])\n",
        "        p = m_estimate_prior(cnts, n, pg, m); ent = entropy_from_probs(p)\n",
        "        g[key] = (p, n, ent)\n",
        "    p_global = pg.copy(); ent_global = entropy_from_probs(p_global)\n",
        "    return g, p_global, ent_global\n",
        "\n",
        "def map_group_priors(df_in, key_col, prior_map, p_global, ent_global, prefix, classes):\n",
        "    df = df_in.copy()\n",
        "    probs_mat = np.zeros((len(df), len(classes)), dtype=np.float32)\n",
        "    counts = np.zeros(len(df), dtype=np.int32)\n",
        "    ents = np.zeros(len(df), dtype=np.float32)\n",
        "    for i, k in enumerate(df[key_col].values):\n",
        "        tpl = prior_map.get(k)\n",
        "        if tpl is None:\n",
        "            probs_mat[i] = p_global; counts[i] = 0; ents[i] = ent_global\n",
        "        else:\n",
        "            p, n, e = tpl; probs_mat[i] = p; counts[i] = n; ents[i] = e\n",
        "    for j, c in enumerate(classes_all):\n",
        "        df[f'{prefix}_p_{c}'] = probs_mat[:, j]\n",
        "    df[f'{prefix}_count'] = np.log1p(counts)\n",
        "    df[f'{prefix}_entropy'] = ents\n",
        "    return df\n",
        "\n",
        "def build_loc_hour_entropy(train_idx_df, m=300):\n",
        "    key = train_idx_df['location'].astype(str) + '|' + train_idx_df['hour_bin'].astype(str)\n",
        "    grp = train_idx_df.assign(k=key).groupby(['k','category_id']).size().unstack(fill_value=0)\n",
        "    classes = sorted(train_idx_df['category_id'].unique().tolist())\n",
        "    for c in classes:\n",
        "        if c not in grp.columns: grp[c] = 0\n",
        "    grp = grp[classes]\n",
        "    total_counts = grp.sum(axis=1).astype(int)\n",
        "    pg = train_idx_df['category_id'].value_counts(normalize=True).reindex(classes).fillna(0).values\n",
        "    ent_map = {}\n",
        "    for k, row in grp.iterrows():\n",
        "        cnts = row.values.astype(float); n = int(total_counts.loc[k])\n",
        "        p = m_estimate_prior(cnts, n, pg, m)\n",
        "        ent_map[k] = (entropy_from_probs(p), n)\n",
        "    ent_global = entropy_from_probs(pg)\n",
        "    return ent_map, ent_global\n",
        "\n",
        "loc_map_full, loc_pg_full, loc_entg_full = build_group_priors(train_fe[['location','category_id']], 'location', classes_all, m=100)\n",
        "rh_map_full,  rh_pg_full,  rh_entg_full  = build_group_priors(train_fe[['rights_holder','category_id']], 'rights_holder', classes_all, m=50)\n",
        "lxh_map_full, lxh_entg_full = build_loc_hour_entropy(train_fe[['location','hour_bin','category_id']].copy(), m=300)\n",
        "X_full = map_group_priors(X_full, 'location', loc_map_full, loc_pg_full, loc_entg_full, 'loc', classes_all)\n",
        "X_full = map_group_priors(X_full, 'rights_holder', rh_map_full,  rh_pg_full,  rh_entg_full,  'rh', classes_all)\n",
        "X_full = map_loc_hour_entropy(X_full, lxh_map_full, lxh_entg_full)\n",
        "X_test = map_group_priors(X_test, 'location', loc_map_full, loc_pg_full, loc_entg_full, 'loc', classes_all)\n",
        "X_test = map_group_priors(X_test, 'rights_holder', rh_map_full,  rh_pg_full,  rh_entg_full,  'rh', classes_all)\n",
        "X_test = map_loc_hour_entropy(X_test, lxh_map_full, lxh_entg_full)\n",
        "\n",
        "use_cols = base_num + ['loc_count','loc_entropy','rh_count','rh_entropy','loc_hour_entropy','loc_hour_count'] + [f'loc_p_{c}' for c in classes_all] + [f'rh_p_{c}' for c in classes_all] + cat_cols\n",
        "cat_idx_full = [use_cols.index(c) for c in cat_cols]\n",
        "pool_test = Pool(X_test[use_cols], cat_features=cat_idx_full)\n",
        "\n",
        "# Predict logits via existing model_full in memory\n",
        "proba_test = model_full.predict_proba(pool_test)\n",
        "logits_test = np.log(np.clip(np.asarray(proba_test), 1e-8, 1.0))\n",
        "\n",
        "test_df = test_fe[['id','seq_id']].copy()\n",
        "seq_map = defaultdict(list)\n",
        "for i, sid in enumerate(test_df['seq_id'].values):\n",
        "    seq_map[sid].append(i)\n",
        "\n",
        "def pool_mean(idxs):\n",
        "    return logits_test[idxs].mean(axis=0)\n",
        "\n",
        "def pool_top2(idxs):\n",
        "    # mean of top-2 frames by max logit\n",
        "    if len(idxs) <= 2:\n",
        "        return logits_test[idxs].mean(axis=0)\n",
        "    scores = logits_test[idxs].max(axis=1)\n",
        "    top2 = np.argsort(-scores)[:2]\n",
        "    sel = [idxs[i] for i in top2]\n",
        "    return logits_test[sel].mean(axis=0)\n",
        "\n",
        "def pool_conf_weight(idxs):\n",
        "    # confidence-weighted mean using softmax(max-logit) as weights\n",
        "    arr = logits_test[idxs]\n",
        "    conf = arr.max(axis=1)\n",
        "    w = np.exp(conf - conf.max())\n",
        "    w = w / (w.sum() + 1e-8)\n",
        "    return (arr * w[:, None]).sum(axis=0)\n",
        "\n",
        "poolers = {'mean': pool_mean, 'top2': pool_top2, 'confw': pool_conf_weight}\n",
        "subs = {}\n",
        "for name, fn in poolers.items():\n",
        "    pred_seq = {}\n",
        "    for sid, idxs in seq_map.items():\n",
        "        m = fn(idxs) + b_opt  # reuse tuned biases\n",
        "        pred_seq[sid] = int(np.argmax(m))\n",
        "    test_pred = test_df['seq_id'].map(pred_seq).astype(int).values\n",
        "    df_out = pd.DataFrame({'id': test_df['id'].astype(str).values, 'category_id': test_pred})\n",
        "    out_path = f'submission_cb_priors_seq_bias_{name}.csv'\n",
        "    df_out.to_csv(out_path, index=False)\n",
        "    subs[name] = out_path\n",
        "    print(f'[ALT-POOL] Saved {out_path} shape', df_out.shape, 'unique classes', df_out['category_id'].nunique())\n",
        "\n",
        "print('[ALT-POOL] Done. Next: run mapping cell to build final submission.csv from the best variant (start with top2).')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALT-POOL] Saved submission_cb_priors_seq_bias_mean.csv shape (16937, 2) unique classes 5\n[ALT-POOL] Saved submission_cb_priors_seq_bias_top2.csv shape (16937, 2) unique classes 5\n[ALT-POOL] Saved submission_cb_priors_seq_bias_confw.csv shape (16937, 2) unique classes 5\n[ALT-POOL] Done. Next: run mapping cell to build final submission.csv from the best variant (start with top2).\n"
          ]
        }
      ]
    },
    {
      "id": "4abef1d7-ba9e-4450-9d69-38fe91b7377c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Map top-2 pooled CB priors submission to original labels and build final submission.csv\n",
        "import pandas as pd, numpy as np\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "pred_raw = pd.read_csv('submission_cb_priors_seq_bias_top2.csv')  # columns: id, category_id (indices 0..13)\n",
        "\n",
        "# Map class indices -> original category_id labels\n",
        "classes_all = sorted(train['category_id'].unique().tolist())\n",
        "idx2label = {i: lab for i, lab in enumerate(classes_all)}\n",
        "\n",
        "# Ensure one row per test id\n",
        "pred_raw = pred_raw.drop_duplicates(subset=['id'], keep='first')\n",
        "pred_raw['mapped'] = pred_raw['category_id'].map(idx2label).astype(int)\n",
        "\n",
        "# Align strictly to test ids and order\n",
        "sub = pd.DataFrame({'Id': test['id'].astype(str)})\n",
        "m = test[['id']].merge(pred_raw[['id','mapped']], on='id', how='left')\n",
        "if m['mapped'].isna().any():\n",
        "    m['mapped'] = m['mapped'].fillna(0).astype(int)\n",
        "sub['Category'] = m['mapped'].astype(int).values\n",
        "\n",
        "assert len(sub) == len(test), f'Row count mismatch: {len(sub)} vs {len(test)}'\n",
        "assert set(sub.columns) == {'Id','Category'}\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[FINAL-SUB-TOP2] submission.csv shape', sub.shape, 'columns', list(sub.columns), 'nunique Category', sub['Category'].nunique())\n",
        "print(sub.head())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FINAL-SUB-TOP2] submission.csv shape (16877, 2) columns ['Id', 'Category'] nunique Category 5\n                                     Id  Category\n0  5998cfa4-23d2-11e8-a6a3-ec086b02610b         0\n1  599fbd89-23d2-11e8-a6a3-ec086b02610b         0\n2  59fae563-23d2-11e8-a6a3-ec086b02610b         0\n3  5a24a741-23d2-11e8-a6a3-ec086b02610b         0\n4  59eab924-23d2-11e8-a6a3-ec086b02610b         0\n"
          ]
        }
      ]
    },
    {
      "id": "ff6d0d8b-d157-491f-81e1-f42363b0942b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Two-stage empty-vs-nonempty gate on seq-averaged logits + biases; build final submission (aligned to test.csv)\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assumes variables from Cell 17/30/31 are in memory: oof_logits, y_all (indices or labels), train_fe, classes_all, b_opt, logits_test, test_fe\n",
        "assert 'oof_logits' in globals() and 'y_all' in globals() and 'train_fe' in globals(), 'Missing OOF artifacts; run model cell first.'\n",
        "assert 'classes_all' in globals() and 'b_opt' in globals() and 'logits_test' in globals() and 'test_fe' in globals(), 'Missing test artifacts; run model cell first.'\n",
        "\n",
        "# Identify index corresponding to original label 0 (empty)\n",
        "idx_empty = classes_all.index(0)\n",
        "\n",
        "def softmax_rows(x):\n",
        "    x = x - x.max(axis=1, keepdims=True)\n",
        "    ex = np.exp(x)\n",
        "    return ex / (ex.sum(axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "# Map y_all to ORIGINAL labels if it's currently indices 0..K-1\n",
        "y_all_arr = np.asarray(y_all)\n",
        "if np.issubdtype(y_all_arr.dtype, np.integer) and y_all_arr.max() < len(classes_all) and set(np.unique(y_all_arr)) == set(range(len(classes_all))):\n",
        "    y_all_labels = np.array([classes_all[i] for i in y_all_arr], dtype=int)\n",
        "else:\n",
        "    y_all_labels = y_all_arr.astype(int)\n",
        "\n",
        "# OOF seq-averaged logits already in oof_logits; apply biases\n",
        "oof_adj = oof_logits + b_opt[None, :]\n",
        "oof_prob = softmax_rows(oof_adj)\n",
        "p0 = oof_prob[:, idx_empty]\n",
        "\n",
        "# Collapse to one row per sequence for gate tuning; use mode target per seq (in ORIGINAL label space)\n",
        "tr_seq = train_fe['seq_id'].values\n",
        "df_tmp = pd.DataFrame({'seq_id': tr_seq, 'y': y_all_labels, 'p0': p0})\n",
        "y_mode = df_tmp.groupby('seq_id')['y'].agg(lambda s: s.mode().iloc[0]).reset_index(name='y')\n",
        "p0_first = df_tmp.groupby('seq_id')['p0'].first().reset_index(name='p0')\n",
        "df_oof = y_mode.merge(p0_first, on='seq_id', how='left')\n",
        "\n",
        "# For non-empty branch, suppress empty logit and optionally apply temperature to spread predictions\n",
        "T_nonempty = 0.7  # temperature <1.0 to increase diversity\n",
        "\n",
        "# Build map seq->first row index for logits lookup (seq-avg oof_adj values are identical per seq anyway)\n",
        "seq_first_idx = {}\n",
        "for i, sid in enumerate(tr_seq):\n",
        "    if sid not in seq_first_idx:\n",
        "        seq_first_idx[sid] = i\n",
        "idxs = np.array([seq_first_idx[sid] for sid in df_oof['seq_id'].values], dtype=int)\n",
        "logits_seq = oof_adj[idxs].copy()\n",
        "\n",
        "# Candidate thresholds for empty gate\n",
        "cands = np.linspace(0.3, 0.95, 66)\n",
        "best_t, best_f1 = 0.5, -1.0\n",
        "for t in cands:\n",
        "    # Decide empty vs non-empty by p0\n",
        "    is_empty = df_oof['p0'].values >= t\n",
        "    # Non-empty branch: suppress empty class and apply temperature scaling\n",
        "    logits_ne = logits_seq.copy()\n",
        "    logits_ne[:, idx_empty] = -1e9\n",
        "    logits_ne = logits_ne / max(T_nonempty, 1e-6)\n",
        "    pred_idx = logits_ne.argmax(axis=1)\n",
        "    pred_labels = np.where(is_empty, 0, np.array([classes_all[j] for j in pred_idx]))\n",
        "    f1 = f1_score(df_oof['y'].values, pred_labels, average='macro')\n",
        "    if f1 > best_f1:\n",
        "        best_f1, best_t = f1, t\n",
        "\n",
        "print(f'[GATE] Best empty gate threshold t={best_t:.3f} (T_nonempty={T_nonempty}) yields OOF seq-avg macro-F1={best_f1:.5f}')\n",
        "\n",
        "# Apply gate to test aligned strictly to test.csv to avoid length mismatches\n",
        "test_true = pd.read_csv('test.csv')\n",
        "# Build mapping from test_fe rows (used to compute logits_test) to their indices\n",
        "df_logits = pd.DataFrame({\n",
        "    'id': test_fe['id'].astype(str).values,\n",
        "    'seq_id': test_fe['seq_id'].values,\n",
        "    'row_idx': np.arange(len(test_fe), dtype=int)\n",
        "})\n",
        "# Map each test.csv id to the corresponding logits_test row index (first occurrence if duplicates)\n",
        "id2idx = {}\n",
        "for rid, rix in zip(df_logits['id'].values, df_logits['row_idx'].values):\n",
        "    if rid not in id2idx:\n",
        "        id2idx[rid] = int(rix)\n",
        "mapped_idx = test_true['id'].astype(str).map(id2idx).values\n",
        "if np.any(pd.isna(mapped_idx)):\n",
        "    mapped_idx = np.where(pd.isna(mapped_idx), 0, mapped_idx).astype(int)\n",
        "else:\n",
        "    mapped_idx = mapped_idx.astype(int)\n",
        "\n",
        "# Group indices by seq_id from test.csv and compute seq-mean logits on those indices\n",
        "from collections import defaultdict\n",
        "seq_to_indices = defaultdict(list)\n",
        "for i, sid in enumerate(test_true['seq_id'].values):\n",
        "    seq_to_indices[sid].append(mapped_idx[i])\n",
        "\n",
        "pred_seq = {}\n",
        "for sid, idxs in seq_to_indices.items():\n",
        "    m = logits_test[idxs].mean(axis=0) + b_opt  # seq-avg + biases\n",
        "    prob = softmax_rows(m[None, :])[0]\n",
        "    if prob[idx_empty] >= best_t:\n",
        "        pred_seq[sid] = 0  # empty label\n",
        "    else:\n",
        "        m_ne = m.copy()\n",
        "        m_ne[idx_empty] = -1e9\n",
        "        m_ne = m_ne / max(T_nonempty, 1e-6)\n",
        "        pred_seq[sid] = classes_all[int(np.argmax(m_ne))]\n",
        "\n",
        "test_pred_labels = pd.Series(test_true['seq_id'].values).map(pred_seq).astype(int).values\n",
        "\n",
        "# Build final submission with correct headers and order\n",
        "sub = pd.DataFrame({'Id': test_true['id'].astype(str).values, 'Category': test_pred_labels})\n",
        "assert len(sub) == len(test_true), f'Length mismatch: {len(sub)} vs {len(test_true)}'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[GATE] Wrote submission.csv with gate. shape', sub.shape, 'nunique Category', sub['Category'].nunique())\n",
        "print(sub.head())"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GATE] Best empty gate threshold t=0.340 (T_nonempty=0.7) yields OOF seq-avg macro-F1=0.68623\n[GATE] Wrote submission.csv with gate. shape (16877, 2) nunique Category 14\n                                     Id  Category\n0  5998cfa4-23d2-11e8-a6a3-ec086b02610b        19\n1  599fbd89-23d2-11e8-a6a3-ec086b02610b         0\n2  59fae563-23d2-11e8-a6a3-ec086b02610b        16\n3  5a24a741-23d2-11e8-a6a3-ec086b02610b        18\n4  59eab924-23d2-11e8-a6a3-ec086b02610b        16\n"
          ]
        }
      ]
    },
    {
      "id": "ded18ac3-7fbb-4406-bc60-1c7bc99b1f65",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CPU embedding extraction: timm resnet18 (160px, GAP) -> 512-D fp16; saves train/test .npy and id/order CSVs\n",
        "import os, time, math, gc, cv2, numpy as np, pandas as pd, torch, timm\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "cv2.setNumThreads(0)\n",
        "torch.set_num_threads(8)\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "IMG_SIZE = 160  # faster than 224; upgrade to 224 or switch to r50 if time allows\n",
        "BATCH_SIZE = 256\n",
        "NUM_WORKERS = 8\n",
        "MODEL_NAME = 'resnet18'\n",
        "OUT_TRAIN = f'emb_train_{MODEL_NAME}_{IMG_SIZE}.npy'\n",
        "OUT_TEST  = f'emb_test_{MODEL_NAME}_{IMG_SIZE}.npy'\n",
        "OUT_TRAIN_IDS = f'emb_train_{MODEL_NAME}_{IMG_SIZE}_ids.csv'\n",
        "OUT_TEST_IDS  = f'emb_test_{MODEL_NAME}_{IMG_SIZE}_ids.csv'\n",
        "\n",
        "mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "\n",
        "def preprocess_pad(img, size=IMG_SIZE):\n",
        "    if img is None:\n",
        "        img = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "    else:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    h, w = img.shape[:2]\n",
        "    scale = size / max(h, w) if max(h, w) > 0 else 1.0\n",
        "    nh, nw = max(1, int(h*scale)), max(1, int(w*scale))\n",
        "    img_rs = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_AREA)\n",
        "    pad = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "    y0 = (size - nh) // 2; x0 = (size - nw) // 2\n",
        "    pad[y0:y0+nh, x0:x0+nw] = img_rs\n",
        "    x = pad.astype(np.float32) / 255.0\n",
        "    x = (x - mean) / std\n",
        "    x = np.transpose(x, (2, 0, 1))\n",
        "    return x\n",
        "\n",
        "class ImgDs(Dataset):\n",
        "    def __init__(self, df, img_dir):\n",
        "        self.ids = df['id'].astype(str).values\n",
        "        self.fns = df['file_name'].values\n",
        "        self.img_dir = img_dir\n",
        "    def __len__(self): return len(self.fns)\n",
        "    def __getitem__(self, i):\n",
        "        fp = os.path.join(self.img_dir, self.fns[i])\n",
        "        img = cv2.imread(fp, cv2.IMREAD_COLOR)\n",
        "        x = preprocess_pad(img)\n",
        "        return torch.from_numpy(x), self.ids[i]\n",
        "\n",
        "def build_loader(df, img_dir):\n",
        "    ds = ImgDs(df, img_dir)\n",
        "    return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=False)\n",
        "\n",
        "def extract_embeddings(df, img_dir, out_npy, out_ids):\n",
        "    n = len(df)\n",
        "    loader = build_loader(df, img_dir)\n",
        "    print(f'[EMB] Loading {MODEL_NAME} pretrained backbone...', flush=True)\n",
        "    model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=0)  # pooled features\n",
        "    model.eval(); model.to(DEVICE); model.to(memory_format=torch.channels_last)\n",
        "    feats = None; ids_all = []\n",
        "    t0 = time.time(); seen = 0\n",
        "    with torch.inference_mode():\n",
        "        for it, (xb, ids) in enumerate(loader, 1):\n",
        "            xb = xb.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            f = model(xb).float().cpu().numpy()  # (B, D)\n",
        "            if feats is None:\n",
        "                D = f.shape[1]\n",
        "                feats = np.memmap(out_npy + '.mmap', mode='w+', dtype=np.float16, shape=(n, D))\n",
        "            bsz = f.shape[0]\n",
        "            feats[seen:seen+bsz] = f.astype(np.float16)\n",
        "            ids_all.extend(list(ids))\n",
        "            seen += bsz\n",
        "            if it % 50 == 0 or seen == n:\n",
        "                dt = time.time()-t0\n",
        "                print(f'  [EMB] it {it} seen {seen}/{n} ({seen/n*100:.1f}%) elapsed {dt/60:.2f}m', flush=True)\n",
        "    # Flush memmap to .npy\n",
        "    arr = np.array(feats, copy=True)  # load to RAM\n",
        "    del feats; gc.collect()\n",
        "    np.save(out_npy, arr)\n",
        "    try:\n",
        "        os.remove(out_npy + '.mmap')\n",
        "    except Exception:\n",
        "        pass\n",
        "    pd.DataFrame({'id': ids_all}).to_csv(out_ids, index=False)\n",
        "    print(f'[EMB] Saved {out_npy} shape {arr.shape} and {out_ids}', flush=True)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "print('[EMB] Starting extraction: train n=', len(train), ' test n=', len(test), ' img_size=', IMG_SIZE, ' bs=', BATCH_SIZE, flush=True)\n",
        "extract_embeddings(train[['id','file_name']], 'train_images', OUT_TRAIN, OUT_TRAIN_IDS)\n",
        "extract_embeddings(test[['id','file_name']],  'test_images',  OUT_TEST,  OUT_TEST_IDS)\n",
        "print('[EMB] Done.')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EMB] Starting extraction: train n= 179422  test n= 16877  img_size= 160  bs= 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EMB] Loading resnet18 pretrained backbone...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 50 seen 12800/179422 (7.1%) elapsed 0.59m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 100 seen 25600/179422 (14.3%) elapsed 1.12m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 150 seen 38400/179422 (21.4%) elapsed 1.65m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 200 seen 51200/179422 (28.5%) elapsed 2.18m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 250 seen 64000/179422 (35.7%) elapsed 2.71m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 300 seen 76800/179422 (42.8%) elapsed 3.24m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 350 seen 89600/179422 (49.9%) elapsed 3.77m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 400 seen 102400/179422 (57.1%) elapsed 4.29m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 450 seen 115200/179422 (64.2%) elapsed 4.83m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 500 seen 128000/179422 (71.3%) elapsed 5.36m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 550 seen 140800/179422 (78.5%) elapsed 5.88m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 600 seen 153600/179422 (85.6%) elapsed 6.41m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 650 seen 166400/179422 (92.7%) elapsed 6.94m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 700 seen 179200/179422 (99.9%) elapsed 7.47m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 701 seen 179422/179422 (100.0%) elapsed 7.47m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EMB] Saved emb_train_resnet18_160.npy shape (179422, 512) and emb_train_resnet18_160_ids.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EMB] Loading resnet18 pretrained backbone...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 50 seen 12800/16877 (75.8%) elapsed 0.56m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [EMB] it 66 seen 16877/16877 (100.0%) elapsed 0.73m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EMB] Saved emb_test_resnet18_160.npy shape (16877, 512) and emb_test_resnet18_160_ids.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EMB] Done.\n"
          ]
        }
      ]
    },
    {
      "id": "66125c6f-b0ef-434f-aaab-258150e70d79",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Logistic Regression on ResNet18 embeddings (+numeric meta), SGKF OOF -> bias tune -> test preds\n",
        "import numpy as np, pandas as pd, time, os\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "classes_all = sorted(train['category_id'].unique().tolist())\n",
        "n_classes = len(classes_all)\n",
        "\n",
        "# Load embeddings and ensure alignment to train/test id order\n",
        "emb_tr = np.load('emb_train_resnet18_160.npy')  # (N_tr, 512)\n",
        "emb_te = np.load('emb_test_resnet18_160.npy')   # (N_te, 512)\n",
        "ids_tr = pd.read_csv('emb_train_resnet18_160_ids.csv')['id'].astype(str).values\n",
        "ids_te = pd.read_csv('emb_test_resnet18_160_ids.csv')['id'].astype(str).values\n",
        "assert emb_tr.shape[0] == len(train) and emb_te.shape[0] == len(test), f'Emb shape mismatch: {emb_tr.shape}, {emb_te.shape}'\n",
        "assert np.all(ids_tr == train['id'].astype(str).values), 'Train embedding id order mismatch'\n",
        "assert np.all(ids_te == test['id'].astype(str).values), 'Test embedding id order mismatch'\n",
        "\n",
        "def fe_base(df):\n",
        "    df = df.copy()\n",
        "    dt = pd.to_datetime(df['date_captured'], errors='coerce')\n",
        "    df['year'] = dt.dt.year.fillna(-1).astype(int)\n",
        "    df['month'] = dt.dt.month.fillna(-1).astype(int)\n",
        "    df['day'] = dt.dt.day.fillna(-1).astype(int)\n",
        "    df['hour'] = dt.dt.hour.fillna(-1).astype(int)\n",
        "    df['is_night'] = ((df['hour'] < 6) | (df['hour'] > 19)).astype(int)\n",
        "    df['frame_num'] = df['frame_num'].fillna(-1).astype(int)\n",
        "    df['seq_num_frames'] = df['seq_num_frames'].fillna(1).astype(int)\n",
        "    df['frame_ratio'] = (df['frame_num'] / df['seq_num_frames']).clip(0,1)\n",
        "    df['is_first'] = (df['frame_num'] <= 1).astype(int)\n",
        "    df['is_last']  = (df['frame_num'] >= df['seq_num_frames']).astype(int)\n",
        "    df['aspect'] = (df['width'] / df['height']).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    return df\n",
        "\n",
        "train_fe = fe_base(train)\n",
        "test_fe  = fe_base(test)\n",
        "\n",
        "# Numeric meta features (keep small to avoid overfitting, categorical omitted for speed)\n",
        "num_cols = ['width','height','aspect','year','month','day','hour','is_night','frame_num','seq_num_frames','frame_ratio','is_first','is_last']\n",
        "X_num_tr = train_fe[num_cols].astype(np.float32).values\n",
        "X_num_te = test_fe[num_cols].astype(np.float32).values\n",
        "\n",
        "# Concatenate embeddings + numeric\n",
        "X_all = np.concatenate([emb_tr.astype(np.float32), X_num_tr], axis=1)\n",
        "X_test = np.concatenate([emb_te.astype(np.float32), X_num_te], axis=1)\n",
        "y_all = train['category_id'].values\n",
        "groups = train['seq_id'].astype(str).values\n",
        "\n",
        "print('[LR-EMB] Shapes: X_all', X_all.shape, 'X_test', X_test.shape, 'classes', n_classes, flush=True)\n",
        "\n",
        "# Build scaler+LR pipeline\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
        "    ('clf', LogisticRegression(\n",
        "        multi_class='multinomial', solver='saga', max_iter=300, C=0.5,\n",
        "        class_weight='balanced', n_jobs=8, verbose=0))\n",
        "])\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_logits = np.full((len(train), n_classes), np.nan, dtype=np.float32)\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(sgkf.split(X_all, y=y_all, groups=groups)):\n",
        "    t_fold = time.time()\n",
        "    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n",
        "    y_tr, y_va = y_all[tr_idx], y_all[va_idx]\n",
        "    pipe.fit(X_tr, y_tr)\n",
        "    proba = pipe.predict_proba(X_va)\n",
        "    logits = np.log(np.clip(proba, 1e-9, 1.0))\n",
        "    # Sequence-average within fold\n",
        "    va_seq = train.loc[va_idx, 'seq_id'].values\n",
        "    logits_seq = logits.copy()\n",
        "    from collections import defaultdict\n",
        "    g = defaultdict(list)\n",
        "    for i, sid in enumerate(va_seq): g[sid].append(i)\n",
        "    for idxs in g.values():\n",
        "        m = logits[idxs].mean(axis=0, keepdims=True)\n",
        "        logits_seq[idxs] = m\n",
        "    oof_logits[va_idx] = logits_seq\n",
        "    f1 = f1_score(y_va, logits_seq.argmax(1), average='macro')\n",
        "    print(f'[LR-EMB] Fold {fold} seq-avg macro-F1={f1:.5f} elapsed {time.time()-t_fold:.1f}s', flush=True)\n",
        "\n",
        "assert not np.isnan(oof_logits).any(), 'NaNs in OOF logits'\n",
        "\n",
        "def optimize_biases(y_true, logits, n_iters=2, grid=np.linspace(-1.5, 1.5, 19)):\n",
        "    b = np.zeros(logits.shape[1], dtype=np.float32)\n",
        "    best = f1_score(y_true, (logits + b).argmax(1), average='macro')\n",
        "    for _ in range(n_iters):\n",
        "        improved = False\n",
        "        for c in range(logits.shape[1]):\n",
        "            bc = b[c]; best_c = bc; best_sc = best\n",
        "            for d in grid:\n",
        "                b[c] = d\n",
        "                sc = f1_score(y_true, (logits + b).argmax(1), average='macro')\n",
        "                if sc > best_sc: best_sc, best_c = sc, d\n",
        "            b[c] = best_c\n",
        "            if best_c != bc: best = best_sc; improved = True\n",
        "        if not improved: break\n",
        "    return b, best\n",
        "\n",
        "b_opt, f1_oof = optimize_biases(y_all, oof_logits)\n",
        "print(f'[LR-EMB] OOF seq-avg macro-F1 (bias-tuned)={f1_oof:.5f}', flush=True)\n",
        "print('[LR-EMB] Biases:', np.round(b_opt, 3))\n",
        "\n",
        "# Fit on full data and predict test\n",
        "t_fit = time.time()\n",
        "pipe.fit(X_all, y_all)\n",
        "print(f'[LR-EMB] Full fit done in {time.time()-t_fit:.1f}s', flush=True)\n",
        "proba_test = pipe.predict_proba(X_test)\n",
        "logits_test = np.log(np.clip(proba_test, 1e-9, 1.0))\n",
        "\n",
        "# Sequence-average test logits and apply biases; predict per seq then broadcast\n",
        "test_df = test[['id','seq_id']].copy()\n",
        "from collections import defaultdict\n",
        "seq_map = defaultdict(list)\n",
        "for i, sid in enumerate(test_df['seq_id'].values): seq_map[sid].append(i)\n",
        "pred_seq = {}\n",
        "for sid, idxs in seq_map.items():\n",
        "    m = logits_test[idxs].mean(axis=0) + b_opt\n",
        "    pred_seq[sid] = int(np.argmax(m))\n",
        "test_pred_idx = test_df['seq_id'].map(pred_seq).astype(int).values\n",
        "\n",
        "# Map class indices -> original labels\n",
        "idx2label = {i: lab for i, lab in enumerate(classes_all)}\n",
        "test_pred = np.vectorize(idx2label.get)(test_pred_idx).astype(int)\n",
        "\n",
        "sub = pd.DataFrame({'Id': test['id'].astype(str), 'Category': test_pred})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('[LR-EMB] Wrote submission.csv shape', sub.shape, 'nunique Category', sub['Category'].nunique())\n",
        "\n",
        "# Expose globals for gate cell reuse\n",
        "globals().update({'oof_logits': oof_logits, 'y_all': y_all, 'b_opt': b_opt, 'logits_test': logits_test, 'train_fe': train_fe, 'test_fe': test_fe, 'classes_all': classes_all})\n",
        "print('[LR-EMB] Exposed oof_logits/y_all/b_opt/logits_test/train_fe/test_fe/classes_all for gating.', flush=True)\n",
        "print('[LR-EMB] Done in {:.1f}s'.format(time.time()-t0), flush=True)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR-EMB] Shapes: X_all (179422, 525) X_test (16877, 525) classes 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR-EMB] Fold 0 seq-avg macro-F1=0.09338 elapsed 736.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "id": "803467da-510c-40e0-ba14-ef33170ebe6f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# XGBoost on ResNet18 embeddings (+numeric meta), 5-fold SGKF -> OOF logits -> bias tune -> test logits\n",
        "import numpy as np, pandas as pd, time, os, sys, subprocess\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Ensure xgboost is installed\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    print('[XGB] Installing xgboost...', flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'xgboost==2.1.1'], check=True)\n",
        "    import xgboost as xgb\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "classes_all = sorted(train['category_id'].unique().tolist())\n",
        "n_classes = len(classes_all)\n",
        "label2idx = {lab: i for i, lab in enumerate(classes_all)}\n",
        "\n",
        "# Load embeddings\n",
        "emb_tr = np.load('emb_train_resnet18_160.npy').astype(np.float32)\n",
        "emb_te = np.load('emb_test_resnet18_160.npy').astype(np.float32)\n",
        "ids_tr = pd.read_csv('emb_train_resnet18_160_ids.csv')['id'].astype(str).values\n",
        "ids_te = pd.read_csv('emb_test_resnet18_160_ids.csv')['id'].astype(str).values\n",
        "assert emb_tr.shape[0] == len(train) and emb_te.shape[0] == len(test)\n",
        "assert np.all(ids_tr == train['id'].astype(str).values)\n",
        "assert np.all(ids_te == test['id'].astype(str).values)\n",
        "\n",
        "def fe_base(df):\n",
        "    df = df.copy()\n",
        "    dt = pd.to_datetime(df['date_captured'], errors='coerce')\n",
        "    df['year'] = dt.dt.year.fillna(-1).astype(int)\n",
        "    df['month'] = dt.dt.month.fillna(-1).astype(int)\n",
        "    df['day'] = dt.dt.day.fillna(-1).astype(int)\n",
        "    df['hour'] = dt.dt.hour.fillna(-1).astype(int)\n",
        "    df['is_night'] = ((df['hour'] < 6) | (df['hour'] > 19)).astype(int)\n",
        "    df['frame_num'] = df['frame_num'].fillna(-1).astype(int)\n",
        "    df['seq_num_frames'] = df['seq_num_frames'].fillna(1).astype(int)\n",
        "    df['frame_ratio'] = (df['frame_num'] / df['seq_num_frames']).clip(0,1)\n",
        "    df['is_first'] = (df['frame_num'] <= 1).astype(int)\n",
        "    df['is_last']  = (df['frame_num'] >= df['seq_num_frames']).astype(int)\n",
        "    df['aspect'] = (df['width'] / df['height']).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    return df\n",
        "\n",
        "train_fe = fe_base(train); test_fe = fe_base(test)\n",
        "num_cols = ['width','height','aspect','year','month','day','hour','is_night','frame_num','seq_num_frames','frame_ratio','is_first','is_last']\n",
        "X_num_tr = train_fe[num_cols].astype(np.float32).values\n",
        "X_num_te = test_fe[num_cols].astype(np.float32).values\n",
        "X_all = np.concatenate([emb_tr, X_num_tr], axis=1)\n",
        "X_test = np.concatenate([emb_te, X_num_te], axis=1)\n",
        "y_all_labels = train['category_id'].values\n",
        "y_all = np.array([label2idx[v] for v in y_all_labels], dtype=np.int32)  # map to 0..K-1\n",
        "groups = train['seq_id'].astype(str).values\n",
        "\n",
        "print('[XGB-EMB] Shapes:', X_all.shape, X_test.shape, 'classes', n_classes, flush=True)\n",
        "\n",
        "params = {\n",
        "    'objective': 'multi:softprob',\n",
        "    'num_class': int(n_classes),\n",
        "    'tree_method': 'hist',\n",
        "    'max_depth': 7,\n",
        "    'learning_rate': 0.05,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'eval_metric': 'mlogloss',\n",
        "    'nthread': -1,\n",
        "}\n",
        "n_estimators = 2000\n",
        "early_stopping = 100\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_logits = np.full((len(train), n_classes), np.nan, dtype=np.float32)\n",
        "best_iteration_last_fold = 1000\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(sgkf.split(X_all, y=y_all, groups=groups)):\n",
        "    t_fold = time.time()\n",
        "    dtr = xgb.DMatrix(X_all[tr_idx], label=y_all[tr_idx])\n",
        "    dva = xgb.DMatrix(X_all[va_idx], label=y_all[va_idx])\n",
        "    watchlist = [(dtr, 'train'), (dva, 'valid')]\n",
        "    model = xgb.train(params, dtr, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping, verbose_eval=False)\n",
        "    best_iteration_last_fold = int(model.best_iteration + 1)\n",
        "    proba = model.predict(dva)\n",
        "    logits = np.log(np.clip(proba, 1e-9, 1.0))\n",
        "    # seq-average within val fold\n",
        "    va_seq = train.loc[va_idx, 'seq_id'].values\n",
        "    from collections import defaultdict\n",
        "    g = defaultdict(list)\n",
        "    for i, sid in enumerate(va_seq): g[sid].append(i)\n",
        "    logits_seq = logits.copy()\n",
        "    for idxs in g.values():\n",
        "        m = logits[idxs].mean(axis=0, keepdims=True)\n",
        "        logits_seq[idxs] = m\n",
        "    oof_logits[va_idx] = logits_seq\n",
        "    f1 = f1_score(y_all[va_idx], logits_seq.argmax(1), average='macro')\n",
        "    print(f'[XGB-EMB] Fold {fold} seq-avg macro-F1={f1:.5f} rounds={model.best_iteration+1} elapsed {time.time()-t_fold:.1f}s', flush=True)\n",
        "\n",
        "assert not np.isnan(oof_logits).any(), 'NaNs in OOF logits'\n",
        "\n",
        "def optimize_biases(y_true, logits, n_iters=2, grid=np.linspace(-1.5, 1.5, 19)):\n",
        "    b = np.zeros(logits.shape[1], dtype=np.float32)\n",
        "    best = f1_score(y_true, (logits + b).argmax(1), average='macro')\n",
        "    for _ in range(n_iters):\n",
        "        improved = False\n",
        "        for c in range(logits.shape[1]):\n",
        "            bc = b[c]; best_c = bc; best_sc = best\n",
        "            for d in grid:\n",
        "                b[c] = d\n",
        "                sc = f1_score(y_true, (logits + b).argmax(1), average='macro')\n",
        "                if sc > best_sc: best_sc, best_c = sc, d\n",
        "            b[c] = best_c\n",
        "            if best_c != bc: best = best_sc; improved = True\n",
        "        if not improved: break\n",
        "    return b, best\n",
        "\n",
        "b_opt, f1_oof = optimize_biases(y_all, oof_logits)\n",
        "print(f'[XGB-EMB] OOF seq-avg macro-F1 (bias-tuned)={f1_oof:.5f}', flush=True)\n",
        "print('[XGB-EMB] Biases:', np.round(b_opt, 3))\n",
        "\n",
        "# Fit full model and predict test\n",
        "dall = xgb.DMatrix(X_all, label=y_all)\n",
        "dte = xgb.DMatrix(X_test)\n",
        "num_round_full = int(max(best_iteration_last_fold, 1000) * 1.1)\n",
        "model_full = xgb.train(params, dall, num_boost_round=num_round_full)\n",
        "proba_test = model_full.predict(dte)\n",
        "logits_test = np.log(np.clip(proba_test, 1e-9, 1.0))\n",
        "\n",
        "# Expose globals for gating and blending\n",
        "globals().update({'oof_logits': oof_logits, 'y_all': y_all, 'b_opt': b_opt, 'logits_test': logits_test, 'train_fe': train_fe, 'test_fe': test_fe, 'classes_all': classes_all})\n",
        "print('[XGB-EMB] Exposed oof/logits for gate. Total time {:.1f}s'.format(time.time()-t0), flush=True)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[XGB-EMB] Shapes: (179422, 525) (16877, 525) classes 14\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     79\u001b[39m dva = xgb.DMatrix(X_all[va_idx], label=y_all[va_idx])\n\u001b[32m     80\u001b[39m watchlist = [(dtr, \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m), (dva, \u001b[33m'\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m model = \u001b[43mxgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwatchlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m best_iteration_last_fold = \u001b[38;5;28mint\u001b[39m(model.best_iteration + \u001b[32m1\u001b[39m)\n\u001b[32m     83\u001b[39m proba = model.predict(dva)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py:726\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    725\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/xgboost/training.py:182\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    180\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    181\u001b[39m     bst.update(dtrain, iteration=i, fobj=obj)\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb_container\u001b[49m\u001b[43m.\u001b[49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    183\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    185\u001b[39m bst = cb_container.after_training(bst)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:258\u001b[39m, in \u001b[36mCallbackContainer.after_iteration\u001b[39m\u001b[34m(self, model, epoch, dtrain, evals)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, name \u001b[38;5;129;01min\u001b[39;00m evals:\n\u001b[32m    257\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m name.find(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m) == -\u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDataset name should not contain `-`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m score: \u001b[38;5;28mstr\u001b[39m = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_output_margin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m metric_score = _parse_eval_str(score)\n\u001b[32m    260\u001b[39m \u001b[38;5;28mself\u001b[39m._update_history(metric_score, epoch)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2212\u001b[39m, in \u001b[36mBooster.eval_set\u001b[39m\u001b[34m(self, evals, iteration, feval, output_margin)\u001b[39m\n\u001b[32m   2209\u001b[39m evnames = c_array(ctypes.c_char_p, [c_str(d[\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m evals])\n\u001b[32m   2210\u001b[39m msg = ctypes.c_char_p()\n\u001b[32m   2211\u001b[39m _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2212\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterEvalOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2213\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdmats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mc_bst_ulong\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2220\u001b[39m )\n\u001b[32m   2221\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m msg.value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2222\u001b[39m res = msg.value.decode()  \u001b[38;5;66;03m# pylint: disable=no-member\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "f38415a4-4bb7-4775-8357-60479ad7a93d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LightGBM on r18 embeddings (+numeric meta), 5-fold SGKF -> OOF logits -> bias tune -> test logits\n",
        "import numpy as np, pandas as pd, time, os, sys, subprocess\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Ensure lightgbm is available\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception:\n",
        "    print('[LGB] Installing lightgbm...', flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'lightgbm==4.6.0'], check=True)\n",
        "    import lightgbm as lgb\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "classes_all = sorted(train['category_id'].unique().tolist())\n",
        "n_classes = len(classes_all)\n",
        "label2idx = {lab: i for i, lab in enumerate(classes_all)}\n",
        "\n",
        "# Load embeddings\n",
        "emb_tr = np.load('emb_train_resnet18_160.npy').astype(np.float32)\n",
        "emb_te = np.load('emb_test_resnet18_160.npy').astype(np.float32)\n",
        "ids_tr = pd.read_csv('emb_train_resnet18_160_ids.csv')['id'].astype(str).values\n",
        "ids_te = pd.read_csv('emb_test_resnet18_160_ids.csv')['id'].astype(str).values\n",
        "assert emb_tr.shape[0] == len(train) and emb_te.shape[0] == len(test)\n",
        "assert np.all(ids_tr == train['id'].astype(str).values)\n",
        "assert np.all(ids_te == test['id'].astype(str).values)\n",
        "\n",
        "def fe_base(df):\n",
        "    df = df.copy()\n",
        "    dt = pd.to_datetime(df['date_captured'], errors='coerce')\n",
        "    df['year'] = dt.dt.year.fillna(-1).astype(int)\n",
        "    df['month'] = dt.dt.month.fillna(-1).astype(int)\n",
        "    df['day'] = dt.dt.day.fillna(-1).astype(int)\n",
        "    df['hour'] = dt.dt.hour.fillna(-1).astype(int)\n",
        "    df['is_night'] = ((df['hour'] < 6) | (df['hour'] > 19)).astype(int)\n",
        "    df['frame_num'] = df['frame_num'].fillna(-1).astype(int)\n",
        "    df['seq_num_frames'] = df['seq_num_frames'].fillna(1).astype(int)\n",
        "    df['frame_ratio'] = (df['frame_num'] / df['seq_num_frames']).clip(0,1)\n",
        "    df['is_first'] = (df['frame_num'] <= 1).astype(int)\n",
        "    df['is_last']  = (df['frame_num'] >= df['seq_num_frames']).astype(int)\n",
        "    df['aspect'] = (df['width'] / df['height']).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    return df\n",
        "\n",
        "train_fe = fe_base(train); test_fe = fe_base(test)\n",
        "num_cols = ['width','height','aspect','year','month','day','hour','is_night','frame_num','seq_num_frames','frame_ratio','is_first','is_last']\n",
        "X_num_tr = train_fe[num_cols].astype(np.float32).values\n",
        "X_num_te = test_fe[num_cols].astype(np.float32).values\n",
        "X_all = np.concatenate([emb_tr, X_num_tr], axis=1)\n",
        "X_test = np.concatenate([emb_te, X_num_te], axis=1)\n",
        "y_all_labels = train['category_id'].values\n",
        "y_all = np.array([label2idx[v] for v in y_all_labels], dtype=np.int32)\n",
        "groups = train['seq_id'].astype(str).values\n",
        "\n",
        "print('[LGB-EMB] Shapes:', X_all.shape, X_test.shape, 'classes', n_classes, flush=True)\n",
        "\n",
        "params = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': int(n_classes),\n",
        "    'metric': 'multi_logloss',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 63,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'min_data_in_leaf': 20,\n",
        "    'verbosity': -1,\n",
        "    'force_col_wise': True\n",
        "}\n",
        "n_estimators = 800\n",
        "early_stopping = 50\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_logits = np.full((len(train), n_classes), np.nan, dtype=np.float32)\n",
        "best_iter_last = 400\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(sgkf.split(X_all, y=y_all, groups=groups)):\n",
        "    t_fold = time.time()\n",
        "    dtr = lgb.Dataset(X_all[tr_idx], label=y_all[tr_idx])\n",
        "    dva = lgb.Dataset(X_all[va_idx], label=y_all[va_idx])\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        dtr,\n",
        "        num_boost_round=n_estimators,\n",
        "        valid_sets=[dtr, dva],\n",
        "        valid_names=['train','valid'],\n",
        "        callbacks=[lgb.early_stopping(early_stopping, verbose=False)]\n",
        "    )\n",
        "    best_iter_last = int(model.best_iteration or n_estimators)\n",
        "    proba = model.predict(X_all[va_idx], num_iteration=model.best_iteration)\n",
        "    logits = np.log(np.clip(proba, 1e-9, 1.0))\n",
        "    # seq-average within val fold\n",
        "    va_seq = train.loc[va_idx, 'seq_id'].values\n",
        "    from collections import defaultdict\n",
        "    g = defaultdict(list)\n",
        "    for i, sid in enumerate(va_seq): g[sid].append(i)\n",
        "    logits_seq = logits.copy()\n",
        "    for idxs in g.values():\n",
        "        m = logits[idxs].mean(axis=0, keepdims=True)\n",
        "        logits_seq[idxs] = m\n",
        "    oof_logits[va_idx] = logits_seq\n",
        "    f1 = f1_score(y_all[va_idx], logits_seq.argmax(1), average='macro')\n",
        "    print(f'[LGB-EMB] Fold {fold} seq-avg macro-F1={f1:.5f} rounds={best_iter_last} elapsed {time.time()-t_fold:.1f}s', flush=True)\n",
        "\n",
        "assert not np.isnan(oof_logits).any(), 'NaNs in OOF logits'\n",
        "\n",
        "def optimize_biases(y_true, logits, n_iters=2, grid=np.linspace(-1.5, 1.5, 19)):\n",
        "    b = np.zeros(logits.shape[1], dtype=np.float32)\n",
        "    best = f1_score(y_true, (logits + b).argmax(1), average='macro')\n",
        "    for _ in range(n_iters):\n",
        "        improved = False\n",
        "        for c in range(logits.shape[1]):\n",
        "            bc = b[c]; best_c = bc; best_sc = best\n",
        "            for d in grid:\n",
        "                b[c] = d\n",
        "                sc = f1_score(y_true, (logits + b).argmax(1), average='macro')\n",
        "                if sc > best_sc: best_sc, best_c = sc, d\n",
        "            b[c] = best_c\n",
        "            if best_c != bc: best = best_sc; improved = True\n",
        "        if not improved: break\n",
        "    return b, best\n",
        "\n",
        "b_opt, f1_oof = optimize_biases(y_all, oof_logits)\n",
        "print(f'[LGB-EMB] OOF seq-avg macro-F1 (bias-tuned)={f1_oof:.5f}', flush=True)\n",
        "print('[LGB-EMB] Biases:', np.round(b_opt, 3))\n",
        "\n",
        "# Fit full model and predict test\n",
        "dall = lgb.Dataset(X_all, label=y_all)\n",
        "model_full = lgb.train(params, dall, num_boost_round=int(best_iter_last*1.1))\n",
        "proba_test = model_full.predict(X_test)\n",
        "logits_test = np.log(np.clip(proba_test, 1e-9, 1.0))\n",
        "\n",
        "# Expose globals\n",
        "globals().update({'oof_logits': oof_logits, 'y_all': y_all, 'b_opt': b_opt, 'logits_test': logits_test, 'train_fe': train_fe, 'test_fe': test_fe, 'classes_all': classes_all})\n",
        "print('[LGB-EMB] Exposed oof/logits. Total time {:.1f}s'.format(time.time()-t0), flush=True)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGB-EMB] Shapes: (179422, 525) (16877, 525) classes 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGB-EMB] Fold 0 seq-avg macro-F1=0.67870 rounds=84 elapsed 65.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGB-EMB] Fold 1 seq-avg macro-F1=0.69600 rounds=73 elapsed 60.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGB-EMB] Fold 2 seq-avg macro-F1=0.68628 rounds=81 elapsed 63.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGB-EMB] Fold 3 seq-avg macro-F1=0.68206 rounds=63 elapsed 55.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGB-EMB] Fold 4 seq-avg macro-F1=0.67563 rounds=58 elapsed 52.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGB-EMB] OOF seq-avg macro-F1 (bias-tuned)=0.69890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGB-EMB] Biases: [-0.667 -0.333  0.    -0.167 -0.333  0.833 -0.167 -0.333  1.333  0.\n  0.167  0.5   -0.5   -0.167]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGB-EMB] Exposed oof/logits. Total time 372.9s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}