{
  "cells": [
    {
      "id": "fd17e001-f5cd-46d1-841e-18d58fdfb6b4",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Freesound Audio Tagging 2019 \u2014 Medal-Driven Notebook (Plan v2.1: Gold-Only Strategy, Mandatory Fixes Applied)\n",
        "\n",
        "Experiment Log and Plan (v2.1 \u2014 incorporates C0 audit + mandatory revisions):\n",
        "- Objective: GOLD medal. Optimize Label-Weighted LRAP (LWLRAP). Focus on SOTA pretrained encoders, robust noisy-label handling, strong CV, TTA, and ensembling.\n",
        "- Data artifacts present: train_curated.csv, train_noisy.csv, train_curated.zip, train_noisy.zip, test.zip, sample_submission.csv.\n",
        "- Protocol: concise notebook, document attempts, backup before major changes, delete stale code, submit for audit at milestones.\n",
        "\n",
        "    Unified Gold Strategy (Single Path Only)\n",
        "1) Encoder: Fine-tune pretrained PANNs (primary: CNN14, backup: ResNet38). Use log-mel frontend matching PANNs defaults.\n",
        "2) CV: 5-fold MultilabelStratifiedKFold (seed=42), stratifying on 80-class binary matrix. Track per-fold/per-class LRAP and global LWLRAP.\n",
        "3) Noisy-label protocol: teacher-student and curriculum with confidence filtering/weighting.\n",
        "4) Inference: strong TTA (multi time-crop), fold-averaging, rank-aware ensembling across diverse models/seeds.\n",
        "\n",
        "Details\n",
        "- Label Space:\n",
        "  - Parse label column (semicolon-separated). Build consistent class list sorted to match sample_submission column order.\n",
        "  - Binarize y for CV and training. Save mapping for inference.\n",
        "\n",
        "- Audio Preprocessing (Log-mel aligned to PANNs):\n",
        "  - Sample rate: 32,000 Hz (resample all audio to 32k).\n",
        "  - Clip duration: 10.0 s target per sample.\n",
        "  - Variable length policy: during training, random time-crop to 10 s if longer; if shorter, loop-pad (tile then trim) with 0.1 probability; else zero-pad to 10 s. At inference, use multi-crop TTA (see below).\n",
        "  - STFT: n_fft=1024, hop_length=320 (10 ms hop), win_length=1024, window=hann, center=True.\n",
        "  - Mel: PRIMARY n_mels=64 (to match PANNs CNN14 pretraining); diversity models may use n_mels=128 or 256. fmin=50 Hz, fmax=16000 Hz, htk=False, norm=None.\n",
        "  - Log scale: use librosa.power_to_db on mel power; then standardize per-frequency bin with dataset mean/std (computed on curated train); clamp to [-10, 10] after standardization for stability.\n",
        "  - Channel: mono (downmix).\n",
        "\n",
        "- Model Architecture:\n",
        "  - Base: PANNs CNN14 pretrained on AudioSet (log-mel, 64 mel). Replace classifier with attention pooling head for 80 classes.\n",
        "  - Pooling: attention pooling over time (linear attention + context gating) instead of mean pooling.\n",
        "  - Head: Dropout p=0.5 before final linear; output 80 logits.\n",
        "  - Alternate diversity models: PANNs ResNet38; CNN14 with 128/256 mels; crop lengths 5 s and 12 s variants.\n",
        "\n",
        "- Losses and Label Handling:\n",
        "  - Primary loss: BCEWithLogitsLoss.\n",
        "  - Class weighting: inverse sqrt class frequency from curated train; normalize weights to mean=1.0.\n",
        "  - Label smoothing: 0.05 on positives (targets y -> y*(1-0.05) + 0.5*0.05), negatives stay at 0.\n",
        "  - Robust noise alternatives: Generalized Cross Entropy (q=0.7) or Symmetric Cross Entropy (alpha=1.0, beta=0.5) if needed in noisy stages.\n",
        "\n",
        "- Optimizer, Schedule, Checkpointing:\n",
        "  - Optimizer: AdamW (betas=(0.9, 0.999), weight_decay=1e-4).\n",
        "  - LR schedule: cosine with warmup. Base LR 2e-4 for head, 1e-4 for encoder (param groups). Warmup 1 epoch (or 1000 steps), then cosine decay.\n",
        "  - Epochs: 20 epochs curated-only warm start; 10\u201315 epochs with noisy curriculum stages (see below).\n",
        "  - Mixed precision (AMP) enabled; gradient clipping at 5.0.\n",
        "  - Batch size: as large as fits GPU (V100 16GB): target 32 for 64-mel 10 s; adjust dynamically.\n",
        "  - Early stopping & checkpoints: monitor validation LWLRAP each epoch, save best checkpoint (highest LWLRAP); patience=3 epochs for early stop. Use best-val checkpoint for inference.\n",
        "\n",
        "- Data Augmentations:\n",
        "  - Time-domain: random gain [-6, +6] dB; pink/gaussian noise injection SNR ~ 20\u201330 dB; time shift \u00b10.5 s; mild pitch shift (\u00b12 semitones) and time stretch (0.9\u20131.1).\n",
        "  - Spectrogram: SpecAugment \u2014 2 freq masks (width up to 20 mel bins), 2 time masks (up to 10% of frames), without masking entire clip.\n",
        "  - MixUp: alpha=0.4 on spectrograms/logits; labels mixed linearly; probability 0.5.\n",
        "  - Random time-crop as above; for multi-scale training, occasionally use 5 s or 12 s crops (p=0.2 each) in diversity runs.\n",
        "\n",
        "- Cross-Validation (explicit):\n",
        "  - Iterative Stratification (MultilabelStratifiedKFold) with n_splits=5, shuffle=True, random_state=42.\n",
        "  - Stratify on 80-dim binary labels from curated train only; keep folds disjoint by filename.\n",
        "  - Metrics per fold: LWLRAP (primary), per-class LRAP, macro/micro AUC for diagnostics; early-stop on LWLRAP.\n",
        "\n",
        "- Noisy Data Protocol (multi-step):\n",
        "  1) Train a strong teacher on curated-only (5-fold, out-of-fold predictions saved for all curated).\n",
        "  2) Use the teacher (fold models averaged) to infer probabilities on train_noisy.\n",
        "  3) Confidence filtering:\n",
        "     - Positive selection: keep labels where teacher prob for that class \u2265 0.8; set others to 0 for that sample.\n",
        "     - Optional addition: add teacher positive pseudo-labels for classes \u2265 0.95 even if not present in weak labels.\n",
        "     - Discard samples with no remaining positives after filtering.\n",
        "  4) Weighting: per-sample weight = max teacher prob among positives; clip to [0.5, 1.0]. Also scale by class weight as above.\n",
        "  5) Curriculum retraining:\n",
        "     - Stage A: fine-tune teacher from curated checkpoint adding filtered noisy with low weight (0.5) for 3\u20135 epochs.\n",
        "     - Stage B: increase noisy sample weights to 0.75\u20131.0 for 5\u201310 epochs; optionally unfreeze more encoder layers.\n",
        "     - If noisy destabilizes LWLRAP, swap to GCE (q=0.7) or SCE (\u03b1=1, \u03b2=0.5).\n",
        "  6) Recompute OOF on curated and evaluate LWLRAP improvements before proceeding.\n",
        "\n",
        "- Sampling Strategy:\n",
        "  - Balanced batch sampler: ensure each batch includes rare classes via inverse-frequency sampling on curated; cap oversampling at 5x.\n",
        "\n",
        "- Inference, Rank Averaging, and Mapping Back to Probabilities:\n",
        "  - TTA per model/fold: K=5 time-crops of 10 s each spaced uniformly; for clips <10 s, use varied pad/loop starts. For long clips, consider K=10 if time allows.\n",
        "  - Compute logits per crop; average logits across crops; apply sigmoid at the end.\n",
        "  - Fold/model ensembling: perform per-class rank averaging across models/folds/TTAs.\n",
        "  - Rank\u2192probability mapping (explicit): per class, min-max normalize averaged ranks to [0,1] using CV distribution; then apply per-class temperature scaling (scalar \u03c4_c learned on OOF/CV by minimizing BCE) to calibrate probabilities. Ensure final outputs \u2208 [0,1].\n",
        "\n",
        "- Ensembling Plan (diversity targets):\n",
        "  - At least 4\u20136 models:\n",
        "    1) CNN14, 64 mels, 10 s crop, BCE+LS, seed 42.\n",
        "    2) CNN14, 64 mels, 10 s crop, BCE+LS, seed 2025.\n",
        "    3) CNN14, 128 mels, 10 s crop.\n",
        "    4) ResNet38, 64 mels, 10 s crop.\n",
        "    5) CNN14, 64 mels, 10 s crop with focal loss (gamma 1.5) variant.\n",
        "    6) CNN14, 64 mels, multi-scale crops (5/10/12 s schedule).\n",
        "  - Blend via rank averaging with per-model weights proportional to CV LWLRAP (normalize weights to sum=1).\n",
        "\n",
        "- Engineering & Efficiency:\n",
        "  - Augmentation vs caching: to preserve augmentation diversity, compute spectrograms on-the-fly during training (torchaudio preferred). Optionally cache only for inference or for fixed crops used in TTA.\n",
        "  - Use PyTorch DataLoader with num_workers=8\u201312, prefetch and pinned memory; cudnn.benchmark=True.\n",
        "  - Determinism: set all seeds; log configs and per-fold metrics; save checkpoints and OOF predictions.\n",
        "  - Notebook backups: programmatically save a copy of agent_notebook.ipynb before major refactors.\n",
        "\n",
        "Milestones & Audits\n",
        "- C0 (this): Gold-only Plan v2.1 with mandatory fixes \u2014 Approved.\n",
        "- C1: Data loading, label parsing, CV split, LWLRAP implementation check, basic EDA (label counts/durations), and mel frontend prototype aligned to PANNs (64 mel).\n",
        "- C2: PANNs CNN14 curated-only 5-fold training; OOF LWLRAP reported; baseline submission (safety).\n",
        "- C3: Noisy protocol (teacher inference, filtering/weighting), curriculum fine-tune; updated CV.\n",
        "- C4: TTA and initial ensembling across folds/seeds; submit improved predictions.\n",
        "- C5: Diversity models (ResNet38, 128/256 mels), final rank-averaged ensemble; final submission.\n",
        "\n",
        "Next Action: Proceed to C1 implementation with this corrected plan (PANNs-aligned 64-mel frontend, explicit rank\u2192prob mapping, and defined checkpointing/early stopping).\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "907d8d42-85fe-402d-a3bb-598e8510cae5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# C1: Data I/O, label parsing, CV split (MLSK), LWLRAP metric, basic EDA, unzip audio\n",
        "import os, zipfile, json, sys, math, random, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "BASE = Path('.')\n",
        "\n",
        "# 1) Unzip datasets if not already extracted\n",
        "def unzip_if_needed(zip_path: Path, dest_dir: Path):\n",
        "    if not dest_dir.exists():\n",
        "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "    # Heuristic: check if dest_dir has any audio files\n",
        "    has_files = any(dest_dir.rglob('*.wav')) or any(dest_dir.rglob('*.mp3')) or any(dest_dir.rglob('*.flac'))\n",
        "    if (not has_files) and zip_path.exists():\n",
        "        print(f\"Extracting {zip_path} -> {dest_dir} ...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "            z.extractall(dest_dir)\n",
        "        print(\"Done.\")\n",
        "    return dest_dir\n",
        "\n",
        "train_curated_csv = BASE / 'train_curated.csv'\n",
        "train_noisy_csv   = BASE / 'train_noisy.csv'\n",
        "sample_sub_csv    = BASE / 'sample_submission.csv'\n",
        "train_curated_zip = BASE / 'train_curated.zip'\n",
        "train_noisy_zip   = BASE / 'train_noisy.zip'\n",
        "test_zip          = BASE / 'test.zip'\n",
        "\n",
        "train_curated_dir = unzip_if_needed(train_curated_zip, BASE / 'train_curated')\n",
        "train_noisy_dir   = unzip_if_needed(train_noisy_zip,   BASE / 'train_noisy')\n",
        "test_dir          = unzip_if_needed(test_zip,          BASE / 'test')\n",
        "\n",
        "# 2) Load CSVs\n",
        "df_cur = pd.read_csv(train_curated_csv)\n",
        "df_noi = pd.read_csv(train_noisy_csv)\n",
        "df_ss  = pd.read_csv(sample_sub_csv)\n",
        "\n",
        "print('Curated shape:', df_cur.shape)\n",
        "print('Noisy shape:', df_noi.shape)\n",
        "print('Sample submission shape:', df_ss.shape)\n",
        "\n",
        "# 3) Determine schema\n",
        "print('Curated head:')\n",
        "print(df_cur.head())\n",
        "print('Noisy head:')\n",
        "print(df_noi.head())\n",
        "\n",
        "# Expect columns: 'fname' and 'labels' (semicolon-separated). Confirm and adapt if needed.\n",
        "fname_col = 'fname' if 'fname' in df_cur.columns else df_cur.columns[0]\n",
        "labels_col = 'labels' if 'labels' in df_cur.columns else df_cur.columns[-1]\n",
        "print(f\"Using columns -> fname: {fname_col}, labels: {labels_col}\")\n",
        "\n",
        "# 4) Build class list from sample_submission column order\n",
        "class_names = [c for c in df_ss.columns if c != 'fname']\n",
        "n_classes = len(class_names)\n",
        "print('Number of classes from sample_submission:', n_classes)\n",
        "\n",
        "# 5) Parse labels to multilabel binary matrix for curated data\n",
        "label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "\n",
        "def encode_labels(label_str: str):\n",
        "    y = np.zeros(n_classes, dtype=np.float32)\n",
        "    if isinstance(label_str, str) and label_str.strip():\n",
        "        for tok in label_str.split(',') if (' ,' in label_str or ',' in label_str and ';' not in label_str) else label_str.split(';'):\n",
        "            t = tok.strip()\n",
        "            if t in label_to_idx:\n",
        "                y[label_to_idx[t]] = 1.0\n",
        "    return y\n",
        "\n",
        "Y_cur = np.stack(df_cur[labels_col].apply(encode_labels).values)\n",
        "print('Curated label matrix shape:', Y_cur.shape, 'positives:', int(Y_cur.sum()))\n",
        "\n",
        "# 6) Implement LWLRAP metric (Kaggle-official style)\n",
        "def lwlrap(truth, scores):\n",
        "    # truth: (n_samples, n_classes) binary; scores: same shape floats\n",
        "    # Computes label-weighted label-ranking average precision\n",
        "    assert truth.shape == scores.shape\n",
        "    n_samples, n_labels = truth.shape\n",
        "    # Per-class sum of precisions\n",
        "    precisions_for_these_labels = np.zeros(n_labels)\n",
        "    labels_per_class = truth.sum(axis=0)\n",
        "    # Avoid divide by zero\n",
        "    labels_per_class = np.maximum(labels_per_class, 1)\n",
        "    for i in range(n_samples):\n",
        "        pos_label_indices = np.where(truth[i] > 0)[0]\n",
        "        if len(pos_label_indices) == 0:\n",
        "            continue\n",
        "        ranking = np.argsort(-scores[i])\n",
        "        ranked_truth = truth[i][ranking]\n",
        "        cumsum = np.cumsum(ranked_truth)\n",
        "        # precision at each true label\n",
        "        pos_rank_indices = np.where(ranked_truth > 0)[0]\n",
        "        precisions = cumsum[pos_rank_indices] / (pos_rank_indices + 1)\n",
        "        # add to corresponding classes\n",
        "        ranked_labels = ranking[pos_rank_indices]\n",
        "        for lbl, prec in zip(ranked_labels, precisions):\n",
        "            precisions_for_these_labels[lbl] += prec\n",
        "    per_class_lwlrap = precisions_for_these_labels / labels_per_class\n",
        "    # label-weighted: weight by prevalence of positives\n",
        "    weights = (truth.sum(axis=0) / np.maximum(truth.sum(), 1))\n",
        "    return float((per_class_lwlrap * weights).sum()), per_class_lwlrap\n",
        "\n",
        "# 7) Create 5-fold Multilabel Stratified K-Fold (iterative stratification)\n",
        "folds = np.full(len(df_cur), -1, dtype=int)\n",
        "mlsk_available = False\n",
        "try:\n",
        "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "    mlsk = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    for k, (_, val_idx) in enumerate(mlsk.split(df_cur[fname_col].values, Y_cur)):\n",
        "        folds[val_idx] = k\n",
        "    mlsk_available = True\n",
        "except Exception as e:\n",
        "    print('iterstrat not available or failed to import:', e)\n",
        "    print('Falling back to regular KFold (WARNING: less reliable for multilabel).')\n",
        "    from sklearn.model_selection import KFold\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    for k, (_, val_idx) in enumerate(kf.split(df_cur)):\n",
        "        folds[val_idx] = k\n",
        "\n",
        "df_cur['fold'] = folds\n",
        "print('Fold distribution (counts):')\n",
        "print(df_cur['fold'].value_counts().sort_index())\n",
        "\n",
        "# 8) Basic EDA: label frequency and co-occurrence summary\n",
        "label_freq = Y_cur.sum(axis=0)\n",
        "top_idx = np.argsort(-label_freq)[:10]\n",
        "print('Top 10 classes by frequency:')\n",
        "for i in top_idx:\n",
        "    print(f\"{class_names[i]}: {int(label_freq[i])}\")\n",
        "print('Num samples with >3 labels:', int((Y_cur.sum(axis=1) > 3).sum()))\n",
        "\n",
        "# 9) Save artifacts: class mapping and folds\n",
        "meta = {\n",
        "    'class_names': class_names,\n",
        "    'label_to_idx': label_to_idx,\n",
        "    'fname_col': fname_col,\n",
        "    'labels_col': labels_col,\n",
        "    'mlsk_available': mlsk_available\n",
        "}\n",
        "with open('metadata.json', 'w') as f:\n",
        "    json.dump(meta, f)\n",
        "df_cur.to_csv('train_curated_folds.csv', index=False)\n",
        "print('Saved metadata.json and train_curated_folds.csv')\n",
        "\n",
        "# 10) Sanity check LWLRAP implementation with a tiny synthetic example\n",
        "truth_demo = np.array([[1,0,1],[0,1,0]], dtype=int)\n",
        "scores_demo = np.array([[0.9, 0.2, 0.8],[0.1,0.7,0.3]], dtype=float)\n",
        "demo_lwlrap, _ = lwlrap(truth_demo, scores_demo)\n",
        "print('LWLRAP demo (should be between 0 and 1):', demo_lwlrap)\n",
        "\n",
        "print('\\nC1 setup complete: data loaded, labels parsed, 5-folds created, LWLRAP ready.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting train_curated.zip -> train_curated ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\nExtracting train_noisy.zip -> train_noisy ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\nExtracting test.zip -> test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\nCurated shape: (4970, 2)\nNoisy shape: (19815, 2)\nSample submission shape: (3361, 81)\nCurated head:\n          fname           labels\n0  0006ae4e.wav             Bark\n1  0019ef41.wav         Raindrop\n2  001ec0ad.wav  Finger_snapping\n3  0026c7cb.wav              Run\n4  0026f116.wav  Finger_snapping\nNoisy head:\n          fname                                          labels\n0  00097e21.wav                    Bathtub_(filling_or_washing)\n1  000b6cfb.wav                                      Motorcycle\n2  00116cd2.wav              Marimba_and_xylophone,Glockenspiel\n3  00127d14.wav  Water_tap_and_faucet,Sink_(filling_or_washing)\n4  0019adae.wav                                        Raindrop\nUsing columns -> fname: fname, labels: labels\nNumber of classes from sample_submission: 80\nCurated label matrix shape: (4970, 80) positives: 5752\niterstrat not available or failed to import: No module named 'iterstrat'\nFalling back to regular KFold (WARNING: less reliable for multilabel).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold distribution (counts):\nfold\n0    994\n1    994\n2    994\n3    994\n4    994\nName: count, dtype: int64\nTop 10 classes by frequency:\nMarimba_and_xylophone: 75\nScreaming: 75\nScissors: 75\nRun: 75\nRaindrop: 75\nPrinter: 75\nMotorcycle: 75\nMicrowave_oven: 75\nMeow: 75\nShatter: 75\nNum samples with >3 labels: 5\nSaved metadata.json and train_curated_folds.csv\nLWLRAP demo (should be between 0 and 1): 1.0\n\nC1 setup complete: data loaded, labels parsed, 5-folds created, LWLRAP ready.\n"
          ]
        }
      ]
    },
    {
      "id": "829412e1-27bc-48bf-9d2f-6b4bcb819e38",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# C1 Remediation: Install MLSKF, robust label parsing, duration EDA, 64-mel frontend prototype, rebuild folds\n",
        "import sys, subprocess, json, math\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "\n",
        "BASE = Path('.')\n",
        "train_curated_csv = BASE / 'train_curated.csv'\n",
        "sample_sub_csv    = BASE / 'sample_submission.csv'\n",
        "train_curated_dir = BASE / 'train_curated'\n",
        "\n",
        "    # 0) Load CSVs and sample submission for class order\n",
        "df_cur = pd.read_csv(train_curated_csv)\n",
        "df_ss  = pd.read_csv(sample_sub_csv)\n",
        "fname_col = 'fname' if 'fname' in df_cur.columns else df_cur.columns[0]\n",
        "labels_col = 'labels' if 'labels' in df_cur.columns else df_cur.columns[-1]\n",
        "class_names = [c for c in df_ss.columns if c != 'fname']\n",
        "label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "n_classes = len(class_names)\n",
        "\n",
        "# 1) Robust label parsing with validation (semicolon-insensitive); fail loudly on unknown labels\n",
        "def parse_labels_str(label_str):\n",
        "    if not isinstance(label_str, str):\n",
        "        return []\n",
        "    toks = [t.strip() for t in label_str.replace(';', ',').split(',') if t.strip()]\n",
        "    # validate\n",
        "    unknown = [t for t in toks if t not in label_to_idx]\n",
        "    if unknown:\n",
        "        raise ValueError(f\"Unknown labels encountered: {unknown[:5]} ... total {len(unknown)}. Check class mappings/CSV.\")\n",
        "    return toks\n",
        "\n",
        "def encode_labels(toks):\n",
        "    y = np.zeros(n_classes, dtype=np.float32)\n",
        "    for t in toks:\n",
        "        y[label_to_idx[t]] = 1.0\n",
        "    return y\n",
        "\n",
        "cur_tokens = df_cur[labels_col].apply(parse_labels_str)\n",
        "Y_cur = np.stack(cur_tokens.apply(encode_labels).values)\n",
        "print('Reparsed curated label matrix:', Y_cur.shape, 'positives:', int(Y_cur.sum()))\n",
        "\n",
        "# 2) Investigate EDA anomaly: label frequency distribution\n",
        "label_freq = Y_cur.sum(axis=0)\n",
        "vals, counts = np.unique(label_freq, return_counts=True)\n",
        "print('Label frequency value_counts (value -> num_classes):')\n",
        "for v, c in zip(vals.astype(int), counts):\n",
        "    print(f'  {v} -> {c}')\n",
        "top10_idx = np.argsort(-label_freq)[:10]\n",
        "print('Top10 classes and counts:')\n",
        "for i in top10_idx:\n",
        "    print(f'  {class_names[i]}: {int(label_freq[i])}')\n",
        "if len(vals) == 1:\n",
        "    print('NOTE: All classes have identical frequency in curated set. Dataset appears artificially balanced. Adjust class weighting accordingly (likely no weighting needed for curated-only).')\n",
        "elif counts.max() > len(class_names)*0.5:\n",
        "    print('NOTE: Majority of classes share the same frequency. Curated set is near-balanced. Use light/none class weighting for curated.')\n",
        "\n",
        "# 3) Audio duration EDA using soundfile metadata (fast, no full decode)\n",
        "audio_paths = df_cur[fname_col].apply(lambda f: str(train_curated_dir / f)).values\n",
        "durations = []\n",
        "missing = 0\n",
        "for p in audio_paths:\n",
        "    try:\n",
        "        info = sf.info(p)\n",
        "        d = info.frames / max(info.samplerate, 1)\n",
        "        durations.append(d)\n",
        "    except Exception:\n",
        "        # fallback to librosa load header if needed\n",
        "        try:\n",
        "            y, sr = librosa.load(p, sr=None, mono=True)\n",
        "            durations.append(len(y)/sr)\n",
        "        except Exception:\n",
        "            missing += 1\n",
        "            durations.append(np.nan)\n",
        "durations = np.array(durations, dtype=float)\n",
        "valid = ~np.isnan(durations)\n",
        "dur = durations[valid]\n",
        "def pct(a, q):\n",
        "    return float(np.percentile(a, q)) if len(a) else float('nan')\n",
        "print(f'Durations stats (n={len(dur)}, missing={missing}): min={dur.min():.3f}s, mean={dur.mean():.3f}s, p50={pct(dur,50):.3f}s, p90={pct(dur,90):.3f}s, p99={pct(dur,99):.3f}s, max={dur.max():.3f}s')\n",
        "print('Implication: use 10.0s target crops; apply random crop if longer, zero/loop pad if shorter as per plan.')\n",
        "\n",
        "# 4) Install iterative-stratification and build 5-fold MultilabelStratifiedKFold (no fallback)\n",
        "need_install = False\n",
        "try:\n",
        "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "except Exception:\n",
        "    need_install = True\n",
        "if need_install:\n",
        "    print('Installing iterative-stratification ...')\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'iterative-stratification'])\n",
        "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "mlsk = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = np.full(len(df_cur), -1, dtype=int)\n",
        "for k, (_, val_idx) in enumerate(mlsk.split(df_cur[fname_col].values, Y_cur)):\n",
        "    folds[val_idx] = k\n",
        "assert (folds >= 0).all(), 'Fold assignment failed.'\n",
        "df_cur['fold'] = folds\n",
        "df_cur.to_csv('train_curated_folds.csv', index=False)\n",
        "print('Rebuilt 5-fold MultilabelStratifiedKFold. Fold counts:')\n",
        "print(df_cur['fold'].value_counts().sort_index())\n",
        "\n",
        "# 5) PANNs-aligned mel-spectrogram frontend prototype (sr=32k, n_mels=64)\n",
        "MEL_CFG = {\n",
        "    'sr': 32000,\n",
        "    'n_fft': 1024,\n",
        "    'hop_length': 320,\n",
        "    'win_length': 1024,\n",
        "    'n_mels': 64,\n",
        "    'fmin': 50,\n",
        "    'fmax': 16000,\n",
        "}\n",
        "\n",
        "def load_audio_32k(path, sr=32000):\n",
        "    y, s = librosa.load(path, sr=sr, mono=True)\n",
        "    return y, s\n",
        "\n",
        "def logmel_64(y, sr, cfg=MEL_CFG):\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=cfg['n_fft'], hop_length=cfg['hop_length'],\n",
        "                                       win_length=cfg['win_length'], window='hann', n_mels=cfg['n_mels'],\n",
        "                                       fmin=cfg['fmin'], fmax=cfg['fmax'], center=True, power=2.0)\n",
        "    S_db = librosa.power_to_db(S, ref=1.0)\n",
        "    return S_db.astype(np.float32)  # shape (n_mels, T)\n",
        "\n",
        "# Demo on a few files\n",
        "demo_files = [str(train_curated_dir / f) for f in df_cur[fname_col].head(3).values]\n",
        "for p in demo_files:\n",
        "    y, sr = load_audio_32k(p, sr=MEL_CFG['sr'])\n",
        "    S = logmel_64(y, sr)\n",
        "    T = S.shape[1]\n",
        "    seconds = T * (MEL_CFG['hop_length'] / MEL_CFG['sr'])\n",
        "    print(f'Mel64 for {Path(p).name}: shape={S.shape}, covers ~{seconds:.2f}s of audio')\n",
        "\n",
        "print('\\nC1 remediation complete: robust parsing, EDA, mel64 prototype, and MLSKF folds rebuilt.')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'soundfile'",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msoundfile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msf\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlibrosa\u001b[39;00m\n\u001b[32m      9\u001b[39m BASE = Path(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'soundfile'"
          ]
        }
      ]
    },
    {
      "id": "af0423c2-63b4-471d-a5a3-4042b799a327",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install missing dependencies for C1 remediation\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install(pkg):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', pkg])\n",
        "        print(f'Installed: {pkg}')\n",
        "    except Exception as e:\n",
        "        print(f'Failed to install {pkg}: {e}')\n",
        "\n",
        "# Ensure soundfile and iterative-stratification are available\n",
        "try:\n",
        "    import soundfile  # noqa: F401\n",
        "    print('soundfile already available')\n",
        "except Exception:\n",
        "    pip_install('soundfile')\n",
        "\n",
        "try:\n",
        "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold  # noqa: F401\n",
        "    print('iterative-stratification already available')\n",
        "except Exception:\n",
        "    pip_install('iterative-stratification')\n",
        "\n",
        "print('Dependency installation step complete.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed: soundfile\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed: iterative-stratification\nDependency installation step complete.\n"
          ]
        }
      ]
    },
    {
      "id": "0d28475c-7d56-4bbe-9478-c398f62404ad",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# C1 Remediation (re-run): Robust parsing, Duration EDA, 64-mel frontend, enforce MLSKF folds (no KFold fallback)\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "BASE = Path('.')\n",
        "train_curated_csv = BASE / 'train_curated.csv'\n",
        "sample_sub_csv    = BASE / 'sample_submission.csv'\n",
        "train_curated_dir = BASE / 'train_curated'\n",
        "\n",
        "# Load CSVs and class order\n",
        "df_cur = pd.read_csv(train_curated_csv)\n",
        "df_ss  = pd.read_csv(sample_sub_csv)\n",
        "fname_col = 'fname' if 'fname' in df_cur.columns else df_cur.columns[0]\n",
        "labels_col = 'labels' if 'labels' in df_cur.columns else df_cur.columns[-1]\n",
        "class_names = [c for c in df_ss.columns if c != 'fname']\n",
        "label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "n_classes = len(class_names)\n",
        "\n",
        "# Robust label parsing with validation\n",
        "def parse_labels_str(label_str):\n",
        "    if not isinstance(label_str, str):\n",
        "        return []\n",
        "    toks = [t.strip() for t in label_str.replace(';', ',').split(',') if t.strip()]\n",
        "    unknown = [t for t in toks if t not in label_to_idx]\n",
        "    if unknown:\n",
        "        raise ValueError(f\"Unknown labels encountered: {unknown[:5]} ... total {len(unknown)}\")\n",
        "    return toks\n",
        "\n",
        "def encode_labels(toks):\n",
        "    y = np.zeros(n_classes, dtype=np.float32)\n",
        "    for t in toks:\n",
        "        y[label_to_idx[t]] = 1.0\n",
        "    return y\n",
        "\n",
        "cur_tokens = df_cur[labels_col].apply(parse_labels_str)\n",
        "Y_cur = np.stack(cur_tokens.apply(encode_labels).values)\n",
        "print('Reparsed curated label matrix:', Y_cur.shape, 'positives:', int(Y_cur.sum()))\n",
        "\n",
        "# Investigate label frequency anomaly\n",
        "label_freq = Y_cur.sum(axis=0)\n",
        "vals, counts = np.unique(label_freq, return_counts=True)\n",
        "print('Label frequency value_counts (value -> num_classes):')\n",
        "for v, c in zip(vals.astype(int), counts):\n",
        "    print(f'  {int(v)} -> {int(c)}')\n",
        "if len(vals) == 1:\n",
        "    print('NOTE: All classes have identical frequency in curated set (artificial balance). Use none/light class weighting on curated.')\n",
        "\n",
        "# Duration EDA (soundfile metadata)\n",
        "audio_paths = df_cur[fname_col].apply(lambda f: str(train_curated_dir / f)).values\n",
        "durations = []\n",
        "missing = 0\n",
        "for p in audio_paths:\n",
        "    try:\n",
        "        info = sf.info(p)\n",
        "        durations.append(info.frames / max(info.samplerate, 1))\n",
        "    except Exception:\n",
        "        try:\n",
        "            y, sr = librosa.load(p, sr=None, mono=True)\n",
        "            durations.append(len(y)/sr)\n",
        "        except Exception:\n",
        "            durations.append(np.nan); missing += 1\n",
        "dur = np.array(durations, dtype=float)\n",
        "valid = np.isfinite(dur)\n",
        "dur = dur[valid]\n",
        "pct = lambda a,q: float(np.percentile(a,q)) if len(a) else float('nan')\n",
        "print(f'Durations stats (n={len(dur)}, missing={missing}): min={dur.min():.3f}s, mean={dur.mean():.3f}s, p50={pct(dur,50):.3f}s, p90={pct(dur,90):.3f}s, p99={pct(dur,99):.3f}s, max={dur.max():.3f}s')\n",
        "print('Implication: 10.0s target crops; random crop if longer; zero/loop pad if shorter.')\n",
        "\n",
        "# Enforce 5-fold MultilabelStratifiedKFold (no fallback)\n",
        "mlsk = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = np.full(len(df_cur), -1, dtype=int)\n",
        "for k, (_, val_idx) in enumerate(mlsk.split(df_cur[fname_col].values, Y_cur)):\n",
        "    folds[val_idx] = k\n",
        "assert (folds >= 0).all(), 'Fold assignment failed.'\n",
        "df_cur['fold'] = folds\n",
        "df_cur.to_csv('train_curated_folds.csv', index=False)\n",
        "print('Rebuilt MLSKF. Fold counts:')\n",
        "print(df_cur['fold'].value_counts().sort_index())\n",
        "\n",
        "# 64-mel frontend prototype (PANNs-aligned)\n",
        "MEL_CFG = dict(sr=32000, n_fft=1024, hop_length=320, win_length=1024, n_mels=64, fmin=50, fmax=16000)\n",
        "def load_audio_32k(path, sr=32000):\n",
        "    y, s = librosa.load(path, sr=sr, mono=True)\n",
        "    return y, s\n",
        "def logmel_64(y, sr, cfg=MEL_CFG):\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=cfg['n_fft'], hop_length=cfg['hop_length'],\n",
        "                                       win_length=cfg['win_length'], window='hann', n_mels=cfg['n_mels'],\n",
        "                                       fmin=cfg['fmin'], fmax=cfg['fmax'], center=True, power=2.0)\n",
        "    S_db = librosa.power_to_db(S, ref=1.0)\n",
        "    return S_db.astype(np.float32)\n",
        "demo_files = [str(train_curated_dir / f) for f in df_cur[fname_col].head(3).values]\n",
        "for p in demo_files:\n",
        "    y, sr = load_audio_32k(p, sr=MEL_CFG['sr'])\n",
        "    S = logmel_64(y, sr)\n",
        "    T = S.shape[1]\n",
        "    seconds = T * (MEL_CFG['hop_length'] / MEL_CFG['sr'])\n",
        "    print(f'Mel64 for {Path(p).name}: shape={S.shape}, covers ~{seconds:.2f}s of audio')\n",
        "\n",
        "print('\\nC1 remediation complete: robust parsing, duration EDA, mel64 prototype, and MLSKF folds rebuilt.')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reparsed curated label matrix: (4970, 80) positives: 5752\nLabel frequency value_counts (value -> num_classes):\n  47 -> 1\n  48 -> 1\n  49 -> 1\n  50 -> 1\n  53 -> 1\n  56 -> 3\n  57 -> 1\n  63 -> 2\n  65 -> 1\n  67 -> 1\n  72 -> 1\n  75 -> 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Durations stats (n=4970, missing=0): min=0.300s, mean=7.648s, p50=4.676s, p90=20.349s, p99=29.000s, max=57.571s\nImplication: 10.0s target crops; random crop if longer; zero/loop pad if shorter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rebuilt MLSKF. Fold counts:\nfold\n0    999\n1    994\n2    992\n3    997\n4    988\nName: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mel64 for 0006ae4e.wav: shape=(64, 704), covers ~7.04s of audio\nMel64 for 0019ef41.wav: shape=(64, 206), covers ~2.06s of audio\nMel64 for 001ec0ad.wav: shape=(64, 152), covers ~1.52s of audio\n\nC1 remediation complete: robust parsing, duration EDA, mel64 prototype, and MLSKF folds rebuilt.\n"
          ]
        }
      ]
    },
    {
      "id": "3a34fa20-d078-430f-923c-83d9ffe8f31b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install librosa and related deps for audio frontend\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install(pkg):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', pkg])\n",
        "        print(f'Installed: {pkg}')\n",
        "    except Exception as e:\n",
        "        print(f'Failed to install {pkg}: {e}')\n",
        "\n",
        "try:\n",
        "    import librosa  # noqa: F401\n",
        "    print('librosa already available')\n",
        "except Exception:\n",
        "    pip_install('librosa')\n",
        "    # optional backends/utilities\n",
        "    pip_install('audioread')\n",
        "\n",
        "print('Librosa installation step complete.')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed: librosa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed: audioread\nLibrosa installation step complete.\n"
          ]
        }
      ]
    },
    {
      "id": "0051aaa0-9a8c-4d88-892d-665c6dea5deb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# C2 Remediation: Pretrained PANNs CNN14 embeddings + fast OVR Logistic Regression (5-fold OOF + submission)\n",
        "# - Uses panns-inference pretrained CNN14 (AudioSet) as frozen encoder to extract 2048-d embeddings per clip.\n",
        "# - Trains scikit-learn OneVsRest LogisticRegression on these embeddings using the existing 5-fold MLSKF splits.\n",
        "# - Produces valid OOF LWLRAP and a competitive submission efficiently. Caches embeddings.\n",
        "\n",
        "import sys, subprocess, os, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import joblib\n",
        "import torch\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Ensure panns data directory and local assets exist\n",
        "os.makedirs('/app/panns_data', exist_ok=True)\n",
        "LABELS_CSV = Path('/app/panns_data/class_labels_indices.csv')\n",
        "CKPT_PATH = Path('/app/panns_data/Cnn14_mAP=0.431.pth')\n",
        "assert LABELS_CSV.exists(), 'Missing /app/panns_data/class_labels_indices.csv. Run the stub-creation cell first.'\n",
        "assert CKPT_PATH.exists(), 'Missing CNN14 checkpoint at /app/panns_data/Cnn14_mAP=0.431.pth. Run the download cell first.'\n",
        "\n",
        "# Install panns-inference if missing\n",
        "try:\n",
        "    import panns_inference\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'panns-inference'])\n",
        "    import panns_inference\n",
        "\n",
        "from panns_inference import AudioTagging\n",
        "\n",
        "BASE = Path('.')\n",
        "SR = 32000\n",
        "CROP_SEC = 10.0\n",
        "EMB_DIM = 2048\n",
        "N_FOLDS = 5\n",
        "\n",
        "# Load metadata and folds (precomputed MLSKF)\n",
        "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
        "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
        "class_names = [c for c in df_ss.columns if c != 'fname']\n",
        "label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "n_classes = len(class_names)\n",
        "train_dir = BASE / 'train_curated'\n",
        "test_dir  = BASE / 'test'\n",
        "\n",
        "def parse_labels_str(s):\n",
        "    if not isinstance(s, str):\n",
        "        return []\n",
        "    toks = [t.strip() for t in s.replace(';', ',').split(',') if t.strip()]\n",
        "    unk = [t for t in toks if t not in label_to_idx]\n",
        "    if unk:\n",
        "        raise ValueError(f'Unknown labels encountered: {unk[:5]} (total {len(unk)})')\n",
        "    return toks\n",
        "\n",
        "def encode_labels(s):\n",
        "    y = np.zeros(n_classes, dtype=np.float32)\n",
        "    for t in parse_labels_str(s):\n",
        "        y[label_to_idx[t]] = 1.0\n",
        "    return y\n",
        "\n",
        "def lwlrap_np(truth, scores):\n",
        "    assert truth.shape == scores.shape\n",
        "    n_samples, n_labels = truth.shape\n",
        "    precisions_for_these_labels = np.zeros(n_labels)\n",
        "    labels_per_class = truth.sum(axis=0)\n",
        "    labels_per_class = np.maximum(labels_per_class, 1)\n",
        "    for i in range(n_samples):\n",
        "        pos_idx = np.where(truth[i] > 0)[0]\n",
        "        if len(pos_idx) == 0:\n",
        "            continue\n",
        "        ranking = np.argsort(-scores[i])\n",
        "        ranked_truth = truth[i][ranking]\n",
        "        cumsum = np.cumsum(ranked_truth)\n",
        "        pos_rank_indices = np.where(ranked_truth > 0)[0]\n",
        "        precisions = cumsum[pos_rank_indices] / (pos_rank_indices + 1)\n",
        "        ranked_labels = ranking[pos_rank_indices]\n",
        "        for lbl, prec in zip(ranked_labels, precisions):\n",
        "            precisions_for_these_labels[lbl] += prec\n",
        "    per_class = precisions_for_these_labels / labels_per_class\n",
        "    weights = (truth.sum(axis=0) / np.maximum(truth.sum(), 1))\n",
        "    return float((per_class * weights).sum()), per_class\n",
        "\n",
        "# Deterministic center crop to 10s (no randomness for embeddings)\n",
        "def load_center_crop_10s(path, sr=SR, crop_sec=CROP_SEC):\n",
        "    y, s = librosa.load(path, sr=sr, mono=True)\n",
        "    target = int(crop_sec * sr)\n",
        "    if len(y) >= target:\n",
        "        start = max(0, (len(y) - target) // 2)\n",
        "        y = y[start:start+target]\n",
        "    else:\n",
        "        y = np.pad(y, (0, target - len(y)), mode='constant')\n",
        "    return y.astype(np.float32)\n",
        "\n",
        "emb_cur_path = BASE / 'embeddings_curated.npy'\n",
        "emb_test_path = BASE / 'embeddings_test.npy'\n",
        "\n",
        "def extract_embeddings(file_list, root_dir):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    # Use local checkpoint to avoid any downloads\n",
        "    at = AudioTagging(checkpoint_path=str(CKPT_PATH), device=device)\n",
        "    X = np.zeros((len(file_list), EMB_DIM), dtype=np.float32)\n",
        "    t0 = time.time()\n",
        "    for i, fname in enumerate(file_list):\n",
        "        wav_path = str(Path(root_dir) / fname)\n",
        "        y = load_center_crop_10s(wav_path, sr=SR, crop_sec=CROP_SEC)\n",
        "        # AudioTagging.inference expects a batched waveform: shape (B, T)\n",
        "        y_batched = np.expand_dims(y, 0)\n",
        "        with torch.no_grad():\n",
        "            out = at.inference(y_batched)\n",
        "        # Some versions return tuple (clipwise_output, embedding)\n",
        "        if isinstance(out, tuple) and len(out) == 2:\n",
        "            embedding = out[1]\n",
        "        elif isinstance(out, dict) and 'embedding' in out:\n",
        "            embedding = out['embedding']\n",
        "        else:\n",
        "            raise RuntimeError(f'Unexpected AudioTagging output type: {type(out)}')\n",
        "        X[i] = np.asarray(embedding, dtype=np.float32)[0]\n",
        "        if (i+1) % 200 == 0:\n",
        "            dt = time.time() - t0\n",
        "            print(f'  Extracted {i+1}/{len(file_list)} embeddings in {dt/60:.1f} min')\n",
        "    return X\n",
        "\n",
        "# Filenames\n",
        "train_files = df_cur['fname'].values\n",
        "test_files  = df_ss['fname'].values\n",
        "\n",
        "if emb_cur_path.exists() and emb_test_path.exists():\n",
        "    X_cur = np.load(emb_cur_path)\n",
        "    X_test = np.load(emb_test_path)\n",
        "    print('Loaded cached embeddings.')\n",
        "else:\n",
        "    print('Extracting curated embeddings ...')\n",
        "    X_cur = extract_embeddings(train_files, train_dir)\n",
        "    np.save(emb_cur_path, X_cur)\n",
        "    print('Extracting test embeddings ...')\n",
        "    X_test = extract_embeddings(test_files, test_dir)\n",
        "    np.save(emb_test_path, X_test)\n",
        "    print('Saved embeddings to disk.')\n",
        "\n",
        "# Targets\n",
        "Y_cur = np.stack(df_cur['labels'].apply(encode_labels).values)\n",
        "\n",
        "# 5-fold OOF using existing folds\n",
        "oof = np.zeros((len(df_cur), n_classes), dtype=np.float32)\n",
        "fold_scores = []\n",
        "\n",
        "for k in range(N_FOLDS):\n",
        "    trn_idx = np.where(df_cur['fold'].values != k)[0]\n",
        "    val_idx = np.where(df_cur['fold'].values == k)[0]\n",
        "    print(f'Fold {k}: train {len(trn_idx)}, val {len(val_idx)}')\n",
        "    X_tr, X_va = X_cur[trn_idx], X_cur[val_idx]\n",
        "    y_tr, y_va = Y_cur[trn_idx], Y_cur[val_idx]\n",
        "    base_lr = LogisticRegression(\n",
        "        solver='lbfgs', max_iter=1000, C=2.0, n_jobs=16, verbose=0\n",
        "    )\n",
        "    clf = OneVsRestClassifier(make_pipeline(StandardScaler(with_mean=True, with_std=True), base_lr), n_jobs=-1)\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    proba = clf.predict_proba(X_va)\n",
        "    oof[val_idx] = proba.astype(np.float32)\n",
        "    lw, _ = lwlrap_np(y_va, proba)\n",
        "    fold_scores.append(lw)\n",
        "    print(f'  Fold {k} LWLRAP={lw:.4f}')\n",
        "\n",
        "oof_lw, _ = lwlrap_np(Y_cur, oof)\n",
        "print(f'OOF LWLRAP={oof_lw:.4f}; per-fold={fold_scores}')\n",
        "np.save('oof_panns_lr.npy', oof)\n",
        "\n",
        "# Train full model and predict test\n",
        "base_lr_full = LogisticRegression(solver='lbfgs', max_iter=1000, C=2.0, n_jobs=16, verbose=0)\n",
        "clf_full = OneVsRestClassifier(make_pipeline(StandardScaler(with_mean=True, with_std=True), base_lr_full), n_jobs=-1)\n",
        "clf_full.fit(X_cur, Y_cur)\n",
        "test_proba = clf_full.predict_proba(X_test).astype(np.float32)\n",
        "\n",
        "sub = pd.DataFrame(test_proba, columns=class_names)\n",
        "sub.insert(0, 'fname', test_files)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv using PANNs CNN14 embeddings + OVR LR. Shape:', sub.shape)\n",
        "\n",
        "# Save classifier for reuse\n",
        "joblib.dump(clf_full, 'ovr_logreg_panns.joblib')\n",
        "print('Saved classifier checkpoint.')\n",
        ""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded cached embeddings.\nFold 0: train 3971, val 999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 0 LWLRAP=0.8091\nFold 1: train 3976, val 994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1 LWLRAP=0.8004\nFold 2: train 3978, val 992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 2 LWLRAP=0.8080\nFold 3: train 3973, val 997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 3 LWLRAP=0.8097\nFold 4: train 3982, val 988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 4 LWLRAP=0.7748\nOOF LWLRAP=0.8001; per-fold=[0.8091265640638446, 0.8004257362345152, 0.8080218131533763, 0.8096867591565071, 0.7748105563971049]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv using PANNs CNN14 embeddings + OVR LR. Shape: (3361, 81)\nSaved classifier checkpoint.\n"
          ]
        }
      ]
    },
    {
      "id": "ab3ec339-0d2b-41c1-89c2-0b47d95e89f6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create offline stub for panns_inference labels to bypass wget during import\n",
        "import os\n",
        "from pathlib import Path\n",
        "labels_dir = Path('/app/panns_data')\n",
        "labels_dir.mkdir(parents=True, exist_ok=True)\n",
        "labels_csv = labels_dir / 'class_labels_indices.csv'\n",
        "if not labels_csv.exists():\n",
        "    # AudioSet has 527 classes; create a stub with correct header and 527 rows\n",
        "    import csv\n",
        "    with open(labels_csv, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['index', 'mid', 'display_name'])\n",
        "        for i in range(527):\n",
        "            writer.writerow([i, f'/m/{i}', f'class_{i:03d}'])\n",
        "    print('Created stub labels CSV at', labels_csv)\n",
        "else:\n",
        "    print('Labels CSV already exists at', labels_csv)\n",
        "\n",
        "print('Stub setup complete. Next: re-run the PANNs embeddings cell to observe weight download behavior and capture expected checkpoint path.')\n",
        ""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created stub labels CSV at /app/panns_data/class_labels_indices.csv\nStub setup complete. Next: re-run the PANNs embeddings cell to observe weight download behavior and capture expected checkpoint path.\n"
          ]
        }
      ]
    },
    {
      "id": "e88047c6-235f-4c90-9ff4-cf895f95ec3f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Download pretrained PANNs CNN14 weights via Python (no wget/apt) to /app/panns_data\n",
        "import os, sys, hashlib\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "\n",
        "dest_dir = Path('/app/panns_data')\n",
        "dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "ckpt_path = dest_dir / 'Cnn14_mAP=0.431.pth'\n",
        "\n",
        "# Zenodo direct download link (shared by colleagues)\n",
        "url = 'https://zenodo.org/record/3987831/files/Cnn14_mAP=0.431.pth?download=1'\n",
        "\n",
        "def download(url, path):\n",
        "    print(f'Downloading CNN14 weights to {path} ...')\n",
        "    with urllib.request.urlopen(url) as resp, open(path, 'wb') as out:\n",
        "        block_size = 1 << 20\n",
        "        while True:\n",
        "            chunk = resp.read(block_size)\n",
        "            if not chunk:\n",
        "                break\n",
        "            out.write(chunk)\n",
        "    print('Download complete.')\n",
        "\n",
        "if ckpt_path.exists() and ckpt_path.stat().st_size > 0:\n",
        "    print('Pretrained weights already present at:', ckpt_path, 'size:', ckpt_path.stat().st_size)\n",
        "else:\n",
        "    download(url, ckpt_path)\n",
        "    print('Saved:', ckpt_path, 'size:', ckpt_path.stat().st_size)\n",
        "\n",
        "print('Next step: modify PANNs embeddings cell to pass checkpoint_path=str(ckpt_path) to AudioTagging and re-run.')\n",
        ""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading CNN14 weights to /app/panns_data/Cnn14_mAP=0.431.pth ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete.\nSaved: /app/panns_data/Cnn14_mAP=0.431.pth size: 327428481\nNext step: modify PANNs embeddings cell to pass checkpoint_path=str(ckpt_path) to AudioTagging and re-run.\n"
          ]
        }
      ]
    },
    {
      "id": "8303a338-fda3-47ca-b071-b121f1529b93",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Diagnose panns_inference.AudioTagging API and verify inference call + output keys with various input shapes\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa, torch, sys, subprocess, os\n",
        "\n",
        "# Ensure assets exist\n",
        "LABELS_CSV = Path('/app/panns_data/class_labels_indices.csv')\n",
        "CKPT_PATH = Path('/app/panns_data/Cnn14_mAP=0.431.pth')\n",
        "assert LABELS_CSV.exists(), 'Missing labels CSV'\n",
        "assert CKPT_PATH.exists(), 'Missing checkpoint file'\n",
        "\n",
        "# Import AudioTagging\n",
        "try:\n",
        "    import panns_inference\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'panns-inference'])\n",
        "    import panns_inference\n",
        "from panns_inference import AudioTagging\n",
        "\n",
        "print('panns_inference version:', getattr(panns_inference, '__version__', 'unknown'))\n",
        "print('AudioTagging public attrs:', [m for m in dir(AudioTagging) if not m.startswith('_')])\n",
        "\n",
        "# Prepare one waveform from curated to test API\n",
        "df_cur = pd.read_csv('train_curated_folds.csv')\n",
        "wav_path = str(Path('train_curated') / df_cur.iloc[0]['fname'])\n",
        "y, sr = librosa.load(wav_path, sr=32000, mono=True)\n",
        "target = 32000 * 10\n",
        "if len(y) >= target:\n",
        "    start = max(0, (len(y) - target)//2)\n",
        "    y = y[start:start+target]\n",
        "else:\n",
        "    y = np.pad(y, (0, target - len(y)))\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device for test:', device)\n",
        "print('Checkpoint path:', CKPT_PATH)\n",
        "print('Using CPU.' if device=='cpu' else 'Using CUDA.')\n",
        "at = AudioTagging(checkpoint_path=str(CKPT_PATH), device=device)\n",
        "\n",
        "def try_call(inp, name):\n",
        "    try:\n",
        "        out = at.inference(inp)\n",
        "        print(f'Call with {name} succeeded; type(out)={type(out)}')\n",
        "        if isinstance(out, dict):\n",
        "            print('  Output keys:', list(out.keys()))\n",
        "            for k in ['embedding', 'clipwise_output', 'tags']:\n",
        "                if k in out:\n",
        "                    v = out[k]\n",
        "                    try:\n",
        "                        print(f'    {k} shape:', getattr(v, 'shape', 'n/a'))\n",
        "                    except Exception:\n",
        "                        pass\n",
        "        else:\n",
        "            # some versions return tuple (clipwise_output, embedding)\n",
        "            try:\n",
        "                print('  Treating as tuple-like, len:', len(out))\n",
        "                for i, v in enumerate(out):\n",
        "                    print(f'    item {i} type={type(v)}, shape={getattr(v, \"shape\", \"n/a\")}')\n",
        "            except Exception as e:\n",
        "                print('  Non-iterable output:', repr(e))\n",
        "    except Exception as e:\n",
        "        print(f'Call with {name} failed:', repr(e))\n",
        "\n",
        "# Try different input shapes\n",
        "try_call(y, '1D waveform (T,)')\n",
        "try_call(np.expand_dims(y, 0), 'batched waveform (1, T)')\n",
        "try_call([y], 'list of waveforms [T]')\n",
        "\n",
        "print('Diagnostics complete. Use the successful calling convention above in the embeddings pipeline.')\n",
        ""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "panns_inference version: 0.1.0\nAudioTagging public attrs: ['inference']\nDevice for test: cpu\nCheckpoint path: /app/panns_data/Cnn14_mAP=0.431.pth\nUsing CPU.\nCheckpoint path: /app/panns_data/Cnn14_mAP=0.431.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU.\nCall with 1D waveform (T,) failed: IndexError('too many indices for tensor of dimension 1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Call with batched waveform (1, T) succeeded; type(out)=<class 'tuple'>\n  Treating as tuple-like, len: 2\n    item 0 type=<class 'numpy.ndarray'>, shape=(1, 527)\n    item 1 type=<class 'numpy.ndarray'>, shape=(1, 2048)\nCall with list of waveforms [T] failed: AttributeError(\"'list' object has no attribute 'dtype'\")\nDiagnostics complete. Use the successful calling convention above in the embeddings pipeline.\n"
          ]
        }
      ]
    },
    {
      "id": "2b8fd828-89bf-43e8-8c7f-872e4e01af62",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Programmatically create a clean, linear notebook: C2_gold_pipeline_final.ipynb\n",
        "import json, sys, subprocess, os, time, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure nbformat/nbconvert are available\n",
        "def pip_install(pkg):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', pkg])\n",
        "    except Exception as e:\n",
        "        print(f'Failed to install {pkg}: {e}')\n",
        "\n",
        "try:\n",
        "    import nbformat\n",
        "except Exception:\n",
        "    pip_install('nbformat')\n",
        "    import nbformat\n",
        "try:\n",
        "    from nbconvert.preprocessors import ExecutePreprocessor\n",
        "except Exception:\n",
        "    pip_install('nbconvert')\n",
        "    from nbconvert.preprocessors import ExecutePreprocessor\n",
        "\n",
        "import nbformat as nbf\n",
        "\n",
        "final_nb_path = Path('C2_gold_pipeline_final.ipynb')\n",
        "\n",
        "# 1) Get plan markdown from current notebook's Cell 0\n",
        "plan_md = None\n",
        "try:\n",
        "    with open('agent_notebook.ipynb', 'r', encoding='utf-8') as f:\n",
        "        cur_nb = nbformat.read(f, as_version=4)\n",
        "    if cur_nb['cells'] and cur_nb['cells'][0]['cell_type'] == 'markdown':\n",
        "        plan_md = cur_nb['cells'][0]['source']\n",
        "except Exception as e:\n",
        "    print('Warning: could not read agent_notebook.ipynb:', e)\n",
        "if plan_md is None:\n",
        "    plan_md = '# Freesound Audio Tagging 2019 \u2014 Gold-Only Plan v2.1 (consolidated)\\nThis notebook is a clean, linear reproduction of the approved plan and winning pipeline.'\n",
        "\n",
        "# 2) Consolidated setup & imports cell (no panns_inference import here to satisfy asset sequencing)\n",
        "cell2 = '''\\\n",
        "# Cell 2: Consolidated setup and imports (single source of truth)\\n\n",
        "import sys, subprocess, os, json, time, warnings\\n\n",
        "from pathlib import Path\\n\n",
        "warnings.filterwarnings('ignore')\\n\n",
        "def pip_install(pkg):\\n\n",
        "    try:\\n\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', pkg])\\n\n",
        "        print(f'Installed: {pkg}')\\n\n",
        "    except Exception as e:\\n\n",
        "        print(f'Failed to install {pkg}: {e}')\\n\n",
        "\\n\n",
        "for pkg in ['soundfile', 'librosa', 'iterative-stratification', 'joblib', 'scikit-learn']:\\n\n",
        "    try:\\n\n",
        "        __import__(pkg.split('==')[0].replace('-', '_'))\\n\n",
        "        print(f'{pkg.split(\"==\")[0]} already available')\\n\n",
        "    except Exception:\\n\n",
        "        pip_install(pkg)\\n\n",
        "\\n\n",
        "import numpy as np\\n\n",
        "import pandas as pd\\n\n",
        "import soundfile as sf\\n\n",
        "import librosa\\n\n",
        "import torch\\n\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\\n\n",
        "from sklearn.linear_model import LogisticRegression\\n\n",
        "from sklearn.multiclass import OneVsRestClassifier\\n\n",
        "from sklearn.preprocessing import StandardScaler\\n\n",
        "from sklearn.pipeline import make_pipeline\\n\n",
        "import joblib\\n\n",
        "BASE = Path('.')\\n\n",
        "np.random.seed(42)\\n\n",
        "'''\n",
        "\n",
        "# 3) Offline PANNs asset preparation\n",
        "cell3 = '''\\\n",
        "# Cell 3: Offline PANNs asset preparation (labels CSV stub + CNN14 checkpoint)\\n\n",
        "from pathlib import Path\\n\n",
        "import urllib.request\\n\n",
        "assets_dir = Path('/app/panns_data')\\n\n",
        "assets_dir.mkdir(parents=True, exist_ok=True)\\n\n",
        "labels_csv = assets_dir / 'class_labels_indices.csv'\\n\n",
        "if not labels_csv.exists():\\n\n",
        "    import csv\\n\n",
        "    with open(labels_csv, 'w', newline='') as f:\\n\n",
        "        w = csv.writer(f)\\n\n",
        "        w.writerow(['index','mid','display_name'])\\n\n",
        "        for i in range(527):\\n\n",
        "            w.writerow([i, f'/m/{i}', f'class_{i:03d}'])\\n\n",
        "    print('Created labels stub at', labels_csv)\\n\n",
        "else:\\n\n",
        "    print('Labels CSV exists at', labels_csv)\\n\n",
        "\\n\n",
        "ckpt_path = assets_dir / 'Cnn14_mAP=0.431.pth'\\n\n",
        "url = 'https://zenodo.org/record/3987831/files/Cnn14_mAP=0.431.pth?download=1'\\n\n",
        "if not ckpt_path.exists() or ckpt_path.stat().st_size == 0:\\n\n",
        "    print('Downloading CNN14 weights ...')\\n\n",
        "    with urllib.request.urlopen(url) as resp, open(ckpt_path, 'wb') as out:\\n\n",
        "        while True:\\n\n",
        "            chunk = resp.read(1<<20)\\n\n",
        "            if not chunk:\\n\n",
        "                break\\n\n",
        "            out.write(chunk)\\n\n",
        "    print('Saved CNN14 to', ckpt_path, 'size:', ckpt_path.stat().st_size)\\n\n",
        "else:\\n\n",
        "    print('CNN14 checkpoint present:', ckpt_path, 'size:', ckpt_path.stat().st_size)\\n\n",
        "'''\n",
        "\n",
        "# 4) Data foundation & CV (no KFold fallback). Define helper funcs once here.\n",
        "cell4 = '''\\\n",
        "# Cell 4: Data foundation, robust label parsing, MLSKF folds (no fallback)\\n\n",
        "BASE = Path('.')\\n\n",
        "df_cur = pd.read_csv(BASE / 'train_curated.csv')\\n\n",
        "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\\n\n",
        "fname_col = 'fname' if 'fname' in df_cur.columns else df_cur.columns[0]\\n\n",
        "labels_col = 'labels' if 'labels' in df_cur.columns else df_cur.columns[-1]\\n\n",
        "class_names = [c for c in df_ss.columns if c != 'fname']\\n\n",
        "label_to_idx = {c:i for i,c in enumerate(class_names)}\\n\n",
        "n_classes = len(class_names)\\n\n",
        "\\n\n",
        "def parse_labels_str(s):\\n\n",
        "    if not isinstance(s, str):\\n\n",
        "        return []\\n\n",
        "    toks = [t.strip() for t in s.replace(';', ',').split(',') if t.strip()]\\n\n",
        "    unknown = [t for t in toks if t not in label_to_idx]\\n\n",
        "    if unknown:\\n\n",
        "        raise ValueError(f'Unknown labels: {unknown[:5]} (total {len(unknown)})')\\n\n",
        "    return toks\\n\n",
        "\\n\n",
        "def encode_tokens(toks):\\n\n",
        "    y = np.zeros(n_classes, dtype=np.float32)\\n\n",
        "    for t in toks:\\n\n",
        "        y[label_to_idx[t]] = 1.0\\n\n",
        "    return y\\n\n",
        "\\n\n",
        "def encode_labels(s):\\n\n",
        "    return encode_tokens(parse_labels_str(s))\\n\n",
        "\\n\n",
        "tokens = df_cur[labels_col].apply(parse_labels_str)\\n\n",
        "Y_cur = np.stack(tokens.apply(encode_tokens).values)\\n\n",
        "mlsk = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n\n",
        "folds = np.full(len(df_cur), -1, dtype=int)\\n\n",
        "for k, (_, val_idx) in enumerate(mlsk.split(df_cur[fname_col].values, Y_cur)):\\n\n",
        "    folds[val_idx] = k\\n\n",
        "assert (folds >= 0).all()\\n\n",
        "df_cur['fold'] = folds\\n\n",
        "df_cur.to_csv('train_curated_folds.csv', index=False)\\n\n",
        "with open('metadata.json', 'w') as f:\\n\n",
        "    json.dump({'class_names': class_names, 'label_to_idx': label_to_idx, 'fname_col': fname_col, 'labels_col': labels_col}, f)\\n\n",
        "print('Saved train_curated_folds.csv and metadata.json; fold counts:')\\n\n",
        "print(df_cur['fold'].value_counts().sort_index())\\n\n",
        "\\n\n",
        "def lwlrap_np(truth, scores):\\n\n",
        "    assert truth.shape == scores.shape\\n\n",
        "    n_samples, n_labels = truth.shape\\n\n",
        "    precisions = np.zeros(n_labels)\\n\n",
        "    labels_per_class = np.maximum(truth.sum(axis=0), 1)\\n\n",
        "    for i in range(n_samples):\\n\n",
        "        pos = np.where(truth[i] > 0)[0]\\n\n",
        "        if pos.size == 0:\\n\n",
        "            continue\\n\n",
        "        ranking = np.argsort(-scores[i])\\n\n",
        "        ranked_truth = truth[i][ranking]\\n\n",
        "        cumsum = np.cumsum(ranked_truth)\\n\n",
        "        pos_rank = np.where(ranked_truth > 0)[0]\\n\n",
        "        prec = cumsum[pos_rank] / (pos_rank + 1)\\n\n",
        "        ranked_labels = ranking[pos_rank]\\n\n",
        "        for lbl, p in zip(ranked_labels, prec):\\n\n",
        "            precisions[lbl] += p\\n\n",
        "    per_class = precisions / labels_per_class\\n\n",
        "    weights = truth.sum(axis=0) / max(truth.sum(), 1)\\n\n",
        "    return float((per_class * weights).sum()), per_class\\n\n",
        "'''\n",
        "\n",
        "# 5) PANNs embeddings + OVR Logistic Regression pipeline (imports panns_inference here, after assets)\n",
        "cell5 = '''\\\n",
        "# Cell 5: PANNs CNN14 embeddings + OVR Logistic Regression (OOF + submission)\\n\n",
        "from pathlib import Path\\n\n",
        "import numpy as np, pandas as pd\\n\n",
        "import librosa, torch, joblib, time\\n\n",
        "from sklearn.linear_model import LogisticRegression\\n\n",
        "from sklearn.multiclass import OneVsRestClassifier\\n\n",
        "from sklearn.preprocessing import StandardScaler\\n\n",
        "from sklearn.pipeline import make_pipeline\\n\n",
        "from panns_inference import AudioTagging\\n\n",
        "\\n\n",
        "BASE = Path('.')\\n\n",
        "SR = 32000\\n\n",
        "CROP_SEC = 10.0\\n\n",
        "EMB_DIM = 2048\\n\n",
        "N_FOLDS = 5\\n\n",
        "CKPT_PATH = Path('/app/panns_data/Cnn14_mAP=0.431.pth')\\n\n",
        "assert CKPT_PATH.exists(), 'CNN14 checkpoint missing; run Cell 3 first.'\\n\n",
        "\\n\n",
        "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\\n\n",
        "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\\n\n",
        "class_names = [c for c in df_ss.columns if c != 'fname']\\n\n",
        "label_to_idx = {c:i for i,c in enumerate(class_names)}\\n\n",
        "n_classes = len(class_names)\\n\n",
        "train_dir = BASE / 'train_curated'\\n\n",
        "test_dir  = BASE / 'test'\\n\n",
        "\\n\n",
        "def load_center_crop_10s(path, sr=SR, crop_sec=CROP_SEC):\\n\n",
        "    y, s = librosa.load(path, sr=sr, mono=True)\\n\n",
        "    target = int(sr * crop_sec)\\n\n",
        "    if len(y) >= target:\\n\n",
        "        start = max(0, (len(y) - target)//2)\\n\n",
        "        y = y[start:start+target]\\n\n",
        "    else:\\n\n",
        "        y = np.pad(y, (0, target-len(y)))\\n\n",
        "    return y.astype(np.float32)\\n\n",
        "\\n\n",
        "emb_cur_path = BASE / 'embeddings_curated.npy'\\n\n",
        "emb_test_path = BASE / 'embeddings_test.npy'\\n\n",
        "\\n\n",
        "def extract_embeddings(file_list, root_dir):\\n\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\\n\n",
        "    at = AudioTagging(checkpoint_path=str(CKPT_PATH), device=device)\\n\n",
        "    X = np.zeros((len(file_list), EMB_DIM), dtype=np.float32)\\n\n",
        "    t0 = time.time()\\n\n",
        "    for i, fname in enumerate(file_list):\\n\n",
        "        y = load_center_crop_10s(str(Path(root_dir) / fname), sr=SR, crop_sec=CROP_SEC)\\n\n",
        "        y_batched = np.expand_dims(y, 0)\\n\n",
        "        with torch.no_grad():\\n\n",
        "            out = at.inference(y_batched)\\n\n",
        "        if isinstance(out, tuple) and len(out) == 2:\\n\n",
        "            embedding = out[1]\\n\n",
        "        elif isinstance(out, dict) and 'embedding' in out:\\n\n",
        "            embedding = out['embedding']\\n\n",
        "        else:\\n\n",
        "            raise RuntimeError(f'Unexpected AudioTagging output: {type(out)}')\\n\n",
        "        X[i] = np.asarray(embedding, dtype=np.float32)[0]\\n\n",
        "        if (i+1) % 200 == 0:\\n\n",
        "            dt = time.time() - t0\\n\n",
        "            print(f'  {i+1}/{len(file_list)} in {dt/60:.1f} min')\\n\n",
        "    return X\\n\n",
        "\\n\n",
        "train_files = df_cur['fname'].values\\n\n",
        "test_files  = df_ss['fname'].values\\n\n",
        "\\n\n",
        "if emb_cur_path.exists() and emb_test_path.exists():\\n\n",
        "    X_cur = np.load(emb_cur_path)\\n\n",
        "    X_test = np.load(emb_test_path)\\n\n",
        "    print('Loaded cached embeddings.')\\n\n",
        "else:\\n\n",
        "    print('Extracting curated embeddings ...')\\n\n",
        "    X_cur = extract_embeddings(train_files, root_dir=BASE/'train_curated')\\n\n",
        "    np.save(emb_cur_path, X_cur)\\n\n",
        "    print('Extracting test embeddings ...')\\n\n",
        "    X_test = extract_embeddings(test_files, root_dir=BASE/'test')\\n\n",
        "    np.save(emb_test_path, X_test)\\n\n",
        "    print('Saved embeddings to disk.')\\n\n",
        "\\n\n",
        "Y_cur = np.stack(df_cur['labels'].apply(encode_labels).values)\\n\n",
        "oof = np.zeros((len(df_cur), n_classes), dtype=np.float32)\\n\n",
        "fold_scores = []\\n\n",
        "for k in range(N_FOLDS):\\n\n",
        "    trn_idx = np.where(df_cur['fold'].values != k)[0]\\n\n",
        "    val_idx = np.where(df_cur['fold'].values == k)[0]\\n\n",
        "    X_tr, X_va = X_cur[trn_idx], X_cur[val_idx]\\n\n",
        "    y_tr, y_va = Y_cur[trn_idx], Y_cur[val_idx]\\n\n",
        "    base_lr = LogisticRegression(solver='lbfgs', max_iter=1000, C=2.0, n_jobs=16, verbose=0)\\n\n",
        "    clf = OneVsRestClassifier(make_pipeline(StandardScaler(with_mean=True, with_std=True), base_lr), n_jobs=-1)\\n\n",
        "    clf.fit(X_tr, y_tr)\\n\n",
        "    proba = clf.predict_proba(X_va)\\n\n",
        "    oof[val_idx] = proba.astype(np.float32)\\n\n",
        "    lw, _ = lwlrap_np(y_va, proba)\\n\n",
        "    fold_scores.append(lw)\\n\n",
        "    print(f'Fold {k} LWLRAP={lw:.4f}')\\n\n",
        "oof_lw, _ = lwlrap_np(Y_cur, oof)\\n\n",
        "print(f'OOF LWLRAP={oof_lw:.4f}; per-fold={fold_scores}')\\n\n",
        "np.save('oof_panns_lr.npy', oof)\\n\n",
        "\\n\n",
        "base_lr_full = LogisticRegression(solver='lbfgs', max_iter=1000, C=2.0, n_jobs=16, verbose=0)\\n\n",
        "clf_full = OneVsRestClassifier(make_pipeline(StandardScaler(with_mean=True, with_std=True), base_lr_full), n_jobs=-1)\\n\n",
        "clf_full.fit(X_cur, Y_cur)\\n\n",
        "test_proba = clf_full.predict_proba(X_test).astype(np.float32)\\n\n",
        "sub = pd.DataFrame(test_proba, columns=class_names)\\n\n",
        "sub.insert(0, 'fname', test_files)\\n\n",
        "sub.to_csv('submission.csv', index=False)\\n\n",
        "print('Saved submission.csv. Shape:', sub.shape)\\n\n",
        "joblib.dump(clf_full, 'ovr_logreg_panns.joblib')\\n\n",
        "print('Saved classifier checkpoint.')\\n\n",
        "'''\n",
        "\n",
        "# Build the new notebook\n",
        "nb = nbf.v4.new_notebook()\n",
        "nb['cells'] = [\n",
        "    nbf.v4.new_markdown_cell(plan_md),\n",
        "    nbf.v4.new_code_cell(cell2),\n",
        "    nbf.v4.new_code_cell(cell3),\n",
        "    nbf.v4.new_code_cell(cell4),\n",
        "    nbf.v4.new_code_cell(cell5),\n",
        "]\n",
        "\n",
        "with open(final_nb_path, 'w', encoding='utf-8') as f:\n",
        "    nbformat.write(nb, f)\n",
        "print('Wrote clean notebook to', final_nb_path)\n",
        "\n",
        "# Clean-room validation: remove cached embeddings and panns assets before executing\n",
        "for p in ['embeddings_curated.npy', 'embeddings_test.npy', 'oof_panns_lr.npy', 'ovr_logreg_panns.joblib']:\n",
        "    fp = Path(p)\n",
        "    if fp.exists():\n",
        "        try:\n",
        "            fp.unlink()\n",
        "            print('Removed cache:', fp)\n",
        "        except Exception as e:\n",
        "            print('Failed to remove', fp, ':', e)\n",
        "assets_dir = Path('/app/panns_data')\n",
        "if assets_dir.exists():\n",
        "    try:\n",
        "        shutil.rmtree(assets_dir)\n",
        "        print('Removed assets dir:', assets_dir)\n",
        "    except Exception as e:\n",
        "        print('Failed to remove assets dir:', e)\n",
        "\n",
        "# Execute the new notebook end-to-end to validate Run All in clean state\n",
        "try:\n",
        "    print('Executing the clean notebook (this may take significant time) ...')\n",
        "    with open(final_nb_path, 'r', encoding='utf-8') as f:\n",
        "        nb_to_run = nbformat.read(f, as_version=4)\n",
        "    ep = ExecutePreprocessor(timeout=36000, kernel_name='python3')\n",
        "    ep.preprocess(nb_to_run, {'metadata': {'path': '.'}})\n",
        "    with open(final_nb_path, 'w', encoding='utf-8') as f:\n",
        "        nbformat.write(nb_to_run, f)\n",
        "    print('Clean notebook executed successfully.')\n",
        "except Exception as e:\n",
        "    print('Execution of the clean notebook failed:', repr(e))\n",
        "    print('Open and Run All manually if needed.')\n",
        "\n",
        "print('Done.')\n",
        ""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote clean notebook to C2_gold_pipeline_final.ipynb\nRemoved cache: embeddings_curated.npy\nRemoved cache: embeddings_test.npy\nRemoved cache: oof_panns_lr.npy\nRemoved cache: ovr_logreg_panns.joblib\nRemoved assets dir: /app/panns_data\nExecuting the clean notebook (this may take significant time) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean notebook executed successfully.\nDone.\n"
          ]
        }
      ]
    },
    {
      "id": "0b3595f2-b6e1-4268-ad05-54b81accaf5f",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# C3 Plan (Revised and Final): De-risked Order, Pure Teacher, Data-Driven Thresholds, and Hygiene\n",
        "\n",
        "Objective: Expand the gold margin on top of the approved C2 baseline (OOF LWLRAP 0.8001) using a rigorously reproducible, low-risk-first sequence. Deliverables: clean, versioned C3 notebooks; ablations per step; improved OOF and final submission.\n",
        "\n",
        "Final priority order (mandatory):\n",
        "1) Multi-Crop Embedding TTA (lowest risk, immediate gain)\n",
        "- Extract K=5 time crops per clip for curated and test: begin, center, end, and two uniform offsets. Average the resulting 2048-d embeddings per clip; cache as embeddings_curated_mc5.npy and embeddings_test_mc5.npy.\n",
        "- Use the same MLSKF splits (seed=42) and retrain the OVR Logistic Regression head on averaged embeddings. Report OOF and per-fold metrics.\n",
        "\n",
        "2) Dual Features (feature fusion)\n",
        "- Augment the 2048-d embeddings with CNN14 clipwise_output (527-d) to form a 2575-d feature vector; re-train LR with identical CV setup.\n",
        "- Cache fused features; update ablation table to show incremental delta from TTA-only to TTA+Fusion.\n",
        "\n",
        "3) Teacher\u2013Student on train_noisy (methodologically pure)\n",
        "- Teacher models: strictly the 5 out-of-fold C2/C3 fold models (no full-data teacher). The teacher prediction for any curated validation fold must exclude that fold\u2019s model to preserve purity. For train_noisy (separate set), use the fold-ensemble average (all 5 folds).\n",
        "- Embed train_noisy with the same TTA settings as curated (K=5), and optionally the fused features if step 2 is adopted. Cache as embeddings_noisy_mc5.npy (and fused variant).\n",
        "- Score train_noisy with the teacher ensemble to obtain per-class probabilities.\n",
        "- Data-driven threshold selection: before filtering, analyze the teacher\u2019s probability distribution per class on train_noisy (histograms/quantiles). Choose thresholds by targeting a precision-first regime with coverage constraints:\n",
        "  - Candidate grids per class: quantiles q \u2208 {0.98, 0.95, 0.90, 0.85} or fixed p \u2208 {0.95, 0.90, 0.85}, pick per-class thresholds to achieve a target retained-rate band (e.g., retain between 20\u201360% of weak positives) while maximizing estimated precision.\n",
        "  - Add new positives (not in weak labels) only at very high confidence (e.g., top 1\u20132% quantile or p \u2265 class-specific high threshold). Cap added positives per clip (\u22643).\n",
        "- Guardrails and logs:\n",
        "  - Log per-class retained counts and retained-rate vs weak labels; fail if median retained-rate < 30% or if any class retains < 10 samples.\n",
        "  - Verify sample_weight propagation: weight each noisy sample by max positive probability (clip to [0.5, 1.0]); log summary stats.\n",
        "  - Abort/raise thresholds if curated OOF drops by >0.01 from the TTA+Fusion baseline.\n",
        "- Retrain LR on curated + filtered noisy (same MLSKF folds for curated; noisy used only in training folds with its sample weights). Re-evaluate curated OOF to validate gains.\n",
        "\n",
        "4) Lightweight Ensemble and Calibration (final polish)\n",
        "- Train several LR heads varying C \u2208 {1.0, 2.0, 4.0} and seed \u2208 {42, 2025}; average probabilities (or rank-average if distributions differ). Keep folds fixed for clean OOF blending.\n",
        "- Per-class temperature scaling: fit temperatures on curated OOF by minimizing BCE; apply to test predictions. Log pre/post calibration metrics.\n",
        "\n",
        "Hygiene, reproducibility, and versioning (mandatory):\n",
        "- Start each C3 milestone from the clean C2 artifact structure. Maintain a linear, 5\u20137 cell notebook per milestone without stale/diagnostic cells. No KFold fallback anywhere.\n",
        "- Versioned notebooks and ablations:\n",
        "  - C3A_TTA.ipynb: Implement K=5 multi-crop TTA on embeddings; report OOF and save oof_tta.npy.\n",
        "  - C3B_Fusion.ipynb: Add 527-d clipwise outputs to form 2575-d features; report OOF and save oof_tta_fusion.npy.\n",
        "  - C3C_TeacherStudent.ipynb: Build pure OOF teacher ensemble, analyze distributions, select data-driven thresholds, filter/weight train_noisy, retrain LR; save artifacts (thresholds.json, oof_tta_fusion_noisy.npy).\n",
        "  - C3D_Ensemble_Calibration.ipynb: Train LR heads with different C/seeds; blend and calibrate; save final oof_ensemble.npy and temperatures.npy.\n",
        "- Artifacts to persist per step: embeddings and/or fused features (.npy), OOF predictions (.npy), per-fold metrics (JSON), selected thresholds per class (JSON), calibration temps (JSON), and final submission.csv.\n",
        "- Determinism: set seeds; pin critical package versions (iterative-stratification, scikit-learn, librosa, panns-inference); log environment versions.\n",
        "- Efficiency: batch panns-inference on GPU (micro-batches 8\u201316), cache all embeddings/features per variant to avoid recompute.\n",
        "\n",
        "Acceptance criteria and safety checks:\n",
        "- Curated OOF must not degrade by >0.01 at any step; if it does, revert the last change or tighten filtering thresholds.\n",
        "- Monitor per-fold LWLRAP and per-class LRAP to prevent collapse on rare classes.\n",
        "- Final deliverable remains linear, offline-first, and re-runnable from a clean state (assets recreated automatically).\n",
        "\n",
        "Next action (C3A): Implement K=5 multi-crop embedding TTA in a clean notebook cloned from C2_gold_pipeline_final.ipynb, retrain the LR head with fixed MLSKF folds, and submit for audit with OOF deltas and cached artifacts."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1186d156-219a-4894-a5d1-8831dd49641b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Programmatically create C3A_TTA_curated.ipynb (K=5 multi-crop TTA embeddings for curated ONLY + OOF LR)\n",
        "import json, sys, subprocess, os, time\n",
        "from pathlib import Path\n",
        "\n",
        "def pip_install(pkg):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', pkg])\n",
        "    except Exception as e:\n",
        "        print('pip install failed for', pkg, ':', e)\n",
        "\n",
        "# Ensure nbformat/nbconvert\n",
        "try:\n",
        "    import nbformat\n",
        "except Exception:\n",
        "    pip_install('nbformat')\n",
        "    import nbformat\n",
        "try:\n",
        "    from nbconvert.preprocessors import ExecutePreprocessor\n",
        "except Exception:\n",
        "    pip_install('nbconvert')\n",
        "    from nbconvert.preprocessors import ExecutePreprocessor\n",
        "import nbformat as nbf\n",
        "\n",
        "nb_path = Path('C3A_TTA_curated.ipynb')\n",
        "\n",
        "# Cell 1: Markdown plan\n",
        "cell1 = \"\"\"\n",
        "# C3A: Multi-Crop Embedding TTA (K=5) \u2014 Curated Only Stage\n",
        "\n",
        "This staged notebook implements K=5 deterministic multi-crop TTA at the embedding level using PANNs CNN14 for curated data only, computes OOF LWLRAP, and caches artifacts.\n",
        "- Crops per clip (target T=10s @ 32kHz): begin, center, end, 25% offset, 75% offset.\n",
        "- Aggregation: mean across the 5 embeddings (no pre-normalization).\n",
        "- CV: fixed 5-fold MLSKF from train_curated_folds.csv (seed=42).\n",
        "- Outputs: embeddings_curated_mc5.npy, metadata_c3a.json; OOF (oof_tta.npy), per-fold LWLRAP.\n",
        "- Test MC5 extraction and submission are deferred to a separate follow-up stage after OOF verification.\n",
        "\"\"\"\n",
        "\n",
        "# Cell 2: Setup and imports (install panns-inference, but do not import yet)\n",
        "cell2 = '''\n",
        "import sys, subprocess, os, json, time, warnings\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings('ignore')\n",
        "def pip_install(pkg):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', pkg])\n",
        "        print(f'Installed: {pkg}')\n",
        "    except Exception as e:\n",
        "        print(f'Failed to install {pkg}: {e}')\n",
        "\n",
        "for pkg in ['soundfile', 'librosa', 'iterative-stratification', 'joblib', 'scikit-learn', 'panns-inference']:\n",
        "    try:\n",
        "        __import__(pkg.split('==')[0].replace('-', '_'))\n",
        "        print(f\"{pkg.split('==')[0]} already available\")\n",
        "    except Exception:\n",
        "        pip_install(pkg)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import torch\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "import joblib\n",
        "BASE = Path('.')\n",
        "np.random.seed(42)\n",
        "print('Versions:', {k:__import__(k).__version__ if hasattr(__import__(k), '__version__') else 'n/a' for k in ['librosa','torch','sklearn']})\n",
        "'''\n",
        "\n",
        "# Cell 3: Offline PANNs assets\n",
        "cell3 = '''\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "assets_dir = Path('/app/panns_data')\n",
        "assets_dir.mkdir(parents=True, exist_ok=True)\n",
        "labels_csv = assets_dir / 'class_labels_indices.csv'\n",
        "if not labels_csv.exists():\n",
        "    import csv\n",
        "    with open(labels_csv, 'w', newline='') as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow(['index','mid','display_name'])\n",
        "        for i in range(527):\n",
        "            w.writerow([i, f'/m/{i}', f'class_{i:03d}'])\n",
        "    print('Created labels stub at', labels_csv)\n",
        "else:\n",
        "    print('Labels CSV exists at', labels_csv)\n",
        "\n",
        "ckpt_path = assets_dir / 'Cnn14_mAP=0.431.pth'\n",
        "url = 'https://zenodo.org/record/3987831/files/Cnn14_mAP=0.431.pth?download=1'\n",
        "if not ckpt_path.exists() or ckpt_path.stat().st_size == 0:\n",
        "    print('Downloading CNN14 weights ...')\n",
        "    with urllib.request.urlopen(url) as resp, open(ckpt_path, 'wb') as out:\n",
        "        while True:\n",
        "            chunk = resp.read(1<<20)\n",
        "            if not chunk:\n",
        "                break\n",
        "            out.write(chunk)\n",
        "    print('Saved CNN14 to', ckpt_path, 'size:', ckpt_path.stat().st_size)\n",
        "else:\n",
        "    print('CNN14 checkpoint present:', ckpt_path, 'size:', ckpt_path.stat().st_size)\n",
        "'''\n",
        "\n",
        "# Cell 4: Load folds, helpers (no rebuild of folds)\n",
        "cell4 = '''\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "import json\n",
        "BASE = Path('.')\n",
        "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
        "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
        "class_names = [c for c in df_ss.columns if c != 'fname']\n",
        "label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "n_classes = len(class_names)\n",
        "\n",
        "def parse_labels_str(s):\n",
        "    if not isinstance(s, str):\n",
        "        return []\n",
        "    toks = [t.strip() for t in s.replace(';', ',').split(',') if t.strip()]\n",
        "    unknown = [t for t in toks if t not in label_to_idx]\n",
        "    if unknown:\n",
        "        raise ValueError(f'Unknown labels: {unknown[:5]} (total {len(unknown)})')\n",
        "    return toks\n",
        "\n",
        "def encode_tokens(toks):\n",
        "    y = np.zeros(n_classes, dtype=np.float32)\n",
        "    for t in toks:\n",
        "        y[label_to_idx[t]] = 1.0\n",
        "    return y\n",
        "\n",
        "def encode_labels(s):\n",
        "    return encode_tokens(parse_labels_str(s))\n",
        "\n",
        "def lwlrap_np(truth, scores):\n",
        "    assert truth.shape == scores.shape\n",
        "    n_samples, n_labels = truth.shape\n",
        "    precisions = np.zeros(n_labels)\n",
        "    labels_per_class = np.maximum(truth.sum(axis=0), 1)\n",
        "    for i in range(n_samples):\n",
        "        pos = np.where(truth[i] > 0)[0]\n",
        "        if pos.size == 0:\n",
        "            continue\n",
        "        ranking = np.argsort(-scores[i])\n",
        "        ranked_truth = truth[i][ranking]\n",
        "        cumsum = np.cumsum(ranked_truth)\n",
        "        pos_rank = np.where(ranked_truth > 0)[0]\n",
        "        prec = cumsum[pos_rank] / (pos_rank + 1)\n",
        "        ranked_labels = ranking[pos_rank]\n",
        "        for lbl, p in zip(ranked_labels, prec):\n",
        "            precisions[lbl] += p\n",
        "    per_class = precisions / labels_per_class\n",
        "    weights = truth.sum(axis=0) / max(truth.sum(), 1)\n",
        "    return float((per_class * weights).sum()), per_class\n",
        "\n",
        "print('Loaded folds and helpers. Fold counts:', df_cur['fold'].value_counts().sort_index().to_dict())\n",
        "'''\n",
        "\n",
        "# Cell 5: Curated-only MC5 embedding extraction (batched crops) and caching\n",
        "cell5 = '''\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, time, json, hashlib\n",
        "import librosa, torch\n",
        "from panns_inference import AudioTagging\n",
        "\n",
        "BASE = Path('.')\n",
        "SR = 32000\n",
        "T_SEC = 10.0\n",
        "T = int(SR * T_SEC)\n",
        "K = 5\n",
        "EMB_DIM = 2048\n",
        "CKPT_PATH = Path('/app/panns_data/Cnn14_mAP=0.431.pth')\n",
        "assert CKPT_PATH.exists(), 'Checkpoint missing; run Cell 3.'\n",
        "\n",
        "def load_audio(path, sr=SR):\n",
        "    y, s = librosa.load(path, sr=sr, mono=True)\n",
        "    return y.astype(np.float32)\n",
        "\n",
        "def crop_starts(L, T):\n",
        "    if L <= T:\n",
        "        return [0, 0, 0, 0, 0]\n",
        "    starts = [0, (L - T)//2, L - T, int(0.25*(L - T)), int(0.75*(L - T))]\n",
        "    starts = [max(0, min(s, L - T)) for s in starts]\n",
        "    return starts\n",
        "\n",
        "def crops_for_wave(y, T):\n",
        "    L = len(y)\n",
        "    starts = crop_starts(L, T)\n",
        "    crops = []\n",
        "    for s in starts:\n",
        "        if L >= T:\n",
        "            crops.append(y[s:s+T])\n",
        "        else:\n",
        "            pad = np.pad(y, (0, T - L))\n",
        "            crops.append(pad)\n",
        "    return np.stack(crops, 0)  # (K, T)\n",
        "\n",
        "def sha1_of_file(path: Path, block_size=1<<20):\n",
        "    h = hashlib.sha1()\n",
        "    with open(path, 'rb') as f:\n",
        "        while True:\n",
        "            b = f.read(block_size)\n",
        "            if not b:\n",
        "                break\n",
        "            h.update(b)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def extract_mc5_embeddings_curated(file_list, root_dir, log_every=100):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    at = AudioTagging(checkpoint_path=str(CKPT_PATH), device=device)\n",
        "    X = np.zeros((len(file_list), EMB_DIM), dtype=np.float32)\n",
        "    t0 = time.time()\n",
        "    for i, fname in enumerate(file_list):\n",
        "        y = load_audio(str(Path(root_dir) / fname), sr=SR)\n",
        "        crops = crops_for_wave(y, T)  # (5, T)\n",
        "        with torch.no_grad():\n",
        "            out = at.inference(crops)  # batched inference on (5, T)\n",
        "        if isinstance(out, tuple) and len(out)==2:\n",
        "            embs = np.asarray(out[1], dtype=np.float32)  # (5, 2048)\n",
        "        elif isinstance(out, dict) and 'embedding' in out:\n",
        "            embs = np.asarray(out['embedding'], dtype=np.float32)\n",
        "        else:\n",
        "            raise RuntimeError('Unexpected AudioTagging output type')\n",
        "        assert embs.ndim == 2 and embs.shape[1] == EMB_DIM, f'Bad embedding shape: {embs.shape}'\n",
        "        X[i] = embs.mean(axis=0)\n",
        "        if (i+1) % log_every == 0:\n",
        "            dt = time.time() - t0\n",
        "            print(f'  Curated MC5: {i+1}/{len(file_list)} in {dt/60:.1f} min')\n",
        "    assert X.shape == (len(file_list), EMB_DIM), f'Output shape mismatch: {X.shape}'\n",
        "    return X\n",
        "\n",
        "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
        "train_files = df_cur['fname'].values\n",
        "\n",
        "emb_cur_mc5_path = BASE / 'embeddings_curated_mc5.npy'\n",
        "if emb_cur_mc5_path.exists():\n",
        "    X_cur = np.load(emb_cur_mc5_path)\n",
        "    print('Loaded cached curated MC5 embeddings.')\n",
        "else:\n",
        "    print('Extracting curated MC5 embeddings ...')\n",
        "    X_cur = extract_mc5_embeddings_curated(train_files, root_dir=BASE/'train_curated', log_every=100)\n",
        "    np.save(emb_cur_mc5_path, X_cur)\n",
        "    print('Saved curated MC5 embeddings.')\n",
        "\n",
        "import torch as _torch, librosa as _librosa\n",
        "ckpt_size = CKPT_PATH.stat().st_size if CKPT_PATH.exists() else None\n",
        "ckpt_sha1 = sha1_of_file(CKPT_PATH) if CKPT_PATH.exists() else None\n",
        "meta = {\n",
        "    'tta': 'mc5',\n",
        "    'sr': SR, 'T_sec': T_SEC, 'T': T,\n",
        "    'crops': 'begin,center,end,25%,75%',\n",
        "    'aggregation': 'mean',\n",
        "    'stage': 'curated_only',\n",
        "    'versions': {'torch': getattr(_torch, '__version__', 'n/a'), 'librosa': getattr(_librosa, '__version__', 'n/a')},\n",
        "    'checkpoint': {'path': str(CKPT_PATH), 'size_bytes': ckpt_size, 'sha1': ckpt_sha1}\n",
        "}\n",
        "with open('metadata_c3a.json', 'w') as f:\n",
        "    json.dump(meta, f)\n",
        "print('metadata_c3a.json written with provenance.')\n",
        "'''\n",
        "\n",
        "# Cell 6: Train LR on curated MC5 embeddings, OOF only (no test/submission in this stage) + Persist metrics\n",
        "cell6 = '''\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, json\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "BASE = Path('.')\n",
        "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
        "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
        "class_names = [c for c in df_ss.columns if c != 'fname']\n",
        "label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "n_classes = len(class_names)\n",
        "\n",
        "def parse_labels_str(s):\n",
        "    if not isinstance(s, str):\n",
        "        return []\n",
        "    toks = [t.strip() for t in s.replace(';', ',').split(',') if t.strip()]\n",
        "    return toks\n",
        "\n",
        "def encode_labels(s):\n",
        "    y = np.zeros(n_classes, dtype=np.float32)\n",
        "    for t in parse_labels_str(s):\n",
        "        if t in label_to_idx:\n",
        "            y[label_to_idx[t]] = 1.0\n",
        "    return y\n",
        "\n",
        "def lwlrap_np(truth, scores):\n",
        "    assert truth.shape == scores.shape\n",
        "    n_samples, n_labels = truth.shape\n",
        "    precisions = np.zeros(n_labels)\n",
        "    labels_per_class = np.maximum(truth.sum(axis=0), 1)\n",
        "    for i in range(n_samples):\n",
        "        pos = np.where(truth[i] > 0)[0]\n",
        "        if pos.size == 0:\n",
        "            continue\n",
        "        ranking = np.argsort(-scores[i])\n",
        "        ranked_truth = truth[i][ranking]\n",
        "        cumsum = np.cumsum(ranked_truth)\n",
        "        pos_rank = np.where(ranked_truth > 0)[0]\n",
        "        prec = cumsum[pos_rank] / (pos_rank + 1)\n",
        "        ranked_labels = ranking[pos_rank]\n",
        "        for lbl, p in zip(ranked_labels, prec):\n",
        "            precisions[lbl] += p\n",
        "    per_class = precisions / labels_per_class\n",
        "    weights = truth.sum(axis=0) / max(truth.sum(), 1)\n",
        "    return float((per_class * weights).sum()), per_class\n",
        "\n",
        "X_cur = np.load('embeddings_curated_mc5.npy')\n",
        "assert X_cur.ndim == 2 and X_cur.shape[1] == 2048, f'Embeddings shape invalid: {X_cur.shape}'\n",
        "Y_cur = np.stack(df_cur['labels'].apply(encode_labels).values).astype(np.float32)\n",
        "\n",
        "oof = np.zeros((len(df_cur), n_classes), dtype=np.float32)\n",
        "fold_scores = []\n",
        "for k in range(5):\n",
        "    trn_idx = np.where(df_cur['fold'].values != k)[0]\n",
        "    val_idx = np.where(df_cur['fold'].values == k)[0]\n",
        "    X_tr, X_va = X_cur[trn_idx], X_cur[val_idx]\n",
        "    y_tr, y_va = Y_cur[trn_idx], Y_cur[val_idx]\n",
        "    base_lr = LogisticRegression(solver='lbfgs', max_iter=1000, C=2.0, n_jobs=16, verbose=0)\n",
        "    clf = OneVsRestClassifier(make_pipeline(StandardScaler(with_mean=True, with_std=True), base_lr), n_jobs=-1)\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    proba = clf.predict_proba(X_va)\n",
        "    oof[val_idx] = proba.astype(np.float32)\n",
        "    lw, _ = lwlrap_np(y_va, proba)\n",
        "    fold_scores.append(lw)\n",
        "    print(f'Fold {k} LWLRAP (MC5 curated)={lw:.4f}')\n",
        "oof_lw, _ = lwlrap_np(Y_cur, oof)\n",
        "print(f'OOF LWLRAP (MC5 curated)={oof_lw:.4f}; per-fold={fold_scores}')\n",
        "np.save('oof_tta.npy', oof)\n",
        "\n",
        "# Mandatory metrics persistence\n",
        "BASELINE = 0.8001\n",
        "delta = float(oof_lw - BASELINE)\n",
        "ck_sha1 = None\n",
        "try:\n",
        "    with open('metadata_c3a.json', 'r') as f:\n",
        "        meta = json.load(f)\n",
        "    ck_sha1 = meta.get('checkpoint', {}).get('sha1')\n",
        "except Exception:\n",
        "    pass\n",
        "metrics = {\n",
        "    'stage': 'C3A_curated_mc5',\n",
        "    'oof_lwlrap': float(oof_lw),\n",
        "    'per_fold_lwlrap': [float(x) for x in fold_scores],\n",
        "    'delta_vs_c2_baseline': delta,\n",
        "    'tta_scheme': 'mc5_mean',\n",
        "    'checkpoint_sha1': ck_sha1\n",
        "}\n",
        "with open('metrics_c3a_curated.json', 'w') as f:\n",
        "    json.dump(metrics, f)\n",
        "print('Persisted metrics to metrics_c3a_curated.json:', metrics)\n",
        "'''\n",
        "\n",
        "# Build notebook\n",
        "nb = nbf.v4.new_notebook()\n",
        "nb['cells'] = [\n",
        "    nbf.v4.new_markdown_cell(cell1),\n",
        "    nbf.v4.new_code_cell(cell2),\n",
        "    nbf.v4.new_code_cell(cell3),\n",
        "    nbf.v4.new_code_cell(cell4),\n",
        "    nbf.v4.new_code_cell(cell5),\n",
        "    nbf.v4.new_code_cell(cell6),\n",
        "]\n",
        "\n",
        "with open(nb_path, 'w', encoding='utf-8') as f:\n",
        "    nbformat.write(nb, f)\n",
        "print('Wrote C3A_TTA_curated.ipynb (curated-only stage) with metrics persistence')\n",
        "\n",
        "print('Note: Not auto-executing due to previous long runtimes. Run this notebook with Run All to extract curated MC5 and compute OOF, then we will add test extraction in a follow-up stage.')\n",
        ""
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote C3A_TTA_curated.ipynb (curated-only stage) with metrics persistence\nNote: Not auto-executing due to previous long runtimes. Run this notebook with Run All to extract curated MC5 and compute OOF, then we will add test extraction in a follow-up stage.\n"
          ]
        }
      ]
    },
    {
      "id": "d105b72b-8469-49ee-b1f5-d3acee9e4cf4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Execute the staged C3A_TTA_curated.ipynb to produce MC5 curated embeddings and OOF\n",
        "import sys, subprocess, time\n",
        "from pathlib import Path\n",
        "\n",
        "def pip_install(pkg):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', pkg])\n",
        "    except Exception as e:\n",
        "        print('pip install failed for', pkg, ':', e)\n",
        "\n",
        "try:\n",
        "    import nbformat\n",
        "except Exception:\n",
        "    pip_install('nbformat')\n",
        "    import nbformat\n",
        "\n",
        "try:\n",
        "    from nbconvert.preprocessors import ExecutePreprocessor\n",
        "except Exception:\n",
        "    pip_install('nbconvert')\n",
        "    from nbconvert.preprocessors import ExecutePreprocessor\n",
        "\n",
        "nb_path = Path('C3A_TTA_curated.ipynb')\n",
        "assert nb_path.exists(), 'C3A_TTA_curated.ipynb not found. Generate it first (Cell 12).'\n",
        "\n",
        "print('Executing C3A_TTA_curated.ipynb ... This may take significant time (GPU-accelerated).')\n",
        "t0 = time.time()\n",
        "with open(nb_path, 'r', encoding='utf-8') as f:\n",
        "    nb_to_run = nbformat.read(f, as_version=4)\n",
        "ep = ExecutePreprocessor(timeout=36000, kernel_name='python3')\n",
        "ep.preprocess(nb_to_run, {'metadata': {'path': '.'}})\n",
        "with open(nb_path, 'w', encoding='utf-8') as f:\n",
        "    nbformat.write(nb_to_run, f)\n",
        "dt = time.time() - t0\n",
        "print(f'Notebook executed successfully in {dt/60:.1f} minutes.')\n",
        "\n",
        "# Quick artifact check\n",
        "for p in ['embeddings_curated_mc5.npy', 'oof_tta.npy', 'metadata_c3a.json']:\n",
        "    print(p, 'exists' if Path(p).exists() else 'MISSING')\n",
        ""
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing C3A_TTA_curated.ipynb ... This may take significant time (GPU-accelerated).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notebook executed successfully in 1.3 minutes.\nembeddings_curated_mc5.npy exists\noof_tta.npy exists\nmetadata_c3a.json exists\n"
          ]
        }
      ]
    },
    {
      "id": "007614db-d699-4cea-99cd-7dd3cb458e81",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# C3A Curated TTA: Verify and/or persist metrics (safety check)\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "metrics_path = Path('metrics_c3a_curated.json')\n",
        "print('metrics_c3a_curated.json exists:', metrics_path.exists())\n",
        "if metrics_path.exists():\n",
        "    with open(metrics_path, 'r') as f:\n",
        "        metrics = json.load(f)\n",
        "    print('Loaded metrics:', metrics)\n",
        "else:\n",
        "    # Fallback: compute OOF LWLRAP from oof_tta.npy and persist metrics now\n",
        "    print('metrics_c3a_curated.json missing. Computing from oof_tta.npy and persisting ...')\n",
        "    oof_path = Path('oof_tta.npy')\n",
        "    assert oof_path.exists(), 'Missing oof_tta.npy to compute metrics.'\n",
        "    oof = np.load(oof_path)\n",
        "    df_cur = pd.read_csv('train_curated_folds.csv')\n",
        "    df_ss  = pd.read_csv('sample_submission.csv')\n",
        "    class_names = [c for c in df_ss.columns if c != 'fname']\n",
        "    label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "    n_classes = len(class_names)\n",
        "    def parse_labels_str(s):\n",
        "        if not isinstance(s, str):\n",
        "            return []\n",
        "        return [t.strip() for t in s.replace(';', ',').split(',') if t.strip()]\n",
        "    def encode_labels(s):\n",
        "        y = np.zeros(n_classes, dtype=np.float32)\n",
        "        for t in parse_labels_str(s):\n",
        "            if t in label_to_idx:\n",
        "                y[label_to_idx[t]] = 1.0\n",
        "        return y\n",
        "    def lwlrap_np(truth, scores):\n",
        "        assert truth.shape == scores.shape\n",
        "        n_samples, n_labels = truth.shape\n",
        "        precisions = np.zeros(n_labels)\n",
        "        labels_per_class = np.maximum(truth.sum(axis=0), 1)\n",
        "        for i in range(n_samples):\n",
        "            pos = np.where(truth[i] > 0)[0]\n",
        "            if pos.size == 0:\n",
        "                continue\n",
        "            ranking = np.argsort(-scores[i])\n",
        "            ranked_truth = truth[i][ranking]\n",
        "            cumsum = np.cumsum(ranked_truth)\n",
        "            pos_rank = np.where(ranked_truth > 0)[0]\n",
        "            prec = cumsum[pos_rank] / (pos_rank + 1)\n",
        "            ranked_labels = ranking[pos_rank]\n",
        "            for lbl, p in zip(ranked_labels, prec):\n",
        "                precisions[lbl] += p\n",
        "        per_class = precisions / labels_per_class\n",
        "        weights = truth.sum(axis=0) / max(truth.sum(), 1)\n",
        "        return float((per_class * weights).sum()), per_class\n",
        "    Y_cur = np.stack(df_cur['labels'].apply(encode_labels).values).astype(np.float32)\n",
        "    oof_lw, _ = lwlrap_np(Y_cur, oof)\n",
        "    BASELINE = 0.8001\n",
        "    delta = float(oof_lw - BASELINE)\n",
        "    # Try to include checkpoint sha1 if available\n",
        "    ck_sha1 = None\n",
        "    try:\n",
        "        with open('metadata_c3a.json', 'r') as f:\n",
        "            meta = json.load(f)\n",
        "        ck_sha1 = meta.get('checkpoint', {}).get('sha1')\n",
        "    except Exception:\n",
        "        pass\n",
        "    metrics = {\n",
        "        'stage': 'C3A_curated_mc5',\n",
        "        'oof_lwlrap': float(oof_lw),\n",
        "        'per_fold_lwlrap': None,\n",
        "        'delta_vs_c2_baseline': delta,\n",
        "        'tta_scheme': 'mc5_mean',\n",
        "        'checkpoint_sha1': ck_sha1\n",
        "    }\n",
        "    with open(metrics_path, 'w') as f:\n",
        "        json.dump(metrics, f)\n",
        "    print('Persisted fallback metrics to metrics_c3a_curated.json:', metrics)\n",
        "\n",
        "# Display quick summary\n",
        "print('Artifacts present:', {\n",
        "    'embeddings_curated_mc5.npy': Path('embeddings_curated_mc5.npy').exists(),\n",
        "    'oof_tta.npy': Path('oof_tta.npy').exists(),\n",
        "    'metadata_c3a.json': Path('metadata_c3a.json').exists(),\n",
        "    'metrics_c3a_curated.json': metrics_path.exists(),\n",
        "})"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metrics_c3a_curated.json exists: True\nLoaded metrics: {'stage': 'C3A_curated_mc5', 'oof_lwlrap': 0.8049036654978639, 'per_fold_lwlrap': [0.8125134119899252, 0.8058232379476366, 0.8127316131414057, 0.8160070710672407, 0.7809753987693859], 'delta_vs_c2_baseline': 0.004803665497863818, 'tta_scheme': 'mc5_mean', 'checkpoint_sha1': '5f73e32676afd7a763ddec6693d975be16859f90'}\nArtifacts present: {'embeddings_curated_mc5.npy': True, 'oof_tta.npy': True, 'metadata_c3a.json': True, 'metrics_c3a_curated.json': True}\n"
          ]
        }
      ]
    },
    {
      "id": "9ba5a3f0-a86f-441f-b7e4-0cfb29021b9b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# C3A Test MC5: Batched 5-crop embeddings for test, fit LR on curated MC5, predict test, write submission.csv (train-test parity)\n",
        "import os, sys, subprocess, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa, torch\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "def pip_install(pkg):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', pkg])\n",
        "    except Exception as e:\n",
        "        print('pip install failed for', pkg, ':', e)\n",
        "\n",
        "# Ensure panns-inference is available\n",
        "try:\n",
        "    import panns_inference  # noqa: F401\n",
        "except Exception:\n",
        "    pip_install('panns-inference')\n",
        "    import panns_inference  # noqa: F401\n",
        "from panns_inference import AudioTagging\n",
        "\n",
        "BASE = Path('.')\n",
        "SR = 32000\n",
        "T_SEC = 10.0\n",
        "T = int(SR * T_SEC)\n",
        "K = 5  # enforce MC5 for strict train-test parity\n",
        "EMB_DIM = 2048\n",
        "CKPT_PATH = Path('/app/panns_data/Cnn14_mAP=0.431.pth')\n",
        "assert CKPT_PATH.exists(), 'CNN14 checkpoint missing. Re-run the C3A asset cell if needed.'\n",
        "\n",
        "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
        "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
        "class_names = [c for c in df_ss.columns if c != 'fname']\n",
        "n_classes = len(class_names)\n",
        "\n",
        "# Load curated MC5 embeddings (training uses MC5 embeddings per plan)\n",
        "emb_cur_mc5_path = BASE / 'embeddings_curated_mc5.npy'\n",
        "assert emb_cur_mc5_path.exists(), 'embeddings_curated_mc5.npy not found. Run curated MC5 extraction first.'\n",
        "X_cur = np.load(emb_cur_mc5_path)\n",
        "assert X_cur.ndim == 2 and X_cur.shape[1] == EMB_DIM\n",
        "\n",
        "def load_audio(path, sr=SR):\n",
        "    y, s = librosa.load(path, sr=sr, mono=True)\n",
        "    return y.astype(np.float32)\n",
        "\n",
        "def crop_starts(L, T, K):\n",
        "    if L <= T:\n",
        "        return [0]*K\n",
        "    if K == 3:\n",
        "        starts = [0, (L - T)//2, L - T]\n",
        "    else:\n",
        "        starts = [0, (L - T)//2, L - T, int(0.25*(L - T)), int(0.75*(L - T))]\n",
        "    return [max(0, min(s, L - T)) for s in starts]\n",
        "\n",
        "def crops_for_wave(y, T, K):\n",
        "    L = len(y)\n",
        "    starts = crop_starts(L, T, K)\n",
        "    crops = []\n",
        "    if L >= T:\n",
        "        for s in starts:\n",
        "            crops.append(y[s:s+T])\n",
        "    else:\n",
        "        pad = np.pad(y, (0, T - L))\n",
        "        crops = [pad for _ in range(K)]\n",
        "    return np.stack(crops, 0)  # (K, T)\n",
        "\n",
        "def extract_mc_embeddings_batched(file_list, root_dir, K=5, batch_size=16, log_every=200):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f'Device: {device.upper()} | Batch size: {batch_size} | K={K}')\n",
        "    at = AudioTagging(checkpoint_path=str(CKPT_PATH), device=device)\n",
        "    N = len(file_list)\n",
        "    X = np.zeros((N, EMB_DIM), dtype=np.float32)\n",
        "    t0 = time.time()\n",
        "    for start_idx in range(0, N, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, N)\n",
        "        batch_files = file_list[start_idx:end_idx]\n",
        "        # Load and crop all in batch\n",
        "        batch_crops = []  # (B*K, T)\n",
        "        for fname in batch_files:\n",
        "            y = load_audio(str(Path(root_dir) / fname), sr=SR)\n",
        "            ck = crops_for_wave(y, T, K)  # (K, T)\n",
        "            batch_crops.append(ck)\n",
        "        batch_crops = np.concatenate(batch_crops, axis=0)  # (B*K, T)\n",
        "        with torch.no_grad():\n",
        "            out = at.inference(batch_crops)\n",
        "        if isinstance(out, tuple) and len(out)==2:\n",
        "            embs = np.asarray(out[1], dtype=np.float32)  # (B*K, 2048)\n",
        "        elif isinstance(out, dict) and 'embedding' in out:\n",
        "            embs = np.asarray(out['embedding'], dtype=np.float32)\n",
        "        else:\n",
        "            raise RuntimeError('Unexpected AudioTagging output type')\n",
        "        # reshape to (B, K, EMB_DIM) and mean over K\n",
        "        B = end_idx - start_idx\n",
        "        embs = embs.reshape(B, K, EMB_DIM).mean(axis=1)\n",
        "        X[start_idx:end_idx] = embs\n",
        "        if (end_idx) % log_every == 0 or end_idx == N:\n",
        "            dt = time.time() - t0\n",
        "            print(f'  Test MC{K}: {end_idx}/{N} in {dt/60:.1f} min')\n",
        "    return X\n",
        "\n",
        "# Extract/Load MC5 test embeddings\n",
        "emb_test_mc_path = BASE / f'embeddings_test_mc{K}.npy'\n",
        "test_files = df_ss['fname'].values\n",
        "if emb_test_mc_path.exists():\n",
        "    X_test = np.load(emb_test_mc_path)\n",
        "    print(f'Loaded cached test MC{K} embeddings.')\n",
        "else:\n",
        "    print(f'Extracting test MC{K} embeddings (batched) ...')\n",
        "    # Heuristic batch size: larger on CPU to leverage MKL, moderate on GPU to fit memory\n",
        "    bs = 64 if not torch.cuda.is_available() else 16\n",
        "    X_test = extract_mc_embeddings_batched(test_files, root_dir=BASE/'test', K=K, batch_size=bs, log_every=256)\n",
        "    np.save(emb_test_mc_path, X_test)\n",
        "    print(f'Saved test MC{K} embeddings to {emb_test_mc_path}.')\n",
        "\n",
        "# Train LR on curated MC5 embeddings and predict test (MC5) with serial n_jobs=1 to avoid parallelism bugs\n",
        "def encode_labels_row(s):\n",
        "    y = np.zeros(n_classes, dtype=np.float32)\n",
        "    if isinstance(s, str):\n",
        "        for t in s.replace(';', ',').split(','):\n",
        "            t = t.strip()\n",
        "            if t and t in class_names:\n",
        "                y[class_names.index(t)] = 1.0\n",
        "    return y\n",
        "\n",
        "Y_cur = np.stack(df_cur['labels'].apply(encode_labels_row).values)\n",
        "base_lr_full = LogisticRegression(solver='lbfgs', max_iter=1000, C=2.0, n_jobs=1, verbose=0)\n",
        "clf_full = OneVsRestClassifier(make_pipeline(StandardScaler(with_mean=True, with_std=True), base_lr_full), n_jobs=1)\n",
        "clf_full.fit(X_cur, Y_cur)\n",
        "test_proba = clf_full.predict_proba(X_test).astype(np.float32)\n",
        "sub = pd.DataFrame(test_proba, columns=class_names)\n",
        "sub.insert(0, 'fname', test_files)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(f'Saved submission.csv (MC{K} TTA, batched, n_jobs=1). Shape:', sub.shape)\n",
        ""
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded cached test MC5 embeddings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (MC5 TTA, batched, n_jobs=1). Shape: (3361, 81)\n"
          ]
        }
      ]
    },
    {
      "id": "b6de8251-9f68-4af5-b2dc-481b1ec6465c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# C3B: Dual Features Fusion (MC5) \u2014 concatenate 2048-d embeddings with 527-d clipwise outputs; retrain LR; submit\n",
        "import os, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa, torch\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import json\n",
        "\n",
        "# Ensure panns-inference is available\n",
        "try:\n",
        "    import panns_inference  # noqa: F401\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'panns-inference'])\n",
        "    import panns_inference  # noqa: F401\n",
        "from panns_inference import AudioTagging\n",
        "\n",
        "BASE = Path('.')\n",
        "SR = 32000\n",
        "T_SEC = 10.0\n",
        "T = int(SR * T_SEC)\n",
        "K = 5  # MC5 parity\n",
        "EMB_DIM = 2048\n",
        "CLIP_DIM = 527\n",
        "CKPT_PATH = Path('/app/panns_data/Cnn14_mAP=0.431.pth')\n",
        "assert CKPT_PATH.exists(), 'CNN14 checkpoint missing. Prepare assets first.'\n",
        "\n",
        "# Data and class schema\n",
        "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
        "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
        "class_names = [c for c in df_ss.columns if c != 'fname']\n",
        "label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "n_classes = len(class_names)\n",
        "train_files = df_cur['fname'].values\n",
        "test_files  = df_ss['fname'].values\n",
        "\n",
        "# Existing MC5 embeddings from C3A\n",
        "X_cur_path = BASE / 'embeddings_curated_mc5.npy'\n",
        "X_test_path = BASE / 'embeddings_test_mc5.npy'\n",
        "assert X_cur_path.exists() and X_test_path.exists(), 'MC5 embeddings missing. Run C3A first.'\n",
        "X_cur = np.load(X_cur_path)\n",
        "X_test = np.load(X_test_path)\n",
        "assert X_cur.shape[1] == EMB_DIM and X_test.shape[1] == EMB_DIM\n",
        "\n",
        "def load_audio(path, sr=SR):\n",
        "    y, s = librosa.load(path, sr=sr, mono=True)\n",
        "    return y.astype(np.float32)\n",
        "\n",
        "def crop_starts(L, T, K=5):\n",
        "    if L <= T:\n",
        "        return [0]*K\n",
        "    return [0, (L - T)//2, L - T, int(0.25*(L - T)), int(0.75*(L - T))]\n",
        "\n",
        "def crops_for_wave(y, T, K=5):\n",
        "    L = len(y)\n",
        "    starts = crop_starts(L, T, K)\n",
        "    crops = []\n",
        "    if L >= T:\n",
        "        for s in starts:\n",
        "            s = max(0, min(s, L - T))\n",
        "            crops.append(y[s:s+T])\n",
        "    else:\n",
        "        pad = np.pad(y, (0, T - L))\n",
        "        crops = [pad for _ in range(K)]\n",
        "    return np.stack(crops, 0)  # (K, T)\n",
        "\n",
        "def extract_mc5_dual_features(file_list, root_dir, batch_size=64, log_every=256):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f'Device: {device.upper()} | Batch size: {batch_size} | Dual dims: emb={EMB_DIM}, clip={CLIP_DIM}')\n",
        "    at = AudioTagging(checkpoint_path=str(CKPT_PATH), device=device)\n",
        "    N = len(file_list)\n",
        "    emb = np.zeros((N, EMB_DIM), dtype=np.float32)\n",
        "    clip = np.zeros((N, CLIP_DIM), dtype=np.float32)\n",
        "    t0 = time.time()\n",
        "    for start_idx in range(0, N, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, N)\n",
        "        batch_files = file_list[start_idx:end_idx]\n",
        "        batch_crops = []  # (B*K, T)\n",
        "        for fname in batch_files:\n",
        "            y = load_audio(str(Path(root_dir) / fname), sr=SR)\n",
        "            ck = crops_for_wave(y, T, K)\n",
        "            batch_crops.append(ck)\n",
        "        batch_crops = np.concatenate(batch_crops, axis=0)\n",
        "        with torch.no_grad():\n",
        "            out = at.inference(batch_crops)\n",
        "        if isinstance(out, tuple) and len(out)==2:\n",
        "            clip_bk = np.asarray(out[0], dtype=np.float32)  # (B*K, 527)\n",
        "            emb_bk  = np.asarray(out[1], dtype=np.float32)  # (B*K, 2048)\n",
        "        elif isinstance(out, dict):\n",
        "            clip_bk = np.asarray(out.get('clipwise_output'), dtype=np.float32)\n",
        "            emb_bk  = np.asarray(out.get('embedding'), dtype=np.float32)\n",
        "        else:\n",
        "            raise RuntimeError('Unexpected AudioTagging output type')\n",
        "        B = end_idx - start_idx\n",
        "        emb_b = emb_bk.reshape(B, K, EMB_DIM).mean(axis=1)\n",
        "        clip_b = clip_bk.reshape(B, K, CLIP_DIM).mean(axis=1)\n",
        "        emb[start_idx:end_idx] = emb_b\n",
        "        clip[start_idx:end_idx] = clip_b\n",
        "        if end_idx % log_every == 0 or end_idx == N:\n",
        "            dt = time.time() - t0\n",
        "            print(f'  Dual MC5: {end_idx}/{N} in {dt/60:.1f} min')\n",
        "    return emb, clip\n",
        "\n",
        "# Paths for clipwise features\n",
        "CL_cur_path = BASE / 'clipwise_curated_mc5.npy'\n",
        "CL_test_path = BASE / 'clipwise_test_mc5.npy'\n",
        "\n",
        "if CL_cur_path.exists():\n",
        "    CL_cur = np.load(CL_cur_path)\n",
        "    print('Loaded cached clipwise_curated_mc5.npy')\n",
        "else:\n",
        "    print('Extracting curated dual features (MC5) ...')\n",
        "    # We already have embeddings; but for consistency, re-extract both and keep embedding for sanity check\n",
        "    _, CL_cur = extract_mc5_dual_features(train_files, BASE/'train_curated', batch_size=64, log_every=200)\n",
        "    np.save(CL_cur_path, CL_cur)\n",
        "    print('Saved clipwise_curated_mc5.npy')\n",
        "\n",
        "if CL_test_path.exists():\n",
        "    CL_test = np.load(CL_test_path)\n",
        "    print('Loaded cached clipwise_test_mc5.npy')\n",
        "else:\n",
        "    print('Extracting test dual features (MC5) ...')\n",
        "    _, CL_test = extract_mc5_dual_features(test_files, BASE/'test', batch_size=64, log_every=256)\n",
        "    np.save(CL_test_path, CL_test)\n",
        "    print('Saved clipwise_test_mc5.npy')\n",
        "\n",
        "assert CL_cur.shape[0] == X_cur.shape[0] and CL_test.shape[0] == X_test.shape[0]\n",
        "assert CL_cur.shape[1] == CLIP_DIM and CL_test.shape[1] == CLIP_DIM\n",
        "\n",
        "# Fuse features: [embeddings | clipwise]\n",
        "X_cur_fused = np.concatenate([X_cur, CL_cur], axis=1)\n",
        "X_test_fused = np.concatenate([X_test, CL_test], axis=1)\n",
        "print('Fused shapes:', X_cur_fused.shape, X_test_fused.shape)\n",
        "assert X_cur_fused.shape[1] == EMB_DIM + CLIP_DIM == 2575\n",
        "\n",
        "# Encode labels\n",
        "def encode_labels_row(s):\n",
        "    y = np.zeros(n_classes, dtype=np.float32)\n",
        "    if isinstance(s, str):\n",
        "        for t in s.replace(';', ',').split(','):\n",
        "            t = t.strip()\n",
        "            if t and t in label_to_idx:\n",
        "                y[label_to_idx[t]] = 1.0\n",
        "    return y\n",
        "Y_cur = np.stack(df_cur['labels'].apply(encode_labels_row).values)\n",
        "\n",
        "# OOF with fused features\n",
        "def lwlrap_np(truth, scores):\n",
        "    assert truth.shape == scores.shape\n",
        "    n_samples, n_labels = truth.shape\n",
        "    precisions = np.zeros(n_labels)\n",
        "    labels_per_class = np.maximum(truth.sum(axis=0), 1)\n",
        "    for i in range(n_samples):\n",
        "        pos = np.where(truth[i] > 0)[0]\n",
        "        if pos.size == 0:\n",
        "            continue\n",
        "        ranking = np.argsort(-scores[i])\n",
        "        ranked_truth = truth[i][ranking]\n",
        "        cumsum = np.cumsum(ranked_truth)\n",
        "        pos_rank = np.where(ranked_truth > 0)[0]\n",
        "        prec = cumsum[pos_rank] / (pos_rank + 1)\n",
        "        ranked_labels = ranking[pos_rank]\n",
        "        for lbl, p in zip(ranked_labels, prec):\n",
        "            precisions[lbl] += p\n",
        "    per_class = precisions / labels_per_class\n",
        "    weights = truth.sum(axis=0) / max(truth.sum(), 1)\n",
        "    return float((per_class * weights).sum()), per_class\n",
        "\n",
        "oof = np.zeros((len(df_cur), n_classes), dtype=np.float32)\n",
        "fold_scores = []\n",
        "for k in range(5):\n",
        "    trn_idx = np.where(df_cur['fold'].values != k)[0]\n",
        "    val_idx = np.where(df_cur['fold'].values == k)[0]\n",
        "    X_tr, X_va = X_cur_fused[trn_idx], X_cur_fused[val_idx]\n",
        "    y_tr, y_va = Y_cur[trn_idx], Y_cur[val_idx]\n",
        "    # Slightly stronger regularization to reduce overfit with higher dim\n",
        "    base_lr = LogisticRegression(solver='lbfgs', max_iter=1000, C=1.0, n_jobs=16, verbose=0)\n",
        "    clf = OneVsRestClassifier(make_pipeline(StandardScaler(with_mean=True, with_std=True), base_lr), n_jobs=-1)\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    proba = clf.predict_proba(X_va)\n",
        "    oof[val_idx] = proba.astype(np.float32)\n",
        "    lw, _ = lwlrap_np(y_va, proba)\n",
        "    fold_scores.append(lw)\n",
        "    print(f'Fold {k} LWLRAP (Fusion MC5)={lw:.4f}')\n",
        "oof_lw, _ = lwlrap_np(Y_cur, oof)\n",
        "print(f'OOF LWLRAP (Fusion MC5)={oof_lw:.4f}; per-fold={fold_scores}')\n",
        "np.save('oof_tta_fusion.npy', oof)\n",
        "\n",
        "# Persist metrics\n",
        "BASELINE_C3A = 0.8049036654978639\n",
        "metrics = {\n",
        "    'stage': 'C3B_fusion_mc5',\n",
        "    'oof_lwlrap': float(oof_lw),\n",
        "    'per_fold_lwlrap': [float(x) for x in fold_scores],\n",
        "    'delta_vs_c3a_baseline': float(oof_lw - BASELINE_C3A),\n",
        "    'tta_scheme': 'mc5_mean',\n",
        "    'feature_dims': {'embedding': EMB_DIM, 'clipwise': CLIP_DIM, 'fused': EMB_DIM+CLIP_DIM}\n",
        "}\n",
        "with open('metrics_c3b_curated.json', 'w') as f:\n",
        "    json.dump(metrics, f)\n",
        "print('Persisted metrics to metrics_c3b_curated.json:', metrics)\n",
        "\n",
        "# Train full model on fused features and predict test\n",
        "base_lr_full = LogisticRegression(solver='lbfgs', max_iter=1000, C=1.0, n_jobs=16, verbose=0)\n",
        "clf_full = OneVsRestClassifier(make_pipeline(StandardScaler(with_mean=True, with_std=True), base_lr_full), n_jobs=-1)\n",
        "clf_full.fit(X_cur_fused, Y_cur)\n",
        "test_proba = clf_full.predict_proba(X_test_fused).astype(np.float32)\n",
        "sub = pd.DataFrame(test_proba, columns=class_names)\n",
        "sub.insert(0, 'fname', test_files)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Fusion MC5). Shape:', sub.shape)\n",
        ""
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting curated dual features (MC5) ...\nDevice: CPU | Batch size: 64 | Dual dims: emb=2048, clip=527\nCheckpoint path: /app/panns_data/Cnn14_mAP=0.431.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 1600/4970 in 9.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 3200/4970 in 19.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 4800/4970 in 29.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 4970/4970 in 30.5 min\nSaved clipwise_curated_mc5.npy\nExtracting test dual features (MC5) ...\nDevice: CPU | Batch size: 64 | Dual dims: emb=2048, clip=527\nCheckpoint path: /app/panns_data/Cnn14_mAP=0.431.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 256/3361 in 1.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 512/3361 in 3.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 768/3361 in 5.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 1024/3361 in 7.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 1280/3361 in 9.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 1536/3361 in 12.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 1792/3361 in 14.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 2048/3361 in 16.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 2304/3361 in 17.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 2560/3361 in 18.9 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 2816/3361 in 20.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 3072/3361 in 21.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 3328/3361 in 23.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Dual MC5: 3361/3361 in 23.5 min\nSaved clipwise_test_mc5.npy\nFused shapes: (4970, 2575) (3361, 2575)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 LWLRAP (Fusion MC5)=0.8106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 LWLRAP (Fusion MC5)=0.8128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 LWLRAP (Fusion MC5)=0.8065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 LWLRAP (Fusion MC5)=0.8059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 LWLRAP (Fusion MC5)=0.7733\nOOF LWLRAP (Fusion MC5)=0.8017; per-fold=[0.8106072890540544, 0.8127845900116647, 0.8064642232857031, 0.8059466790179934, 0.773337002183789]\nPersisted metrics to metrics_c3b_curated.json: {'stage': 'C3B_fusion_mc5', 'oof_lwlrap': 0.8017233014450034, 'per_fold_lwlrap': [0.8106072890540544, 0.8127845900116647, 0.8064642232857031, 0.8059466790179934, 0.773337002183789], 'delta_vs_c3a_baseline': -0.003180364052860485, 'tta_scheme': 'mc5_mean', 'feature_dims': {'embedding': 2048, 'clipwise': 527, 'fused': 2575}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (Fusion MC5). Shape: (3361, 81)\n"
          ]
        }
      ]
    },
    {
      "id": "fa33398e-c38a-4c63-9a0d-0230f3ee68e6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# C3C: Pure Teacher-Student on train_noisy (center-crop fast path, BATCHED) \u2014 OOF guardrails + new submission\n",
        "# Rationale: Use batched center-crop embeddings to enable fast teacher inference over 19.8k noisy clips.\n",
        "# Teachers are strictly 5 OOF LR models trained on curated center-crop embeddings (no full-data leak).\n",
        "# Select high-confidence positives among weak labels (precision-first), weight samples, retrain student,\n",
        "# validate curated OOF (must not drop >0.01 vs C2 baseline 0.8001), then produce a new submission on test center-crop.\n",
        "\n",
        "import os, json, time, math\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa, torch\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import joblib\n",
        "\n",
        "# Ensure panns_inference for feature extraction (center-crop) on noisy if needed\n",
        "try:\n",
        "    from panns_inference import AudioTagging\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'panns-inference'])\n",
        "    from panns_inference import AudioTagging\n",
        "\n",
        "BASE = Path('.')\n",
        "SR = 32000\n",
        "CROP_SEC = 10.0\n",
        "T = int(SR * CROP_SEC)\n",
        "EMB_DIM = 2048\n",
        "N_FOLDS = 5\n",
        "CKPT_PATH = Path('/app/panns_data/Cnn14_mAP=0.431.pth')\n",
        "assert CKPT_PATH.exists(), 'Missing CNN14 checkpoint; run earlier asset cell.'\n",
        "\n",
        "# Load dataframes and schema\n",
        "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
        "df_noi = pd.read_csv(BASE / 'train_noisy.csv')\n",
        "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
        "class_names = [c for c in df_ss.columns if c != 'fname']\n",
        "label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "n_classes = len(class_names)\n",
        "\n",
        "def parse_labels_str(s):\n",
        "    if not isinstance(s, str):\n",
        "        return []\n",
        "    return [t.strip() for t in s.replace(';', ',').split(',') if t.strip()]\n",
        "\n",
        "def encode_labels(s):\n",
        "    y = np.zeros(n_classes, dtype=np.float32)\n",
        "    for t in parse_labels_str(s):\n",
        "        if t in label_to_idx:\n",
        "            y[label_to_idx[t]] = 1.0\n",
        "    return y\n",
        "\n",
        "def lwlrap_np(truth, scores):\n",
        "    assert truth.shape == scores.shape\n",
        "    n_samples, n_labels = truth.shape\n",
        "    precisions = np.zeros(n_labels)\n",
        "    labels_per_class = np.maximum(truth.sum(axis=0), 1)\n",
        "    for i in range(n_samples):\n",
        "        pos = np.where(truth[i] > 0)[0]\n",
        "        if pos.size == 0:\n",
        "            continue\n",
        "        ranking = np.argsort(-scores[i])\n",
        "        ranked_truth = truth[i][ranking]\n",
        "        cumsum = np.cumsum(ranked_truth)\n",
        "        pos_rank = np.where(ranked_truth > 0)[0]\n",
        "        prec = cumsum[pos_rank] / (pos_rank + 1)\n",
        "        ranked_labels = ranking[pos_rank]\n",
        "        for lbl, p in zip(ranked_labels, prec):\n",
        "            precisions[lbl] += p\n",
        "    per_class = precisions / labels_per_class\n",
        "    weights = truth.sum(axis=0) / max(truth.sum(), 1)\n",
        "    return float((per_class * weights).sum()), per_class\n",
        "\n",
        "# Load curated/test center-crop embeddings (C2 artifacts)\n",
        "emb_cur_path = BASE / 'embeddings_curated.npy'\n",
        "emb_test_path = BASE / 'embeddings_test.npy'\n",
        "assert emb_cur_path.exists() and emb_test_path.exists(), 'Center-crop embeddings missing; run C2 pipeline.'\n",
        "X_cur = np.load(emb_cur_path)\n",
        "X_test = np.load(emb_test_path)\n",
        "assert X_cur.shape[1] == EMB_DIM and X_test.shape[1] == EMB_DIM\n",
        "Y_cur = np.stack(df_cur['labels'].apply(encode_labels).values).astype(np.float32)\n",
        "\n",
        "# 1) Train 5 pure OOF teacher models on curated center-crop embeddings\n",
        "teachers = []\n",
        "oof = np.zeros((len(df_cur), n_classes), dtype=np.float32)\n",
        "fold_scores = []\n",
        "for k in range(N_FOLDS):\n",
        "    trn_idx = np.where(df_cur['fold'].values != k)[0]\n",
        "    val_idx = np.where(df_cur['fold'].values == k)[0]\n",
        "    X_tr, X_va = X_cur[trn_idx], X_cur[val_idx]\n",
        "    y_tr, y_va = Y_cur[trn_idx], Y_cur[val_idx]\n",
        "    base_lr = LogisticRegression(solver='lbfgs', max_iter=1000, C=2.0, n_jobs=16, verbose=0)\n",
        "    clf = OneVsRestClassifier(make_pipeline(StandardScaler(with_mean=True, with_std=True), base_lr), n_jobs=-1)\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    teachers.append(clf)\n",
        "    proba = clf.predict_proba(X_va)\n",
        "    oof[val_idx] = proba.astype(np.float32)\n",
        "    lw, _ = lwlrap_np(y_va, proba)\n",
        "    fold_scores.append(lw)\n",
        "    print(f'Teacher fold {k} LWLRAP (center-crop)={lw:.4f}')\n",
        "oof_lw, _ = lwlrap_np(Y_cur, oof)\n",
        "print(f'Teacher OOF LWLRAP (center-crop curated)={oof_lw:.4f}; per-fold={fold_scores}')\n",
        "\n",
        "# 2) Extract center-crop embeddings for train_noisy (BATCHED) if not cached\n",
        "emb_noisy_path = BASE / 'embeddings_noisy.npy'\n",
        "def load_center_crop_10s(path, sr=SR, crop_sec=CROP_SEC):\n",
        "    y, s = librosa.load(path, sr=sr, mono=True)\n",
        "    if y.dtype != np.float32:\n",
        "        y = y.astype(np.float32)\n",
        "    target = int(sr * crop_sec)\n",
        "    if len(y) >= target:\n",
        "        start = max(0, (len(y) - target)//2)\n",
        "        y = y[start:start+target]\n",
        "    else:\n",
        "        y = np.pad(y, (0, target-len(y)))\n",
        "    return y\n",
        "\n",
        "def extract_noisy_embeddings_batched(file_list, root_dir, batch_size=128, log_every=1024):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f'Extracting noisy embeddings batched | Device: {device.upper()} | batch_size={batch_size}')\n",
        "    at = AudioTagging(checkpoint_path=str(CKPT_PATH), device=device)\n",
        "    N = len(file_list)\n",
        "    X = np.zeros((N, EMB_DIM), dtype=np.float32)\n",
        "    t0 = time.time()\n",
        "    for start in range(0, N, batch_size):\n",
        "        end = min(start + batch_size, N)\n",
        "        batch_files = file_list[start:end]\n",
        "        batch_waves = []\n",
        "        for fname in batch_files:\n",
        "            y = load_center_crop_10s(str(Path(root_dir) / fname))\n",
        "            batch_waves.append(y)\n",
        "        batch_waves = np.stack(batch_waves, 0)  # (B, T)\n",
        "        with torch.no_grad():\n",
        "            out = at.inference(batch_waves)\n",
        "        if isinstance(out, tuple) and len(out)==2:\n",
        "            emb_b = np.asarray(out[1], dtype=np.float32)  # (B, 2048)\n",
        "        elif isinstance(out, dict) and 'embedding' in out:\n",
        "            emb_b = np.asarray(out['embedding'], dtype=np.float32)\n",
        "        else:\n",
        "            raise RuntimeError('Unexpected AudioTagging output for noisy batch')\n",
        "        X[start:end] = emb_b\n",
        "        if (end % log_every == 0) or (end == N):\n",
        "            dt = time.time() - t0\n",
        "            print(f'  Noisy {end}/{N} in {dt/60:.1f} min')\n",
        "    return X\n",
        "\n",
        "if emb_noisy_path.exists():\n",
        "    X_noisy = np.load(emb_noisy_path)\n",
        "    print('Loaded cached embeddings_noisy.npy')\n",
        "else:\n",
        "    noisy_files = df_noi['fname'].values\n",
        "    root = BASE / 'train_noisy'\n",
        "    # Heuristic: bigger batch on CPU, moderate on GPU\n",
        "    bs = 128 if not torch.cuda.is_available() else 64\n",
        "    X_noisy = extract_noisy_embeddings_batched(noisy_files, root, batch_size=bs, log_every=2048)\n",
        "    np.save(emb_noisy_path, X_noisy)\n",
        "    print('Saved embeddings_noisy.npy')\n",
        "\n",
        "# 3) Teacher ensemble predictions on train_noisy (average of fold teachers)\n",
        "print('Scoring train_noisy with teacher ensemble ...')\n",
        "probs_noisy = np.zeros((len(df_noi), n_classes), dtype=np.float32)\n",
        "for t_idx, clf in enumerate(teachers):\n",
        "    p = clf.predict_proba(X_noisy).astype(np.float32)\n",
        "    probs_noisy += p\n",
        "probs_noisy /= len(teachers)\n",
        "\n",
        "# 4) Data-driven thresholds per class (precision-first among weak positives)\n",
        "print('Selecting thresholds and filtering weak positives ...')\n",
        "weak_lists = df_noi['labels'].apply(parse_labels_str).values\n",
        "thr = np.full(n_classes, 0.95, dtype=np.float32)  # default min threshold\n",
        "for c, name in enumerate(class_names):\n",
        "    idx = [i for i, toks in enumerate(weak_lists) if name in toks]\n",
        "    if len(idx) >= 50:\n",
        "        vals = probs_noisy[idx, c]\n",
        "        q95 = float(np.quantile(vals, 0.95))\n",
        "        thr[c] = max(0.90, q95)\n",
        "    elif len(idx) >= 10:\n",
        "        vals = probs_noisy[idx, c]\n",
        "        q90 = float(np.quantile(vals, 0.90))\n",
        "        thr[c] = max(0.92, q90)\n",
        "    else:\n",
        "        thr[c] = 0.95\n",
        "\n",
        "# Build selected noisy labels matrix based on thresholds, restricted to weak positives only (no new positives)\n",
        "Y_noisy_sel = np.zeros((len(df_noi), n_classes), dtype=np.float32)\n",
        "sel_mask = np.zeros(len(df_noi), dtype=bool)\n",
        "weights_noisy = np.zeros(len(df_noi), dtype=np.float32)\n",
        "for i, toks in enumerate(weak_lists):\n",
        "    if not toks:\n",
        "        continue\n",
        "    cls_idx = [label_to_idx[t] for t in toks if t in label_to_idx]\n",
        "    if not cls_idx:\n",
        "        continue\n",
        "    preds = probs_noisy[i, cls_idx]\n",
        "    keep = [j for j, p in zip(cls_idx, preds) if p >= thr[j]]\n",
        "    if keep:\n",
        "        Y_noisy_sel[i, keep] = 1.0\n",
        "        sel_mask[i] = True\n",
        "        weights_noisy[i] = float(np.clip(probs_noisy[i, keep].max(), 0.5, 1.0))\n",
        "\n",
        "selected_idx = np.where(sel_mask)[0]\n",
        "print('Selected noisy samples:', len(selected_idx), 'out of', len(df_noi), f'({len(selected_idx)/len(df_noi)*100:.1f}%)')\n",
        "assert len(selected_idx) > 0, 'No noisy samples selected; thresholds too strict.'\n",
        "\n",
        "# Persist thresholds and selection summary\n",
        "thr_dict = {name: float(thr[i]) for i, name in enumerate(class_names)}\n",
        "with open('thresholds_c3c.json', 'w') as f:\n",
        "    json.dump({'per_class_thresholds': thr_dict, 'selection_count': int(len(selected_idx))}, f)\n",
        "print('Saved thresholds_c3c.json')\n",
        "\n",
        "# 5) OOF validation on curated with noisy augmentation in training folds\n",
        "print('Validating curated OOF with noisy-augmented training ...')\n",
        "oof_student = np.zeros((len(df_cur), n_classes), dtype=np.float32)\n",
        "fold_scores_student = []\n",
        "BASELINE_C2 = 0.8001  # curated center-crop baseline\n",
        "\n",
        "def predict_oof_weighted_lr(X_tr, y_tr, X_va, sw_tr, C=2.0, max_iter=1000):\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr_s = scaler.fit_transform(X_tr)\n",
        "    X_va_s = scaler.transform(X_va)\n",
        "    proba = np.zeros((X_va.shape[0], n_classes), dtype=np.float32)\n",
        "    for c in range(n_classes):\n",
        "        lr = LogisticRegression(solver='lbfgs', max_iter=max_iter, C=C, n_jobs=16, verbose=0)\n",
        "        lr.fit(X_tr_s, y_tr[:, c], sample_weight=sw_tr)\n",
        "        proba[:, c] = lr.predict_proba(X_va_s)[:, 1]\n",
        "    return proba\n",
        "\n",
        "for k in range(N_FOLDS):\n",
        "    trn_idx = np.where(df_cur['fold'].values != k)[0]\n",
        "    val_idx = np.where(df_cur['fold'].values == k)[0]\n",
        "    X_tr = np.concatenate([X_cur[trn_idx], X_noisy[selected_idx]], axis=0)\n",
        "    y_tr = np.concatenate([Y_cur[trn_idx], Y_noisy_sel[selected_idx]], axis=0)\n",
        "    sw_tr = np.concatenate([np.ones(len(trn_idx), dtype=np.float32), weights_noisy[selected_idx]], axis=0)\n",
        "    X_va = X_cur[val_idx]\n",
        "    y_va = Y_cur[val_idx]\n",
        "    proba = predict_oof_weighted_lr(X_tr, y_tr, X_va, sw_tr, C=2.0, max_iter=1000)\n",
        "    oof_student[val_idx] = proba.astype(np.float32)\n",
        "    lw, _ = lwlrap_np(y_va, proba)\n",
        "    fold_scores_student.append(lw)\n",
        "    print(f'Student fold {k} LWLRAP={lw:.4f}')\n",
        "oof_lw_student, _ = lwlrap_np(Y_cur, oof_student)\n",
        "print(f'Student OOF LWLRAP={oof_lw_student:.4f}; per-fold={fold_scores_student}')\n",
        "\n",
        "# Guardrail: If degradation > 0.01, tighten thresholds and recompute once\n",
        "if oof_lw_student + 0.01 < BASELINE_C2:\n",
        "    print('WARNING: OOF degraded by >0.01 vs baseline. Tightening thresholds to 0.98 minimum and recomputing selection ...')\n",
        "    thr = np.maximum(thr, 0.98)\n",
        "    Y_noisy_sel[:] = 0.0\n",
        "    sel_mask[:] = False\n",
        "    weights_noisy[:] = 0.0\n",
        "    for i, toks in enumerate(weak_lists):\n",
        "        if not toks:\n",
        "            continue\n",
        "        cls_idx = [label_to_idx[t] for t in toks if t in label_to_idx]\n",
        "        if not cls_idx:\n",
        "            continue\n",
        "        preds = probs_noisy[i, cls_idx]\n",
        "        keep = [j for j, p in zip(cls_idx, preds) if p >= thr[j]]\n",
        "        if keep:\n",
        "            Y_noisy_sel[i, keep] = 1.0\n",
        "            sel_mask[i] = True\n",
        "            weights_noisy[i] = float(np.clip(probs_noisy[i, keep].max(), 0.5, 1.0))\n",
        "    selected_idx = np.where(sel_mask)[0]\n",
        "    print('After tightening, selected noisy samples:', len(selected_idx))\n",
        "    oof_student[:] = 0.0\n",
        "    fold_scores_student = []\n",
        "    for k in range(N_FOLDS):\n",
        "        trn_idx = np.where(df_cur['fold'].values != k)[0]\n",
        "        val_idx = np.where(df_cur['fold'].values == k)[0]\n",
        "        X_tr = np.concatenate([X_cur[trn_idx], X_noisy[selected_idx]], axis=0)\n",
        "        y_tr = np.concatenate([Y_cur[trn_idx], Y_noisy_sel[selected_idx]], axis=0)\n",
        "        sw_tr = np.concatenate([np.ones(len(trn_idx), dtype=np.float32), weights_noisy[selected_idx]], axis=0)\n",
        "        X_va = X_cur[val_idx]\n",
        "        y_va = Y_cur[val_idx]\n",
        "        proba = predict_oof_weighted_lr(X_tr, y_tr, X_va, sw_tr, C=2.0, max_iter=1000)\n",
        "        oof_student[val_idx] = proba.astype(np.float32)\n",
        "        lw, _ = lwlrap_np(y_va, proba)\n",
        "        fold_scores_student.append(lw)\n",
        "        print(f'(Tight) Student fold {k} LWLRAP={lw:.4f}')\n",
        "    oof_lw_student, _ = lwlrap_np(Y_cur, oof_student)\n",
        "    print(f'(Tight) Student OOF LWLRAP={oof_lw_student:.4f}')\n",
        "\n",
        "# Persist C3C curated metrics\n",
        "metrics_c3c = {\n",
        "    'stage': 'C3C_teacher_student_center_batched',\n",
        "    'teacher_oof_lwlrap': float(oof_lw),\n",
        "    'student_oof_lwlrap': float(oof_lw_student),\n",
        "    'student_per_fold': [float(x) for x in fold_scores_student],\n",
        "    'baseline_c2_center': 0.8001,\n",
        "    'selected_noisy': int(len(selected_idx))\n",
        "}\n",
        "with open('metrics_c3c_curated.json', 'w') as f:\n",
        "    json.dump(metrics_c3c, f)\n",
        "print('Saved metrics_c3c_curated.json:', metrics_c3c)\n",
        "\n",
        "# 6) Train full student on curated + selected noisy and predict test (center-crop submission)\n",
        "def predict_full_weighted_lr(X_full, Y_full, W_full, X_eval, C=2.0, max_iter=1000):\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_full_s = scaler.fit_transform(X_full)\n",
        "    X_eval_s = scaler.transform(X_eval)\n",
        "    proba = np.zeros((X_eval.shape[0], n_classes), dtype=np.float32)\n",
        "    for c in range(n_classes):\n",
        "        lr = LogisticRegression(solver='lbfgs', max_iter=max_iter, C=C, n_jobs=16, verbose=0)\n",
        "        lr.fit(X_full_s, Y_full[:, c], sample_weight=W_full)\n",
        "        proba[:, c] = lr.predict_proba(X_eval_s)[:, 1]\n",
        "    return proba\n",
        "\n",
        "X_full = np.concatenate([X_cur, X_noisy[selected_idx]], axis=0)\n",
        "Y_full = np.concatenate([Y_cur, Y_noisy_sel[selected_idx]], axis=0)\n",
        "W_full = np.concatenate([np.ones(len(X_cur), dtype=np.float32), weights_noisy[selected_idx]], axis=0)\n",
        "\n",
        "test_proba = predict_full_weighted_lr(X_full, Y_full, W_full, X_test, C=2.0, max_iter=1000)\n",
        "sub = pd.DataFrame(test_proba, columns=class_names)\n",
        "sub.insert(0, 'fname', df_ss['fname'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (C3C center-crop student, batched). Shape:', sub.shape)\n",
        "\n",
        "# Save student scaler and a note (full per-class models are not serialized to keep notebook light)\n",
        "joblib.dump({'note': 'Per-class LR models trained ad-hoc; reproduce by re-running this cell', 'selected_noisy': selected_idx.shape[0]}, 'ovr_logreg_student_c3c_center_note.joblib')\n",
        "print('Saved student training note checkpoint.')\n",
        ""
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher fold 0 LWLRAP (center-crop)=0.8091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher fold 1 LWLRAP (center-crop)=0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher fold 2 LWLRAP (center-crop)=0.8086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher fold 3 LWLRAP (center-crop)=0.8101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher fold 4 LWLRAP (center-crop)=0.7745\nTeacher OOF LWLRAP (center-crop curated)=0.8001; per-fold=[0.8091147409599597, 0.7999997415260673, 0.8085873055161429, 0.8101376948155439, 0.7744689058892884]\nLoaded cached embeddings_noisy.npy\nScoring train_noisy with teacher ensemble ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting thresholds and filtering weak positives ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected noisy samples: 1183 out of 19815 (6.0%)\nSaved thresholds_c3c.json\nValidating curated OOF with noisy-augmented training ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student fold 0 LWLRAP=0.8109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student fold 1 LWLRAP=0.8039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student fold 2 LWLRAP=0.8078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student fold 3 LWLRAP=0.8150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student fold 4 LWLRAP=0.7795\nStudent OOF LWLRAP=0.8034; per-fold=[0.8108990560912546, 0.8039474681684713, 0.807789676386343, 0.8150056151141913, 0.7795498747685645]\nSaved metrics_c3c_curated.json: {'stage': 'C3C_teacher_student_center_batched', 'teacher_oof_lwlrap': 0.8000558604499433, 'student_oof_lwlrap': 0.8034271510417962, 'student_per_fold': [0.8108990560912546, 0.8039474681684713, 0.807789676386343, 0.8150056151141913, 0.7795498747685645], 'baseline_c2_center': 0.8001, 'selected_noisy': 1183}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (C3C center-crop student, batched). Shape: (3361, 81)\nSaved student training note checkpoint.\n"
          ]
        }
      ]
    },
    {
      "id": "68df5e05-75e4-4b01-9445-a19e9970b14b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# C3C (Remediated): Pure Teacher-Student on train_noisy with strict MC5 parity and class-specific weights\n",
        "# Foundation: C3A MC5 embeddings (best validated model, OOF=0.8049). No center-crop anywhere.\n",
        "# - Train 5 pure OOF teachers on curated MC5\n",
        "# - Extract MC5 embeddings for train_noisy (batched)\n",
        "# - Teacher ensemble on noisy -> per-class thresholds (precision-first, retain 20\u201360% of weak positives)\n",
        "# - Select positives (weak-only) + optionally add ultra-confident new positives (p>=0.98, cap 3/clip)\n",
        "# - Train student with class-specific sample weights per class (noisy negatives get weight 0)\n",
        "# - Validate curated OOF against true baseline (0.8049); persist metrics; produce submission on test MC5.\n",
        "\n",
        "import os, json, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa, torch\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# panns-inference\n",
        "try:\n",
        "    from panns_inference import AudioTagging\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'panns-inference'])\n",
        "    from panns_inference import AudioTagging\n",
        "\n",
        "BASE = Path('.')\n",
        "SR = 32000\n",
        "T_SEC = 10.0\n",
        "T = int(SR * T_SEC)\n",
        "K = 5\n",
        "EMB_DIM = 2048\n",
        "N_FOLDS = 5\n",
        "CKPT_PATH = Path('/app/panns_data/Cnn14_mAP=0.431.pth')\n",
        "assert CKPT_PATH.exists(), 'CNN14 checkpoint missing; prepare assets first.'\n",
        "\n",
        "# Data\n",
        "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
        "df_noi = pd.read_csv(BASE / 'train_noisy.csv')\n",
        "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
        "class_names = [c for c in df_ss.columns if c != 'fname']\n",
        "label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "n_classes = len(class_names)\n",
        "\n",
        "def parse_labels_str(s):\n",
        "    if not isinstance(s, str):\n",
        "        return []\n",
        "    return [t.strip() for t in s.replace(';', ',').split(',') if t.strip()]\n",
        "\n",
        "def encode_labels(s):\n",
        "    y = np.zeros(n_classes, dtype=np.float32)\n",
        "    for t in parse_labels_str(s):\n",
        "        if t in label_to_idx:\n",
        "            y[label_to_idx[t]] = 1.0\n",
        "    return y\n",
        "\n",
        "def lwlrap_np(truth, scores):\n",
        "    assert truth.shape == scores.shape\n",
        "    n_samples, n_labels = truth.shape\n",
        "    precisions = np.zeros(n_labels)\n",
        "    labels_per_class = np.maximum(truth.sum(axis=0), 1)\n",
        "    for i in range(n_samples):\n",
        "        pos = np.where(truth[i] > 0)[0]\n",
        "        if pos.size == 0:\n",
        "            continue\n",
        "        ranking = np.argsort(-scores[i])\n",
        "        ranked_truth = truth[i][ranking]\n",
        "        cumsum = np.cumsum(ranked_truth)\n",
        "        pos_rank = np.where(ranked_truth > 0)[0]\n",
        "        prec = cumsum[pos_rank] / (pos_rank + 1)\n",
        "        ranked_labels = ranking[pos_rank]\n",
        "        for lbl, p in zip(ranked_labels, prec):\n",
        "            precisions[lbl] += p\n",
        "    per_class = precisions / labels_per_class\n",
        "    weights = truth.sum(axis=0) / max(truth.sum(), 1)\n",
        "    return float((per_class * weights).sum()), per_class\n",
        "\n",
        "# Load curated/test MC5 embeddings (foundation)\n",
        "X_cur_path = BASE / 'embeddings_curated_mc5.npy'\n",
        "X_test_path = BASE / 'embeddings_test_mc5.npy'\n",
        "assert X_cur_path.exists() and X_test_path.exists(), 'Missing MC5 embeddings; run C3A first.'\n",
        "X_cur = np.load(X_cur_path)\n",
        "X_test = np.load(X_test_path)\n",
        "assert X_cur.shape[1] == EMB_DIM and X_test.shape[1] == EMB_DIM\n",
        "Y_cur = np.stack(df_cur['labels'].apply(encode_labels).values).astype(np.float32)\n",
        "\n",
        "# 1) Train 5 pure OOF teachers on curated MC5\n",
        "teachers = []\n",
        "oof = np.zeros((len(df_cur), n_classes), dtype=np.float32)\n",
        "fold_scores = []\n",
        "for k in range(N_FOLDS):\n",
        "    trn_idx = np.where(df_cur['fold'].values != k)[0]\n",
        "    val_idx = np.where(df_cur['fold'].values == k)[0]\n",
        "    X_tr, X_va = X_cur[trn_idx], X_cur[val_idx]\n",
        "    y_tr, y_va = Y_cur[trn_idx], Y_cur[val_idx]\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr_s = scaler.fit_transform(X_tr)\n",
        "    X_va_s = scaler.transform(X_va)\n",
        "    proba_va = np.zeros((len(val_idx), n_classes), dtype=np.float32)\n",
        "    models_k = []\n",
        "    for c in range(n_classes):\n",
        "        lr = LogisticRegression(solver='lbfgs', max_iter=1000, C=2.0, n_jobs=16, verbose=0)\n",
        "        lr.fit(X_tr_s, y_tr[:, c])\n",
        "        proba_va[:, c] = lr.predict_proba(X_va_s)[:, 1]\n",
        "        models_k.append(lr)\n",
        "    teachers.append((scaler, models_k))\n",
        "    oof[val_idx] = proba_va\n",
        "    lw, _ = lwlrap_np(y_va, proba_va)\n",
        "    fold_scores.append(lw)\n",
        "    print(f'Teacher fold {k} LWLRAP (MC5)={lw:.4f}')\n",
        "oof_lw, _ = lwlrap_np(Y_cur, oof)\n",
        "print(f'Teacher OOF LWLRAP (MC5 curated)={oof_lw:.4f}; per-fold={fold_scores}')\n",
        "\n",
        "# 2) Extract MC5 embeddings for train_noisy (batched, cached)\n",
        "def load_audio(path, sr=SR):\n",
        "    y, s = librosa.load(path, sr=sr, mono=True)\n",
        "    return y.astype(np.float32)\n",
        "\n",
        "def crop_starts(L, T, K=5):\n",
        "    if L <= T:\n",
        "        return [0]*K\n",
        "    return [0, (L - T)//2, L - T, int(0.25*(L - T)), int(0.75*(L - T))]\n",
        "\n",
        "def crops_for_wave(y, T, K=5):\n",
        "    L = len(y)\n",
        "    starts = crop_starts(L, T, K)\n",
        "    if L >= T:\n",
        "        crops = [y[max(0, min(s, L - T)):max(0, min(s, L - T))+T] for s in starts]\n",
        "    else:\n",
        "        pad = np.pad(y, (0, T - L))\n",
        "        crops = [pad for _ in range(K)]\n",
        "    return np.stack(crops, 0)  # (K, T)\n",
        "\n",
        "def extract_mc_embeddings_batched(file_list, root_dir, K=5, batch_size=128, log_every=2048):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f'Noisy MC{K} extraction | Device: {device.upper()} | batch_size={batch_size}')\n",
        "    at = AudioTagging(checkpoint_path=str(CKPT_PATH), device=device)\n",
        "    N = len(file_list)\n",
        "    X = np.zeros((N, EMB_DIM), dtype=np.float32)\n",
        "    t0 = time.time()\n",
        "    for start_idx in range(0, N, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, N)\n",
        "        batch_files = file_list[start_idx:end_idx]\n",
        "        batch_crops = []\n",
        "        for fname in batch_files:\n",
        "            y = load_audio(str(Path(root_dir) / fname), sr=SR)\n",
        "            ck = crops_for_wave(y, T, K)\n",
        "            batch_crops.append(ck)\n",
        "        batch_crops = np.concatenate(batch_crops, axis=0)  # (B*K, T)\n",
        "        with torch.no_grad():\n",
        "            out = at.inference(batch_crops)\n",
        "        if isinstance(out, tuple) and len(out)==2:\n",
        "            emb_bk = np.asarray(out[1], dtype=np.float32)\n",
        "        elif isinstance(out, dict) and 'embedding' in out:\n",
        "            emb_bk = np.asarray(out['embedding'], dtype=np.float32)\n",
        "        else:\n",
        "            raise RuntimeError('Unexpected AudioTagging output type')\n",
        "        B = end_idx - start_idx\n",
        "        embs = emb_bk.reshape(B, K, EMB_DIM).mean(axis=1)\n",
        "        X[start_idx:end_idx] = embs\n",
        "        if (end_idx % log_every == 0) or (end_idx == N):\n",
        "            dt = time.time() - t0\n",
        "            print(f'  Noisy MC{K}: {end_idx}/{N} in {dt/60:.1f} min')\n",
        "    return X\n",
        "\n",
        "emb_noisy_mc_path = BASE / 'embeddings_noisy_mc5.npy'\n",
        "if emb_noisy_mc_path.exists():\n",
        "    X_noisy = np.load(emb_noisy_mc_path)\n",
        "    print('Loaded cached embeddings_noisy_mc5.npy')\n",
        "else:\n",
        "    noisy_files = df_noi['fname'].values\n",
        "    root = BASE / 'train_noisy'\n",
        "    bs = 128 if not torch.cuda.is_available() else 16\n",
        "    X_noisy = extract_mc_embeddings_batched(noisy_files, root, K=K, batch_size=bs, log_every=2048)\n",
        "    np.save(emb_noisy_mc_path, X_noisy)\n",
        "    print('Saved embeddings_noisy_mc5.npy')\n",
        "\n",
        "# 3) Teacher ensemble predictions on train_noisy (average of fold teachers)\n",
        "print('Scoring train_noisy (MC5) with teacher ensemble ...')\n",
        "probs_noisy = np.zeros((len(df_noi), n_classes), dtype=np.float32)\n",
        "for (scaler, models_k) in teachers:\n",
        "    Xn_s = scaler.transform(X_noisy)\n",
        "    p = np.zeros_like(probs_noisy)\n",
        "    for c in range(n_classes):\n",
        "        p[:, c] = models_k[c].predict_proba(Xn_s)[:, 1]\n",
        "    probs_noisy += p.astype(np.float32)\n",
        "probs_noisy /= len(teachers)\n",
        "\n",
        "# 4) Per-class thresholds among weak positives; aim retain ~20\u201360% coverage\n",
        "weak_lists = df_noi['labels'].apply(parse_labels_str).values\n",
        "thr = np.full(n_classes, 0.90, dtype=np.float32)\n",
        "retained_per_class = np.zeros(n_classes, dtype=int)\n",
        "total_weak_per_class = np.zeros(n_classes, dtype=int)\n",
        "for c, name in enumerate(class_names):\n",
        "    idx = [i for i, toks in enumerate(weak_lists) if name in toks]\n",
        "    total_weak_per_class[c] = len(idx)\n",
        "    if len(idx) == 0:\n",
        "        thr[c] = 0.95\n",
        "        continue\n",
        "    vals = probs_noisy[idx, c]\n",
        "    q = 0.90\n",
        "    qv = float(np.quantile(vals, q))\n",
        "    t = max(0.85, qv)\n",
        "    retained = int((vals >= t).sum())\n",
        "    if retained < 20 and len(idx) >= 50:\n",
        "        q = 0.85\n",
        "        qv = float(np.quantile(vals, q))\n",
        "        t = max(0.85, qv)\n",
        "        retained = int((vals >= t).sum())\n",
        "    thr[c] = t\n",
        "    retained_per_class[c] = retained\n",
        "\n",
        "# Build noisy label matrix restricted to weak positives; add ultra-confident new positives (p>=0.98, cap 3/clip)\n",
        "Y_noisy_sel = np.zeros((len(df_noi), n_classes), dtype=np.float32)\n",
        "sel_mask = np.zeros(len(df_noi), dtype=bool)\n",
        "for i, toks in enumerate(weak_lists):\n",
        "    cls_idx = [label_to_idx[t] for t in toks if t in label_to_idx]\n",
        "    for j in cls_idx:\n",
        "        if probs_noisy[i, j] >= thr[j]:\n",
        "            Y_noisy_sel[i, j] = 1.0\n",
        "    not_weak = [j for j in range(n_classes) if j not in cls_idx]\n",
        "    if not_weak:\n",
        "        top_new = [(j, probs_noisy[i, j]) for j in not_weak if probs_noisy[i, j] >= 0.98]\n",
        "        top_new.sort(key=lambda x: -x[1])\n",
        "        for j, _p in top_new[:3]:\n",
        "            Y_noisy_sel[i, j] = 1.0\n",
        "    if Y_noisy_sel[i].sum() > 0:\n",
        "        sel_mask[i] = True\n",
        "\n",
        "selected_idx = np.where(sel_mask)[0]\n",
        "print('Selected noisy samples (any positives after filtering):', len(selected_idx), 'out of', len(df_noi), f'({len(selected_idx)/len(df_noi)*100:.1f}%)')\n",
        "assert len(selected_idx) > 0, 'No noisy samples selected; thresholds too strict.'\n",
        "\n",
        "# Persist thresholds and selection summary\n",
        "thr_dict = {name: float(thr[i]) for i, name in enumerate(class_names)}\n",
        "summary = {\n",
        "    'per_class_thresholds': thr_dict,\n",
        "    'selection_count': int(len(selected_idx)),\n",
        "    'retained_per_class': {class_names[i]: int(retained_per_class[i]) for i in range(n_classes)},\n",
        "    'total_weak_per_class': {class_names[i]: int(total_weak_per_class[i]) for i in range(n_classes)}\n",
        "}\n",
        "with open('thresholds_c3c.json', 'w') as f:\n",
        "    json.dump(summary, f)\n",
        "print('Saved thresholds_c3c.json (MC5).')\n",
        "\n",
        "# 5) OOF validation on curated with noisy augmentation, class-specific sample weights (noisy negatives weight=0)\n",
        "print('Validating curated OOF (student, MC5, class-specific weights) ...')\n",
        "oof_student = np.zeros((len(df_cur), n_classes), dtype=np.float32)\n",
        "fold_scores_student = []\n",
        "BASELINE_C3A = 0.8049036654978639\n",
        "\n",
        "def predict_oof_student_mc5(X_tr, y_tr, X_va, probs_noisy_sel, C=2.0, max_iter=1000):\n",
        "    n_cur = y_tr.shape[0] - probs_noisy_sel.shape[0]\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr_s = scaler.fit_transform(X_tr)\n",
        "    X_va_s = scaler.transform(X_va)\n",
        "    proba = np.zeros((X_va.shape[0], n_classes), dtype=np.float32)\n",
        "    # y_tr noisy block for masking\n",
        "    y_noi = y_tr[n_cur:]\n",
        "    for c in range(n_classes):\n",
        "        w_cur = np.ones(n_cur, dtype=np.float32)\n",
        "        # class-specific weights: use prob only for positive pseudo-labels; negatives get 0.0\n",
        "        w_noi_c = np.where(y_noi[:, c] > 0.5, np.clip(probs_noisy_sel[:, c], 0.5, 1.0), 0.0).astype(np.float32)\n",
        "        w = np.concatenate([w_cur, w_noi_c], axis=0)\n",
        "        lr = LogisticRegression(solver='lbfgs', max_iter=max_iter, C=C, n_jobs=16, verbose=0)\n",
        "        lr.fit(X_tr_s, y_tr[:, c], sample_weight=w)\n",
        "        proba[:, c] = lr.predict_proba(X_va_s)[:, 1]\n",
        "    return proba\n",
        "\n",
        "for k in range(N_FOLDS):\n",
        "    trn_idx = np.where(df_cur['fold'].values != k)[0]\n",
        "    val_idx = np.where(df_cur['fold'].values == k)[0]\n",
        "    X_tr_cur, y_tr_cur = X_cur[trn_idx], Y_cur[trn_idx]\n",
        "    X_va, y_va = X_cur[val_idx], Y_cur[val_idx]\n",
        "    X_tr = np.concatenate([X_tr_cur, X_noisy[selected_idx]], axis=0)\n",
        "    y_tr = np.concatenate([y_tr_cur, Y_noisy_sel[selected_idx]], axis=0)\n",
        "    proba = predict_oof_student_mc5(X_tr, y_tr, X_va, probs_noisy[selected_idx], C=2.0, max_iter=1000)\n",
        "    oof_student[val_idx] = proba\n",
        "    lw, _ = lwlrap_np(y_va, proba)\n",
        "    fold_scores_student.append(lw)\n",
        "    print(f'Student fold {k} LWLRAP={lw:.4f}')\n",
        "oof_lw_student, _ = lwlrap_np(Y_cur, oof_student)\n",
        "print(f'Student OOF LWLRAP (MC5)={oof_lw_student:.4f}; per-fold={fold_scores_student}')\n",
        "\n",
        "metrics_c3c = {\n",
        "    'stage': 'C3C_teacher_student_mc5_batched',\n",
        "    'teacher_oof_lwlrap': float(oof_lw),\n",
        "    'student_oof_lwlrap': float(oof_lw_student),\n",
        "    'student_per_fold': [float(x) for x in fold_scores_student],\n",
        "    'baseline_c3a_mc5': float(BASELINE_C3A),\n",
        "    'selected_noisy': int(len(selected_idx))\n",
        "}\n",
        "with open('metrics_c3c_curated.json', 'w') as f:\n",
        "    json.dump(metrics_c3c, f)\n",
        "print('Saved metrics_c3c_curated.json (MC5):', metrics_c3c)\n",
        "\n",
        "# 6) Train full student on curated + selected noisy (MC5) and predict test (MC5) -> submission.csv\n",
        "def predict_full_student_mc5(X_full, Y_full, probs_noisy_sel, X_eval, n_cur, C=2.0, max_iter=1000):\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_full_s = scaler.fit_transform(X_full)\n",
        "    X_eval_s = scaler.transform(X_eval)\n",
        "    proba = np.zeros((X_eval.shape[0], n_classes), dtype=np.float32)\n",
        "    y_noi = Y_full[n_cur:]\n",
        "    for c in range(n_classes):\n",
        "        w_cur = np.ones(n_cur, dtype=np.float32)\n",
        "        w_noi_c = np.where(y_noi[:, c] > 0.5, np.clip(probs_noisy_sel[:, c], 0.5, 1.0), 0.0).astype(np.float32)\n",
        "        w = np.concatenate([w_cur, w_noi_c], axis=0)\n",
        "        lr = LogisticRegression(solver='lbfgs', max_iter=max_iter, C=C, n_jobs=16, verbose=0)\n",
        "        lr.fit(X_full_s, Y_full[:, c], sample_weight=w)\n",
        "        proba[:, c] = lr.predict_proba(X_eval_s)[:, 1]\n",
        "    return proba\n",
        "\n",
        "X_full = np.concatenate([X_cur, X_noisy[selected_idx]], axis=0)\n",
        "Y_full = np.concatenate([Y_cur, Y_noisy_sel[selected_idx]], axis=0)\n",
        "n_cur = X_cur.shape[0]\n",
        "test_proba = predict_full_student_mc5(X_full, Y_full, probs_noisy[selected_idx], X_test, n_cur=n_cur, C=2.0, max_iter=1000)\n",
        "sub = pd.DataFrame(test_proba, columns=class_names)\n",
        "sub.insert(0, 'fname', df_ss['fname'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (C3C MC5 student). Shape:', sub.shape)\n",
        "\n",
        "# Save note artifact\n",
        "joblib.dump({'note': 'MC5 teacher-student per-class LR trained on-the-fly (class-specific weights); reproduce by re-running this cell', 'selected_noisy': int(len(selected_idx))}, 'ovr_logreg_student_c3c_mc5_note.joblib')\n",
        "print('Saved student training note (MC5).')\n",
        ""
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher fold 0 LWLRAP (MC5)=0.8121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher fold 1 LWLRAP (MC5)=0.8054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher fold 2 LWLRAP (MC5)=0.8123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher fold 3 LWLRAP (MC5)=0.8147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher fold 4 LWLRAP (MC5)=0.7801\nTeacher OOF LWLRAP (MC5 curated)=0.8049; per-fold=[0.8120782509891866, 0.805387697867566, 0.8122972082814656, 0.8147039939573124, 0.7801080960536076]\nNoisy MC5 extraction | Device: CPU | batch_size=128\nCheckpoint path: /app/panns_data/Cnn14_mAP=0.431.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Noisy MC5: 2048/19815 in 11.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Noisy MC5: 4096/19815 in 22.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Noisy MC5: 6144/19815 in 37.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Noisy MC5: 8192/19815 in 51.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Noisy MC5: 10240/19815 in 64.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Noisy MC5: 12288/19815 in 79.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Noisy MC5: 14336/19815 in 90.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Noisy MC5: 16384/19815 in 106.6 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Noisy MC5: 18432/19815 in 123.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Noisy MC5: 19815/19815 in 135.5 min\nSaved embeddings_noisy_mc5.npy\nScoring train_noisy (MC5) with teacher ensemble ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected noisy samples (any positives after filtering): 4446 out of 19815 (22.4%)\nSaved thresholds_c3c.json (MC5).\nValidating curated OOF (student, MC5, class-specific weights) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student fold 0 LWLRAP=0.8190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student fold 1 LWLRAP=0.8099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student fold 2 LWLRAP=0.8235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student fold 3 LWLRAP=0.8197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student fold 4 LWLRAP=0.7886\nStudent OOF LWLRAP (MC5)=0.8121; per-fold=[0.8189625167044805, 0.8099130328822854, 0.8235343965782844, 0.819744053492511, 0.7885782685976188]\nSaved metrics_c3c_curated.json (MC5): {'stage': 'C3C_teacher_student_mc5_batched', 'teacher_oof_lwlrap': 0.8049036654978639, 'student_oof_lwlrap': 0.8121370323443532, 'student_per_fold': [0.8189625167044805, 0.8099130328822854, 0.8235343965782844, 0.819744053492511, 0.7885782685976188], 'baseline_c3a_mc5': 0.8049036654978639, 'selected_noisy': 4446}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (C3C MC5 student). Shape: (3361, 81)\nSaved student training note (MC5).\n"
          ]
        }
      ]
    },
    {
      "id": "5e7cee4f-b9c4-42f9-827b-84e5b465be4e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# C3D: Lightweight Ensemble + Per-class Temperature Scaling (MC5 parity)\n",
        "# - Rebuild MC5 teachers, rescore train_noisy to recompute selection (deterministic)\n",
        "# - Train student heads with different C values, fold-OOF ensemble\n",
        "# - Fit per-class temperature scaling on OOF, apply to test, write submission.csv\n",
        "# - Persist artifacts: metrics_c3d_curated.json, oof_ensemble.npy, temps_c3d.json\n",
        "\n",
        "import os, json, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa, torch\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "try:\n",
        "    from panns_inference import AudioTagging\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'panns-inference'])\n",
        "    from panns_inference import AudioTagging\n",
        "\n",
        "BASE = Path('.')\n",
        "SR = 32000\n",
        "T_SEC = 10.0\n",
        "T = int(SR * T_SEC)\n",
        "K = 5\n",
        "EMB_DIM = 2048\n",
        "N_FOLDS = 5\n",
        "CKPT_PATH = Path('/app/panns_data/Cnn14_mAP=0.431.pth')\n",
        "assert CKPT_PATH.exists(), 'CNN14 checkpoint missing.'\n",
        "\n",
        "# Data\n",
        "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
        "df_noi = pd.read_csv(BASE / 'train_noisy.csv')\n",
        "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
        "class_names = [c for c in df_ss.columns if c != 'fname']\n",
        "label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "n_classes = len(class_names)\n",
        "\n",
        "def parse_labels_str(s):\n",
        "    if not isinstance(s, str):\n",
        "        return []\n",
        "    return [t.strip() for t in s.replace(';', ',').split(',') if t.strip()]\n",
        "\n",
        "def encode_labels(s):\n",
        "    y = np.zeros(n_classes, dtype=np.float32)\n",
        "    for t in parse_labels_str(s):\n",
        "        if t in label_to_idx:\n",
        "            y[label_to_idx[t]] = 1.0\n",
        "    return y\n",
        "\n",
        "def lwlrap_np(truth, scores):\n",
        "    assert truth.shape == scores.shape\n",
        "    n_samples, n_labels = truth.shape\n",
        "    precisions = np.zeros(n_labels)\n",
        "    labels_per_class = np.maximum(truth.sum(axis=0), 1)\n",
        "    for i in range(n_samples):\n",
        "        pos = np.where(truth[i] > 0)[0]\n",
        "        if pos.size == 0:\n",
        "            continue\n",
        "        ranking = np.argsort(-scores[i])\n",
        "        ranked_truth = truth[i][ranking]\n",
        "        cumsum = np.cumsum(ranked_truth)\n",
        "        pos_rank = np.where(ranked_truth > 0)[0]\n",
        "        prec = cumsum[pos_rank] / (pos_rank + 1)\n",
        "        ranked_labels = ranking[pos_rank]\n",
        "        for lbl, p in zip(ranked_labels, prec):\n",
        "            precisions[lbl] += p\n",
        "    per_class = precisions / labels_per_class\n",
        "    weights = truth.sum(axis=0) / max(truth.sum(), 1)\n",
        "    return float((per_class * weights).sum()), per_class\n",
        "\n",
        "# Load MC5 embeddings\n",
        "X_cur = np.load('embeddings_curated_mc5.npy')\n",
        "X_test = np.load('embeddings_test_mc5.npy')\n",
        "X_noisy = np.load('embeddings_noisy_mc5.npy') if Path('embeddings_noisy_mc5.npy').exists() else None\n",
        "assert X_noisy is not None, 'embeddings_noisy_mc5.npy missing; run C3C MC5 first.'\n",
        "Y_cur = np.stack(df_cur['labels'].apply(encode_labels).values).astype(np.float32)\n",
        "\n",
        "# 1) Rebuild teachers on curated MC5 (pure OOF)\n",
        "teachers = []\n",
        "oof_teacher = np.zeros((len(df_cur), n_classes), dtype=np.float32)\n",
        "for k in range(N_FOLDS):\n",
        "    trn_idx = np.where(df_cur['fold'].values != k)[0]\n",
        "    val_idx = np.where(df_cur['fold'].values == k)[0]\n",
        "    X_tr, X_va = X_cur[trn_idx], X_cur[val_idx]\n",
        "    y_tr, y_va = Y_cur[trn_idx], Y_cur[val_idx]\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr_s = scaler.fit_transform(X_tr)\n",
        "    X_va_s = scaler.transform(X_va)\n",
        "    proba_va = np.zeros((len(val_idx), n_classes), dtype=np.float32)\n",
        "    models_k = []\n",
        "    for c in range(n_classes):\n",
        "        lr = LogisticRegression(solver='lbfgs', max_iter=1000, C=2.0, n_jobs=1)\n",
        "        lr.fit(X_tr_s, y_tr[:, c])\n",
        "        proba_va[:, c] = lr.predict_proba(X_va_s)[:, 1]\n",
        "        models_k.append(lr)\n",
        "    teachers.append((scaler, models_k))\n",
        "    oof_teacher[val_idx] = proba_va\n",
        "teacher_oof_lw, _ = lwlrap_np(Y_cur, oof_teacher)\n",
        "print(f'Teacher OOF (MC5) rebuilt: {teacher_oof_lw:.4f}')\n",
        "\n",
        "# 2) Score train_noisy with teacher ensemble to recompute selection\n",
        "print('Scoring noisy with teacher ensemble ...')\n",
        "probs_noisy = np.zeros((len(df_noi), n_classes), dtype=np.float32)\n",
        "for scaler, models_k in teachers:\n",
        "    Xn_s = scaler.transform(X_noisy)\n",
        "    p = np.zeros_like(probs_noisy)\n",
        "    for c in range(n_classes):\n",
        "        p[:, c] = models_k[c].predict_proba(Xn_s)[:, 1]\n",
        "    probs_noisy += p.astype(np.float32)\n",
        "probs_noisy /= len(teachers)\n",
        "\n",
        "# Thresholds aiming for ~20\u201360% retention among weak positives with ultra-confident additions\n",
        "weak_lists = df_noi['labels'].apply(parse_labels_str).values\n",
        "thr = np.full(n_classes, 0.90, dtype=np.float32)\n",
        "for c, name in enumerate(class_names):\n",
        "    idx = [i for i, toks in enumerate(weak_lists) if name in toks]\n",
        "    if len(idx) == 0:\n",
        "        thr[c] = 0.95\n",
        "        continue\n",
        "    vals = probs_noisy[idx, c]\n",
        "    qv = float(np.quantile(vals, 0.90))\n",
        "    t = max(0.85, qv)\n",
        "    if (vals >= t).sum() < 20 and len(idx) >= 50:\n",
        "        qv = float(np.quantile(vals, 0.85))\n",
        "        t = max(0.85, qv)\n",
        "    thr[c] = t\n",
        "\n",
        "Y_noisy_sel = np.zeros((len(df_noi), n_classes), dtype=np.float32)\n",
        "sel_mask = np.zeros(len(df_noi), dtype=bool)\n",
        "for i, toks in enumerate(weak_lists):\n",
        "    cls_idx = [label_to_idx[t] for t in toks if t in label_to_idx]\n",
        "    for j in cls_idx:\n",
        "        if probs_noisy[i, j] >= thr[j]:\n",
        "            Y_noisy_sel[i, j] = 1.0\n",
        "    # ultra-confident new positives (cap 3)\n",
        "    not_weak = [j for j in range(n_classes) if j not in cls_idx]\n",
        "    top_new = [(j, probs_noisy[i, j]) for j in not_weak if probs_noisy[i, j] >= 0.98]\n",
        "    top_new.sort(key=lambda x: -x[1])\n",
        "    for j, _p in top_new[:3]:\n",
        "        Y_noisy_sel[i, j] = 1.0\n",
        "    if Y_noisy_sel[i].sum() > 0:\n",
        "        sel_mask[i] = True\n",
        "selected_idx = np.where(sel_mask)[0]\n",
        "print('Selected noisy:', len(selected_idx), f'({len(selected_idx)/len(df_noi)*100:.1f}%)')\n",
        "assert len(selected_idx) > 0\n",
        "\n",
        "# 3) OOF ensemble of students with different C; class-specific weights (noisy negatives weight=0)\n",
        "C_LIST = [1.0, 2.0, 4.0]\n",
        "oof_ensemble = np.zeros((len(df_cur), n_classes), dtype=np.float32)\n",
        "fold_scores = []\n",
        "for k in range(N_FOLDS):\n",
        "    trn_idx = np.where(df_cur['fold'].values != k)[0]\n",
        "    val_idx = np.where(df_cur['fold'].values == k)[0]\n",
        "    X_tr_cur, y_tr_cur = X_cur[trn_idx], Y_cur[trn_idx]\n",
        "    X_va, y_va = X_cur[val_idx], Y_cur[val_idx]\n",
        "    X_tr = np.concatenate([X_tr_cur, X_noisy[selected_idx]], axis=0)\n",
        "    y_tr = np.concatenate([y_tr_cur, Y_noisy_sel[selected_idx]], axis=0)\n",
        "    n_cur = X_tr_cur.shape[0]\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr_s = scaler.fit_transform(X_tr)\n",
        "    X_va_s = scaler.transform(X_va)\n",
        "    y_noi = y_tr[n_cur:]\n",
        "    proba_accum = np.zeros((len(val_idx), n_classes), dtype=np.float32)\n",
        "    for Cval in C_LIST:\n",
        "        proba_cfg = np.zeros((len(val_idx), n_classes), dtype=np.float32)\n",
        "        for c in range(n_classes):\n",
        "            w_cur = np.ones(n_cur, dtype=np.float32)\n",
        "            w_noi_c = np.where(y_noi[:, c] > 0.5, np.clip(probs_noisy[selected_idx, c], 0.5, 1.0), 0.0).astype(np.float32)\n",
        "            w = np.concatenate([w_cur, w_noi_c], axis=0)\n",
        "            lr = LogisticRegression(solver='lbfgs', max_iter=1000, C=Cval, n_jobs=1)\n",
        "            lr.fit(X_tr_s, y_tr[:, c], sample_weight=w)\n",
        "            proba_cfg[:, c] = lr.predict_proba(X_va_s)[:, 1]\n",
        "        proba_accum += proba_cfg\n",
        "    proba_avg = proba_accum / len(C_LIST)\n",
        "    oof_ensemble[val_idx] = proba_avg\n",
        "    lw, _ = lwlrap_np(y_va, proba_avg)\n",
        "    fold_scores.append(lw)\n",
        "    print(f'Fold {k} ensemble LWLRAP={lw:.4f}')\n",
        "oof_lw_ens, _ = lwlrap_np(Y_cur, oof_ensemble)\n",
        "print(f'OOF LWLRAP ensemble (MC5 student, {len(C_LIST)} heads) = {oof_lw_ens:.4f}; per-fold={fold_scores}')\n",
        "np.save('oof_ensemble.npy', oof_ensemble)\n",
        "\n",
        "# 4) Per-class temperature scaling on OOF to calibrate probabilities\n",
        "def safe_logit(p, eps=1e-6):\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p / (1 - p))\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "def bce_loss(y, p, eps=1e-7):\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return float(-(y * np.log(p) + (1 - y) * np.log(1 - p)).mean())\n",
        "\n",
        "temps = np.ones(n_classes, dtype=np.float32)\n",
        "for c in range(n_classes):\n",
        "    y = Y_cur[:, c]\n",
        "    p = oof_ensemble[:, c]\n",
        "    z = safe_logit(p)\n",
        "    best_t, best_loss = 1.0, bce_loss(y, p)\n",
        "    # grid over tau in [0.5, 2.0]\n",
        "    for tau in np.linspace(0.5, 2.0, 16):\n",
        "        p_cal = sigmoid(z / tau)\n",
        "        loss = bce_loss(y, p_cal)\n",
        "        if loss < best_loss:\n",
        "            best_loss, best_t = loss, float(tau)\n",
        "    temps[c] = best_t\n",
        "print('Calibrated per-class temperatures (min/max):', float(temps.min()), float(temps.max()))\n",
        "with open('temps_c3d.json', 'w') as f:\n",
        "    json.dump({class_names[i]: float(temps[i]) for i in range(n_classes)}, f)\n",
        "print('Saved temps_c3d.json')\n",
        "\n",
        "# 5) Train full ensemble on curated+selected noisy and predict test; apply temperature scaling\n",
        "X_full = np.concatenate([X_cur, X_noisy[selected_idx]], axis=0)\n",
        "Y_full = np.concatenate([Y_cur, Y_noisy_sel[selected_idx]], axis=0)\n",
        "n_cur = X_cur.shape[0]\n",
        "scaler_full = StandardScaler(with_mean=True, with_std=True)\n",
        "X_full_s = scaler_full.fit_transform(X_full)\n",
        "X_test_s = scaler_full.transform(X_test)\n",
        "\n",
        "proba_test_accum = np.zeros((X_test.shape[0], n_classes), dtype=np.float32)\n",
        "y_noi_full = Y_full[n_cur:]\n",
        "for Cval in C_LIST:\n",
        "    proba_test_cfg = np.zeros_like(proba_test_accum)\n",
        "    for c in range(n_classes):\n",
        "        w_cur = np.ones(n_cur, dtype=np.float32)\n",
        "        w_noi_c = np.where(y_noi_full[:, c] > 0.5, np.clip(probs_noisy[selected_idx, c], 0.5, 1.0), 0.0).astype(np.float32)\n",
        "        w = np.concatenate([w_cur, w_noi_c], axis=0)\n",
        "        lr = LogisticRegression(solver='lbfgs', max_iter=1000, C=Cval, n_jobs=1)\n",
        "        lr.fit(X_full_s, Y_full[:, c], sample_weight=w)\n",
        "        proba_test_cfg[:, c] = lr.predict_proba(X_test_s)[:, 1]\n",
        "    proba_test_accum += proba_test_cfg\n",
        "proba_test = proba_test_accum / len(C_LIST)\n",
        "\n",
        "# Apply temperature scaling\n",
        "logits_test = safe_logit(proba_test)\n",
        "for c in range(n_classes):\n",
        "    logits_test[:, c] = logits_test[:, c] / max(temps[c], 1e-6)\n",
        "proba_test_cal = sigmoid(logits_test)\n",
        "\n",
        "# 6) Save submission and metrics\n",
        "sub = pd.DataFrame(proba_test_cal, columns=class_names)\n",
        "sub.insert(0, 'fname', df_ss['fname'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (C3D ensemble + temp scaling). Shape:', sub.shape)\n",
        "\n",
        "metrics = {\n",
        "    'stage': 'C3D_ensemble_calibration_mc5',\n",
        "    'teacher_oof_lwlrap': float(teacher_oof_lw),\n",
        "    'student_oof_lwlrap_ensemble': float(oof_lw_ens),\n",
        "    'per_fold_student_ens': [float(x) for x in fold_scores],\n",
        "    'C_list': C_LIST,\n",
        "    'selected_noisy': int(len(selected_idx)),\n",
        "    'temps_min': float(temps.min()),\n",
        "    'temps_max': float(temps.max())\n",
        "}\n",
        "with open('metrics_c3d_curated.json', 'w') as f:\n",
        "    json.dump(metrics, f)\n",
        "print('Saved metrics_c3d_curated.json:', metrics)\n",
        ""
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher OOF (MC5) rebuilt: 0.8049\nScoring noisy with teacher ensemble ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected noisy: 4506 (22.7%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 ensemble LWLRAP=0.8230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 ensemble LWLRAP=0.8116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 ensemble LWLRAP=0.8236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 ensemble LWLRAP=0.8228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 ensemble LWLRAP=0.7925\nOOF LWLRAP ensemble (MC5 student, 3 heads) = 0.8147; per-fold=[0.8230405728471376, 0.8115917422894654, 0.8235739410416323, 0.8228305234799349, 0.7924861952710641]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibrated per-class temperatures (min/max): 1.100000023841858 2.0\nSaved temps_c3d.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (C3D ensemble + temp scaling). Shape: (3361, 81)\nSaved metrics_c3d_curated.json: {'stage': 'C3D_ensemble_calibration_mc5', 'teacher_oof_lwlrap': 0.8048759167920174, 'student_oof_lwlrap_ensemble': 0.814695573093225, 'per_fold_student_ens': [0.8230405728471376, 0.8115917422894654, 0.8235739410416323, 0.8228305234799349, 0.7924861952710641], 'C_list': [1.0, 2.0, 4.0], 'selected_noisy': 4506, 'temps_min': 1.100000023841858, 'temps_max': 2.0}\n"
          ]
        }
      ]
    },
    {
      "id": "a2157a3b-8624-4f55-a167-013d0443331f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# C4 Phase 2 \u2014 Step 1: Mandatory duration diagnostics (curated vs test)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import soundfile as sf\n",
        "\n",
        "BASE = Path('.')\n",
        "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
        "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
        "cur_paths = [str(BASE / 'train_curated' / f) for f in df_cur['fname'].values]\n",
        "test_paths = [str(BASE / 'test' / f) for f in df_ss['fname'].values]\n",
        "\n",
        "def fast_duration(p):\n",
        "    try:\n",
        "        info = sf.info(p)\n",
        "        return info.frames / max(info.samplerate, 1)\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "def describe_durations(paths, name):\n",
        "    durs = np.array([fast_duration(p) for p in paths], dtype=float)\n",
        "    durs = durs[np.isfinite(durs)]\n",
        "    if durs.size == 0:\n",
        "        print(f\"{name}: no valid durations found\")\n",
        "        return\n",
        "    def pct(a, q):\n",
        "        return float(np.percentile(a, q))\n",
        "    print(f\"{name} durations (n={len(durs)}): min={durs.min():.3f}s, mean={durs.mean():.3f}s, p50={pct(durs,50):.3f}s, p75={pct(durs,75):.3f}s, p90={pct(durs,90):.3f}s, p95={pct(durs,95):.3f}s, p99={pct(durs,99):.3f}s, max={durs.max():.3f}s\")\n",
        "\n",
        "print('Duration diagnostics \u2014 curated vs test\\n')\n",
        "describe_durations(cur_paths, 'Curated')\n",
        "describe_durations(test_paths, 'Test')\n",
        "print('\\nImplications: If test median >> curated median or tail is longer, prioritize multi-crop TTA (MC7) and robust handling of long clips. If test median << curated, ensure pad/loop policies are consistent.')\n",
        ""
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration diagnostics \u2014 curated vs test\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Curated durations (n=4970): min=0.300s, mean=7.648s, p50=4.676s, p75=11.146s, p90=20.349s, p95=24.497s, p99=29.000s, max=57.571s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test durations (n=3361): min=0.320s, mean=10.489s, p50=8.160s, p75=16.160s, p90=23.700s, p95=27.000s, p99=29.580s, max=30.000s\n\nImplications: If test median >> curated median or tail is longer, prioritize multi-crop TTA (MC7) and robust handling of long clips. If test median << curated, ensure pad/loop policies are consistent.\n"
          ]
        }
      ]
    },
    {
      "id": "1ecc797e-9624-46e1-a92e-02bf99e5bb8e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# C4 Phase 2 \u2014 Step 2: Strict-parity MC7 embeddings for curated and test (begin, 16%, 33%, 50%, 66%, 83%, end) with batched inference\n",
        "import time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa, torch\n",
        "\n",
        "BASE = Path('.')\n",
        "SR = 32000\n",
        "T_SEC = 10.0\n",
        "T = int(SR * T_SEC)\n",
        "K = 7\n",
        "EMB_DIM = 2048\n",
        "CKPT_PATH = Path('/app/panns_data/Cnn14_mAP=0.431.pth')\n",
        "assert CKPT_PATH.exists(), 'CNN14 checkpoint missing; prepare assets first.'\n",
        "\n",
        "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
        "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
        "train_files = df_cur['fname'].values\n",
        "test_files  = df_ss['fname'].values\n",
        "\n",
        "def load_audio(path, sr=SR):\n",
        "    y, s = librosa.load(path, sr=sr, mono=True)\n",
        "    return y.astype(np.float32)\n",
        "\n",
        "def mc7_starts(L: int, T: int):\n",
        "    if L <= T:\n",
        "        return [0]*7\n",
        "    span = max(L - T, 1)\n",
        "    # begin, 16%, 33%, center, 66%, 83%, end\n",
        "    ratios = [0.0, 1.0/6.0, 2.0/6.0, 0.5, 4.0/6.0, 5.0/6.0, 1.0]\n",
        "    starts = [int(round(r * span)) for r in ratios]\n",
        "    return [max(0, min(s, L - T)) for s in starts]\n",
        "\n",
        "def crops_for_wave(y: np.ndarray, T: int, K: int = 7):\n",
        "    L = len(y)\n",
        "    starts = mc7_starts(L, T)\n",
        "    if L >= T:\n",
        "        crops = [y[s:s+T] for s in starts]\n",
        "    else:\n",
        "        pad = np.pad(y, (0, T - L))\n",
        "        crops = [pad for _ in range(K)]\n",
        "    return np.stack(crops, 0)\n",
        "\n",
        "def extract_mc7_embeddings(file_list, root_dir, batch_size=16, log_every=200):\n",
        "    from panns_inference import AudioTagging\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f'Device: {device.upper()} | Batch size: {batch_size} | K=7')\n",
        "    at = AudioTagging(checkpoint_path=str(CKPT_PATH), device=device)\n",
        "    N = len(file_list)\n",
        "    X = np.zeros((N, EMB_DIM), dtype=np.float32)\n",
        "    t0 = time.time()\n",
        "    for start_idx in range(0, N, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, N)\n",
        "        batch_files = file_list[start_idx:end_idx]\n",
        "        batch_crops = []  # (B*K, T)\n",
        "        for fname in batch_files:\n",
        "            y = load_audio(str(Path(root_dir) / fname), sr=SR)\n",
        "            ck = crops_for_wave(y, T, 7)\n",
        "            batch_crops.append(ck)\n",
        "        batch_crops = np.concatenate(batch_crops, axis=0)\n",
        "        with torch.no_grad():\n",
        "            out = at.inference(batch_crops)\n",
        "        if isinstance(out, tuple) and len(out)==2:\n",
        "            embs = np.asarray(out[1], dtype=np.float32)\n",
        "        elif isinstance(out, dict) and 'embedding' in out:\n",
        "            embs = np.asarray(out['embedding'], dtype=np.float32)\n",
        "        else:\n",
        "            raise RuntimeError('Unexpected AudioTagging output type')\n",
        "        B = end_idx - start_idx\n",
        "        embs = embs.reshape(B, 7, EMB_DIM).mean(axis=1)\n",
        "        X[start_idx:end_idx] = embs\n",
        "        if (end_idx % log_every == 0) or (end_idx == N):\n",
        "            dt = time.time() - t0\n",
        "            print(f'  {end_idx}/{N} in {dt/60:.1f} min')\n",
        "    return X\n",
        "\n",
        "cur_path = BASE / 'embeddings_curated_mc7.npy'\n",
        "tst_path = BASE / 'embeddings_test_mc7.npy'\n",
        "\n",
        "if cur_path.exists():\n",
        "    X_cur_mc7 = np.load(cur_path)\n",
        "    print('Loaded cached embeddings_curated_mc7.npy')\n",
        "else:\n",
        "    bs = 16 if torch.cuda.is_available() else 64\n",
        "    print('Extracting curated MC7 embeddings ...')\n",
        "    X_cur_mc7 = extract_mc7_embeddings(train_files, BASE/'train_curated', batch_size=bs, log_every=200)\n",
        "    np.save(cur_path, X_cur_mc7)\n",
        "    print('Saved embeddings_curated_mc7.npy')\n",
        "\n",
        "if tst_path.exists():\n",
        "    X_test_mc7 = np.load(tst_path)\n",
        "    print('Loaded cached embeddings_test_mc7.npy')\n",
        "else:\n",
        "    bs = 16 if torch.cuda.is_available() else 64\n",
        "    print('Extracting test MC7 embeddings ...')\n",
        "    X_test_mc7 = extract_mc7_embeddings(test_files, BASE/'test', batch_size=bs, log_every=256)\n",
        "    np.save(tst_path, X_test_mc7)\n",
        "    print('Saved embeddings_test_mc7.npy')\n",
        "\n",
        "print('MC7 extraction complete. Shapes:', X_cur_mc7.shape, X_test_mc7.shape)\n",
        ""
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting curated MC7 embeddings ...\nDevice: CPU | Batch size: 64 | K=7\nCheckpoint path: /app/panns_data/Cnn14_mAP=0.431.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1600/4970 in 15.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  3200/4970 in 31.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  4800/4970 in 49.1 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  4970/4970 in 50.8 min\nSaved embeddings_curated_mc7.npy\nExtracting test MC7 embeddings ...\nDevice: CPU | Batch size: 64 | K=7\nCheckpoint path: /app/panns_data/Cnn14_mAP=0.431.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  256/3361 in 2.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  512/3361 in 4.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  768/3361 in 6.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1024/3361 in 8.3 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1280/3361 in 10.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1536/3361 in 12.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1792/3361 in 15.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2048/3361 in 18.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2304/3361 in 21.2 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2560/3361 in 23.8 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2816/3361 in 26.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  3072/3361 in 28.4 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  3328/3361 in 30.5 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  3361/3361 in 30.8 min\nSaved embeddings_test_mc7.npy\nMC7 extraction complete. Shapes: (4970, 2048) (3361, 2048)\n"
          ]
        }
      ]
    },
    {
      "id": "45da6591-5f74-4bdb-8feb-2346fb9fc0a8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# C4 Phase 2 \u2014 Step 3\u20135: Curated-only MC7 Ensemble (LR grid, PCA+LR, kNN) + Rank Blending + Per-class Temp Scaling -> submission.csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import json\n",
        "\n",
        "BASE = Path('.')\n",
        "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
        "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
        "class_names = [c for c in df_ss.columns if c != 'fname']\n",
        "n_classes = len(class_names)\n",
        "test_files = df_ss['fname'].values\n",
        "\n",
        "X_cur = np.load('embeddings_curated_mc7.npy')\n",
        "X_test = np.load('embeddings_test_mc7.npy')\n",
        "assert X_cur.ndim==2 and X_test.ndim==2 and X_cur.shape[1]==X_test.shape[1]==2048\n",
        "\n",
        "def parse_labels_str(s):\n",
        "    if not isinstance(s, str):\n",
        "        return []\n",
        "    return [t.strip() for t in s.replace(';', ',').split(',') if t.strip()]\n",
        "\n",
        "label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "def encode_labels(s):\n",
        "    y = np.zeros(n_classes, dtype=np.float32)\n",
        "    for t in parse_labels_str(s):\n",
        "        if t in label_to_idx:\n",
        "            y[label_to_idx[t]] = 1.0\n",
        "    return y\n",
        "Y_cur = np.stack(df_cur['labels'].apply(encode_labels).values).astype(np.float32)\n",
        "\n",
        "def lwlrap_np(truth, scores):\n",
        "    assert truth.shape == scores.shape\n",
        "    n_samples, n_labels = truth.shape\n",
        "    precisions = np.zeros(n_labels)\n",
        "    labels_per_class = np.maximum(truth.sum(axis=0), 1)\n",
        "    for i in range(n_samples):\n",
        "        pos = np.where(truth[i] > 0)[0]\n",
        "        if pos.size == 0:\n",
        "            continue\n",
        "        ranking = np.argsort(-scores[i])\n",
        "        ranked_truth = truth[i][ranking]\n",
        "        cumsum = np.cumsum(ranked_truth)\n",
        "        pos_rank = np.where(ranked_truth > 0)[0]\n",
        "        prec = cumsum[pos_rank] / (pos_rank + 1)\n",
        "        ranked_labels = ranking[pos_rank]\n",
        "        for lbl, p in zip(ranked_labels, prec):\n",
        "            precisions[lbl] += p\n",
        "    per_class = precisions / labels_per_class\n",
        "    weights = truth.sum(axis=0) / max(truth.sum(), 1)\n",
        "    return float((per_class * weights).sum()), per_class\n",
        "\n",
        "def ranks_0_1(x):\n",
        "    # x: (N,) vector, return ranks in [0,1]\n",
        "    order = np.argsort(x)\n",
        "    ranks = np.empty_like(order, dtype=np.float32)\n",
        "    ranks[order] = np.arange(len(x), dtype=np.float32)\n",
        "    if len(x) > 1:\n",
        "        ranks /= (len(x) - 1)\n",
        "    else:\n",
        "        ranks[:] = 0.0\n",
        "    return ranks\n",
        "\n",
        "def safe_logit(p, eps=1e-6):\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p / (1 - p))\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "def bce_loss(y, p, eps=1e-7):\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return float(-(y * np.log(p) + (1 - y) * np.log(1 - p)).mean())\n",
        "\n",
        "# Head definitions\n",
        "LR_C_GRID = [0.5, 1.0, 2.0, 4.0, 8.0]\n",
        "SEEDS = [42, 2025, 7]\n",
        "PCA_DIMS = [512, 1024]\n",
        "KNN_K = [50, 100]\n",
        "\n",
        "heads_info = []  # list of dicts: {name, oof, test, oof_lw}\n",
        "\n",
        "folds = df_cur['fold'].values.astype(int)\n",
        "N_FOLDS = np.unique(folds).size\n",
        "\n",
        "# Utility: fit OneVsRest LR head with StandardScaler on fold\n",
        "def fit_ovr_lr(X_tr, y_tr, X_va, X_te, C=2.0, seed=42):\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr_s = scaler.fit_transform(X_tr)\n",
        "    X_va_s = scaler.transform(X_va)\n",
        "    X_te_s = scaler.transform(X_te)\n",
        "    proba_va = np.zeros((X_va.shape[0], n_classes), dtype=np.float32)\n",
        "    proba_te = np.zeros((X_te.shape[0], n_classes), dtype=np.float32)\n",
        "    for c in range(n_classes):\n",
        "        lr = LogisticRegression(solver='lbfgs', max_iter=1000, C=C, n_jobs=1, random_state=seed)\n",
        "        lr.fit(X_tr_s, y_tr[:, c])\n",
        "        proba_va[:, c] = lr.predict_proba(X_va_s)[:, 1]\n",
        "        proba_te[:, c] = lr.predict_proba(X_te_s)[:, 1]\n",
        "    return proba_va, proba_te\n",
        "\n",
        "def fit_pca_lr(X_tr, y_tr, X_va, X_te, n_comp=512, C=2.0, seed=42):\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr_s = scaler.fit_transform(X_tr)\n",
        "    X_va_s = scaler.transform(X_va)\n",
        "    X_te_s = scaler.transform(X_te)\n",
        "    pca = PCA(n_components=n_comp, svd_solver='auto', random_state=seed)\n",
        "    X_tr_p = pca.fit_transform(X_tr_s)\n",
        "    X_va_p = pca.transform(X_va_s)\n",
        "    X_te_p = pca.transform(X_te_s)\n",
        "    proba_va = np.zeros((X_va.shape[0], n_classes), dtype=np.float32)\n",
        "    proba_te = np.zeros((X_te.shape[0], n_classes), dtype=np.float32)\n",
        "    for c in range(n_classes):\n",
        "        lr = LogisticRegression(solver='lbfgs', max_iter=1000, C=C, n_jobs=1, random_state=seed)\n",
        "        lr.fit(X_tr_p, y_tr[:, c])\n",
        "        proba_va[:, c] = lr.predict_proba(X_va_p)[:, 1]\n",
        "        proba_te[:, c] = lr.predict_proba(X_te_p)[:, 1]\n",
        "    return proba_va, proba_te\n",
        "\n",
        "def fit_knn(X_tr, y_tr, X_va, X_te, k=50):\n",
        "    # Multioutput KNN classification with cosine distance; use probability via distance weights\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr_s = scaler.fit_transform(X_tr)\n",
        "    X_va_s = scaler.transform(X_va)\n",
        "    X_te_s = scaler.transform(X_te)\n",
        "    knn = KNeighborsClassifier(n_neighbors=k, metric='cosine', weights='distance', n_jobs=1)\n",
        "    knn.fit(X_tr_s, y_tr)\n",
        "    # predict_proba returns list of arrays (n_classes long), each (N, 2)\n",
        "    def knn_proba(X):\n",
        "        probs_list = knn.predict_proba(X)\n",
        "        # Convert list to (N, C) of positive class probabilities\n",
        "        out = np.zeros((X.shape[0], n_classes), dtype=np.float32)\n",
        "        for c, pc in enumerate(probs_list):\n",
        "            # pc shape (N, n_classes_of_target_c). For binary, columns correspond to [class0, class1]\n",
        "            if pc.ndim == 2 and pc.shape[1] == 2:\n",
        "                out[:, c] = pc[:, 1].astype(np.float32)\n",
        "            else:\n",
        "                # fallback: decision function not available => use predictions\n",
        "                out[:, c] = (knn.predict(X)[:, c] > 0.5).astype(np.float32)\n",
        "        return out\n",
        "    return knn_proba(X_va_s), knn_proba(X_te_s)\n",
        "\n",
        "def run_head(name, fit_fn, *args, **kwargs):\n",
        "    oof = np.zeros((len(df_cur), n_classes), dtype=np.float32)\n",
        "    te = np.zeros((len(X_test), n_classes), dtype=np.float32)\n",
        "    for k in range(N_FOLDS):\n",
        "        trn_idx = np.where(folds != k)[0]\n",
        "        val_idx = np.where(folds == k)[0]\n",
        "        X_tr, y_tr = X_cur[trn_idx], Y_cur[trn_idx]\n",
        "        X_va, y_va = X_cur[val_idx], Y_cur[val_idx]\n",
        "        proba_va, proba_te = fit_fn(X_tr, y_tr, X_va, X_test, *args, **kwargs)\n",
        "        oof[val_idx] = proba_va.astype(np.float32)\n",
        "        te += proba_te.astype(np.float32)\n",
        "    te /= N_FOLDS\n",
        "    oof_lw, _ = lwlrap_np(Y_cur, oof)\n",
        "    heads_info.append({'name': name, 'oof': oof, 'test': te, 'oof_lw': float(oof_lw)})\n",
        "    print(f\"Head {name}: OOF LWLRAP={oof_lw:.4f}\")\n",
        "\n",
        "print('Training curated-only heads (leak-proof per fold) ...')\n",
        "# LR heads grid\n",
        "for C in LR_C_GRID:\n",
        "    for seed in SEEDS:\n",
        "        run_head(f'LR_C{C}_S{seed}', fit_ovr_lr, C, seed)\n",
        "\n",
        "# PCA+LR heads (reduced C grid for speed)\n",
        "for n_comp in PCA_DIMS:\n",
        "    for C in [1.0, 2.0]:\n",
        "        for seed in SEEDS:\n",
        "            run_head(f'PCA{n_comp}_LR_C{C}_S{seed}', fit_pca_lr, n_comp, C, seed)\n",
        "\n",
        "# kNN heads\n",
        "for k in KNN_K:\n",
        "    run_head(f'kNN_k{k}', fit_knn, k)\n",
        "\n",
        "# Rank blending with weights proportional to head OOF LWLRAP\n",
        "weights = np.array([h['oof_lw'] for h in heads_info], dtype=np.float64)\n",
        "w_sum = weights.sum()\n",
        "assert w_sum > 0\n",
        "\n",
        "def blend_by_rank(heads, split='oof'):\n",
        "    if split == 'oof':\n",
        "        N = heads[0]['oof'].shape[0]\n",
        "        blended = np.zeros((N, n_classes), dtype=np.float32)\n",
        "        for c in range(n_classes):\n",
        "            acc = np.zeros(N, dtype=np.float64)\n",
        "            for w, h in zip(weights, heads):\n",
        "                ranks = ranks_0_1(h['oof'][:, c])\n",
        "                acc += w * ranks\n",
        "            blended[:, c] = (acc / w_sum).astype(np.float32)\n",
        "        return blended\n",
        "    else:\n",
        "        N = heads[0]['test'].shape[0]\n",
        "        blended = np.zeros((N, n_classes), dtype=np.float32)\n",
        "        for c in range(n_classes):\n",
        "            acc = np.zeros(N, dtype=np.float64)\n",
        "            for w, h in zip(weights, heads):\n",
        "                ranks = ranks_0_1(h['test'][:, c])\n",
        "                acc += w * ranks\n",
        "            blended[:, c] = (acc / w_sum).astype(np.float32)\n",
        "        return blended\n",
        "\n",
        "blend_oof = blend_by_rank(heads_info, 'oof')\n",
        "blend_test = blend_by_rank(heads_info, 'test')\n",
        "blend_oof_lw, _ = lwlrap_np(Y_cur, blend_oof)\n",
        "print(f'Blended OOF LWLRAP (rank-avg, weighted) = {blend_oof_lw:.4f}')\n",
        "\n",
        "# Guardrail\n",
        "BASELINE = 0.8049  # C3A MC5 OOF, target to exceed here as curated-only MC7\n",
        "if blend_oof_lw < BASELINE - 1e-6:\n",
        "    print('WARNING: Blended OOF is below baseline. Investigate before trusting LB.')\n",
        "\n",
        "# Per-class Temperature Scaling on blended OOF ranks (treated as probs in [0,1])\n",
        "temps = np.ones(n_classes, dtype=np.float32)\n",
        "for c in range(n_classes):\n",
        "    y = Y_cur[:, c]\n",
        "    p = blend_oof[:, c]\n",
        "    z = safe_logit(p)\n",
        "    best_t, best_loss = 1.0, bce_loss(y, p)\n",
        "    for tau in np.linspace(0.5, 2.0, 16):\n",
        "        p_cal = sigmoid(z / tau)\n",
        "        loss = bce_loss(y, p_cal)\n",
        "        if loss < best_loss:\n",
        "            best_loss, best_t = loss, float(tau)\n",
        "    temps[c] = best_t\n",
        "print('Temperature scaling range (min/max):', float(temps.min()), float(temps.max()))\n",
        "\n",
        "# Apply temps to test blended ranks\n",
        "logits_test = safe_logit(blend_test)\n",
        "for c in range(n_classes):\n",
        "    logits_test[:, c] = logits_test[:, c] / max(temps[c], 1e-6)\n",
        "proba_test_final = sigmoid(logits_test).astype(np.float32)\n",
        "\n",
        "# Save submission and metrics\n",
        "sub = pd.DataFrame(proba_test_final, columns=class_names)\n",
        "sub.insert(0, 'fname', test_files)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "metrics = {\n",
        "    'stage': 'C4_curated_only_mc7_ensemble_rankblend_temp',\n",
        "    'n_heads': len(heads_info),\n",
        "    'heads': [{k: (float(v) if isinstance(v, (int,float)) else v) for k,v in {'name':h['name'], 'oof_lw':h['oof_lw']}.items()} for h in heads_info],\n",
        "    'blended_oof_lw': float(blend_oof_lw),\n",
        "    'temps_min': float(temps.min()), 'temps_max': float(temps.max())\n",
        "}\n",
        "with open('metrics_c4_curated_mc7.json', 'w') as f:\n",
        "    json.dump(metrics, f)\n",
        "print('Saved submission.csv and metrics_c4_curated_mc7.json. Heads used:', len(heads_info))\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}