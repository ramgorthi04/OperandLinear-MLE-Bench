{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b007db8",
   "metadata": {},
   "source": [
    "# C3A: Multi-Crop Embedding TTA (K=5)\n",
    "\n",
    "This notebook implements K=5 deterministic multi-crop TTA at the embedding level using PANNs CNN14.\n",
    "- Crops per clip (target T=10s @ 32kHz): begin, center, end, 25% offset, 75% offset.\n",
    "- For clips shorter than T: zero-pad to T and use same crops (effectively identical).\n",
    "- Aggregation: mean across the 5 embeddings (no pre-normalization).\n",
    "- CV: fixed 5-fold MLSKF from train_curated_folds.csv (seed=42).\n",
    "- Outputs: embeddings_curated_mc5.npy, embeddings_test_mc5.npy, metadata_c3a.json; OOF (oof_tta.npy), per-fold LWLRAP, submission.csv.\n",
    "- Environment: offline PANNs assets; versions logged for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed78d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess, os, json, time, warnings\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "def pip_install(pkg):\n",
    "\n",
    "    try:\n",
    "\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', pkg])\n",
    "\n",
    "        print(f'Installed: {pkg}')\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f'Failed to install {pkg}: {e}')\n",
    "\n",
    "\n",
    "\n",
    "for pkg in ['soundfile', 'librosa', 'iterative-stratification', 'joblib', 'scikit-learn']:\n",
    "\n",
    "    try:\n",
    "\n",
    "        __import__(pkg.split('==')[0].replace('-', '_'))\n",
    "\n",
    "        print(f'{pkg.split(\"==\")[0]} already available')\n",
    "\n",
    "    except Exception:\n",
    "\n",
    "        pip_install(pkg)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import joblib\n",
    "\n",
    "BASE = Path('.')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print('Versions:', {k:__import__(k).__version__ if hasattr(__import__(k), '__version__') else 'n/a' for k in ['librosa','torch','sklearn']})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60495dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "assets_dir = Path('/app/panns_data')\n",
    "\n",
    "assets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "labels_csv = assets_dir / 'class_labels_indices.csv'\n",
    "\n",
    "if not labels_csv.exists():\n",
    "\n",
    "    import csv\n",
    "\n",
    "    with open(labels_csv, 'w', newline='') as f:\n",
    "\n",
    "        w = csv.writer(f)\n",
    "\n",
    "        w.writerow(['index','mid','display_name'])\n",
    "\n",
    "        for i in range(527):\n",
    "\n",
    "            w.writerow([i, f'/m/{i}', f'class_{i:03d}'])\n",
    "\n",
    "    print('Created labels stub at', labels_csv)\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Labels CSV exists at', labels_csv)\n",
    "\n",
    "\n",
    "\n",
    "ckpt_path = assets_dir / 'Cnn14_mAP=0.431.pth'\n",
    "\n",
    "url = 'https://zenodo.org/record/3987831/files/Cnn14_mAP=0.431.pth?download=1'\n",
    "\n",
    "if not ckpt_path.exists() or ckpt_path.stat().st_size == 0:\n",
    "\n",
    "    print('Downloading CNN14 weights ...')\n",
    "\n",
    "    with urllib.request.urlopen(url) as resp, open(ckpt_path, 'wb') as out:\n",
    "\n",
    "        while True:\n",
    "\n",
    "            chunk = resp.read(1<<20)\n",
    "\n",
    "            if not chunk:\n",
    "\n",
    "                break\n",
    "\n",
    "            out.write(chunk)\n",
    "\n",
    "    print('Saved CNN14 to', ckpt_path, 'size:', ckpt_path.stat().st_size)\n",
    "\n",
    "else:\n",
    "\n",
    "    print('CNN14 checkpoint present:', ckpt_path, 'size:', ckpt_path.stat().st_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "BASE = Path('.')\n",
    "\n",
    "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
    "\n",
    "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
    "\n",
    "class_names = [c for c in df_ss.columns if c != 'fname']\n",
    "\n",
    "label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
    "\n",
    "n_classes = len(class_names)\n",
    "\n",
    "\n",
    "\n",
    "def parse_labels_str(s):\n",
    "\n",
    "    if not isinstance(s, str):\n",
    "\n",
    "        return []\n",
    "\n",
    "    toks = [t.strip() for t in s.replace(';', ',').split(',') if t.strip()]\n",
    "\n",
    "    unknown = [t for t in toks if t not in label_to_idx]\n",
    "\n",
    "    if unknown:\n",
    "\n",
    "        raise ValueError(f'Unknown labels: {unknown[:5]} (total {len(unknown)})')\n",
    "\n",
    "    return toks\n",
    "\n",
    "\n",
    "\n",
    "def encode_tokens(toks):\n",
    "\n",
    "    y = np.zeros(n_classes, dtype=np.float32)\n",
    "\n",
    "    for t in toks:\n",
    "\n",
    "        y[label_to_idx[t]] = 1.0\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def encode_labels(s):\n",
    "\n",
    "    return encode_tokens(parse_labels_str(s))\n",
    "\n",
    "\n",
    "\n",
    "def lwlrap_np(truth, scores):\n",
    "\n",
    "    assert truth.shape == scores.shape\n",
    "\n",
    "    n_samples, n_labels = truth.shape\n",
    "\n",
    "    precisions = np.zeros(n_labels)\n",
    "\n",
    "    labels_per_class = np.maximum(truth.sum(axis=0), 1)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "\n",
    "        pos = np.where(truth[i] > 0)[0]\n",
    "\n",
    "        if pos.size == 0:\n",
    "\n",
    "            continue\n",
    "\n",
    "        ranking = np.argsort(-scores[i])\n",
    "\n",
    "        ranked_truth = truth[i][ranking]\n",
    "\n",
    "        cumsum = np.cumsum(ranked_truth)\n",
    "\n",
    "        pos_rank = np.where(ranked_truth > 0)[0]\n",
    "\n",
    "        prec = cumsum[pos_rank] / (pos_rank + 1)\n",
    "\n",
    "        ranked_labels = ranking[pos_rank]\n",
    "\n",
    "        for lbl, p in zip(ranked_labels, prec):\n",
    "\n",
    "            precisions[lbl] += p\n",
    "\n",
    "    per_class = precisions / labels_per_class\n",
    "\n",
    "    weights = truth.sum(axis=0) / max(truth.sum(), 1)\n",
    "\n",
    "    return float((per_class * weights).sum()), per_class\n",
    "\n",
    "\n",
    "\n",
    "print('Loaded folds and helpers. Fold counts:', df_cur['fold'].value_counts().sort_index().to_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb200c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np, pandas as pd, time, json\n",
    "\n",
    "import librosa, torch\n",
    "\n",
    "from panns_inference import AudioTagging\n",
    "\n",
    "\n",
    "\n",
    "BASE = Path('.')\n",
    "\n",
    "SR = 32000\n",
    "\n",
    "T_SEC = 10.0\n",
    "\n",
    "T = int(SR * T_SEC)\n",
    "\n",
    "K = 5\n",
    "\n",
    "EMB_DIM = 2048\n",
    "\n",
    "CKPT_PATH = Path('/app/panns_data/Cnn14_mAP=0.431.pth')\n",
    "\n",
    "assert CKPT_PATH.exists(), 'Checkpoint missing; run Cell 3.'\n",
    "\n",
    "\n",
    "\n",
    "def load_audio(path, sr=SR):\n",
    "\n",
    "    y, s = librosa.load(path, sr=sr, mono=True)\n",
    "\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "def crop_starts(L, T):\n",
    "\n",
    "    if L <= T:\n",
    "\n",
    "        return [0, 0, 0, 0, 0]\n",
    "\n",
    "    # begin, center, end, 25%, 75% (clamped)\n",
    "\n",
    "    starts = [0, (L - T)//2, L - T, int(0.25*(L - T)), int(0.75*(L - T))]\n",
    "\n",
    "    starts = [max(0, min(s, L - T)) for s in starts]\n",
    "\n",
    "    return starts\n",
    "\n",
    "\n",
    "\n",
    "def crops_for_wave(y, T):\n",
    "\n",
    "    L = len(y)\n",
    "\n",
    "    starts = crop_starts(L, T)\n",
    "\n",
    "    crops = []\n",
    "\n",
    "    for s in starts:\n",
    "\n",
    "        if L >= T:\n",
    "\n",
    "            crops.append(y[s:s+T])\n",
    "\n",
    "        else:\n",
    "\n",
    "            pad = np.pad(y, (0, T - L))\n",
    "\n",
    "            crops.append(pad)\n",
    "\n",
    "    return np.stack(crops, 0)  # (K, T)\n",
    "\n",
    "\n",
    "\n",
    "def extract_mc5_embeddings(file_list, root_dir):\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    at = AudioTagging(checkpoint_path=str(CKPT_PATH), device=device)\n",
    "\n",
    "    X = np.zeros((len(file_list), EMB_DIM), dtype=np.float32)\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i, fname in enumerate(file_list):\n",
    "\n",
    "        y = load_audio(str(Path(root_dir) / fname), sr=SR)\n",
    "\n",
    "        crops = crops_for_wave(y, T)  # (5, T)\n",
    "\n",
    "        embs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for k in range(crops.shape[0]):\n",
    "\n",
    "                out = at.inference(np.expand_dims(crops[k], 0))\n",
    "\n",
    "                if isinstance(out, tuple) and len(out)==2:\n",
    "\n",
    "                    emb = out[1][0]\n",
    "\n",
    "                elif isinstance(out, dict) and 'embedding' in out:\n",
    "\n",
    "                    emb = np.asarray(out['embedding'], dtype=np.float32)[0]\n",
    "\n",
    "                else:\n",
    "\n",
    "                    raise RuntimeError('Unexpected AudioTagging output type')\n",
    "\n",
    "                embs.append(emb.astype(np.float32))\n",
    "\n",
    "        X[i] = np.mean(np.stack(embs, 0), axis=0)\n",
    "\n",
    "        if (i+1) % 200 == 0:\n",
    "\n",
    "            dt = time.time() - t0\n",
    "\n",
    "            print(f'  {i+1}/{len(file_list)} files processed in {dt/60:.1f} min')\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
    "\n",
    "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
    "\n",
    "train_files = df_cur['fname'].values\n",
    "\n",
    "test_files  = df_ss['fname'].values\n",
    "\n",
    "\n",
    "\n",
    "emb_cur_mc5_path = BASE / 'embeddings_curated_mc5.npy'\n",
    "\n",
    "emb_test_mc5_path = BASE / 'embeddings_test_mc5.npy'\n",
    "\n",
    "\n",
    "\n",
    "if emb_cur_mc5_path.exists() and emb_test_mc5_path.exists():\n",
    "\n",
    "    X_cur = np.load(emb_cur_mc5_path)\n",
    "\n",
    "    X_test = np.load(emb_test_mc5_path)\n",
    "\n",
    "    print('Loaded cached MC5 embeddings.')\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Extracting curated MC5 embeddings ...')\n",
    "\n",
    "    X_cur = extract_mc5_embeddings(train_files, root_dir=BASE/'train_curated')\n",
    "\n",
    "    np.save(emb_cur_mc5_path, X_cur)\n",
    "\n",
    "    print('Extracting test MC5 embeddings ...')\n",
    "\n",
    "    X_test = extract_mc5_embeddings(test_files, root_dir=BASE/'test')\n",
    "\n",
    "    np.save(emb_test_mc5_path, X_test)\n",
    "\n",
    "    print('Saved MC5 embeddings.')\n",
    "\n",
    "\n",
    "\n",
    "meta = {\n",
    "\n",
    "    'tta': 'mc5',\n",
    "\n",
    "    'sr': SR, 'T_sec': T_SEC, 'T': T,\n",
    "\n",
    "    'crops': 'begin,center,end,25%,75%',\n",
    "\n",
    "    'aggregation': 'mean',\n",
    "\n",
    "}\n",
    "\n",
    "with open('metadata_c3a.json', 'w') as f:\n",
    "\n",
    "    json.dump(meta, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191edfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "\n",
    "BASE = Path('.')\n",
    "\n",
    "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
    "\n",
    "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
    "\n",
    "class_names = [c for c in df_ss.columns if c != 'fname']\n",
    "\n",
    "label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
    "\n",
    "n_classes = len(class_names)\n",
    "\n",
    "\n",
    "\n",
    "def parse_labels_str(s):\n",
    "\n",
    "    if not isinstance(s, str):\n",
    "\n",
    "        return []\n",
    "\n",
    "    toks = [t.strip() for t in s.replace(';', ',').split(',') if t.strip()]\n",
    "\n",
    "    return toks\n",
    "\n",
    "\n",
    "\n",
    "def encode_labels(s):\n",
    "\n",
    "    y = np.zeros(n_classes, dtype=np.float32)\n",
    "\n",
    "    for t in parse_labels_str(s):\n",
    "\n",
    "        if t in label_to_idx:\n",
    "\n",
    "            y[label_to_idx[t]] = 1.0\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def lwlrap_np(truth, scores):\n",
    "\n",
    "    assert truth.shape == scores.shape\n",
    "\n",
    "    n_samples, n_labels = truth.shape\n",
    "\n",
    "    precisions = np.zeros(n_labels)\n",
    "\n",
    "    labels_per_class = np.maximum(truth.sum(axis=0), 1)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "\n",
    "        pos = np.where(truth[i] > 0)[0]\n",
    "\n",
    "        if pos.size == 0:\n",
    "\n",
    "            continue\n",
    "\n",
    "        ranking = np.argsort(-scores[i])\n",
    "\n",
    "        ranked_truth = truth[i][ranking]\n",
    "\n",
    "        cumsum = np.cumsum(ranked_truth)\n",
    "\n",
    "        pos_rank = np.where(ranked_truth > 0)[0]\n",
    "\n",
    "        prec = cumsum[pos_rank] / (pos_rank + 1)\n",
    "\n",
    "        ranked_labels = ranking[pos_rank]\n",
    "\n",
    "        for lbl, p in zip(ranked_labels, prec):\n",
    "\n",
    "            precisions[lbl] += p\n",
    "\n",
    "    per_class = precisions / labels_per_class\n",
    "\n",
    "    weights = truth.sum(axis=0) / max(truth.sum(), 1)\n",
    "\n",
    "    return float((per_class * weights).sum()), per_class\n",
    "\n",
    "\n",
    "\n",
    "X_cur = np.load('embeddings_curated_mc5.npy')\n",
    "\n",
    "X_test = np.load('embeddings_test_mc5.npy')\n",
    "\n",
    "Y_cur = np.stack(df_cur['labels'].apply(encode_labels).values).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "oof = np.zeros((len(df_cur), n_classes), dtype=np.float32)\n",
    "\n",
    "fold_scores = []\n",
    "\n",
    "for k in range(5):\n",
    "\n",
    "    trn_idx = np.where(df_cur['fold'].values != k)[0]\n",
    "\n",
    "    val_idx = np.where(df_cur['fold'].values == k)[0]\n",
    "\n",
    "    X_tr, X_va = X_cur[trn_idx], X_cur[val_idx]\n",
    "\n",
    "    y_tr, y_va = Y_cur[trn_idx], Y_cur[val_idx]\n",
    "\n",
    "    base_lr = LogisticRegression(solver='lbfgs', max_iter=1000, C=2.0, n_jobs=16, verbose=0)\n",
    "\n",
    "    clf = OneVsRestClassifier(make_pipeline(StandardScaler(with_mean=True, with_std=True), base_lr), n_jobs=-1)\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    proba = clf.predict_proba(X_va)\n",
    "\n",
    "    oof[val_idx] = proba.astype(np.float32)\n",
    "\n",
    "    lw, _ = lwlrap_np(y_va, proba)\n",
    "\n",
    "    fold_scores.append(lw)\n",
    "\n",
    "    print(f'Fold {k} LWLRAP={lw:.4f}')\n",
    "\n",
    "oof_lw, _ = lwlrap_np(Y_cur, oof)\n",
    "\n",
    "print(f'OOF LWLRAP (MC5)={oof_lw:.4f}; per-fold={fold_scores}')\n",
    "\n",
    "np.save('oof_tta.npy', oof)\n",
    "\n",
    "\n",
    "\n",
    "base_lr_full = LogisticRegression(solver='lbfgs', max_iter=1000, C=2.0, n_jobs=16, verbose=0)\n",
    "\n",
    "clf_full = OneVsRestClassifier(make_pipeline(StandardScaler(with_mean=True, with_std=True), base_lr_full), n_jobs=-1)\n",
    "\n",
    "clf_full.fit(X_cur, Y_cur)\n",
    "\n",
    "test_proba = clf_full.predict_proba(X_test).astype(np.float32)\n",
    "\n",
    "sub = pd.DataFrame(test_proba, columns=class_names)\n",
    "\n",
    "sub.insert(0, 'fname', df_ss['fname'].values)\n",
    "\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('Saved submission.csv (MC5). Shape:', sub.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
