{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc8d39d",
   "metadata": {},
   "source": [
    "# Freesound Audio Tagging 2019 — Medal-Driven Notebook (Plan v2.1: Gold-Only Strategy, Mandatory Fixes Applied)\n",
    "\n",
    "Experiment Log and Plan (v2.1 — incorporates C0 audit + mandatory revisions):\n",
    "- Objective: GOLD medal. Optimize Label-Weighted LRAP (LWLRAP). Focus on SOTA pretrained encoders, robust noisy-label handling, strong CV, TTA, and ensembling.\n",
    "- Data artifacts present: train_curated.csv, train_noisy.csv, train_curated.zip, train_noisy.zip, test.zip, sample_submission.csv.\n",
    "- Protocol: concise notebook, document attempts, backup before major changes, delete stale code, submit for audit at milestones.\n",
    "\n",
    "    Unified Gold Strategy (Single Path Only)\n",
    "1) Encoder: Fine-tune pretrained PANNs (primary: CNN14, backup: ResNet38). Use log-mel frontend matching PANNs defaults.\n",
    "2) CV: 5-fold MultilabelStratifiedKFold (seed=42), stratifying on 80-class binary matrix. Track per-fold/per-class LRAP and global LWLRAP.\n",
    "3) Noisy-label protocol: teacher-student and curriculum with confidence filtering/weighting.\n",
    "4) Inference: strong TTA (multi time-crop), fold-averaging, rank-aware ensembling across diverse models/seeds.\n",
    "\n",
    "Details\n",
    "- Label Space:\n",
    "  - Parse label column (semicolon-separated). Build consistent class list sorted to match sample_submission column order.\n",
    "  - Binarize y for CV and training. Save mapping for inference.\n",
    "\n",
    "- Audio Preprocessing (Log-mel aligned to PANNs):\n",
    "  - Sample rate: 32,000 Hz (resample all audio to 32k).\n",
    "  - Clip duration: 10.0 s target per sample.\n",
    "  - Variable length policy: during training, random time-crop to 10 s if longer; if shorter, loop-pad (tile then trim) with 0.1 probability; else zero-pad to 10 s. At inference, use multi-crop TTA (see below).\n",
    "  - STFT: n_fft=1024, hop_length=320 (10 ms hop), win_length=1024, window=hann, center=True.\n",
    "  - Mel: PRIMARY n_mels=64 (to match PANNs CNN14 pretraining); diversity models may use n_mels=128 or 256. fmin=50 Hz, fmax=16000 Hz, htk=False, norm=None.\n",
    "  - Log scale: use librosa.power_to_db on mel power; then standardize per-frequency bin with dataset mean/std (computed on curated train); clamp to [-10, 10] after standardization for stability.\n",
    "  - Channel: mono (downmix).\n",
    "\n",
    "- Model Architecture:\n",
    "  - Base: PANNs CNN14 pretrained on AudioSet (log-mel, 64 mel). Replace classifier with attention pooling head for 80 classes.\n",
    "  - Pooling: attention pooling over time (linear attention + context gating) instead of mean pooling.\n",
    "  - Head: Dropout p=0.5 before final linear; output 80 logits.\n",
    "  - Alternate diversity models: PANNs ResNet38; CNN14 with 128/256 mels; crop lengths 5 s and 12 s variants.\n",
    "\n",
    "- Losses and Label Handling:\n",
    "  - Primary loss: BCEWithLogitsLoss.\n",
    "  - Class weighting: inverse sqrt class frequency from curated train; normalize weights to mean=1.0.\n",
    "  - Label smoothing: 0.05 on positives (targets y -> y*(1-0.05) + 0.5*0.05), negatives stay at 0.\n",
    "  - Robust noise alternatives: Generalized Cross Entropy (q=0.7) or Symmetric Cross Entropy (alpha=1.0, beta=0.5) if needed in noisy stages.\n",
    "\n",
    "- Optimizer, Schedule, Checkpointing:\n",
    "  - Optimizer: AdamW (betas=(0.9, 0.999), weight_decay=1e-4).\n",
    "  - LR schedule: cosine with warmup. Base LR 2e-4 for head, 1e-4 for encoder (param groups). Warmup 1 epoch (or 1000 steps), then cosine decay.\n",
    "  - Epochs: 20 epochs curated-only warm start; 10–15 epochs with noisy curriculum stages (see below).\n",
    "  - Mixed precision (AMP) enabled; gradient clipping at 5.0.\n",
    "  - Batch size: as large as fits GPU (V100 16GB): target 32 for 64-mel 10 s; adjust dynamically.\n",
    "  - Early stopping & checkpoints: monitor validation LWLRAP each epoch, save best checkpoint (highest LWLRAP); patience=3 epochs for early stop. Use best-val checkpoint for inference.\n",
    "\n",
    "- Data Augmentations:\n",
    "  - Time-domain: random gain [-6, +6] dB; pink/gaussian noise injection SNR ~ 20–30 dB; time shift ±0.5 s; mild pitch shift (±2 semitones) and time stretch (0.9–1.1).\n",
    "  - Spectrogram: SpecAugment — 2 freq masks (width up to 20 mel bins), 2 time masks (up to 10% of frames), without masking entire clip.\n",
    "  - MixUp: alpha=0.4 on spectrograms/logits; labels mixed linearly; probability 0.5.\n",
    "  - Random time-crop as above; for multi-scale training, occasionally use 5 s or 12 s crops (p=0.2 each) in diversity runs.\n",
    "\n",
    "- Cross-Validation (explicit):\n",
    "  - Iterative Stratification (MultilabelStratifiedKFold) with n_splits=5, shuffle=True, random_state=42.\n",
    "  - Stratify on 80-dim binary labels from curated train only; keep folds disjoint by filename.\n",
    "  - Metrics per fold: LWLRAP (primary), per-class LRAP, macro/micro AUC for diagnostics; early-stop on LWLRAP.\n",
    "\n",
    "- Noisy Data Protocol (multi-step):\n",
    "  1) Train a strong teacher on curated-only (5-fold, out-of-fold predictions saved for all curated).\n",
    "  2) Use the teacher (fold models averaged) to infer probabilities on train_noisy.\n",
    "  3) Confidence filtering:\n",
    "     - Positive selection: keep labels where teacher prob for that class ≥ 0.8; set others to 0 for that sample.\n",
    "     - Optional addition: add teacher positive pseudo-labels for classes ≥ 0.95 even if not present in weak labels.\n",
    "     - Discard samples with no remaining positives after filtering.\n",
    "  4) Weighting: per-sample weight = max teacher prob among positives; clip to [0.5, 1.0]. Also scale by class weight as above.\n",
    "  5) Curriculum retraining:\n",
    "     - Stage A: fine-tune teacher from curated checkpoint adding filtered noisy with low weight (0.5) for 3–5 epochs.\n",
    "     - Stage B: increase noisy sample weights to 0.75–1.0 for 5–10 epochs; optionally unfreeze more encoder layers.\n",
    "     - If noisy destabilizes LWLRAP, swap to GCE (q=0.7) or SCE (α=1, β=0.5).\n",
    "  6) Recompute OOF on curated and evaluate LWLRAP improvements before proceeding.\n",
    "\n",
    "- Sampling Strategy:\n",
    "  - Balanced batch sampler: ensure each batch includes rare classes via inverse-frequency sampling on curated; cap oversampling at 5x.\n",
    "\n",
    "- Inference, Rank Averaging, and Mapping Back to Probabilities:\n",
    "  - TTA per model/fold: K=5 time-crops of 10 s each spaced uniformly; for clips <10 s, use varied pad/loop starts. For long clips, consider K=10 if time allows.\n",
    "  - Compute logits per crop; average logits across crops; apply sigmoid at the end.\n",
    "  - Fold/model ensembling: perform per-class rank averaging across models/folds/TTAs.\n",
    "  - Rank→probability mapping (explicit): per class, min-max normalize averaged ranks to [0,1] using CV distribution; then apply per-class temperature scaling (scalar τ_c learned on OOF/CV by minimizing BCE) to calibrate probabilities. Ensure final outputs ∈ [0,1].\n",
    "\n",
    "- Ensembling Plan (diversity targets):\n",
    "  - At least 4–6 models:\n",
    "    1) CNN14, 64 mels, 10 s crop, BCE+LS, seed 42.\n",
    "    2) CNN14, 64 mels, 10 s crop, BCE+LS, seed 2025.\n",
    "    3) CNN14, 128 mels, 10 s crop.\n",
    "    4) ResNet38, 64 mels, 10 s crop.\n",
    "    5) CNN14, 64 mels, 10 s crop with focal loss (gamma 1.5) variant.\n",
    "    6) CNN14, 64 mels, multi-scale crops (5/10/12 s schedule).\n",
    "  - Blend via rank averaging with per-model weights proportional to CV LWLRAP (normalize weights to sum=1).\n",
    "\n",
    "- Engineering & Efficiency:\n",
    "  - Augmentation vs caching: to preserve augmentation diversity, compute spectrograms on-the-fly during training (torchaudio preferred). Optionally cache only for inference or for fixed crops used in TTA.\n",
    "  - Use PyTorch DataLoader with num_workers=8–12, prefetch and pinned memory; cudnn.benchmark=True.\n",
    "  - Determinism: set all seeds; log configs and per-fold metrics; save checkpoints and OOF predictions.\n",
    "  - Notebook backups: programmatically save a copy of agent_notebook.ipynb before major refactors.\n",
    "\n",
    "Milestones & Audits\n",
    "- C0 (this): Gold-only Plan v2.1 with mandatory fixes — Approved.\n",
    "- C1: Data loading, label parsing, CV split, LWLRAP implementation check, basic EDA (label counts/durations), and mel frontend prototype aligned to PANNs (64 mel).\n",
    "- C2: PANNs CNN14 curated-only 5-fold training; OOF LWLRAP reported; baseline submission (safety).\n",
    "- C3: Noisy protocol (teacher inference, filtering/weighting), curriculum fine-tune; updated CV.\n",
    "- C4: TTA and initial ensembling across folds/seeds; submit improved predictions.\n",
    "- C5: Diversity models (ResNet38, 128/256 mels), final rank-averaged ensemble; final submission.\n",
    "\n",
    "Next Action: Proceed to C1 implementation with this corrected plan (PANNs-aligned 64-mel frontend, explicit rank→prob mapping, and defined checkpointing/early stopping).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "157e80f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T14:01:26.607294Z",
     "iopub.status.busy": "2025-08-11T14:01:26.606495Z",
     "iopub.status.idle": "2025-08-11T14:01:35.231372Z",
     "shell.execute_reply": "2025-08-11T14:01:35.230331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soundfile already available\n",
      "librosa already available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed: iterative-stratification\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joblib already available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed: scikit-learn\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Consolidated setup and imports (single source of truth)\n",
    "\n",
    "import sys, subprocess, os, json, time, warnings\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def pip_install(pkg):\n",
    "\n",
    "    try:\n",
    "\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', pkg])\n",
    "\n",
    "        print(f'Installed: {pkg}')\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f'Failed to install {pkg}: {e}')\n",
    "\n",
    "\n",
    "\n",
    "for pkg in ['soundfile', 'librosa', 'iterative-stratification', 'joblib', 'scikit-learn']:\n",
    "\n",
    "    try:\n",
    "\n",
    "        __import__(pkg.split('==')[0].replace('-', '_'))\n",
    "\n",
    "        print(f'{pkg.split(\"==\")[0]} already available')\n",
    "\n",
    "    except Exception:\n",
    "\n",
    "        pip_install(pkg)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import joblib\n",
    "\n",
    "BASE = Path('.')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1970aa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T14:01:35.236593Z",
     "iopub.status.busy": "2025-08-11T14:01:35.236108Z",
     "iopub.status.idle": "2025-08-11T14:03:36.186449Z",
     "shell.execute_reply": "2025-08-11T14:03:36.185396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created labels stub at /app/panns_data/class_labels_indices.csv\n",
      "Downloading CNN14 weights ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CNN14 to /app/panns_data/Cnn14_mAP=0.431.pth size: 327428481\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Offline PANNs asset preparation (labels CSV stub + CNN14 checkpoint)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "assets_dir = Path('/app/panns_data')\n",
    "\n",
    "assets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "labels_csv = assets_dir / 'class_labels_indices.csv'\n",
    "\n",
    "if not labels_csv.exists():\n",
    "\n",
    "    import csv\n",
    "\n",
    "    with open(labels_csv, 'w', newline='') as f:\n",
    "\n",
    "        w = csv.writer(f)\n",
    "\n",
    "        w.writerow(['index','mid','display_name'])\n",
    "\n",
    "        for i in range(527):\n",
    "\n",
    "            w.writerow([i, f'/m/{i}', f'class_{i:03d}'])\n",
    "\n",
    "    print('Created labels stub at', labels_csv)\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Labels CSV exists at', labels_csv)\n",
    "\n",
    "\n",
    "\n",
    "ckpt_path = assets_dir / 'Cnn14_mAP=0.431.pth'\n",
    "\n",
    "url = 'https://zenodo.org/record/3987831/files/Cnn14_mAP=0.431.pth?download=1'\n",
    "\n",
    "if not ckpt_path.exists() or ckpt_path.stat().st_size == 0:\n",
    "\n",
    "    print('Downloading CNN14 weights ...')\n",
    "\n",
    "    with urllib.request.urlopen(url) as resp, open(ckpt_path, 'wb') as out:\n",
    "\n",
    "        while True:\n",
    "\n",
    "            chunk = resp.read(1<<20)\n",
    "\n",
    "            if not chunk:\n",
    "\n",
    "                break\n",
    "\n",
    "            out.write(chunk)\n",
    "\n",
    "    print('Saved CNN14 to', ckpt_path, 'size:', ckpt_path.stat().st_size)\n",
    "\n",
    "else:\n",
    "\n",
    "    print('CNN14 checkpoint present:', ckpt_path, 'size:', ckpt_path.stat().st_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5450f8c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T14:03:36.191467Z",
     "iopub.status.busy": "2025-08-11T14:03:36.191159Z",
     "iopub.status.idle": "2025-08-11T14:03:36.462114Z",
     "shell.execute_reply": "2025-08-11T14:03:36.461230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train_curated_folds.csv and metadata.json; fold counts:\n",
      "fold\n",
      "0    999\n",
      "1    994\n",
      "2    992\n",
      "3    997\n",
      "4    988\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Data foundation, robust label parsing, MLSKF folds (no fallback)\n",
    "\n",
    "BASE = Path('.')\n",
    "\n",
    "df_cur = pd.read_csv(BASE / 'train_curated.csv')\n",
    "\n",
    "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
    "\n",
    "fname_col = 'fname' if 'fname' in df_cur.columns else df_cur.columns[0]\n",
    "\n",
    "labels_col = 'labels' if 'labels' in df_cur.columns else df_cur.columns[-1]\n",
    "\n",
    "class_names = [c for c in df_ss.columns if c != 'fname']\n",
    "\n",
    "label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
    "\n",
    "n_classes = len(class_names)\n",
    "\n",
    "\n",
    "\n",
    "def parse_labels_str(s):\n",
    "\n",
    "    if not isinstance(s, str):\n",
    "\n",
    "        return []\n",
    "\n",
    "    toks = [t.strip() for t in s.replace(';', ',').split(',') if t.strip()]\n",
    "\n",
    "    unknown = [t for t in toks if t not in label_to_idx]\n",
    "\n",
    "    if unknown:\n",
    "\n",
    "        raise ValueError(f'Unknown labels: {unknown[:5]} (total {len(unknown)})')\n",
    "\n",
    "    return toks\n",
    "\n",
    "\n",
    "\n",
    "def encode_tokens(toks):\n",
    "\n",
    "    y = np.zeros(n_classes, dtype=np.float32)\n",
    "\n",
    "    for t in toks:\n",
    "\n",
    "        y[label_to_idx[t]] = 1.0\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def encode_labels(s):\n",
    "\n",
    "    return encode_tokens(parse_labels_str(s))\n",
    "\n",
    "\n",
    "\n",
    "tokens = df_cur[labels_col].apply(parse_labels_str)\n",
    "\n",
    "Y_cur = np.stack(tokens.apply(encode_tokens).values)\n",
    "\n",
    "mlsk = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "folds = np.full(len(df_cur), -1, dtype=int)\n",
    "\n",
    "for k, (_, val_idx) in enumerate(mlsk.split(df_cur[fname_col].values, Y_cur)):\n",
    "\n",
    "    folds[val_idx] = k\n",
    "\n",
    "assert (folds >= 0).all()\n",
    "\n",
    "df_cur['fold'] = folds\n",
    "\n",
    "df_cur.to_csv('train_curated_folds.csv', index=False)\n",
    "\n",
    "with open('metadata.json', 'w') as f:\n",
    "\n",
    "    json.dump({'class_names': class_names, 'label_to_idx': label_to_idx, 'fname_col': fname_col, 'labels_col': labels_col}, f)\n",
    "\n",
    "print('Saved train_curated_folds.csv and metadata.json; fold counts:')\n",
    "\n",
    "print(df_cur['fold'].value_counts().sort_index())\n",
    "\n",
    "\n",
    "\n",
    "def lwlrap_np(truth, scores):\n",
    "\n",
    "    assert truth.shape == scores.shape\n",
    "\n",
    "    n_samples, n_labels = truth.shape\n",
    "\n",
    "    precisions = np.zeros(n_labels)\n",
    "\n",
    "    labels_per_class = np.maximum(truth.sum(axis=0), 1)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "\n",
    "        pos = np.where(truth[i] > 0)[0]\n",
    "\n",
    "        if pos.size == 0:\n",
    "\n",
    "            continue\n",
    "\n",
    "        ranking = np.argsort(-scores[i])\n",
    "\n",
    "        ranked_truth = truth[i][ranking]\n",
    "\n",
    "        cumsum = np.cumsum(ranked_truth)\n",
    "\n",
    "        pos_rank = np.where(ranked_truth > 0)[0]\n",
    "\n",
    "        prec = cumsum[pos_rank] / (pos_rank + 1)\n",
    "\n",
    "        ranked_labels = ranking[pos_rank]\n",
    "\n",
    "        for lbl, p in zip(ranked_labels, prec):\n",
    "\n",
    "            precisions[lbl] += p\n",
    "\n",
    "    per_class = precisions / labels_per_class\n",
    "\n",
    "    weights = truth.sum(axis=0) / max(truth.sum(), 1)\n",
    "\n",
    "    return float((per_class * weights).sum()), per_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a513060a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T14:03:36.466987Z",
     "iopub.status.busy": "2025-08-11T14:03:36.466442Z",
     "iopub.status.idle": "2025-08-11T15:34:54.820806Z",
     "shell.execute_reply": "2025-08-11T15:34:54.819183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting curated embeddings ...\n",
      "Checkpoint path: /app/panns_data/Cnn14_mAP=0.431.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  200/4970 in 4.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  400/4970 in 13.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  600/4970 in 22.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  800/4970 in 32.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1000/4970 in 40.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1200/4970 in 45.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1400/4970 in 49.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1600/4970 in 50.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1800/4970 in 51.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2000/4970 in 52.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2200/4970 in 53.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2400/4970 in 54.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2600/4970 in 55.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2800/4970 in 56.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3000/4970 in 57.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3200/4970 in 58.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3400/4970 in 59.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3600/4970 in 60.9 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3800/4970 in 62.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4000/4970 in 63.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4200/4970 in 64.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4400/4970 in 65.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4600/4970 in 66.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4800/4970 in 67.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test embeddings ...\n",
      "Checkpoint path: /app/panns_data/Cnn14_mAP=0.431.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  200/3361 in 1.3 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  400/3361 in 2.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  600/3361 in 3.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  800/3361 in 5.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1000/3361 in 6.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1200/3361 in 7.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1400/3361 in 8.7 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1600/3361 in 9.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1800/3361 in 11.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2000/3361 in 12.2 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2200/3361 in 13.5 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2400/3361 in 14.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2600/3361 in 16.1 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2800/3361 in 17.4 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3000/3361 in 18.6 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3200/3361 in 19.8 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 LWLRAP=0.8091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 LWLRAP=0.8004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 LWLRAP=0.8080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 LWLRAP=0.8097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 LWLRAP=0.7748\n",
      "OOF LWLRAP=0.8001; per-fold=[0.8091265640638446, 0.8004257362345152, 0.8080218131533763, 0.8096867591565071, 0.7748105563971049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission.csv. Shape: (3361, 81)\n",
      "Saved classifier checkpoint.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: PANNs CNN14 embeddings + OVR Logistic Regression (OOF + submission)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "import librosa, torch, joblib, time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from panns_inference import AudioTagging\n",
    "\n",
    "\n",
    "\n",
    "BASE = Path('.')\n",
    "\n",
    "SR = 32000\n",
    "\n",
    "CROP_SEC = 10.0\n",
    "\n",
    "EMB_DIM = 2048\n",
    "\n",
    "N_FOLDS = 5\n",
    "\n",
    "CKPT_PATH = Path('/app/panns_data/Cnn14_mAP=0.431.pth')\n",
    "\n",
    "assert CKPT_PATH.exists(), 'CNN14 checkpoint missing; run Cell 3 first.'\n",
    "\n",
    "\n",
    "\n",
    "df_cur = pd.read_csv(BASE / 'train_curated_folds.csv')\n",
    "\n",
    "df_ss  = pd.read_csv(BASE / 'sample_submission.csv')\n",
    "\n",
    "class_names = [c for c in df_ss.columns if c != 'fname']\n",
    "\n",
    "label_to_idx = {c:i for i,c in enumerate(class_names)}\n",
    "\n",
    "n_classes = len(class_names)\n",
    "\n",
    "train_dir = BASE / 'train_curated'\n",
    "\n",
    "test_dir  = BASE / 'test'\n",
    "\n",
    "\n",
    "\n",
    "def load_center_crop_10s(path, sr=SR, crop_sec=CROP_SEC):\n",
    "\n",
    "    y, s = librosa.load(path, sr=sr, mono=True)\n",
    "\n",
    "    target = int(sr * crop_sec)\n",
    "\n",
    "    if len(y) >= target:\n",
    "\n",
    "        start = max(0, (len(y) - target)//2)\n",
    "\n",
    "        y = y[start:start+target]\n",
    "\n",
    "    else:\n",
    "\n",
    "        y = np.pad(y, (0, target-len(y)))\n",
    "\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "emb_cur_path = BASE / 'embeddings_curated.npy'\n",
    "\n",
    "emb_test_path = BASE / 'embeddings_test.npy'\n",
    "\n",
    "\n",
    "\n",
    "def extract_embeddings(file_list, root_dir):\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    at = AudioTagging(checkpoint_path=str(CKPT_PATH), device=device)\n",
    "\n",
    "    X = np.zeros((len(file_list), EMB_DIM), dtype=np.float32)\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i, fname in enumerate(file_list):\n",
    "\n",
    "        y = load_center_crop_10s(str(Path(root_dir) / fname), sr=SR, crop_sec=CROP_SEC)\n",
    "\n",
    "        y_batched = np.expand_dims(y, 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            out = at.inference(y_batched)\n",
    "\n",
    "        if isinstance(out, tuple) and len(out) == 2:\n",
    "\n",
    "            embedding = out[1]\n",
    "\n",
    "        elif isinstance(out, dict) and 'embedding' in out:\n",
    "\n",
    "            embedding = out['embedding']\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise RuntimeError(f'Unexpected AudioTagging output: {type(out)}')\n",
    "\n",
    "        X[i] = np.asarray(embedding, dtype=np.float32)[0]\n",
    "\n",
    "        if (i+1) % 200 == 0:\n",
    "\n",
    "            dt = time.time() - t0\n",
    "\n",
    "            print(f'  {i+1}/{len(file_list)} in {dt/60:.1f} min')\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "train_files = df_cur['fname'].values\n",
    "\n",
    "test_files  = df_ss['fname'].values\n",
    "\n",
    "\n",
    "\n",
    "if emb_cur_path.exists() and emb_test_path.exists():\n",
    "\n",
    "    X_cur = np.load(emb_cur_path)\n",
    "\n",
    "    X_test = np.load(emb_test_path)\n",
    "\n",
    "    print('Loaded cached embeddings.')\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Extracting curated embeddings ...')\n",
    "\n",
    "    X_cur = extract_embeddings(train_files, root_dir=BASE/'train_curated')\n",
    "\n",
    "    np.save(emb_cur_path, X_cur)\n",
    "\n",
    "    print('Extracting test embeddings ...')\n",
    "\n",
    "    X_test = extract_embeddings(test_files, root_dir=BASE/'test')\n",
    "\n",
    "    np.save(emb_test_path, X_test)\n",
    "\n",
    "    print('Saved embeddings to disk.')\n",
    "\n",
    "\n",
    "\n",
    "Y_cur = np.stack(df_cur['labels'].apply(encode_labels).values)\n",
    "\n",
    "oof = np.zeros((len(df_cur), n_classes), dtype=np.float32)\n",
    "\n",
    "fold_scores = []\n",
    "\n",
    "for k in range(N_FOLDS):\n",
    "\n",
    "    trn_idx = np.where(df_cur['fold'].values != k)[0]\n",
    "\n",
    "    val_idx = np.where(df_cur['fold'].values == k)[0]\n",
    "\n",
    "    X_tr, X_va = X_cur[trn_idx], X_cur[val_idx]\n",
    "\n",
    "    y_tr, y_va = Y_cur[trn_idx], Y_cur[val_idx]\n",
    "\n",
    "    base_lr = LogisticRegression(solver='lbfgs', max_iter=1000, C=2.0, n_jobs=16, verbose=0)\n",
    "\n",
    "    clf = OneVsRestClassifier(make_pipeline(StandardScaler(with_mean=True, with_std=True), base_lr), n_jobs=-1)\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    proba = clf.predict_proba(X_va)\n",
    "\n",
    "    oof[val_idx] = proba.astype(np.float32)\n",
    "\n",
    "    lw, _ = lwlrap_np(y_va, proba)\n",
    "\n",
    "    fold_scores.append(lw)\n",
    "\n",
    "    print(f'Fold {k} LWLRAP={lw:.4f}')\n",
    "\n",
    "oof_lw, _ = lwlrap_np(Y_cur, oof)\n",
    "\n",
    "print(f'OOF LWLRAP={oof_lw:.4f}; per-fold={fold_scores}')\n",
    "\n",
    "np.save('oof_panns_lr.npy', oof)\n",
    "\n",
    "\n",
    "\n",
    "base_lr_full = LogisticRegression(solver='lbfgs', max_iter=1000, C=2.0, n_jobs=16, verbose=0)\n",
    "\n",
    "clf_full = OneVsRestClassifier(make_pipeline(StandardScaler(with_mean=True, with_std=True), base_lr_full), n_jobs=-1)\n",
    "\n",
    "clf_full.fit(X_cur, Y_cur)\n",
    "\n",
    "test_proba = clf_full.predict_proba(X_test).astype(np.float32)\n",
    "\n",
    "sub = pd.DataFrame(test_proba, columns=class_names)\n",
    "\n",
    "sub.insert(0, 'fname', test_files)\n",
    "\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('Saved submission.csv. Shape:', sub.shape)\n",
    "\n",
    "joblib.dump(clf_full, 'ovr_logreg_panns.joblib')\n",
    "\n",
    "print('Saved classifier checkpoint.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
