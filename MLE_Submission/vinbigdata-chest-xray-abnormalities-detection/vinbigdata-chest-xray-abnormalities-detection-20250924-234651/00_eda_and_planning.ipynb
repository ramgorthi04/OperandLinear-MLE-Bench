{
  "cells": [
    {
      "id": "c97fb8f3-be17-4710-bb38-306befb597e0",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: VinBigData Chest X-ray Abnormalities Detection\n",
        "\n",
        "Objectives:\n",
        "- Establish GPU-ready environment and fast, correct baseline.\n",
        "- Build robust data pipeline: DICOM -> PNG with proper windowing; labels -> YOLO format.\n",
        "- Lock CV and training procedure; produce OOF for diagnostics.\n",
        "- Train a strong object detector (Ultralytics YOLO, pretrained) with efficient epochs.\n",
        "- Inference on test; format per competition requirements; iterate and improve; aim for medal.\n",
        "\n",
        "Phases:\n",
        "1) Environment & GPU check\n",
        "   - Verify nvidia-smi and torch CUDA 12.1 install.\n",
        "   - Set constraints to avoid torch drift.\n",
        "\n",
        "2) Data audit & EDA\n",
        "   - Inspect train.csv, sample_submission.csv; count images/classes; check box stats.\n",
        "   - Verify DICOM integrity; test reading and conversion.\n",
        "\n",
        "3) Preprocessing\n",
        "   - DICOM -> 8-bit PNG (or JPEG) with chest-appropriate windowing and histogram normalization; cache at 1024px.\n",
        "   - Create YOLO labels per image (one .txt with class_id x_center y_center w h in normalized coords).\n",
        "   - Handle 'No finding' properly (no boxes).\n",
        "\n",
        "4) Validation protocol\n",
        "   - Stratified KFold by presence of each class (multilabel stratification) into 5 folds; save folds.\n",
        "\n",
        "5) Baseline model\n",
        "   - Ultralytics YOLOv8n or v5s pretrained on COCO; train 5-10 epochs @ 1024 with strong aug (mixup off initially).\n",
        "   - Use GPU, AMP, cosine LR, patience; log per-epoch mAP on val.\n",
        "\n",
        "6) Inference & submission\n",
        "   - TTA (flip) optional; confidence/NMS tuned on OOF.\n",
        "   - Convert YOLO outputs to competition format (class_name/conf/xmin ymin xmax ymax ...).\n",
        "   - Save submission.csv.\n",
        "\n",
        "7) Iterations for medal\n",
        "   - Resolution sweep (640 -> 1024), model size sweep (n/s), augment tuning, EMA.\n",
        "   - Ensemble diverse seeds/models.\n",
        "   - Error analysis on OOF by class; adjust thresholds per class.\n",
        "\n",
        "Checkpoints for expert review:\n",
        "- After this plan.\n",
        "- After data/labels pipeline built & validated.\n",
        "- After baseline training (OOF metrics).\n",
        "- After first LB submission; then iterate for improvements."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "7e3bb4eb-dffa-4a4b-9ea5-47050ea2d7ed",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment & GPU check: ensure CUDA 12.1 torch stack is correctly installed\n",
        "import os, sys, subprocess, shutil, time\n",
        "from pathlib import Path\n",
        "\n",
        "def run(cmd):\n",
        "    print('>>', ' '.join(cmd), flush=True)\n",
        "    return subprocess.run(cmd, check=False, text=True, capture_output=True)\n",
        "\n",
        "print('Checking nvidia-smi...', flush=True)\n",
        "res = run(['bash','-lc','nvidia-smi || true'])\n",
        "print(res.stdout)\n",
        "print(res.stderr, file=sys.stderr)\n",
        "\n",
        "# Uninstall any existing torch stack to avoid conflicts\n",
        "for pkg in ('torch','torchvision','torchaudio'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "\n",
        "# Clean stray site dirs that can shadow correct wheels (idempotent)\n",
        "for d in (\n",
        "    '/app/.pip-target/torch',\n",
        "    '/app/.pip-target/torchvision',\n",
        "    '/app/.pip-target/torchaudio',\n",
        "    '/app/.pip-target/torchgen',\n",
        "    '/app/.pip-target/functorch',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d, flush=True)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "def pip(*args):\n",
        "    print('> pip', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "# Install EXACT cu121 stack\n",
        "pip('install',\n",
        "    '--index-url','https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url','https://pypi.org/simple',\n",
        "    'torch==2.4.1','torchvision==0.19.1','torchaudio==2.4.1'\n",
        ")\n",
        "\n",
        "# Freeze versions\n",
        "Path('constraints.txt').write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'CUDA build:', getattr(torch.version, 'cuda', None), flush=True)\n",
        "print('CUDA available:', torch.cuda.is_available(), flush=True)\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemExit('CUDA not available. Exiting to avoid wasting time.')\n",
        "assert str(getattr(torch.version,'cuda','')).startswith('12.1'), f'Wrong CUDA build: {torch.version.cuda}'\n",
        "print('GPU:', torch.cuda.get_device_name(0), flush=True)\n",
        "\n",
        "print('Environment ready.', flush=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking nvidia-smi...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> bash -lc nvidia-smi || true\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep 25 02:14:50 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     182MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torch as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchvision as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchaudio as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 MB 381.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 391.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 487.6 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 79.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 144.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 182.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 481.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 122.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 42.6 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 258.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 475.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 81.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 379.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 120.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 138.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 233.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 443.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 394.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 440.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 166.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 107.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 196.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 282.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 512.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.4.1+cu121 CUDA build: 12.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: NVIDIA A10-24Q\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment ready.\n"
          ]
        }
      ]
    },
    {
      "id": "2dea6fbe-0101-4c72-a409-16fc23e3c42f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick EDA: inspect CSVs and dataset structure\n",
        "import pandas as pd, os, json, sys\n",
        "from collections import Counter\n",
        "\n",
        "print('Listing data dirs...', flush=True)\n",
        "train_dir = 'train'\n",
        "test_dir = 'test'\n",
        "print('train files:', len(os.listdir(train_dir)) if os.path.exists(train_dir) else 'missing')\n",
        "print('test files:', len(os.listdir(test_dir)) if os.path.exists(test_dir) else 'missing')\n",
        "\n",
        "print('Reading CSVs...', flush=True)\n",
        "train_csv = pd.read_csv('train.csv')\n",
        "ss = pd.read_csv('sample_submission.csv')\n",
        "print('train.csv shape:', train_csv.shape)\n",
        "print('train.csv columns:', list(train_csv.columns))\n",
        "print(train_csv.head(3))\n",
        "print('sample_submission.csv shape:', ss.shape)\n",
        "print(ss.head(3))\n",
        "\n",
        "# Basic label stats\n",
        "if {'image_id','class_name','class_id','x_min','y_min','x_max','y_max'}.issubset(train_csv.columns):\n",
        "    n_images = train_csv['image_id'].nunique()\n",
        "    classes = sorted(train_csv['class_id'].unique().tolist())\n",
        "    print('Unique images:', n_images)\n",
        "    print('Classes:', classes)\n",
        "    cnt = train_csv['class_id'].value_counts().sort_index()\n",
        "    print('Counts per class_id:\\n', cnt.to_string())\n",
        "    no_find_mask = (train_csv['class_name'].str.lower()=='no finding') if 'class_name' in train_csv.columns else (train_csv['class_id']==14)\n",
        "    print('Images with only No finding (approx by rows labeled No finding):', no_find_mask.sum())\n",
        "else:\n",
        "    print('Unexpected train.csv schema; will inspect later in detail.')\n",
        "\n",
        "# Validate DICOM presence for a few samples\n",
        "sample_ids = train_csv['image_id'].drop_duplicates().head(5).tolist()\n",
        "missing = 0\n",
        "for iid in sample_ids:\n",
        "    p = os.path.join(train_dir, f'{iid}.dicom')\n",
        "    if not os.path.exists(p):\n",
        "        missing += 1\n",
        "print('Missing dicoms among 5-sample check:', missing, 'out of', len(sample_ids))\n",
        "print('EDA done.', flush=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing data dirs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train files: 13500\ntest files: 1500\nReading CSVs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.csv shape: (61171, 8)\ntrain.csv columns: ['image_id', 'class_name', 'class_id', 'rad_id', 'x_min', 'y_min', 'x_max', 'y_max']\n                           image_id    class_name  class_id rad_id  x_min  \\\n0  50a418190bc3fb1ef1633bf9678929b3    No finding        14    R11    NaN   \n1  21a10246a5ec7af151081d0cd6d65dc9    No finding        14     R7    NaN   \n2  9a5094b2563a1ef3ff50dc5c7ff71345  Cardiomegaly         3    R10  691.0   \n\n    y_min   x_max   y_max  \n0     NaN     NaN     NaN  \n1     NaN     NaN     NaN  \n2  1375.0  1653.0  1831.0  \nsample_submission.csv shape: (1500, 2)\n                           image_id PredictionString\n0  24b3c4ccc0e19044935c8f40ab37fc18     14 1 0 0 1 1\n1  295add70002001e13d65c0d0d4a100a0     14 1 0 0 1 1\n2  c2a691b7f3af90af7d2b09985f75ae2d     14 1 0 0 1 1\nUnique images: 13500\nClasses: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\nCounts per class_id:\n class_id\n0      6479\n1       249\n2       827\n3      4894\n4       507\n5       879\n6      1127\n7      2253\n8      2371\n9      2010\n10     2237\n11     4347\n12      203\n13     4165\n14    28623\nImages with only No finding (approx by rows labeled No finding): 28623\nMissing dicoms among 5-sample check: 0 out of 5\nEDA done.\n"
          ]
        }
      ]
    },
    {
      "id": "0841f50b-dbde-40b3-95d1-0548b7f3c46e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install non-torch dependencies (honor torch constraints)\n",
        "import sys, subprocess, os\n",
        "def pip(*args):\n",
        "    print('> pip', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "constrained = os.path.exists('constraints.txt')\n",
        "cmd = ['install']\n",
        "if constrained:\n",
        "    cmd += ['-c','constraints.txt','--upgrade-strategy','only-if-needed']\n",
        "cmd += [\n",
        "    'pydicom==2.4.4',\n",
        "    'opencv-python-headless==4.10.0.84',\n",
        "    'tqdm==4.66.5',\n",
        "    'iterative-stratification==0.1.7',\n",
        "    'scikit-learn==1.5.2',\n",
        "    'albumentations==1.4.18',\n",
        "    'matplotlib==3.9.2',\n",
        "    'ensemble-boxes==1.0.9',\n",
        "    'ultralytics==8.3.60'\n",
        "]\n",
        "pip(*cmd)\n",
        "print('Deps installed.', flush=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install -c constraints.txt --upgrade-strategy only-if-needed pydicom==2.4.4 opencv-python-headless==4.10.0.84 tqdm==4.66.5 iterative-stratification==0.1.7 scikit-learn==1.5.2 albumentations==1.4.18 matplotlib==3.9.2 ensemble-boxes==1.0.9 ultralytics==8.3.60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydicom==2.4.4\n  Downloading pydicom-2.4.4-py3-none-any.whl (1.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.8/1.8 MB 51.2 MB/s eta 0:00:00\nCollecting opencv-python-headless==4.10.0.84\n  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 49.9/49.9 MB 226.2 MB/s eta 0:00:00\nCollecting tqdm==4.66.5\n  Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.4/78.4 KB 440.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting iterative-stratification==0.1.7\n  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\nCollecting scikit-learn==1.5.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13.3/13.3 MB 112.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations==1.4.18\n  Downloading albumentations-1.4.18-py3-none-any.whl (224 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 224.0/224.0 KB 183.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplotlib==3.9.2\n  Downloading matplotlib-3.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8.3/8.3 MB 114.6 MB/s eta 0:00:00\nCollecting ensemble-boxes==1.0.9\n  Downloading ensemble_boxes-1.0.9-py3-none-any.whl (23 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics==8.3.60\n  Downloading ultralytics-8.3.60-py3-none-any.whl (906 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 906.9/906.9 KB 377.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.17.0\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 173.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 143.9 MB/s eta 0:00:00\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 522.5 MB/s eta 0:00:00\nCollecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nCollecting eval-type-backport\n  Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-image>=0.21.0\n  Downloading scikit_image-0.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.8/14.8 MB 305.1 MB/s eta 0:00:00\nCollecting albucore==0.0.17\n  Downloading albucore-0.0.17-py3-none-any.whl (10 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyYAML\n  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 763.0/763.0 KB 542.7 MB/s eta 0:00:00\nCollecting pydantic>=2.7.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 444.9/444.9 KB 36.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fonttools>=4.22.0\n  Downloading fonttools-4.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5.0/5.0 MB 239.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow>=8\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 228.0 MB/s eta 0:00:00\nCollecting python-dateutil>=2.7\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.9/229.9 KB 486.1 MB/s eta 0:00:00\nCollecting packaging>=20.0\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 428.7 MB/s eta 0:00:00\nCollecting contourpy>=1.0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 355.2/355.2 KB 479.7 MB/s eta 0:00:00\nCollecting cycler>=0.10\n  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\nCollecting pyparsing>=2.3.1\n  Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 113.9/113.9 KB 476.1 MB/s eta 0:00:00\nCollecting kiwisolver>=1.3.1\n  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 405.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numba\n  Downloading numba-0.62.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.5/3.5 MB 82.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas\n  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.4/12.4 MB 325.6 MB/s eta 0:00:00\nCollecting psutil\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 291.2/291.2 KB 536.3 MB/s eta 0:00:00\nCollecting torch>=1.8.0\n  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.1/797.1 MB 86.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting py-cpuinfo\n  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\nCollecting ultralytics-thop>=2.0.0\n  Downloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\nCollecting torchvision>=0.9.0\n  Downloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.0/7.0 MB 325.9 MB/s eta 0:00:00\nCollecting seaborn>=0.11.0\n  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 294.9/294.9 KB 478.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python>=4.6.0\n  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 67.0/67.0 MB 236.6 MB/s eta 0:00:00\nCollecting requests>=2.23.0\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 343.1 MB/s eta 0:00:00\nCollecting opencv-python>=4.6.0\n  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 63.0/63.0 MB 68.0 MB/s eta 0:00:00\nCollecting tzdata>=2022.7\n  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 347.8/347.8 KB 517.1 MB/s eta 0:00:00\nCollecting pytz>=2020.1\n  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 509.2/509.2 KB 518.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting annotated-types>=0.6.0\n  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nCollecting typing-inspection>=0.4.0\n  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\nCollecting typing-extensions>=4.12.2\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 378.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic-core==2.33.2\n  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 387.1 MB/s eta 0:00:00\nCollecting six>=1.5\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 496.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 448.5 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 450.8 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 409.0 MB/s eta 0:00:00\nCollecting networkx>=3.0\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 362.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tifffile>=2022.8.12\n  Downloading tifffile-2025.9.20-py3-none-any.whl (230 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 230.1/230.1 KB 106.0 MB/s eta 0:00:00\nCollecting imageio!=2.35.0,>=2.33\n  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 315.8/315.8 KB 471.9 MB/s eta 0:00:00\nCollecting lazy-loader>=0.4\n  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 175.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 206.5 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 165.2 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 124.7 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 227.9 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 217.6 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 167.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 130.6 MB/s eta 0:00:00\nCollecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 207.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 482.9 MB/s eta 0:00:00\nCollecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 114.3 MB/s eta 0:00:00\nCollecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 484.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 431.5 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 181.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 269.2 MB/s eta 0:00:00\nCollecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 153.9 MB/s eta 0:00:00\nCollecting llvmlite<0.46,>=0.45.0dev0\n  Downloading llvmlite-0.45.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.3/56.3 MB 74.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 502.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pytz, py-cpuinfo, mpmath, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, six, PyYAML, pyparsing, pydicom, psutil, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, llvmlite, kiwisolver, joblib, idna, fsspec, fonttools, filelock, eval-type-backport, cycler, charset_normalizer, certifi, annotated-types, typing-inspection, triton, tifffile, scipy, requests, python-dateutil, pydantic-core, opencv-python-headless, opencv-python, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, lazy-loader, jinja2, imageio, contourpy, scikit-learn, scikit-image, pydantic, pandas, nvidia-cusolver-cu12, matplotlib, albucore, torch, seaborn, iterative-stratification, ensemble-boxes, albumentations, ultralytics-thop, torchvision, ultralytics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 PyYAML-6.0.2 albucore-0.0.17 albumentations-1.4.18 annotated-types-0.7.0 certifi-2025.8.3 charset_normalizer-3.4.3 contourpy-1.3.3 cycler-0.12.1 ensemble-boxes-1.0.9 eval-type-backport-0.2.2 filelock-3.19.1 fonttools-4.60.0 fsspec-2025.9.0 idna-3.10 imageio-2.37.0 iterative-stratification-0.1.7 jinja2-3.1.6 joblib-1.5.2 kiwisolver-1.4.9 lazy-loader-0.4 llvmlite-0.45.0 matplotlib-3.9.2 mpmath-1.3.0 networkx-3.5 numba-0.62.0 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 opencv-python-4.11.0.86 opencv-python-headless-4.10.0.84 packaging-25.0 pandas-2.3.2 pillow-11.3.0 psutil-7.1.0 py-cpuinfo-9.0.0 pydantic-2.11.9 pydantic-core-2.33.2 pydicom-2.4.4 pyparsing-3.2.5 python-dateutil-2.9.0.post0 pytz-2025.2 requests-2.32.5 scikit-image-0.25.2 scikit-learn-1.5.2 scipy-1.16.2 seaborn-0.13.2 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 tifffile-2025.9.20 torch-2.4.1 torchvision-0.19.1 tqdm-4.66.5 triton-3.0.0 typing-extensions-4.15.0 typing-inspection-0.4.1 tzdata-2025.2 ultralytics-8.3.60 ultralytics-thop-2.0.17 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/torchvision already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchvision.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.4.5.107.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.1.0.106.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/triton-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec-2025.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/MarkupSafe-3.0.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.1.3.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.0.2.54.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.2.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.20.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.9.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deps installed.\n"
          ]
        }
      ]
    },
    {
      "id": "6dba55c5-273b-4f5b-bc29-ac597aec8c44",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install system libraries required by OpenCV (fix libGL error)\n",
        "import subprocess, sys\n",
        "def sh(cmd):\n",
        "    print('>>', cmd, flush=True)\n",
        "    subprocess.run(['bash','-lc', cmd], check=True)\n",
        "sh('apt-get update -y')\n",
        "sh('apt-get install -y libgl1 libglib2.0-0')\n",
        "print('System deps installed.')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> apt-get update -y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E: List directory /var/lib/apt/lists/partial is missing. - Acquire (30: Read-only file system)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command '['bash', '-lc', 'apt-get update -y']' returned non-zero exit status 100.",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m>>\u001b[39m\u001b[33m'\u001b[39m, cmd, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m     subprocess.run([\u001b[33m'\u001b[39m\u001b[33mbash\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m-lc\u001b[39m\u001b[33m'\u001b[39m, cmd], check=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43msh\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mapt-get update -y\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m sh(\u001b[33m'\u001b[39m\u001b[33mapt-get install -y libgl1 libglib2.0-0\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mSystem deps installed.\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36msh\u001b[39m\u001b[34m(cmd)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msh\u001b[39m(cmd):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m>>\u001b[39m\u001b[33m'\u001b[39m, cmd, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbash\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-lc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/subprocess.py:569\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     retcode = process.poll()\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    570\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
            "\u001b[31mCalledProcessError\u001b[39m: Command '['bash', '-lc', 'apt-get update -y']' returned non-zero exit status 100."
          ]
        }
      ]
    },
    {
      "id": "3f543509-89d0-4fcf-8b06-c96e99e90e0f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Preprocessing: DICOM -> multi-window PNG@1024 and YOLO labels + 5-fold multilabel CV (with WBF + proper MONOCHROME1 handling)\n",
        "import os, json, math, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import pydicom\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from PIL import Image\n",
        "from ensemble_boxes import weighted_boxes_fusion\n",
        "\n",
        "TRAIN_DIR = Path('train')\n",
        "TEST_DIR = Path('test')\n",
        "IMG_OUT_DIR = Path('images_1024')  # cached 1024x1024 PNGs\n",
        "LBL_OUT_DIR = Path('labels_yolo')  # YOLO txt labels (train only, excluding class 14)\n",
        "META_DIR = Path('meta')\n",
        "for d in (IMG_OUT_DIR, LBL_OUT_DIR, META_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def load_dicom_pixel(ds: pydicom.Dataset) -> np.ndarray:\n",
        "    arr = ds.pixel_array.astype(np.float32)\n",
        "    # Apply rescale slope/intercept\n",
        "    slope = float(getattr(ds, 'RescaleSlope', 1.0))\n",
        "    inter = float(getattr(ds, 'RescaleIntercept', 0.0))\n",
        "    arr = arr * slope + inter\n",
        "    # Do NOT invert here for MONOCHROME1; we'll invert the final 8-bit image after windowing\n",
        "    # Optional clamp to robust HU range\n",
        "    arr = np.clip(arr, -2000, 2000)\n",
        "    return arr\n",
        "\n",
        "def window_image(img: np.ndarray, center: float, width: float) -> np.ndarray:\n",
        "    low = center - width / 2.0\n",
        "    high = center + width / 2.0\n",
        "    img_w = np.clip(img, low, high)\n",
        "    img_w = (img_w - low) / max(1e-6, (high - low))\n",
        "    return (img_w * 255.0).astype(np.uint8)\n",
        "\n",
        "def multi_window_stack(img: np.ndarray) -> np.ndarray:\n",
        "    # Clinical windows: lung, soft tissue (mediastinum), bone\n",
        "    lung = window_image(img, center=-600, width=1500)\n",
        "    soft = window_image(img, center=40, width=400)\n",
        "    bone = window_image(img, center=300, width=2000)\n",
        "    return np.stack([lung, soft, bone], axis=-1)\n",
        "\n",
        "def letterbox_square(img: np.ndarray, target: int = 1024):\n",
        "    # img: HxWxC uint8\n",
        "    h, w = img.shape[:2]\n",
        "    scale = target / max(h, w)\n",
        "    nh, nw = int(round(h * scale)), int(round(w * scale))\n",
        "    pil = Image.fromarray(img)\n",
        "    resized = pil.resize((nw, nh), resample=Image.BILINEAR)\n",
        "    canvas = Image.new('RGB', (target, target), (0, 0, 0))\n",
        "    top = (target - nh) // 2\n",
        "    left = (target - nw) // 2\n",
        "    canvas.paste(resized, (left, top))\n",
        "    meta = {'orig_h': h, 'orig_w': w, 'scale': scale, 'top': top, 'left': left, 'target': target}\n",
        "    return np.array(canvas), meta\n",
        "\n",
        "def save_png(img: np.ndarray, out_path: Path):\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    Image.fromarray(img).save(str(out_path), format='PNG', compress_level=3)\n",
        "\n",
        "def fuse_gt_wbf(df_img: pd.DataFrame, orig_w: int, orig_h: int, iou_thr: float = 0.5):\n",
        "    # df_img contains rows for a single image (all classes and rads); we'll fuse per class_id != 14 across rad_id\n",
        "    fused = []\n",
        "    df_img = df_img[df_img['class_id'] != 14]\n",
        "    if df_img.empty:\n",
        "        return fused\n",
        "    for cid, df_c in df_img.groupby('class_id'):\n",
        "        # group by radiologist to build per-detector lists\n",
        "        box_lists, score_lists, label_lists = [], [], []\n",
        "        for rad, df_r in df_c.groupby('rad_id'):\n",
        "            boxes = []\n",
        "            for _, r in df_r.iterrows():\n",
        "                x1, y1, x2, y2 = r['x_min'], r['y_min'], r['x_max'], r['y_max']\n",
        "                if not (np.isfinite(x1) and np.isfinite(y1) and np.isfinite(x2) and np.isfinite(y2)):\n",
        "                    continue\n",
        "                if x2 <= x1 or y2 <= y1:\n",
        "                    continue\n",
        "                # normalize to [0,1]\n",
        "                boxes.append([x1 / orig_w, y1 / orig_h, x2 / orig_w, y2 / orig_h])\n",
        "            if len(boxes) == 0:\n",
        "                continue\n",
        "            box_lists.append(boxes)\n",
        "            score_lists.append([1.0] * len(boxes))\n",
        "            label_lists.append([int(cid)] * len(boxes))\n",
        "        if len(box_lists) == 0:\n",
        "            continue\n",
        "        b, s, l = weighted_boxes_fusion(box_lists, score_lists, label_lists, iou_thr=iou_thr, skip_box_thr=0.0)\n",
        "        # denormalize back to pixels\n",
        "        for (x1n, y1n, x2n, y2n), lab, sc in zip(b, l, s):\n",
        "            x1 = float(x1n * orig_w); y1 = float(y1n * orig_h); x2 = float(x2n * orig_w); y2 = float(y2n * orig_h)\n",
        "            fused.append({'class_id': int(cid), 'x_min': x1, 'y_min': y1, 'x_max': x2, 'y_max': y2})\n",
        "    return fused\n",
        "\n",
        "def convert_boxes_to_yolo(rows: pd.DataFrame, meta: dict) -> list[str]:\n",
        "    # rows contains columns: class_id, x_min, y_min, x_max, y_max in ORIGINAL pixels\n",
        "    S = meta['scale']; L = meta['left']; T = meta['top']; tgt = meta['target']\n",
        "    lines = []\n",
        "    for _, r in rows.iterrows():\n",
        "        cid = int(r['class_id'])\n",
        "        x1, y1, x2, y2 = float(r['x_min']), float(r['y_min']), float(r['x_max']), float(r['y_max'])\n",
        "        if not (np.isfinite(x1) and np.isfinite(y1) and np.isfinite(x2) and np.isfinite(y2)):\n",
        "            continue\n",
        "        if x2 <= x1 or y2 <= y1:\n",
        "            continue\n",
        "        # scale and letterbox shift\n",
        "        x1p = x1 * S + L; x2p = x2 * S + L\n",
        "        y1p = y1 * S + T; y2p = y2 * S + T\n",
        "        # clip\n",
        "        x1p = np.clip(x1p, 0, tgt); x2p = np.clip(x2p, 0, tgt)\n",
        "        y1p = np.clip(y1p, 0, tgt); y2p = np.clip(y2p, 0, tgt)\n",
        "        if x2p <= x1p or y2p <= y1p:\n",
        "            continue\n",
        "        xc = (x1p + x2p) / 2.0 / tgt\n",
        "        yc = (y1p + y2p) / 2.0 / tgt\n",
        "        bw = (x2p - x1p) / tgt\n",
        "        bh = (y2p - y1p) / tgt\n",
        "        lines.append(f\"{cid} {xc:.6f} {yc:.6f} {bw:.6f} {bh:.6f}\")\n",
        "    return lines\n",
        "\n",
        "def process_split(csv_path='train.csv', limit=None):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    img_ids = df['image_id'].unique().tolist()\n",
        "    if limit is not None:\n",
        "        img_ids = img_ids[:limit]\n",
        "    meta_records = {}\n",
        "    for iid in tqdm(img_ids, desc='DICOM->PNG'):\n",
        "        dcm_path = TRAIN_DIR / f'{iid}.dicom'\n",
        "        try:\n",
        "            ds = pydicom.dcmread(str(dcm_path))\n",
        "            base = load_dicom_pixel(ds)\n",
        "            rgb = multi_window_stack(base)\n",
        "            # Proper MONOCHROME1 handling: invert AFTER windowing\n",
        "            if getattr(ds, 'PhotometricInterpretation', 'MONOCHROME2') == 'MONOCHROME1':\n",
        "                rgb = 255 - rgb\n",
        "            out_img_path = IMG_OUT_DIR / f'{iid}.png'\n",
        "            img_1024, meta = letterbox_square(rgb, 1024)\n",
        "            save_png(img_1024, out_img_path)\n",
        "            meta_records[iid] = meta\n",
        "            # labels: WBF merge across rad_id per class\n",
        "            orig_h, orig_w = int(getattr(ds, 'Rows', img_1024.shape[0])), int(getattr(ds, 'Columns', img_1024.shape[1]))\n",
        "            fused = fuse_gt_wbf(df[df.image_id == iid], orig_w=orig_w, orig_h=orig_h, iou_thr=0.5)\n",
        "            if len(fused):\n",
        "                rows = pd.DataFrame(fused)\n",
        "            else:\n",
        "                rows = pd.DataFrame(columns=['class_id','x_min','y_min','x_max','y_max'])\n",
        "            yolo_lines = convert_boxes_to_yolo(rows, meta)\n",
        "            out_lbl_path = LBL_OUT_DIR / f'{iid}.txt'\n",
        "            with open(out_lbl_path, 'w') as f:\n",
        "                if len(yolo_lines):\n",
        "                    f.write('\\n'.join(yolo_lines))\n",
        "                else:\n",
        "                    f.write('')\n",
        "        except Exception as e:\n",
        "            print(f'Failed {iid}: {e}', flush=True)\n",
        "    # Save meta for later inverse-mapping to original size\n",
        "    with open(META_DIR / 'image_meta.json', 'w') as f:\n",
        "        json.dump(meta_records, f)\n",
        "    print('Processed images:', len(meta_records))\n",
        "\n",
        "def build_folds(csv_path='train.csv', n_splits=5, seed=42):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # Build multilabel presence per image for classes 0-13 (exclude 14 No finding)\n",
        "    pos = df[df['class_id'] != 14][['image_id','class_id']].dropna()\n",
        "    pos['class_id'] = pos['class_id'].astype(int)\n",
        "    classes = list(range(14))\n",
        "    img_ids = df['image_id'].unique()\n",
        "    img_to_idx = {iid:i for i,iid in enumerate(img_ids)}\n",
        "    Y = np.zeros((len(img_ids), len(classes)), dtype=int)\n",
        "    for iid, cid in zip(pos['image_id'].values, pos['class_id'].values):\n",
        "        Y[img_to_idx[iid], cid] = 1\n",
        "    mskf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    folds = np.full(len(img_ids), -1, dtype=int)\n",
        "    for k, (_, val_idx) in enumerate(mskf.split(np.zeros(len(img_ids)), Y)):\n",
        "        folds[val_idx] = k\n",
        "    folds_df = pd.DataFrame({'image_id': img_ids, 'fold': folds})\n",
        "    folds_df.to_csv(META_DIR / 'folds.csv', index=False)\n",
        "    print('Folds saved:', (META_DIR / 'folds.csv'))\n",
        "\n",
        "print('Preprocessing utilities ready. Next steps:')\n",
        "print('- Run process_split(limit=50) for smoke test, then full run (may take minutes).')\n",
        "print('- Run build_folds() to create 5-fold split for training.')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing utilities ready. Next steps:\n- Run process_split(limit=50) for smoke test, then full run (may take minutes).\n- Run build_folds() to create 5-fold split for training.\n"
          ]
        }
      ]
    },
    {
      "id": "11d88818-c226-4130-a9ff-15c9e1dcb0d3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Smoke test: convert 50 images and build 5-folds\n",
        "import time\n",
        "t0 = time.time()\n",
        "print('Starting smoke conversion for 50 images...', flush=True)\n",
        "process_split(csv_path='train.csv', limit=50)\n",
        "print('Building folds...', flush=True)\n",
        "build_folds(csv_path='train.csv', n_splits=5, seed=42)\n",
        "print('Elapsed: %.2f sec' % (time.time() - t0), flush=True)\n",
        "print('Check outputs: images_1024/*.png, labels_yolo/*.txt, meta/image_meta.json, meta/folds.csv', flush=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting smoke conversion for 50 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:   0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/pydicom/pixel_data_handlers/pillow_handler.py:238: UserWarning: The (0028,0101) 'Bits Stored' value (14-bit) doesn't match the JPEG 2000 data (16-bit). It's recommended that you change the 'Bits Stored' value\n  warnings.warn(\n\rDICOM->PNG:   2%|\u258f         | 1/50 [00:01<01:00,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:   4%|\u258d         | 2/50 [00:01<00:29,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/pydicom/pixel_data_handlers/pillow_handler.py:238: UserWarning: The (0028,0101) 'Bits Stored' value (12-bit) doesn't match the JPEG 2000 data (16-bit). It's recommended that you change the 'Bits Stored' value\n  warnings.warn(\n\rDICOM->PNG:   6%|\u258c         | 3/50 [00:02<00:40,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:   8%|\u258a         | 4/50 [00:03<00:49,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  12%|\u2588\u258f        | 6/50 [00:04<00:24,  1.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  14%|\u2588\u258d        | 7/50 [00:05<00:33,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  16%|\u2588\u258c        | 8/50 [00:05<00:25,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  18%|\u2588\u258a        | 9/50 [00:07<00:34,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  20%|\u2588\u2588        | 10/50 [00:07<00:24,  1.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  22%|\u2588\u2588\u258f       | 11/50 [00:07<00:18,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  24%|\u2588\u2588\u258d       | 12/50 [00:08<00:28,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  26%|\u2588\u2588\u258c       | 13/50 [00:08<00:20,  1.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  28%|\u2588\u2588\u258a       | 14/50 [00:08<00:15,  2.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  32%|\u2588\u2588\u2588\u258f      | 16/50 [00:10<00:16,  2.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  34%|\u2588\u2588\u2588\u258d      | 17/50 [00:10<00:13,  2.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  36%|\u2588\u2588\u2588\u258c      | 18/50 [00:11<00:22,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  38%|\u2588\u2588\u2588\u258a      | 19/50 [00:11<00:16,  1.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  40%|\u2588\u2588\u2588\u2588      | 20/50 [00:13<00:25,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  42%|\u2588\u2588\u2588\u2588\u258f     | 21/50 [00:14<00:27,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  46%|\u2588\u2588\u2588\u2588\u258c     | 23/50 [00:14<00:15,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  48%|\u2588\u2588\u2588\u2588\u258a     | 24/50 [00:15<00:12,  2.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  50%|\u2588\u2588\u2588\u2588\u2588     | 25/50 [00:15<00:09,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 26/50 [00:15<00:07,  3.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 28/50 [00:15<00:04,  4.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 30/50 [00:15<00:03,  5.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 31/50 [00:16<00:07,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 32/50 [00:16<00:05,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 33/50 [00:17<00:04,  3.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 34/50 [00:17<00:03,  4.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 35/50 [00:17<00:03,  4.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 36/50 [00:17<00:02,  5.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 37/50 [00:17<00:02,  6.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 38/50 [00:17<00:01,  6.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 39/50 [00:17<00:01,  6.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 40/50 [00:17<00:01,  7.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 41/50 [00:18<00:01,  7.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 42/50 [00:18<00:01,  7.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 44/50 [00:18<00:00,  8.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 46/50 [00:20<00:01,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 48/50 [00:21<00:01,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 49/50 [00:21<00:00,  2.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [00:21<00:00,  2.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [00:21<00:00,  2.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed images: 50\nBuilding folds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folds saved: meta/folds.csv\nElapsed: 21.99 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check outputs: images_1024/*.png, labels_yolo/*.txt, meta/image_meta.json, meta/folds.csv\n"
          ]
        }
      ]
    },
    {
      "id": "af092088-b9c1-4d6e-863d-5397f0f708df",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Utilities: Full conversion runners (train full, test), and YOLO fold file lists + data.yaml\n",
        "import pandas as pd, json, os\n",
        "from pathlib import Path\n",
        "\n",
        "IMG_OUT_DIR = Path('images_1024')\n",
        "IMG_OUT_TEST_DIR = Path('images_1024_test')\n",
        "META_DIR = Path('meta')\n",
        "TRAIN_DIR = Path('train')\n",
        "TEST_DIR = Path('test')\n",
        "LBL_OUT_DIR = Path('labels_yolo')\n",
        "IMG_OUT_TEST_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def process_test(target=1024):\n",
        "    from tqdm import tqdm\n",
        "    import pydicom, numpy as np\n",
        "    meta_records = {}\n",
        "    iids = [p.stem for p in sorted(TEST_DIR.glob('*.dicom'))]\n",
        "    for iid in tqdm(iids, desc='TEST DICOM->PNG'):\n",
        "        dcm_path = TEST_DIR / f'{iid}.dicom'\n",
        "        try:\n",
        "            ds = pydicom.dcmread(str(dcm_path))\n",
        "            base = load_dicom_pixel(ds)\n",
        "            rgb = multi_window_stack(base)\n",
        "            if getattr(ds, 'PhotometricInterpretation', 'MONOCHROME2') == 'MONOCHROME1':\n",
        "                rgb = 255 - rgb\n",
        "            out_img_path = IMG_OUT_TEST_DIR / f'{iid}.png'\n",
        "            img_1024, meta = letterbox_square(rgb, target)\n",
        "            save_png(img_1024, out_img_path)\n",
        "            meta_records[iid] = meta\n",
        "        except Exception as e:\n",
        "            print(f'Failed TEST {iid}: {e}', flush=True)\n",
        "    with open(META_DIR / 'test_image_meta.json', 'w') as f:\n",
        "        json.dump(meta_records, f)\n",
        "    print('Processed TEST images:', len(meta_records))\n",
        "\n",
        "def prepare_yolo_folds_files(folds_csv='meta/folds.csv', out_dir='meta/fold_files'):\n",
        "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    folds = pd.read_csv(folds_csv)\n",
        "    # ensure all images exist\n",
        "    exist_set = set(p.stem for p in IMG_OUT_DIR.glob('*.png'))\n",
        "    folds = folds[folds['image_id'].isin(exist_set)].copy()\n",
        "    # write per-fold train/val lists\n",
        "    for k in sorted(folds['fold'].unique()):\n",
        "        val_ids = folds.loc[folds['fold']==k, 'image_id'].tolist()\n",
        "        train_ids = folds.loc[folds['fold']!=k, 'image_id'].tolist()\n",
        "        with open(out_dir / f'train_fold{k}.txt', 'w') as f:\n",
        "            for iid in train_ids:\n",
        "                f.write(str(IMG_OUT_DIR / f'{iid}.png') + '\\n')\n",
        "        with open(out_dir / f'val_fold{k}.txt', 'w') as f:\n",
        "            for iid in val_ids:\n",
        "                f.write(str(IMG_OUT_DIR / f'{iid}.png') + '\\n')\n",
        "    print('Wrote YOLO filelists to', out_dir)\n",
        "\n",
        "def write_classes_and_yaml(train_csv='train.csv', out_dir='meta'):\n",
        "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    df = pd.read_csv(train_csv)\n",
        "    df = df[df['class_id'] != 14]\n",
        "    # Build class names mapping 0..13\n",
        "    names = df[['class_id','class_name']].drop_duplicates().sort_values('class_id')\n",
        "    # Some datasets may not list all 0..13 in this subset; ensure full 0..13 order by filling names if missing\n",
        "    class_names = {}\n",
        "    for cid in range(14):\n",
        "        rows = names[names['class_id']==cid]['class_name'].tolist()\n",
        "        class_names[cid] = (rows[0] if rows else f'class_{cid}')\n",
        "    with open(out_dir / 'classes.json','w') as f:\n",
        "        json.dump(class_names, f, indent=2)\n",
        "    # Write a base data.yaml (we will override train/val per fold when launching training)\n",
        "    yaml_content = [\n",
        "        'path: .',\n",
        "        f'train: {str((Path(out_dir)/\"fold_files\"/\"train_fold0.txt\").as_posix())}',\n",
        "        f'val: {str((Path(out_dir)/\"fold_files\"/\"val_fold0.txt\").as_posix())}',\n",
        "        'nc: 14',\n",
        "        'names: [' + ', '.join([f'\"{class_names[i]}\"' for i in range(14)]) + ']'\n",
        "    ]\n",
        "    with open(out_dir / 'data.yaml','w') as f:\n",
        "        f.write('\\n'.join(yaml_content))\n",
        "    print('Wrote classes.json and base data.yaml in', out_dir)\n",
        "\n",
        "print('Runners ready: process_test(), prepare_yolo_folds_files(), write_classes_and_yaml().')\n",
        "print('Next: run full train conversion (process_split with limit=None), process_test(), make fold lists, and write data.yaml.')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runners ready: process_test(), prepare_yolo_folds_files(), write_classes_and_yaml().\nNext: run full train conversion (process_split with limit=None), process_test(), make fold lists, and write data.yaml.\n"
          ]
        }
      ]
    },
    {
      "id": "30bdfe18-90f6-47b3-aada-fb7dd4413dd0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Full conversion + lists/yaml\n",
        "import time\n",
        "t0 = time.time()\n",
        "print('Starting FULL train conversion...', flush=True)\n",
        "process_split(csv_path='train.csv', limit=None)\n",
        "print('Train conversion done in %.2f sec' % (time.time()-t0), flush=True)\n",
        "t1 = time.time()\n",
        "print('Processing TEST conversion...', flush=True)\n",
        "process_test(target=1024)\n",
        "print('Test conversion done in %.2f sec' % (time.time()-t1), flush=True)\n",
        "print('Preparing YOLO fold filelists...', flush=True)\n",
        "prepare_yolo_folds_files(folds_csv='meta/folds.csv', out_dir='meta/fold_files')\n",
        "print('Writing classes and base data.yaml...', flush=True)\n",
        "write_classes_and_yaml(train_csv='train.csv', out_dir='meta')\n",
        "print('All preprocessing steps completed in %.2f sec' % (time.time()-t0), flush=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting FULL train conversion...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:   0%|          | 0/13500 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/pydicom/pixel_data_handlers/pillow_handler.py:238: UserWarning: The (0028,0101) 'Bits Stored' value (14-bit) doesn't match the JPEG 2000 data (16-bit). It's recommended that you change the 'Bits Stored' value\n  warnings.warn(\n\rDICOM->PNG:   0%|          | 1/13500 [00:01<4:35:59,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:   0%|          | 2/13500 [00:01<2:14:46,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/pydicom/pixel_data_handlers/pillow_handler.py:238: UserWarning: The (0028,0101) 'Bits Stored' value (12-bit) doesn't match the JPEG 2000 data (16-bit). It's recommended that you change the 'Bits Stored' value\n  warnings.warn(\n\rDICOM->PNG:   0%|          | 3/13500 [00:02<3:12:53,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:   0%|          | 4/13500 [00:03<4:03:48,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDICOM->PNG:   0%|          | 6/13500 [00:04<2:06:15,  1.78it/s]"
          ]
        }
      ]
    },
    {
      "id": "7dfa771e-e97e-4414-a013-a71a814e5d41",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# YOLOv8m training utilities (per-fold) - tuned per expert advice\n",
        "from ultralytics import YOLO\n",
        "import yaml, shutil, os, time\n",
        "from pathlib import Path\n",
        "\n",
        "META_DIR = Path('meta')\n",
        "FOLDS_DIR = META_DIR / 'fold_files'\n",
        "RUNS_DIR = Path('runs')\n",
        "RUNS_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "def make_fold_yaml(base_yaml='meta/data.yaml', fold=0) -> str:\n",
        "    base = yaml.safe_load(Path(base_yaml).read_text())\n",
        "    base['train'] = str((FOLDS_DIR / f'train_fold{fold}.txt').as_posix())\n",
        "    base['val'] = str((FOLDS_DIR / f'val_fold{fold}.txt').as_posix())\n",
        "    out = META_DIR / f'data_fold{fold}.yaml'\n",
        "    out.write_text(yaml.safe_dump(base))\n",
        "    return str(out)\n",
        "\n",
        "def train_yolov8m_fold(\n",
        "    fold=0,\n",
        "    epochs=25,\n",
        "    batch=16,\n",
        "    imgsz=1024,\n",
        "    seed=42,\n",
        "    workers=8,\n",
        "    cache='ram',  # 'ram' or 'disk' or False\n",
        "    mosaic=0.3,\n",
        "    close_mosaic=10,\n",
        "    degrees=5.0,\n",
        "    scale=0.2,\n",
        "    translate=0.05,\n",
        "    fliplr=0.5,\n",
        "    hsv_v=0.1,\n",
        "):\n",
        "    data_yaml = make_fold_yaml(fold=fold)\n",
        "    model = YOLO('yolov8m.pt')\n",
        "    name = f'v8m_{imgsz}_fold{fold}_e{epochs}_b{batch}'\n",
        "    print(f'Start training: {name}', flush=True)\n",
        "    t0 = time.time()\n",
        "    results = model.train(\n",
        "        data=data_yaml,\n",
        "        epochs=epochs,\n",
        "        imgsz=imgsz,\n",
        "        batch=batch,\n",
        "        device=0,\n",
        "        seed=seed,\n",
        "        workers=workers,\n",
        "        project=str(RUNS_DIR),\n",
        "        name=name,\n",
        "        pretrained=True,\n",
        "        amp=True,\n",
        "        ema=True,\n",
        "        cos_lr=True,\n",
        "        warmup_epochs=3,\n",
        "        patience=5,\n",
        "        cache=cache,\n",
        "        rect=False,\n",
        "        # Augs (medical-safe)\n",
        "        fliplr=fliplr,\n",
        "        flipud=0.0,\n",
        "        degrees=degrees,\n",
        "        scale=scale,\n",
        "        translate=translate,\n",
        "        shear=0.0,\n",
        "        hsv_h=0.0, hsv_s=0.0, hsv_v=hsv_v,\n",
        "        mixup=0.0,\n",
        "        copy_paste=0.0,\n",
        "        mosaic=mosaic,\n",
        "        close_mosaic=close_mosaic,\n",
        "    )\n",
        "    print(f'Training done in {(time.time()-t0)/60:.1f} min', flush=True)\n",
        "    return results\n",
        "\n",
        "print('Training utilities ready. After full conversion completes, call train_yolov8m_fold(fold=0) as a smoke, then scale to 5 folds.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "d0573214-3c38-414b-854c-0c6d2864d66a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inspect sanity training metrics (results.csv) live - posonly2 run\n",
        "import os, time, pandas as pd, glob\n",
        "from pathlib import Path\n",
        "\n",
        "# Point to the corrected sanity run\n",
        "run_dir = Path('runs/partial_v8m_1024_fold0_e4_b12_posonly2')\n",
        "csv_path = run_dir / 'results.csv'\n",
        "if not csv_path.exists():\n",
        "    print('results.csv not found at', csv_path)\n",
        "else:\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print('results.csv rows:', len(df), 'cols:', list(df.columns))\n",
        "        print(df.tail(5))\n",
        "    except Exception as e:\n",
        "        print('Failed to read results.csv:', e)\n",
        "    st = os.stat(csv_path)\n",
        "    print('Last modified (epoch):', st.st_mtime, '->', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(st.st_mtime)))\n",
        "    print('results.csv size (bytes):', os.path.getsize(csv_path))\n",
        "\n",
        "    # List artifacts to gauge training progress\n",
        "    files = sorted(glob.glob(str(run_dir / '*')))\n",
        "    print('Artifacts:', [Path(f).name for f in files])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "b10b8e74-49bc-44d4-ab4d-252b597ff577",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inference, WBF, and submission utilities (prep while conversion runs)\n",
        "import json, os, math, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "from ensemble_boxes import weighted_boxes_fusion\n",
        "\n",
        "META_DIR = Path('meta')\n",
        "IMG_TEST_DIR = Path('images_1024_test')\n",
        "\n",
        "def load_meta(kind='train'):\n",
        "    if kind == 'train':\n",
        "        mp = META_DIR / 'image_meta.json'\n",
        "    else:\n",
        "        mp = META_DIR / 'test_image_meta.json'\n",
        "    return json.loads(mp.read_text()) if mp.exists() else {}\n",
        "\n",
        "def unletterbox_to_original(xyxy, meta):\n",
        "    # xyxy in letterboxed 1024 space -> back to original pixels, then normalize to [0,1]\n",
        "    x1, y1, x2, y2, conf, cid = xyxy\n",
        "    S = meta['scale']; L = meta['left']; T = meta['top']; tgt = meta['target']\n",
        "    orig_w = meta['orig_w']; orig_h = meta['orig_h']\n",
        "    # remove letterbox/pad\n",
        "    x1p = (x1 - L) / max(S, 1e-6)\n",
        "    x2p = (x2 - L) / max(S, 1e-6)\n",
        "    y1p = (y1 - T) / max(S, 1e-6)\n",
        "    y2p = (y2 - T) / max(S, 1e-6)\n",
        "    # clip to original frame\n",
        "    x1p = float(np.clip(x1p, 0, orig_w)); x2p = float(np.clip(x2p, 0, orig_w))\n",
        "    y1p = float(np.clip(y1p, 0, orig_h)); y2p = float(np.clip(y2p, 0, orig_h))\n",
        "    if x2p <= x1p or y2p <= y1p:\n",
        "        return None\n",
        "    # normalize to [0,1] and return class id, conf, normalized xyxy\n",
        "    return int(cid), float(conf), x1p / orig_w, y1p / orig_h, x2p / orig_w, y2p / orig_h\n",
        "\n",
        "def predict_single_with_tta(model, img_path, tta_hflip=True):\n",
        "    # Returns list of (boxes_xyxy, scores, labels) for each TTA view in letterboxed 1024 coords\n",
        "    outs = []\n",
        "    # base\n",
        "    r = model.predict(source=str(img_path), conf=0.001, iou=0.5, device=0, imgsz=1024, half=False, verbose=False, max_det=300)[0]\n",
        "    if r.boxes is not None and len(r.boxes) > 0:\n",
        "        b = r.boxes.xyxy.cpu().numpy(); s = r.boxes.conf.cpu().numpy(); l = r.boxes.cls.cpu().numpy().astype(int)\n",
        "        outs.append((b, s, l))\n",
        "    else:\n",
        "        outs.append((np.zeros((0,4)), np.zeros((0,)), np.zeros((0,), int)))\n",
        "    if tta_hflip:\n",
        "        r = model.predict(source=str(img_path), conf=0.001, iou=0.5, device=0, imgsz=1024, half=False, verbose=False, max_det=300, augment=True)[0]\n",
        "        # Ultralytics augment includes hflip among others; we don't need to manually invert\n",
        "        if r.boxes is not None and len(r.boxes) > 0:\n",
        "            b = r.boxes.xyxy.cpu().numpy(); s = r.boxes.conf.cpu().numpy(); l = r.boxes.cls.cpu().numpy().astype(int)\n",
        "            outs.append((b, s, l))\n",
        "        else:\n",
        "            outs.append((np.zeros((0,4)), np.zeros((0,)), np.zeros((0,), int)))\n",
        "    return outs\n",
        "\n",
        "def wbf_fuse_views(views, iou_thr=0.55):\n",
        "    # views: list of (boxes_xyxy, scores, labels) in SAME coord space\n",
        "    if not views:\n",
        "        return np.zeros((0,4)), np.zeros((0,)), np.zeros((0,), int)\n",
        "    bxs = []; scs = []; lbs = []\n",
        "    for b, s, l in views:\n",
        "        # normalize to [0,1] for WBF fusion in 1024 space\n",
        "        if len(b) == 0:\n",
        "            bxs.append([]); scs.append([]); lbs.append([]); continue\n",
        "        b_norm = b / 1024.0\n",
        "        b_norm = b_norm.tolist()\n",
        "        bxs.append(b_norm); scs.append(s.tolist()); lbs.append(l.tolist())\n",
        "    if sum(len(x) for x in bxs) == 0:\n",
        "        return np.zeros((0,4)), np.zeros((0,)), np.zeros((0,), int)\n",
        "    fb, fs, fl = weighted_boxes_fusion(bxs, scs, lbs, iou_thr=iou_thr, skip_box_thr=0.0, conf_type='avg')\n",
        "    fb = np.asarray(fb) * 1024.0\n",
        "    fs = np.asarray(fs); fl = np.asarray(fl, int)\n",
        "    return fb, fs, fl\n",
        "\n",
        "def build_submission_from_preds(preds_norm):\n",
        "    # preds_norm: dict[iid] -> list of tuples (cid, conf, x1n, y1n, x2n, y2n) on original normalized coords\n",
        "    rows = []\n",
        "    for iid, items in preds_norm.items():\n",
        "        if not items:\n",
        "            rows.append({'image_id': iid, 'PredictionString': '14 1 0 0 1 1'})\n",
        "            continue\n",
        "        parts = []\n",
        "        for cid, conf, x1, y1, x2, y2 in items:\n",
        "            parts.extend([str(cid), f'{conf:.6f}', f'{x1:.6f}', f'{y1:.6f}', f'{x2:.6f}', f'{y2:.6f}'])\n",
        "        rows.append({'image_id': iid, 'PredictionString': ' '.join(parts) if parts else '14 1 0 0 1 1'})\n",
        "    return pd.DataFrame(rows, columns=['image_id','PredictionString'])\n",
        "\n",
        "def infer_test_fold(model_path, tta_hflip=True, min_conf_per_class=None):\n",
        "    # min_conf_per_class: optional dict[cid] -> threshold to filter after WBF\n",
        "    model = YOLO(model_path)\n",
        "    test_meta = load_meta('test')\n",
        "    preds_norm = {}\n",
        "    iids = [p.stem for p in sorted(IMG_TEST_DIR.glob('*.png'))]\n",
        "    for i, iid in enumerate(iids):\n",
        "        img_path = IMG_TEST_DIR / f'{iid}.png'\n",
        "        views = predict_single_with_tta(model, img_path, tta_hflip=tta_hflip)\n",
        "        fb, fs, fl = wbf_fuse_views(views, iou_thr=0.55)\n",
        "        # per-class thresholding\n",
        "        keep = []\n",
        "        for (x1, y1, x2, y2), sc, cl in zip(fb, fs, fl):\n",
        "            if min_conf_per_class is not None:\n",
        "                thr = min_conf_per_class.get(int(cl), 0.0)\n",
        "                if sc < thr:\n",
        "                    continue\n",
        "            keep.append((x1, y1, x2, y2, sc, cl))\n",
        "        items = []\n",
        "        meta = test_meta.get(iid, None)\n",
        "        if meta is None:\n",
        "            preds_norm[iid] = []\n",
        "            continue\n",
        "        for obj in keep:\n",
        "            unm = unletterbox_to_original(obj, meta)\n",
        "            if unm is not None:\n",
        "                items.append(unm)\n",
        "        preds_norm[iid] = items\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Infer {i+1}/{len(iids)} done', flush=True)\n",
        "    sub_df = build_submission_from_preds(preds_norm)\n",
        "    return sub_df\n",
        "\n",
        "print('Inference utilities ready: use infer_test_fold(model_path, tta_hflip=True, min_conf_per_class=...) after training completes.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "9259d88b-be13-42ff-86c3-05db1f573792",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Launch fold-0 full training after preprocessing completes\n",
        "import os, glob, time\n",
        "from pathlib import Path\n",
        "\n",
        "def clear_ultralytics_caches():\n",
        "    removed = 0\n",
        "    for p in list(Path('.').glob('*.cache')) + list(Path('meta').glob('*.cache')) + list(Path('meta/fold_files').glob('*.cache')):\n",
        "        try:\n",
        "            p.unlink(); removed += 1\n",
        "        except Exception:\n",
        "            pass\n",
        "    print('Cleared cache files:', removed, flush=True)\n",
        "\n",
        "def sanity_preflight():\n",
        "    # quick checks before training\n",
        "    imgs = list(Path('images_1024').glob('*.png'))\n",
        "    lbls = list(Path('labels_yolo').glob('*.txt'))\n",
        "    print('Found images:', len(imgs), 'labels:', len(lbls), flush=True)\n",
        "    assert Path('meta/folds.csv').exists(), 'Missing meta/folds.csv'\n",
        "    assert Path('meta/fold_files/train_fold0.txt').exists(), 'Missing fold filelists; run prepare_yolo_folds_files()'\n",
        "    assert Path('meta/data.yaml').exists(), 'Missing meta/data.yaml'\n",
        "\n",
        "def train_fold0():\n",
        "    clear_ultralytics_caches()\n",
        "    sanity_preflight()\n",
        "    # Start training with tuned settings (adjust batch if OOM)\n",
        "    train_yolov8m_fold(\n",
        "        fold=0,\n",
        "        epochs=25,\n",
        "        batch=16,\n",
        "        imgsz=1024,\n",
        "        workers=8,\n",
        "        cache='ram',\n",
        "        mosaic=0.3,\n",
        "        close_mosaic=10,\n",
        "        degrees=5.0,\n",
        "        scale=0.2,\n",
        "        translate=0.05,\n",
        "        fliplr=0.5,\n",
        "        hsv_v=0.1,\n",
        "    )\n",
        "\n",
        "print('When Cell 8 finishes, run train_fold0() to launch full fold-0 training.', flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}