{
  "cells": [
    {
      "id": "46fc0c03-6f81-47d7-8ed7-b1ffdc4fe096",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n",
        "import sys\n",
        "print(subprocess.run(['bash', '-lc', 'nvidia-smi || true'], capture_output=True, text=True).stdout)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep 28 05:55:25 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     182MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\n"
          ]
        }
      ]
    },
    {
      "id": "ffb4b7d7-6624-4a29-83b9-3820cea4414d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "# Load train metadata\n",
        "with open('nybg2020/train/metadata.json', 'r') as f:\n",
        "    train_meta = json.load(f)\n",
        "\n",
        "print('Type of train_meta:', type(train_meta))\n",
        "print('Keys in train_meta:', list(train_meta.keys()))\n",
        "\n",
        "# Extract images and annotations\n",
        "train_images = train_meta['images']\n",
        "train_annotations = train_meta['annotations']\n",
        "\n",
        "print('Number of train images:', len(train_images))\n",
        "print('Number of annotations:', len(train_annotations))\n",
        "\n",
        "# Create DataFrames\n",
        "train_df = pd.DataFrame(train_images)\n",
        "annotations_df = pd.DataFrame(train_annotations)\n",
        "\n",
        "print('Train images columns:', list(train_df.columns))\n",
        "print('Annotations columns:', list(annotations_df.columns))\n",
        "\n",
        "# Merge on image id (images have 'id', annotations have 'image_id')\n",
        "train_df = train_df.merge(annotations_df, left_on='id', right_on='image_id', how='left')\n",
        "print('Merged train shape:', train_df.shape)\n",
        "print(train_df.head())\n",
        "\n",
        "print('\\nClass distribution:')\n",
        "class_counts = Counter(train_df['category_id'])\n",
        "print(f'Number of unique classes: {len(class_counts)}')\n",
        "print(f'Min images per class: {min(class_counts.values())}')\n",
        "print(f'Max images per class: {max(class_counts.values())}')\n",
        "print(f'Mean images per class: {np.mean(list(class_counts.values())):.2f}')\n",
        "print(f'Total train samples: {len(train_df)}')\n",
        "\n",
        "# Load test metadata\n",
        "with open('nybg2020/test/metadata.json', 'r') as f:\n",
        "    test_meta = json.load(f)\n",
        "\n",
        "print('\\nType of test_meta:', type(test_meta))\n",
        "print('Keys in test_meta:', list(test_meta.keys()))\n",
        "\n",
        "test_images = test_meta['images']\n",
        "test_df = pd.DataFrame(test_images)\n",
        "print('Test images columns:', list(test_df.columns))\n",
        "print('Test shape:', test_df.shape)\n",
        "print(test_df.head())\n",
        "\n",
        "# Check sample submission\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "print('\\nSample submission shape:', sample_sub.shape)\n",
        "print(sample_sub.head())\n",
        "print('\\nSubmission format: id, Predicted')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of train_meta: <class 'dict'>\nKeys in train_meta: ['annotations', 'categories', 'images', 'info', 'licenses', 'regions']\nNumber of train images: 811623\nNumber of annotations: 811623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images columns: ['file_name', 'height', 'id', 'license', 'width']\nAnnotations columns: ['category_id', 'id', 'image_id', 'region_id']\nMerged train shape: (811623, 9)\n                  file_name  height    id_x  license  width  category_id  \\\n0  images/156/72/124136.jpg    1000  124136        1    661        15672   \n1    images/156/72/5327.jpg    1000    5327        1    661        15672   \n2  images/156/72/449419.jpg    1000  449419        1    662        15672   \n3   images/156/72/29079.jpg    1000   29079        1    661        15672   \n4  images/156/72/368979.jpg    1000  368979        1    667        15672   \n\n     id_y  image_id  region_id  \n0  124136    124136          1  \n1    5327      5327          1  \n2  449419    449419          1  \n3   29079     29079          1  \n4  368979    368979          1  \n\nClass distribution:\nNumber of unique classes: 32093\nMin images per class: 1\nMax images per class: 1412\nMean images per class: 25.29\nTotal train samples: 811623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nType of test_meta: <class 'dict'>\nKeys in test_meta: ['images', 'info', 'licenses']\nTest images columns: ['file_name', 'height', 'id', 'license', 'width']\nTest shape: (219124, 5)\n          file_name  height id  license  width\n0  images/000/0.jpg    1000  0        1    667\n1  images/000/1.jpg    1000  1        1    667\n2  images/000/2.jpg    1000  2        1    675\n3  images/000/3.jpg    1000  3        1    676\n4  images/000/4.jpg    1000  4        1    678\n\nSample submission shape: (219124, 2)\n   Id  Predicted\n0   0          0\n1   1          0\n2   2          0\n3   3          0\n4   4          0\n\nSubmission format: id, Predicted\n"
          ]
        }
      ]
    },
    {
      "id": "110589e8-edbb-4451-b623-66c33b282d38",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Revised Plan for Herbarium 2020 (Post-Expert Review)\n",
        "\n",
        "## Dataset Overview\n",
        "- Train: 811k images, 32k classes, highly imbalanced (long-tail, min 1, max 1412, mean 25)\n",
        "- Test: 219k images\n",
        "- Metric: Macro F1 score\n",
        "- Goal: Gold medal \u22650.63151\n",
        "\n",
        "## Updated Strategy (Incorporating Expert Advice)\n",
        "1. **Environment Setup**: Install PyTorch 2.4.1+cu121, timm, albumentations, scikit-learn following best practices. Ensure AMP for mixed precision.\n",
        "2. **Quick Baseline (Retrieval/Embeddings - First Submission)**:\n",
        "   - Use timm tf_efficientnet_b3_ns (num_classes=0) to extract embeddings on train/test (input 512px, center crop).\n",
        "   - Compute class prototypes: mean embedding per category_id (use up to 5 images/class, at least 1).\n",
        "   - For test: Embed, predict nearest prototype via cosine similarity (FAISS if possible, else batched matmul).\n",
        "   - Add simple TTA: hflip + original, average embeddings.\n",
        "   - Train linear head on train embeddings with Balanced Softmax or CB-Focal, class-balanced sampler.\n",
        "   - This gets a fast submission (target Bronze/Silver), then iterate.\n",
        "3. **Data Pipeline**:\n",
        "   - Custom Dataset: Load from nybg2020/train/images/{dir}/{file} using file_name from metadata.\n",
        "   - Preprocess: Tight non-white crop (threshold margins), resize shorter to 512, center crop 480, normalize ImageNet.\n",
        "   - Augmentations (for training): HorizontalFlip, small Rotate(\u00b115\u00b0), BrightnessContrast (mild), Mixup \u03b1=0.2, CutMix p=0.2, Label Smoothing 0.05. Avoid heavy rotation/blur/perspective.\n",
        "4. **Handling Imbalance**:\n",
        "   - CB-Focal Loss (beta=0.9999, gamma=1.5-2.0) or LDAM-DRW.\n",
        "   - Sampler: Class-Aware (uniform over classes) or sqrt-frequency.\n",
        "   - Two-stage: Stage 1 instance-balanced full data; Stage 2 class-balanced head fine-tune.\n",
        "   - CV: Single stratified split (90/10) or 2 folds max for OOF macro F1.\n",
        "5. **Full Models**:\n",
        "   - Primary: tf_efficientnetv2_m or tf_efficientnet_b5_ns via timm, GeM pooling if easy.\n",
        "   - Diversity: resnest101e at 384px.\n",
        "   - Train: AdamW lr 3e-4, cosine scheduler, warmup 500 steps, bs 64 (AMP), EMA 0.9998.\n",
        "   - Fine-tune at 512px for 1-1.5 epochs, lr 1e-4, no Mixup/CutMix.\n",
        "6. **Evaluation & Inference**:\n",
        "   - Macro F1 on OOF; early stop on val macro F1.\n",
        "   - TTA: orig + hflip (2-4x), average logits.\n",
        "   - Ensemble: Weighted average of 1-2 models by OOF F1.\n",
        "   - Submission: Ensure Id matches test id, Predicted = original category_id (no remapping).\n",
        "\n",
        "## 24h Timeline\n",
        "- 0-1h: PyTorch install, implement tight crop, quick embedding baseline in '01_quick_baseline.ipynb'.\n",
        "- 1-4h: Extract embeddings, compute prototypes, generate first submission.csv (retrieval).\n",
        "- 4-6h: Train linear head on embeddings, evaluate OOF macro F1.\n",
        "- 6-13h: Train Model A (tf_efficientnetv2_m 384px) with CB-Focal, Class-Aware sampler, AMP/EMA.\n",
        "- 13-15h: Fine-tune Model A at 512px.\n",
        "- 15-21h: Train Model B (resnest101e 384px), fine-tune at 512px.\n",
        "- 21-23h: TTA inference, blend, generate final submission.\n",
        "- 23-24h: Sanity check, submit.\n",
        "\n",
        "## Avoided Mistakes\n",
        "- No top-class subsampling; ensure tail coverage.\n",
        "- Mild augs only; preserve botanical details.\n",
        "- 1-2 folds max; full data training.\n",
        "- Verify paths (use file_name), id mapping, submission format.\n",
        "- Log macro F1 per epoch; monitor rare class performance.\n",
        "\n",
        "## Next Steps\n",
        "- Create '01_quick_baseline.ipynb': PyTorch setup, embedding extraction, prototype retrieval for fast submission.\n",
        "- After first OOF, request expert review with score to decide on second model or two-stage fine-tune.\n",
        "- If retrieval baseline scores well, pivot to light fine-tune."
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}