{
  "cells": [
    {
      "id": "5b5998fd-e221-4c79-b81b-7f9ed735aabc",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# iMet 2020 FGVC7: Plan\n",
        "\n",
        "Objectives:\n",
        "- Get a strong baseline quickly; iterate to medal.\n",
        "- Maintain rigorous CV and logging; avoid long blind runs.\n",
        "\n",
        "Initial Baseline (Phase 1):\n",
        "- Environment check: confirm GPU and correct torch stack.\n",
        "- Data sanity check: train.csv, labels.csv, sample_submission.csv, image paths.\n",
        "- CV: 5-fold Multilabel Stratified KFold (iterative stratification).\n",
        "- Model: timm pretrained CNN (e.g., tf_efficientnet_b3_ns or nfnet_l0 if VRAM allows), multilabel BCEWithLogitsLoss.\n",
        "- Image size 384\u2192512 (start 384 for speed), AMP + gradient accumulation if needed.\n",
        "- Augmentations: A.Resize->A.RandomResizedCrop(384), HFlip, ColorJitter(soft), Cutout optional; Normalize as timm pretrained.\n",
        "- Optimizer: AdamW, cosine schedule with warmup. Early stopping on F1.\n",
        "- Thresholding: per-fold global threshold via sweep maximizing micro-F1 on OOF; save per-fold thresholds.\n",
        "- Artifacts: save OOF logits, test logits, model weights per fold. Log times/folds.\n",
        "\n",
        "Improvements (Phase 2):\n",
        "- Larger resolution (512), stronger aug (Mixup/Cutmix careful with multilabel), EMA, label-smoothing.\n",
        "- Try different backbones (Swin-T/S, ConvNeXt-T, EfficientNetV2-S/B3).\n",
        "- TTA (hflip + minor scale).\n",
        "- Blend diverse seeds/backbones via logit averaging and re-threshold using OOF.\n",
        "\n",
        "Validation Discipline:\n",
        "- Single fold split saved and reused across runs.\n",
        "- Avoid leakage: fit transforms inside folds only; no peeking.\n",
        "- Track micro-F1 OOF; expect strong baseline ~0.60\u20130.63 at 384, improve to \u22650.65 with 512/backbone/ensemble.\n",
        "\n",
        "Execution Plan:\n",
        "1) Env check (GPU, torch install).\n",
        "2) Data EDA: counts, classes, label freq, basic sanity.\n",
        "3) Implement training pipeline script (train.py) with cfg and logging.\n",
        "4) Smoke test on 1 fold, 1000 images, few epochs to verify.\n",
        "5) Full 5-fold at 384. Save OOF/test logits.\n",
        "6) Threshold sweep, create submission. Request expert review.\n",
        "7) Iterate with improved backbone/resolution/ensembles until medal CV.\n",
        "\n",
        "We will solicit expert advice at each major milestone."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "ca27d13b-e870-4d8d-9bd8-14c41e35795b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, json, time, shutil, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== Environment Check ===', flush=True)\n",
        "try:\n",
        "    out = subprocess.run(['bash','-lc','nvidia-smi || true'], capture_output=True, text=True, check=False)\n",
        "    print(out.stdout)\n",
        "except Exception as e:\n",
        "    print('nvidia-smi failed:', e)\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print('torch:', torch.__version__, 'CUDA avail:', torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print('GPU name:', torch.cuda.get_device_name(0))\n",
        "except Exception as e:\n",
        "    print('torch not available yet:', e)\n",
        "\n",
        "print('=== Data Sanity ===', flush=True)\n",
        "base = Path('.')\n",
        "train_dir = base/'train'\n",
        "test_dir = base/'test'\n",
        "\n",
        "# Count any files (any extension) to avoid extension mismatch issues\n",
        "train_files = [p.name for p in train_dir.iterdir() if p.is_file()]\n",
        "test_files = [p.name for p in test_dir.iterdir() if p.is_file()]\n",
        "print('train files:', len(train_files))\n",
        "print('test files:', len(test_files))\n",
        "print('sample train files:', train_files[:5])\n",
        "print('sample test files:', test_files[:5])\n",
        "\n",
        "train_csv = pd.read_csv(base/'train.csv')\n",
        "labels_csv = pd.read_csv(base/'labels.csv')\n",
        "sub_csv = pd.read_csv(base/'sample_submission.csv')\n",
        "print('train.csv shape:', train_csv.shape)\n",
        "print('labels.csv shape:', labels_csv.shape)\n",
        "print('sample_submission.csv shape:', sub_csv.shape)\n",
        "print('train.csv head:\\n', train_csv.head(3))\n",
        "print('labels.csv head:\\n', labels_csv.head(3))\n",
        "\n",
        "# Determine extension by probing first few ids\n",
        "def find_ext_for_id(img_id: str, roots):\n",
        "    for ext in ('.jpg', '.jpeg', '.png', '.webp', '.bmp'):\n",
        "        for root in roots:\n",
        "            p = root/f'{img_id}{ext}'\n",
        "            if p.exists():\n",
        "                return ext\n",
        "    # fallback: scan by prefix\n",
        "    for root in roots:\n",
        "        cands = list(root.glob(f'{img_id}.*'))\n",
        "        if cands:\n",
        "            return cands[0].suffix\n",
        "    return None\n",
        "\n",
        "train_id_col = 'id' if 'id' in train_csv.columns else ('image_id' if 'image_id' in train_csv.columns else None)\n",
        "test_id_col = 'id' if 'id' in sub_csv.columns else ('image_id' if 'image_id' in sub_csv.columns else None)\n",
        "print('Detected id columns -> train:', train_id_col, ' test:', test_id_col)\n",
        "\n",
        "probe_ids = list(train_csv[train_id_col].head(5)) if train_id_col else []\n",
        "probe_exts = {pid: find_ext_for_id(pid, [train_dir]) for pid in probe_ids}\n",
        "print('Probe extensions:', probe_exts)\n",
        "default_ext = None\n",
        "vals = [e for e in probe_exts.values() if e]\n",
        "if vals:\n",
        "    default_ext = max(set(vals), key=vals.count)\n",
        "print('Chosen default ext:', default_ext)\n",
        "\n",
        "# Basic existence checks using detected extension or prefix matching\n",
        "missing_train = 0\n",
        "for img in train_csv[train_id_col].head(1000):\n",
        "    if default_ext:\n",
        "        exists = (train_dir/f'{img}{default_ext}').exists()\n",
        "    else:\n",
        "        exists = any((train_dir/f).name.startswith(img) for f in train_dir.iterdir())\n",
        "    if not exists:\n",
        "        missing_train += 1\n",
        "print('Missing among first 1000 train ids:', missing_train)\n",
        "\n",
        "missing_test = 0\n",
        "for img in sub_csv[test_id_col].head(1000):\n",
        "    if default_ext:\n",
        "        exists = (test_dir/f'{img}{default_ext}').exists()\n",
        "    else:\n",
        "        exists = any((test_dir/f).name.startswith(img) for f in test_dir.iterdir())\n",
        "    if not exists:\n",
        "        missing_test += 1\n",
        "print('Missing among first 1000 test ids:', missing_test)\n",
        "\n",
        "print('Unique labels in labels.csv:', labels_csv['attribute_id'].nunique() if 'attribute_id' in labels_csv.columns else 'N/A')\n",
        "print('Done.')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Environment Check ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 27 16:44:14 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     182MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\ntorch not available yet: No module named 'torch'\n=== Data Sanity ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train files: 120801\ntest files: 21318\nsample train files: ['f4e684acbb6f2b33b45a16e586f87369.png', '727510fa873bed3a7a5b9902567b0d9f.png', 'e6b8484abfef0045ce9e4577f2ffd9e8.png', '8a074a477f1ccbd865151a9866419940.png', 'b22aa8832224499fdff061348f07bed2.png']\nsample test files: ['4e0ba2b09affaf8525695752214b1dc4.png', 'ac5b7b0322f8c2ef4035b394956da403.png', 'df1d4aa72346aacaa4d4d1cc880fcc78.png', '878e96135e73845501059fb1d022d459.png', 'b007aba41e9d5a4c2df63fa452bf1640.png']\ntrain.csv shape: (120801, 2)\nlabels.csv shape: (3474, 2)\nsample_submission.csv shape: (21318, 2)\ntrain.csv head:\n                                  id                attribute_ids\n0  4d0f6eada4ccb283551bc2f75e2ba588  3077 3187 3418 448 1625 782\n1  75a9baea36b82e81263716fac427e416        2802 287 370 1419 784\n2  cc7cbf14ef9e9261508ba27f9d2f4f28                      922 785\nlabels.csv head:\n    attribute_id        attribute_name\n0             0  country::afghanistan\n1             1     country::alamania\n2             2      country::algeria\nDetected id columns -> train: id  test: id\nProbe extensions: {'4d0f6eada4ccb283551bc2f75e2ba588': '.png', '75a9baea36b82e81263716fac427e416': '.png', 'cc7cbf14ef9e9261508ba27f9d2f4f28': '.png', '12fbc761e4a216a9d8e593a42cce0a0d': '.png', 'da4c839851b8b7c517bd6dab80ba0db0': '.png'}\nChosen default ext: .png\nMissing among first 1000 train ids: 0\nMissing among first 1000 test ids: 0\nUnique labels in labels.csv: 3474\nDone.\n"
          ]
        }
      ]
    },
    {
      "id": "5d8163f3-3dc8-4d85-91f2-ec8bf4637c2a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print('> pip', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "print('=== Install CUDA 12.1 torch stack and deps ===', flush=True)\n",
        "# Uninstall any existing torch stack\n",
        "for pkg in ('torch','torchvision','torchaudio'):\n",
        "    try:\n",
        "        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
        "    except Exception as e:\n",
        "        print('uninstall error:', pkg, e)\n",
        "\n",
        "# Clean potential stray site dirs that can shadow correct wheels\n",
        "for d in (\n",
        "    '/app/.pip-target/torch',\n",
        "    '/app/.pip-target/torchvision',\n",
        "    '/app/.pip-target/torchaudio',\n",
        "    '/app/.pip-target/torch-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torch-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.23.0.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.19.1.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchgen',\n",
        "    '/app/.pip-target/functorch',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# Install exact cu121 torch stack\n",
        "pip('install',\n",
        "    '--index-url', 'https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url', 'https://pypi.org/simple',\n",
        "    'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1')\n",
        "\n",
        "# Freeze versions for later installs\n",
        "Path('constraints.txt').write_text('torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n')\n",
        "\n",
        "# Core deps\n",
        "pip('install', '-c', 'constraints.txt',\n",
        "    'timm==1.0.9',\n",
        "    'albumentations==1.4.14',\n",
        "    'scikit-learn==1.5.2',\n",
        "    'iterative-stratification==0.1.7',\n",
        "    'opencv-python-headless==4.10.0.84',\n",
        "    'pandas',\n",
        "    '--upgrade-strategy', 'only-if-needed')\n",
        "\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'CUDA build:', getattr(torch.version, 'cuda', None))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "assert str(getattr(torch.version, 'cuda', '12.1')).startswith('12.1'), f'Wrong CUDA build: {torch.version.cuda}'\n",
        "assert torch.cuda.is_available(), 'CUDA not available'\n",
        "print('GPU:', torch.cuda.get_device_name(0))\n",
        "print('Done installing.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Install CUDA 12.1 torch stack and deps ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 MB 392.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 382.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 233.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 128.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 273.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 269.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 287.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 243.6 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 238.7 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 316.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 258.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 246.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 345.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 471.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 287.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 461.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 446.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 451.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 500.3 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 257.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 261.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 244.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 194.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 530.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install -c constraints.txt timm==1.0.9 albumentations==1.4.14 scikit-learn==1.5.2 iterative-stratification==0.1.7 opencv-python-headless==4.10.0.84 pandas --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm==1.0.9\n  Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.3/2.3 MB 67.3 MB/s eta 0:00:00\nCollecting albumentations==1.4.14\n  Downloading albumentations-1.4.14-py3-none-any.whl (177 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 178.0/178.0 KB 283.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.5.2\n  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13.3/13.3 MB 205.3 MB/s eta 0:00:00\nCollecting iterative-stratification==0.1.7\n  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python-headless==4.10.0.84\n  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 49.9/49.9 MB 213.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas\n  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.4/12.4 MB 211.6 MB/s eta 0:00:00\nCollecting pyyaml\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 521.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch\n  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.1/797.1 MB 316.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 563.3/563.3 KB 508.8 MB/s eta 0:00:00\nCollecting torchvision\n  Downloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.0/7.0 MB 386.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 270.9 MB/s eta 0:00:00\nCollecting typing-extensions>=4.9.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 398.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.24.4\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 241.4 MB/s eta 0:00:00\nCollecting eval-type-backport\n  Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic>=2.7.0\n  Downloading pydantic-2.11.9-py3-none-any.whl (444 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 444.9/444.9 KB 494.3 MB/s eta 0:00:00\nCollecting albucore>=0.0.13\n  Downloading albucore-0.0.33-py3-none-any.whl (18 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy>=1.10.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 264.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-image>=0.21.0\n  Downloading scikit_image-0.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.8/14.8 MB 250.2 MB/s eta 0:00:00\nCollecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 466.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dateutil>=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.9/229.9 KB 454.3 MB/s eta 0:00:00\nCollecting tzdata>=2022.7\n  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 347.8/347.8 KB 473.5 MB/s eta 0:00:00\nCollecting pytz>=2020.1\n  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 509.2/509.2 KB 483.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting simsimd>=5.9.2\n  Downloading simsimd-6.5.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1/1.1 MB 206.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stringzilla>=3.10.4\n  Downloading stringzilla-4.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (496 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 496.5/496.5 KB 355.6 MB/s eta 0:00:00\nCollecting typing-inspection>=0.4.0\n  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\nCollecting annotated-types>=0.6.0\n  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic-core==2.33.2\n  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 517.7 MB/s eta 0:00:00\nCollecting six>=1.5\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nCollecting imageio!=2.35.0,>=2.33\n  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 315.8/315.8 KB 493.6 MB/s eta 0:00:00\nCollecting lazy-loader>=0.4\n  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow>=10.1\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 285.9 MB/s eta 0:00:00\nCollecting tifffile>=2022.8.12\n  Downloading tifffile-2025.9.20-py3-none-any.whl (230 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 230.1/230.1 KB 458.3 MB/s eta 0:00:00\nCollecting packaging>=21\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 412.7 MB/s eta 0:00:00\nCollecting networkx>=3.0\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 519.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting fsspec>=2023.5.0\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 477.1 MB/s eta 0:00:00\nCollecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 527.4 MB/s eta 0:00:00\nCollecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 385.1 MB/s eta 0:00:00\nCollecting tqdm>=4.42.1\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 415.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 246.5 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 227.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 460.9 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 277.7 MB/s eta 0:00:00\nCollecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 397.6 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 223.8 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 230.6 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 307.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 489.7 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 258.4 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 229.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 525.8 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 280.7 MB/s eta 0:00:00\nCollecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 287.1 MB/s eta 0:00:00\nCollecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 161.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 428.3 MB/s eta 0:00:00\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 449.3 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 426.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 483.6 MB/s eta 0:00:00\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 428.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: simsimd, pytz, mpmath, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, stringzilla, six, safetensors, pyyaml, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, joblib, idna, hf-xet, fsspec, filelock, eval-type-backport, charset_normalizer, certifi, annotated-types, typing-inspection, triton, tifffile, scipy, requests, python-dateutil, pydantic-core, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cudnn-cu12, lazy-loader, jinja2, imageio, scikit-learn, scikit-image, pydantic, pandas, nvidia-cusolver-cu12, huggingface_hub, albucore, torch, iterative-stratification, albumentations, torchvision, timm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 albucore-0.0.33 albumentations-1.4.14 annotated-types-0.7.0 certifi-2025.8.3 charset_normalizer-3.4.3 eval-type-backport-0.2.2 filelock-3.19.1 fsspec-2025.9.0 hf-xet-1.1.10 huggingface_hub-0.35.1 idna-3.10 imageio-2.37.0 iterative-stratification-0.1.7 jinja2-3.1.6 joblib-1.5.2 lazy-loader-0.4 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 opencv-python-headless-4.10.0.84 packaging-25.0 pandas-2.3.2 pillow-11.3.0 pydantic-2.11.9 pydantic-core-2.33.2 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.3 requests-2.32.5 safetensors-0.6.2 scikit-image-0.25.2 scikit-learn-1.5.2 scipy-1.16.2 simsimd-6.5.3 six-1.17.0 stringzilla-4.0.14 sympy-1.14.0 threadpoolctl-3.6.0 tifffile-2025.9.20 timm-1.0.9 torch-2.4.1 torchvision-0.19.1 tqdm-4.67.1 triton-3.0.0 typing-extensions-4.15.0 typing-inspection-0.4.1 tzdata-2025.2 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/torchvision already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchvision.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.4.5.107.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.1.0.106.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/triton-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec-2025.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/MarkupSafe-3.0.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.1.3.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.0.2.54.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.2.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.20.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.9.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.4.1+cu121 CUDA build: 12.1\nCUDA available: True\nGPU: NVIDIA A10-24Q\nDone installing.\n"
          ]
        }
      ]
    },
    {
      "id": "88977898-69e2-41ee-9092-ab1a77b335c1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create and save 5-fold Multilabel Stratified CV splits\n",
        "import time, numpy as np, pandas as pd\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "t0 = time.time()\n",
        "train_df = pd.read_csv('train.csv')\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "num_labels = labels_df['attribute_id'].nunique()\n",
        "print('Num samples:', len(train_df), 'Num labels:', num_labels, flush=True)\n",
        "\n",
        "# Parse attribute_ids -> list of ints\n",
        "attrs = train_df['attribute_ids'].fillna('').apply(lambda s: [int(x) for x in str(s).split() if x!=''])\n",
        "lens = attrs.apply(len)\n",
        "print('Label cardinality: mean', lens.mean(), 'median', lens.median(), 'max', lens.max(), flush=True)\n",
        "\n",
        "# Build dense indicator matrix (bool) for stratification\n",
        "y = np.zeros((len(train_df), num_labels), dtype=np.uint8)\n",
        "t1 = time.time()\n",
        "for i, lab_list in enumerate(attrs):\n",
        "    if lab_list:\n",
        "        y[i, lab_list] = 1\n",
        "    if (i+1) % 20000 == 0:\n",
        "        print(f'..filled {i+1}/{len(train_df)} rows in {time.time()-t1:.1f}s', flush=True)\n",
        "\n",
        "skf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = np.full(len(train_df), -1, dtype=np.int16)\n",
        "for fold, (_, val_idx) in enumerate(skf.split(train_df.index.values, y)):\n",
        "    folds[val_idx] = fold\n",
        "    print(f'Assigned fold {fold}: {len(val_idx)} samples', flush=True)\n",
        "assert (folds >= 0).all()\n",
        "\n",
        "train_folds = train_df.copy()\n",
        "train_folds['fold'] = folds\n",
        "train_folds.to_csv('train_folds.csv', index=False)\n",
        "print('Saved train_folds.csv. Time:', round(time.time()-t0,1), 's', flush=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num samples: 120801 Num labels: 3474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label cardinality: mean 4.421097507470964 median 4.0 max 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..filled 20000/120801 rows in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..filled 40000/120801 rows in 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..filled 60000/120801 rows in 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..filled 80000/120801 rows in 0.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..filled 100000/120801 rows in 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..filled 120000/120801 rows in 0.2s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m skf = MultilabelStratifiedKFold(n_splits=\u001b[32m5\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     26\u001b[39m folds = np.full(\u001b[38;5;28mlen\u001b[39m(train_df), -\u001b[32m1\u001b[39m, dtype=np.int16)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mskf\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAssigned fold \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m samples\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/model_selection/_split.py:416\u001b[39m, in \u001b[36m_BaseKFold.split\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_splits > n_samples:\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    410\u001b[39m         (\n\u001b[32m    411\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m greater\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    413\u001b[39m         ).format(\u001b[38;5;28mself\u001b[39m.n_splits, n_samples)\n\u001b[32m    414\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/model_selection/_split.py:147\u001b[39m, in \u001b[36mBaseCrossValidator.split\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m    145\u001b[39m X, y, groups = indexable(X, y, groups)\n\u001b[32m    146\u001b[39m indices = np.arange(_num_samples(X))\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_test_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogical_not\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m]\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/iterstrat/ml_stratifiers.py:183\u001b[39m, in \u001b[36mMultilabelStratifiedKFold._iter_test_masks\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iter_test_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, X=\u001b[38;5;28;01mNone\u001b[39;00m, y=\u001b[38;5;28;01mNone\u001b[39;00m, groups=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     test_folds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_test_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.n_splits):\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m test_folds == i\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/iterstrat/ml_stratifiers.py:178\u001b[39m, in \u001b[36mMultilabelStratifiedKFold._make_test_folds\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    174\u001b[39m     y = y[indices]\n\u001b[32m    176\u001b[39m r = np.asarray([\u001b[32m1\u001b[39m / \u001b[38;5;28mself\u001b[39m.n_splits] * \u001b[38;5;28mself\u001b[39m.n_splits)\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m test_folds = \u001b[43mIterativeStratification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m test_folds[np.argsort(indices)]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/iterstrat/ml_stratifiers.py:56\u001b[39m, in \u001b[36mIterativeStratification\u001b[39m\u001b[34m(labels, r, random_state)\u001b[39m\n\u001b[32m     51\u001b[39m labels_not_processed_mask = np.ones(n_samples, dtype=\u001b[38;5;28mbool\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m np.any(labels_not_processed_mask):\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# Find the label with the fewest (but at least one) remaining examples,\u001b[39;00m\n\u001b[32m     55\u001b[39m     \u001b[38;5;66;03m# breaking ties randomly\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     num_labels = \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabels_not_processed_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     \u001b[38;5;66;03m# Handle case where only all-zero labels are left by distributing\u001b[39;00m\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# across all folds as evenly as possible (not in original algorithm but\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# mentioned in the text). (By handling this case separately, some\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# code redundancy is introduced; however, this approach allows for\u001b[39;00m\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# decreased execution time when there are a relatively large number\u001b[39;00m\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# of all-zero labels.)\u001b[39;00m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m num_labels.sum() == \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:49\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     48\u001b[39m          initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "292c24ea-bdff-4090-9c9a-83df1d9e5f15",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys, subprocess\n",
        "def pip(*args):\n",
        "    print('> pip', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "print('Fixing albumentations/albucore mismatch by downgrading albumentations to 1.3.1', flush=True)\n",
        "pip('install', '-c', 'constraints.txt', 'albumentations==1.3.1', '--upgrade-strategy', 'only-if-needed')\n",
        "import albumentations as A\n",
        "print('albumentations version:', A.__version__)\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "print('Albumentations import OK')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixing albumentations/albucore mismatch by downgrading albumentations to 1.3.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install -c constraints.txt albumentations==1.3.1 --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations==1.3.1\n  Downloading albumentations-1.3.1-py3-none-any.whl (125 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 125.7/125.7 KB 6.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy>=1.1.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 208.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python-headless>=4.1.1\n  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 54.0/54.0 MB 218.1 MB/s eta 0:00:00\nCollecting scikit-image>=0.16.1\n  Downloading scikit_image-0.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.8/14.8 MB 246.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyYAML\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 545.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.11.1\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 187.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qudida>=0.0.4\n  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\nCollecting opencv-python-headless>=4.1.1\n  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 50.0/50.0 MB 269.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn>=0.19.1\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 292.7 MB/s eta 0:00:00\nCollecting typing-extensions\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 383.1 MB/s eta 0:00:00\nCollecting packaging>=21\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 421.1 MB/s eta 0:00:00\nCollecting imageio!=2.35.0,>=2.33\n  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 315.8/315.8 KB 528.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx>=3.0\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 561.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow>=10.1\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 253.2 MB/s eta 0:00:00\nCollecting tifffile>=2022.8.12\n  Downloading tifffile-2025.9.20-py3-none-any.whl (230 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 230.1/230.1 KB 498.5 MB/s eta 0:00:00\nCollecting lazy-loader>=0.4\n  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 516.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: typing-extensions, threadpoolctl, PyYAML, pillow, packaging, numpy, networkx, joblib, tifffile, scipy, opencv-python-headless, lazy-loader, imageio, scikit-learn, scikit-image, qudida, albumentations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed PyYAML-6.0.3 albumentations-1.3.1 imageio-2.37.0 joblib-1.5.2 lazy-loader-0.4 networkx-3.5 numpy-1.26.4 opencv-python-headless-4.11.0.86 packaging-25.0 pillow-11.3.0 qudida-0.0.4 scikit-image-0.25.2 scikit-learn-1.7.2 scipy-1.16.2 threadpoolctl-3.6.0 tifffile-2025.9.20 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/albumentations already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_image-0.25.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/skimage already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sklearn already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_learn.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/imageio-2.37.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/imageio already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/lazy_loader-0.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/lazy_loader already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/cv2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/opencv_python_headless.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy-1.16.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tifffile-2025.9.20.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tifffile already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib-1.5.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging-25.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pyyaml-6.0.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/_yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl-3.6.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'preserve_channel_dim' from 'albucore.utils' (/app/.pip-target/albucore/utils.py)",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFixing albumentations/albucore mismatch by downgrading albumentations to 1.3.1\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m pip(\u001b[33m'\u001b[39m\u001b[33minstall\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m-c\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mconstraints.txt\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33malbumentations==1.3.1\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m--upgrade-strategy\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33monly-if-needed\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbumentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mA\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33malbumentations version:\u001b[39m\u001b[33m'\u001b[39m, A.__version__)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbumentations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToTensorV2\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/albumentations/__init__.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbumentations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheck_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_for_updates\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maugmentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomposition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mserialization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/albumentations/augmentations/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mblur\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mblur\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcrops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/albumentations/augmentations/blur/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/albumentations/augmentations/blur/functional.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbucore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clipped, maybe_process_in_chunks, preserve_channel_dim\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbumentations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maugmentations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convolve\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01malbumentations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maugmentations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m scale\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'preserve_channel_dim' from 'albucore.utils' (/app/.pip-target/albucore/utils.py)"
          ]
        }
      ]
    },
    {
      "id": "39939d2a-e63f-46ad-903a-a063859009e9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys, subprocess, importlib, os\n",
        "def run_pip(cmd):\n",
        "    print('> pip', *cmd, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *cmd], check=True)\n",
        "\n",
        "print('Hard-reset albumentations to 1.3.1 (remove albucore), forcing overwrite', flush=True)\n",
        "run_pip(['uninstall', '-y', 'albumentations', 'albucore'])\n",
        "run_pip(['install', '--no-cache-dir', 'albumentations==1.3.1'])\n",
        "import albumentations as A\n",
        "print('albumentations version:', A.__version__)\n",
        "print('albumentations file:', A.__file__)\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "print('Albumentations import OK')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hard-reset albumentations to 1.3.1 (remove albucore), forcing overwrite\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip uninstall -y albumentations albucore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: albumentations 1.3.1\nUninstalling albumentations-1.3.1:\n  Successfully uninstalled albumentations-1.3.1\n> pip install --no-cache-dir albumentations==1.3.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping albucore as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations==1.3.1\n  Downloading albumentations-1.3.1-py3-none-any.whl (125 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 125.7/125.7 KB 5.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy>=1.1.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 169.0 MB/s eta 0:00:00\nCollecting scikit-image>=0.16.1\n  Downloading scikit_image-0.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.8/14.8 MB 240.5 MB/s eta 0:00:00\nCollecting qudida>=0.0.4\n  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\nCollecting opencv-python-headless>=4.1.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 54.0/54.0 MB 191.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.11.1\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 319.6 MB/s eta 0:00:00\nCollecting PyYAML\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 524.7 MB/s eta 0:00:00\nCollecting opencv-python-headless>=4.1.1\n  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 50.0/50.0 MB 176.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn>=0.19.1\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 208.1 MB/s eta 0:00:00\nCollecting typing-extensions\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 388.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow>=10.1\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 540.1 MB/s eta 0:00:00\nCollecting imageio!=2.35.0,>=2.33\n  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 315.8/315.8 KB 511.4 MB/s eta 0:00:00\nCollecting packaging>=21\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 426.5 MB/s eta 0:00:00\nCollecting networkx>=3.0\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 542.1 MB/s eta 0:00:00\nCollecting tifffile>=2022.8.12\n  Downloading tifffile-2025.9.20-py3-none-any.whl (230 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 230.1/230.1 KB 497.9 MB/s eta 0:00:00\nCollecting lazy-loader>=0.4\n  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 529.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: typing-extensions, threadpoolctl, PyYAML, pillow, packaging, numpy, networkx, joblib, tifffile, scipy, opencv-python-headless, lazy-loader, imageio, scikit-learn, scikit-image, qudida, albumentations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed PyYAML-6.0.3 albumentations-1.3.1 imageio-2.37.0 joblib-1.5.2 lazy-loader-0.4 networkx-3.5 numpy-1.26.4 opencv-python-headless-4.11.0.86 packaging-25.0 pillow-11.3.0 qudida-0.0.4 scikit-image-0.25.2 scikit-learn-1.7.2 scipy-1.16.2 threadpoolctl-3.6.0 tifffile-2025.9.20 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/qudida-0.0.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/qudida already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_image-0.25.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/skimage already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_learn-1.7.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sklearn already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_learn.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/imageio-2.37.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/imageio already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/lazy_loader-0.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/lazy_loader already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/cv2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/opencv_python_headless-4.11.0.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/opencv_python_headless.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy-1.16.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tifffile-2025.9.20.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tifffile already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib-1.5.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging-25.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pyyaml-6.0.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/_yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl-3.6.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "albumentations version: 1.3.1\nalbumentations file: /app/.pip-target/albumentations/__init__.py\nAlbumentations import OK\n"
          ]
        }
      ]
    },
    {
      "id": "02c23467-a425-45ea-8fb1-2db4d62d86cb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, time, shlex\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "print('=== Smoke: precompute fast folds (cardinality) + tiny 1-fold run (no-pretrained, unbuffered) ===', flush=True)\n",
        "# 1) Fast folds via label-cardinality bins to avoid slow MSKF in subprocess\n",
        "folds_path = Path('train_folds_smoke.csv')\n",
        "if folds_path.exists():\n",
        "    folds_path.unlink()\n",
        "    print('Deleted existing train_folds_smoke.csv')\n",
        "train_df = pd.read_csv('train.csv')\n",
        "attrs = train_df['attribute_ids'].fillna('').astype(str).apply(lambda s: [int(x) for x in s.split() if x!=''])\n",
        "card = attrs.apply(len).values\n",
        "bins = np.clip(card, 0, 8)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = np.full(len(train_df), -1, dtype=np.int16)\n",
        "for f, (_, vidx) in enumerate(skf.split(np.zeros(len(train_df)), bins)):\n",
        "    folds[vidx] = f\n",
        "train_df2 = train_df.copy()\n",
        "train_df2['fold'] = folds\n",
        "train_df2.to_csv(folds_path, index=False)\n",
        "print('Wrote fast folds to train_folds_smoke.csv (cardinality SKFold)', flush=True)\n",
        "\n",
        "# 2) Run a tiny smoke of train.py using these folds\n",
        "cmd = [\n",
        "    sys.executable, '-u', 'train.py',\n",
        "    '--model', 'tf_efficientnet_b3_ns',\n",
        "    '--img-size', '224',\n",
        "    '--epochs', '1',\n",
        "    '--batch-size', '64',\n",
        "    '--val-batch-size', '96',\n",
        "    '--num-workers', '4',\n",
        "    '--folds', '0',\n",
        "    '--folds-csv', 'train_folds_smoke.csv',\n",
        "    '--out-dir', 'out_smoke_fast',\n",
        "    '--limit-train-steps', '30',\n",
        "    '--limit-val-steps', '10',\n",
        "    '--early-stop-patience', '1',\n",
        "    '--no-pretrained'\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd), flush=True)\n",
        "t0 = time.time()\n",
        "env = dict(os.environ)\n",
        "env['PYTHONUNBUFFERED'] = '1'\n",
        "p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, env=env)\n",
        "try:\n",
        "    for line in p.stdout:\n",
        "        print(line, end='')\n",
        "finally:\n",
        "    rc = p.wait()\n",
        "elapsed = time.time() - t0\n",
        "print(f'Exit code: {rc}, elapsed {elapsed/60:.1f} min', flush=True)\n",
        "assert rc == 0, 'Smoke training failed'\n",
        "print('Smoke training completed.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Smoke: precompute fast folds (cardinality) + tiny 1-fold run (no-pretrained, unbuffered) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted existing train_folds_smoke.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote fast folds to train_folds_smoke.csv (cardinality SKFold)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running: /usr/bin/python3.11 -u train.py --model tf_efficientnet_b3_ns --img-size 224 --epochs 1 --batch-size 64 --val-batch-size 96 --num-workers 4 --folds 0 --folds-csv train_folds_smoke.csv --out-dir out_smoke_fast --limit-train-steps 30 --limit-val-steps 10 --early-stop-patience 1 --no-pretrained\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected image extension: .png\n==== Fold 0 start ====\n/app/.pip-target/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b3_ns to current tf_efficientnet_b3.ns_jft_in1k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:215: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:244: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 1 ===\nval_size=24161 probs_shape=(960, 3474) tgts_shape=(960, 3474)\nprobs_range=[0.000001,0.167815]\ntgt_pos_rate=0.00126745 mean_pos_per_img=4.403\nthr=0.2 pred_pos_rate=0.00000000 mean_pred_per_img=0.000 empty_frac=1.000000 TP=0 FP=0 FN=4227 f1@0.2=0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 val micro-f1 0.17101 @ thr 0.090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Fold 0 done: best_f1 0.17101 thr 0.090 ====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF micro-f1 0.08716 @ thr 0.050\n/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:355: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(pth, map_location='cpu')\n/app/.pip-target/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b3_ns to current tf_efficientnet_b3.ns_jft_in1k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exit code: 0, elapsed 2.7 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoke training completed.\n"
          ]
        }
      ]
    },
    {
      "id": "37f7c593-4279-4cdb-9fed-08f8d9ee5e76",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, time, shlex\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== PRODUCTION RUN: b3@384 5-fold, EMA+TTA (cardinality folds) ===', flush=True)\n",
        "cmd = [\n",
        "    sys.executable, 'train.py',\n",
        "    '--model', 'tf_efficientnet_b3_ns',\n",
        "    '--img-size', '384',\n",
        "    '--epochs', '10',\n",
        "    '--batch-size', '56',\n",
        "    '--val-batch-size', '96',\n",
        "    '--num-workers', '10',\n",
        "    '--lr', '2e-4',\n",
        "    '--use-ema',\n",
        "    '--tta',\n",
        "    '--early-stop-patience', '3',\n",
        "    '--folds', '0,1,2,3,4',\n",
        "    '--folds-csv', 'train_folds_smoke.csv',\n",
        "    '--out-dir', 'out_b3_384_card',\n",
        "    '--pretrained'\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd), flush=True)\n",
        "t0 = time.time()\n",
        "p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)\n",
        "try:\n",
        "    for line in p.stdout:\n",
        "        print(line, end='')\n",
        "finally:\n",
        "    rc = p.wait()\n",
        "elapsed = time.time() - t0\n",
        "print(f'Exit code: {rc}, elapsed {elapsed/3600:.2f} h', flush=True)\n",
        "assert rc == 0, 'Production run failed'\n",
        "print('Production run completed.')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PRODUCTION RUN: b3@384 5-fold, EMA+TTA (cardinality folds) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running: /usr/bin/python3.11 train.py --model tf_efficientnet_b3_ns --img-size 384 --epochs 10 --batch-size 56 --val-batch-size 96 --num-workers 10 --lr 2e-4 --use-ema --tta --early-stop-patience 3 --folds 0,1,2,3,4 --folds-csv train_folds_smoke.csv --out-dir out_b3_384_card --pretrained\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected image extension: .png\n==== Fold 0 start ====\n/app/.pip-target/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b3_ns to current tf_efficientnet_b3.ns_jft_in1k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:215: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:244: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 100/1725 loss 2.4412 elapsed 0.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 200/1725 loss 2.4382 elapsed 1.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 300/1725 loss 2.4356 elapsed 1.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 400/1725 loss 2.4342 elapsed 2.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 500/1725 loss 2.4334 elapsed 2.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 600/1725 loss 2.4328 elapsed 3.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 700/1725 loss 2.4324 elapsed 4.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 800/1725 loss 2.4321 elapsed 4.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 900/1725 loss 2.4319 elapsed 5.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1000/1725 loss 2.4317 elapsed 5.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1100/1725 loss 2.4316 elapsed 6.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1200/1725 loss 2.4314 elapsed 6.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1300/1725 loss 2.4313 elapsed 7.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1400/1725 loss 2.4312 elapsed 7.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1500/1725 loss 2.4312 elapsed 8.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1600/1725 loss 2.4311 elapsed 8.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1700/1725 loss 2.4310 elapsed 9.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 1 ===\nval_size=24161 probs_shape=(24161, 3474) tgts_shape=(24161, 3474)\nprobs_range=[0.000001,0.674683]\ntgt_pos_rate=0.00127242 mean_pos_per_img=4.420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.00584288 mean_pred_per_img=20.298 empty_frac=0.002235 TP=44515 FP=445909 FN=62286 f1@0.2=0.149073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 val micro-f1 0.18500 @ thr 0.350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 100/1725 loss 2.4300 elapsed 11.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 200/1725 loss 2.4300 elapsed 12.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 300/1725 loss 2.4300 elapsed 12.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 400/1725 loss 2.4300 elapsed 13.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 500/1725 loss 2.4300 elapsed 13.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 600/1725 loss 2.4300 elapsed 14.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 700/1725 loss 2.4300 elapsed 14.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 800/1725 loss 2.4300 elapsed 15.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 900/1725 loss 2.4300 elapsed 15.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1000/1725 loss 2.4300 elapsed 16.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1100/1725 loss 2.4300 elapsed 16.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1200/1725 loss 2.4300 elapsed 17.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1300/1725 loss 2.4300 elapsed 17.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1400/1725 loss 2.4300 elapsed 18.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1500/1725 loss 2.4300 elapsed 18.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1600/1725 loss 2.4300 elapsed 19.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1700/1725 loss 2.4300 elapsed 19.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 2 ===\nval_size=24161 probs_shape=(24161, 3474) tgts_shape=(24161, 3474)\nprobs_range=[0.000001,0.939251]\ntgt_pos_rate=0.00127242 mean_pos_per_img=4.420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.00633276 mean_pred_per_img=22.000 empty_frac=0.000000 TP=46455 FP=485087 FN=60346 f1@0.2=0.145549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 val micro-f1 0.14555 @ thr 0.050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 100/1725 loss 2.4300 elapsed 21.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 200/1725 loss 2.4300 elapsed 22.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 300/1725 loss 2.4300 elapsed 22.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 400/1725 loss 2.4300 elapsed 23.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 500/1725 loss 2.4300 elapsed 23.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 600/1725 loss 2.4300 elapsed 24.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 700/1725 loss 2.4300 elapsed 24.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 800/1725 loss 2.4300 elapsed 25.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 900/1725 loss 2.4300 elapsed 25.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1000/1725 loss 2.4300 elapsed 26.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1100/1725 loss 2.4300 elapsed 26.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1200/1725 loss 2.4300 elapsed 27.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1300/1725 loss 2.4300 elapsed 27.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1400/1725 loss 2.4300 elapsed 28.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1500/1725 loss 2.4300 elapsed 28.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1600/1725 loss 2.4300 elapsed 29.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1700/1725 loss 2.4300 elapsed 29.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 3 ===\nval_size=24161 probs_shape=(24161, 3474) tgts_shape=(24161, 3474)\nprobs_range=[0.000001,0.968680]\ntgt_pos_rate=0.00127242 mean_pos_per_img=4.420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.00633276 mean_pred_per_img=22.000 empty_frac=0.000000 TP=46455 FP=485087 FN=60346 f1@0.2=0.145549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 val micro-f1 0.14555 @ thr 0.480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 100/1725 loss 2.4300 elapsed 31.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 200/1725 loss 2.4300 elapsed 32.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 300/1725 loss 2.4300 elapsed 32.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 400/1725 loss 2.4300 elapsed 33.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 500/1725 loss 2.4300 elapsed 33.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 600/1725 loss 2.4300 elapsed 34.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 700/1725 loss 2.4300 elapsed 34.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 800/1725 loss 2.4300 elapsed 35.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 900/1725 loss 2.4300 elapsed 35.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1000/1725 loss 2.4300 elapsed 36.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1100/1725 loss 2.4300 elapsed 36.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1200/1725 loss 2.4300 elapsed 37.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1300/1725 loss 2.4300 elapsed 37.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1400/1725 loss 2.4300 elapsed 38.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1500/1725 loss 2.4300 elapsed 39.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1600/1725 loss 2.4300 elapsed 39.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1700/1725 loss 2.4300 elapsed 40.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 4 ===\nval_size=24161 probs_shape=(24161, 3474) tgts_shape=(24161, 3474)\nprobs_range=[0.000001,0.984481]\ntgt_pos_rate=0.00127242 mean_pos_per_img=4.420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.00633276 mean_pred_per_img=22.000 empty_frac=0.000000 TP=46455 FP=485087 FN=60346 f1@0.2=0.145549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 val micro-f1 0.14555 @ thr 0.050\nEarly stopping at epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Fold 0 done: best_f1 0.18500 thr 0.350 ====\n==== Fold 1 start ====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 100/1725 loss 2.4413 elapsed 0.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 200/1725 loss 2.4379 elapsed 1.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 300/1725 loss 2.4347 elapsed 1.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 400/1725 loss 2.4330 elapsed 2.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 500/1725 loss 2.4320 elapsed 2.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 600/1725 loss 2.4313 elapsed 3.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 700/1725 loss 2.4308 elapsed 3.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 800/1725 loss 2.4305 elapsed 4.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 900/1725 loss 2.4302 elapsed 4.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1000/1725 loss 2.4300 elapsed 5.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1100/1725 loss 2.4298 elapsed 5.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1200/1725 loss 2.4296 elapsed 6.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1300/1725 loss 2.4295 elapsed 6.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1400/1725 loss 2.4294 elapsed 7.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1500/1725 loss 2.4293 elapsed 7.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1600/1725 loss 2.4292 elapsed 8.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1700/1725 loss 2.4291 elapsed 8.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 1 epoch 1 ===\nval_size=24160 probs_shape=(24160, 3474) tgts_shape=(24160, 3474)\nprobs_range=[0.000001,0.520331]\ntgt_pos_rate=0.00127227 mean_pos_per_img=4.420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.00415277 mean_pred_per_img=14.427 empty_frac=0.006416 TP=37122 FP=311428 FN=69662 f1@0.2=0.163054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 val micro-f1 0.18269 @ thr 0.250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 100/1725 loss 2.4279 elapsed 10.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 200/1725 loss 2.4279 elapsed 11.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 300/1725 loss 2.4279 elapsed 11.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 400/1725 loss 2.4279 elapsed 12.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 500/1725 loss 2.4279 elapsed 12.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 600/1725 loss 2.4279 elapsed 13.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 700/1725 loss 2.4279 elapsed 13.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 800/1725 loss 2.4279 elapsed 14.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 900/1725 loss 2.4279 elapsed 15.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1000/1725 loss 2.4279 elapsed 15.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1100/1725 loss 2.4279 elapsed 16.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1200/1725 loss 2.4279 elapsed 16.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1300/1725 loss 2.4279 elapsed 17.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1400/1725 loss 2.4279 elapsed 17.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1500/1725 loss 2.4279 elapsed 18.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1600/1725 loss 2.4279 elapsed 18.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1700/1725 loss 2.4279 elapsed 19.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 1 epoch 2 ===\nval_size=24160 probs_shape=(24160, 3474) tgts_shape=(24160, 3474)\nprobs_range=[0.000001,0.917003]\ntgt_pos_rate=0.00127227 mean_pos_per_img=4.420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.00719632 mean_pred_per_img=25.000 empty_frac=0.000000 TP=48827 FP=555173 FN=57957 f1@0.2=0.137389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 val micro-f1 0.13739 @ thr 0.050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 100/1725 loss 2.4279 elapsed 21.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 200/1725 loss 2.4279 elapsed 21.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 300/1725 loss 2.4279 elapsed 22.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 400/1725 loss 2.4279 elapsed 22.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 500/1725 loss 2.4279 elapsed 23.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 600/1725 loss 2.4279 elapsed 23.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 700/1725 loss 2.4279 elapsed 24.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 800/1725 loss 2.4279 elapsed 24.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 900/1725 loss 2.4279 elapsed 25.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1000/1725 loss 2.4279 elapsed 25.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1100/1725 loss 2.4279 elapsed 26.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1200/1725 loss 2.4279 elapsed 26.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1300/1725 loss 2.4279 elapsed 27.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1400/1725 loss 2.4279 elapsed 27.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1500/1725 loss 2.4279 elapsed 28.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1600/1725 loss 2.4279 elapsed 28.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1700/1725 loss 2.4279 elapsed 29.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 1 epoch 3 ===\nval_size=24160 probs_shape=(24160, 3474) tgts_shape=(24160, 3474)\nprobs_range=[0.000001,0.871343]\ntgt_pos_rate=0.00127227 mean_pos_per_img=4.420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.00719632 mean_pred_per_img=25.000 empty_frac=0.000000 TP=48827 FP=555173 FN=57957 f1@0.2=0.137389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 val micro-f1 0.13818 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 100/1725 loss 2.4279 elapsed 31.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 200/1725 loss 2.4279 elapsed 31.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 300/1725 loss 2.4279 elapsed 32.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 400/1725 loss 2.4279 elapsed 32.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 500/1725 loss 2.4279 elapsed 33.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 600/1725 loss 2.4279 elapsed 33.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 700/1725 loss 2.4279 elapsed 34.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 800/1725 loss 2.4279 elapsed 34.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 900/1725 loss 2.4279 elapsed 35.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1000/1725 loss 2.4279 elapsed 35.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1100/1725 loss 2.4279 elapsed 36.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1200/1725 loss 2.4279 elapsed 36.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1300/1725 loss 2.4279 elapsed 37.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1400/1725 loss 2.4279 elapsed 37.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1500/1725 loss 2.4279 elapsed 38.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1600/1725 loss 2.4279 elapsed 38.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1700/1725 loss 2.4279 elapsed 39.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 1 epoch 4 ===\nval_size=24160 probs_shape=(24160, 3474) tgts_shape=(24160, 3474)\nprobs_range=[0.000001,0.964950]\ntgt_pos_rate=0.00127227 mean_pos_per_img=4.420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.00719632 mean_pred_per_img=25.000 empty_frac=0.000000 TP=48827 FP=555173 FN=57957 f1@0.2=0.137389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 val micro-f1 0.13739 @ thr 0.050\nEarly stopping at epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Fold 1 done: best_f1 0.18269 thr 0.250 ====\n==== Fold 2 start ====\n"
          ]
        }
      ]
    },
    {
      "id": "c9235397-a39e-4301-b774-61d3d63d1f9b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Precompute and lock 5-fold MSKF folds (seed=42) to train_folds.csv\n",
        "import time, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "t0 = time.time()\n",
        "train_df = pd.read_csv('train.csv')\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "attr_ids = sorted(labels_df['attribute_id'].unique().tolist())\n",
        "attr_to_idx = {a:i for i,a in enumerate(attr_ids)}\n",
        "n = len(train_df); C = len(attr_ids)\n",
        "print(f'n={n} classes={C}', flush=True)\n",
        "\n",
        "# Build sparse label matrix via rows/cols\n",
        "rows, cols = [], []\n",
        "for i, s in enumerate(train_df['attribute_ids'].fillna('').astype(str).tolist()):\n",
        "    if s:\n",
        "        for a in map(int, s.split()):\n",
        "            j = attr_to_idx.get(a, None)\n",
        "            if j is not None:\n",
        "                rows.append(i); cols.append(j)\n",
        "print(f'nonzeros={len(rows)}', flush=True)\n",
        "y_sparse = csr_matrix((np.ones(len(rows), dtype=np.uint8), (rows, cols)), shape=(n, C), dtype=np.uint8)\n",
        "\n",
        "# Dense conversion (fits in RAM ~420MB) for iterstrat; one-time cost\n",
        "t1 = time.time()\n",
        "y = y_sparse.toarray(order='C')\n",
        "print(f'dense built in {time.time()-t1:.1f}s, shape={y.shape}, dtype={y.dtype}', flush=True)\n",
        "\n",
        "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = np.full(n, -1, np.int16)\n",
        "t2 = time.time()\n",
        "for f, (_, vidx) in enumerate(mskf.split(np.zeros(n), y)):\n",
        "    folds[vidx] = f\n",
        "    print(f'fold {f}: {len(vidx)}', flush=True)\n",
        "print(f'MSKF split time: {time.time()-t2:.1f}s', flush=True)\n",
        "assert (folds >= 0).all()\n",
        "\n",
        "out = train_df.copy(); out['fold'] = folds\n",
        "out.to_csv('train_folds.csv', index=False)\n",
        "print(f'Saved train_folds.csv in {time.time()-t0:.1f}s')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n=120801 classes=3474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nonzeros=534073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dense built in 0.0s, shape=(120801, 3474), dtype=uint8\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m folds = np.full(n, -\u001b[32m1\u001b[39m, np.int16)\n\u001b[32m     33\u001b[39m t2 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvidx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmskf\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvidx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfold \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mf\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvidx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/model_selection/_split.py:416\u001b[39m, in \u001b[36m_BaseKFold.split\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_splits > n_samples:\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    410\u001b[39m         (\n\u001b[32m    411\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m greater\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    413\u001b[39m         ).format(\u001b[38;5;28mself\u001b[39m.n_splits, n_samples)\n\u001b[32m    414\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/model_selection/_split.py:147\u001b[39m, in \u001b[36mBaseCrossValidator.split\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m    145\u001b[39m X, y, groups = indexable(X, y, groups)\n\u001b[32m    146\u001b[39m indices = np.arange(_num_samples(X))\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_test_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogical_not\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m]\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/iterstrat/ml_stratifiers.py:183\u001b[39m, in \u001b[36mMultilabelStratifiedKFold._iter_test_masks\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iter_test_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, X=\u001b[38;5;28;01mNone\u001b[39;00m, y=\u001b[38;5;28;01mNone\u001b[39;00m, groups=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     test_folds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_test_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.n_splits):\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m test_folds == i\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/iterstrat/ml_stratifiers.py:178\u001b[39m, in \u001b[36mMultilabelStratifiedKFold._make_test_folds\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    174\u001b[39m     y = y[indices]\n\u001b[32m    176\u001b[39m r = np.asarray([\u001b[32m1\u001b[39m / \u001b[38;5;28mself\u001b[39m.n_splits] * \u001b[38;5;28mself\u001b[39m.n_splits)\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m test_folds = \u001b[43mIterativeStratification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m test_folds[np.argsort(indices)]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/iterstrat/ml_stratifiers.py:56\u001b[39m, in \u001b[36mIterativeStratification\u001b[39m\u001b[34m(labels, r, random_state)\u001b[39m\n\u001b[32m     51\u001b[39m labels_not_processed_mask = np.ones(n_samples, dtype=\u001b[38;5;28mbool\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m np.any(labels_not_processed_mask):\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# Find the label with the fewest (but at least one) remaining examples,\u001b[39;00m\n\u001b[32m     55\u001b[39m     \u001b[38;5;66;03m# breaking ties randomly\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     num_labels = \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabels_not_processed_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     \u001b[38;5;66;03m# Handle case where only all-zero labels are left by distributing\u001b[39;00m\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# across all folds as evenly as possible (not in original algorithm but\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# mentioned in the text). (By handling this case separately, some\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# code redundancy is introduced; however, this approach allows for\u001b[39;00m\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# decreased execution time when there are a relatively large number\u001b[39;00m\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# of all-zero labels.)\u001b[39;00m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m num_labels.sum() == \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:49\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     48\u001b[39m          initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "a34e6304-96ae-42ee-b0df-1d8f979b4e6e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, subprocess, time, shlex\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "print('=== Diagnostic single-fold run: b3@384, epochs=5, pretrained ===', flush=True)\n",
        "# Ensure fast cardinality folds exist (for speed)\n",
        "folds_path = Path('train_folds_smoke.csv')\n",
        "if not folds_path.exists():\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    attrs = train_df['attribute_ids'].fillna('').astype(str).apply(lambda s: [int(x) for x in s.split() if x!=''])\n",
        "    card = attrs.apply(len).values\n",
        "    bins = np.clip(card, 0, 8)\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    folds = np.full(len(train_df), -1, dtype=np.int16)\n",
        "    for f, (_, vidx) in enumerate(skf.split(np.zeros(len(train_df)), bins)):\n",
        "        folds[vidx] = f\n",
        "    train_df2 = train_df.copy()\n",
        "    train_df2['fold'] = folds\n",
        "    train_df2.to_csv(folds_path, index=False)\n",
        "    print('Wrote train_folds_smoke.csv')\n",
        "else:\n",
        "    print('Using existing train_folds_smoke.csv')\n",
        "\n",
        "cmd = [\n",
        "    sys.executable, '-u', 'train.py',\n",
        "    '--model', 'tf_efficientnet_b3_ns',\n",
        "    '--img-size', '384',\n",
        "    '--epochs', '5',\n",
        "    '--batch-size', '32',\n",
        "    '--val-batch-size', '64',\n",
        "    '--num-workers', '8',\n",
        "    '--folds', '0',\n",
        "    '--folds-csv', 'train_folds_smoke.csv',\n",
        "    '--out-dir', 'out_debug',\n",
        "    '--early-stop-patience', '2',\n",
        "    '--pretrained'\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd), flush=True)\n",
        "t0 = time.time()\n",
        "env = dict(os.environ)\n",
        "env['PYTHONUNBUFFERED'] = '1'\n",
        "p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, env=env)\n",
        "try:\n",
        "    for line in p.stdout:\n",
        "        print(line, end='')\n",
        "finally:\n",
        "    rc = p.wait()\n",
        "elapsed = time.time() - t0\n",
        "print(f'Exit code: {rc}, elapsed {elapsed/60:.1f} min', flush=True)\n",
        "assert rc == 0, 'Diagnostic run failed'\n",
        "print('Diagnostic run completed.')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Diagnostic single-fold run: b3@384, epochs=5, pretrained ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing train_folds_smoke.csv\nRunning: /usr/bin/python3.11 -u train.py --model tf_efficientnet_b3_ns --img-size 384 --epochs 5 --batch-size 32 --val-batch-size 64 --num-workers 8 --folds 0 --folds-csv train_folds_smoke.csv --out-dir out_debug --early-stop-patience 2 --pretrained\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected image extension: .png\n==== Fold 0 start ====\n/app/.pip-target/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b3_ns to current tf_efficientnet_b3.ns_jft_in1k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:214: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:246: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 100/3020 loss 0.0014 elapsed 0.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 200/3020 loss 0.0014 elapsed 0.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 300/3020 loss 0.0013 elapsed 1.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 400/3020 loss 0.0013 elapsed 1.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 500/3020 loss 0.0012 elapsed 1.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 600/3020 loss 0.0012 elapsed 1.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 700/3020 loss 0.0011 elapsed 2.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 800/3020 loss 0.0011 elapsed 2.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 900/3020 loss 0.0011 elapsed 2.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1000/3020 loss 0.0011 elapsed 3.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1100/3020 loss 0.0010 elapsed 3.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1200/3020 loss 0.0010 elapsed 3.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1300/3020 loss 0.0010 elapsed 3.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1400/3020 loss 0.0010 elapsed 4.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1500/3020 loss 0.0010 elapsed 4.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1600/3020 loss 0.0010 elapsed 4.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1700/3020 loss 0.0010 elapsed 5.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1800/3020 loss 0.0009 elapsed 5.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1900/3020 loss 0.0009 elapsed 5.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2000/3020 loss 0.0009 elapsed 5.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2100/3020 loss 0.0009 elapsed 6.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2200/3020 loss 0.0009 elapsed 6.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2300/3020 loss 0.0009 elapsed 6.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2400/3020 loss 0.0009 elapsed 7.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2500/3020 loss 0.0009 elapsed 7.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2600/3020 loss 0.0009 elapsed 7.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2700/3020 loss 0.0009 elapsed 7.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2800/3020 loss 0.0009 elapsed 8.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2900/3020 loss 0.0009 elapsed 8.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 3000/3020 loss 0.0009 elapsed 8.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 1 ===\nval_size=24161 probs_shape=(24161, 3474) tgts_shape=(24161, 3474)\nprobs_range=[0.000000,0.999465]\ntgt_pos_rate=0.00127242 mean_pos_per_img=4.420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.02936688 mean_pred_per_img=102.021 empty_frac=0.000000 TP=100599 FP=2364319 FN=6202 f1@0.2=0.078235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 val micro-f1 0.54314 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 100/3020 loss 0.0007 elapsed 10.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 200/3020 loss 0.0006 elapsed 11.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 300/3020 loss 0.0007 elapsed 11.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 400/3020 loss 0.0007 elapsed 11.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 500/3020 loss 0.0007 elapsed 11.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 600/3020 loss 0.0007 elapsed 12.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 700/3020 loss 0.0007 elapsed 12.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 800/3020 loss 0.0007 elapsed 12.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 900/3020 loss 0.0007 elapsed 13.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1000/3020 loss 0.0007 elapsed 13.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1100/3020 loss 0.0007 elapsed 13.5m\n"
          ]
        }
      ]
    },
    {
      "id": "31a3628f-19be-4c72-8f6d-71cb1700f327",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, time, shlex, subprocess\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "print('=== Build fast MSKF folds: top-512 labels + cardinality bins (one-hot) ===', flush=True)\n",
        "t0 = time.time()\n",
        "train_df = pd.read_csv('train.csv')\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "attr_ids = labels_df['attribute_id'].astype(int).tolist()\n",
        "n = len(train_df)\n",
        "print('n samples:', n, 'num labels:', len(attr_ids), flush=True)\n",
        "\n",
        "# Parse labels\n",
        "labs = train_df['attribute_ids'].fillna('').astype(str).str.split()\n",
        "\n",
        "# Count label frequencies\n",
        "from collections import Counter\n",
        "cnt = Counter()\n",
        "for s in labs:\n",
        "    for x in s:\n",
        "        cnt[int(x)] += 1\n",
        "topK = 512\n",
        "top_attrs = [a for a,_ in cnt.most_common(topK)]\n",
        "top_map = {a:i for i,a in enumerate(top_attrs)}\n",
        "\n",
        "# Build reduced multilabel matrix: K (top labels) + 9 bin one-hots for cardinality (0..8, 8=8+)\n",
        "K = len(top_attrs)\n",
        "B = 9\n",
        "y = np.zeros((n, K + B), dtype=np.uint8)\n",
        "for i, s in enumerate(labs):\n",
        "    if s:\n",
        "        # top-K one-hots\n",
        "        for x in s:\n",
        "            j = top_map.get(int(x))\n",
        "            if j is not None:\n",
        "                y[i, j] = 1\n",
        "        # cardinality bin one-hot\n",
        "        c = len(s)\n",
        "        b = min(c, B-1)\n",
        "        y[i, K + b] = 1\n",
        "    if (i+1) % 20000 == 0:\n",
        "        print(f'..processed {i+1}/{n}', flush=True)\n",
        "\n",
        "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "folds = np.full(n, -1, np.int16)\n",
        "for f, (_, vidx) in enumerate(mskf.split(np.zeros(n), y)):\n",
        "    folds[vidx] = f\n",
        "    print('fold', f, 'size', len(vidx), flush=True)\n",
        "assert (folds >= 0).all()\n",
        "\n",
        "out_path = Path('train_folds_top512.csv')\n",
        "out = train_df.copy(); out['fold'] = folds\n",
        "out.to_csv(out_path, index=False)\n",
        "print('Saved', str(out_path), 'in', f'{time.time()-t0:.1f}s', flush=True)\n",
        "\n",
        "print('=== Launch 5-fold b3@384 EMA+TTA with new folds ===', flush=True)\n",
        "cmd = [\n",
        "    sys.executable, '-u', 'train.py',\n",
        "    '--model', 'tf_efficientnet_b3_ns',\n",
        "    '--img-size', '384',\n",
        "    '--epochs', '10',\n",
        "    '--batch-size', '56',\n",
        "    '--val-batch-size', '96',\n",
        "    '--num-workers', '10',\n",
        "    '--lr', '2e-4',\n",
        "    '--use-ema',\n",
        "    '--tta',\n",
        "    '--early-stop-patience', '3',\n",
        "    '--folds', '0,1,2,3,4',\n",
        "    '--folds-csv', 'train_folds_top512.csv',\n",
        "    '--out-dir', 'out_b3_384_top512',\n",
        "    '--pretrained'\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd), flush=True)\n",
        "t1 = time.time()\n",
        "env = dict(os.environ); env['PYTHONUNBUFFERED'] = '1'\n",
        "p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, env=env)\n",
        "try:\n",
        "    for line in p.stdout:\n",
        "        print(line, end='')\n",
        "finally:\n",
        "    rc = p.wait()\n",
        "print(f'Exit code: {rc}, elapsed {(time.time()-t1)/3600:.2f} h', flush=True)\n",
        "assert rc == 0, 'Production run failed'\n",
        "print('Production run completed.')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Build fast MSKF folds: top-512 labels + cardinality bins (one-hot) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n samples: 120801 num labels: 3474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..processed 20000/120801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..processed 40000/120801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..processed 60000/120801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..processed 80000/120801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..processed 100000/120801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..processed 120000/120801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 size 24189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 size 24136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 size 24188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 3 size 24167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 4 size 24121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved train_folds_top512.csv in 15.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Launch 5-fold b3@384 EMA+TTA with new folds ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running: /usr/bin/python3.11 -u train.py --model tf_efficientnet_b3_ns --img-size 384 --epochs 10 --batch-size 56 --val-batch-size 96 --num-workers 10 --lr 2e-4 --use-ema --tta --early-stop-patience 3 --folds 0,1,2,3,4 --folds-csv train_folds_top512.csv --out-dir out_b3_384_top512 --pretrained\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected image extension: .png\n==== Fold 0 start ====\n/app/.pip-target/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b3_ns to current tf_efficientnet_b3.ns_jft_in1k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:214: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:246: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 100/1725 loss 0.0014 elapsed 0.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 200/1725 loss 0.0014 elapsed 1.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 300/1725 loss 0.0013 elapsed 1.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 400/1725 loss 0.0012 elapsed 2.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 500/1725 loss 0.0012 elapsed 2.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 600/1725 loss 0.0011 elapsed 3.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 700/1725 loss 0.0011 elapsed 4.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 800/1725 loss 0.0011 elapsed 4.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 900/1725 loss 0.0010 elapsed 5.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1000/1725 loss 0.0010 elapsed 5.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1100/1725 loss 0.0010 elapsed 6.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1200/1725 loss 0.0010 elapsed 6.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1300/1725 loss 0.0010 elapsed 7.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1400/1725 loss 0.0010 elapsed 7.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1500/1725 loss 0.0009 elapsed 8.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1600/1725 loss 0.0009 elapsed 8.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1700/1725 loss nan elapsed 9.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 1 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.012800,0.977794]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.03767297 mean_pred_per_img=130.876 empty_frac=0.000000 TP=98246 FP=3067511 FN=8629 f1@0.2=0.060041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 val micro-f1 0.49668 @ thr 0.460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 100/1725 loss 0.0007 elapsed 11.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 200/1725 loss 0.0007 elapsed 12.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 300/1725 loss 0.0007 elapsed 12.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 400/1725 loss 0.0007 elapsed 13.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 500/1725 loss 0.0007 elapsed 13.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 600/1725 loss 0.0007 elapsed 14.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 700/1725 loss 0.0007 elapsed 14.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 800/1725 loss 0.0007 elapsed 15.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 900/1725 loss 0.0007 elapsed 15.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1000/1725 loss 0.0007 elapsed 16.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1100/1725 loss 0.0007 elapsed 16.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1200/1725 loss 0.0007 elapsed 17.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1300/1725 loss 0.0007 elapsed 17.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1400/1725 loss 0.0007 elapsed 18.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1500/1725 loss 0.0007 elapsed 18.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1600/1725 loss 0.0007 elapsed 19.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1700/1725 loss 0.0007 elapsed 19.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 2 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000094,0.998641]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.02908528 mean_pred_per_img=101.042 empty_frac=0.000000 TP=101958 FP=2342153 FN=4917 f1@0.2=0.079936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 val micro-f1 0.58421 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 100/1725 loss 0.0006 elapsed 21.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 200/1725 loss 0.0006 elapsed 22.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 300/1725 loss 0.0006 elapsed 22.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 400/1725 loss 0.0006 elapsed 23.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 500/1725 loss 0.0006 elapsed 23.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 600/1725 loss 0.0006 elapsed 24.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 700/1725 loss 0.0006 elapsed 24.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 800/1725 loss 0.0006 elapsed 25.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 900/1725 loss 0.0006 elapsed 25.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1000/1725 loss 0.0006 elapsed 26.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1100/1725 loss 0.0006 elapsed 26.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1200/1725 loss 0.0006 elapsed 27.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1300/1725 loss 0.0006 elapsed 28.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1400/1725 loss 0.0006 elapsed 28.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1500/1725 loss 0.0006 elapsed 29.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1600/1725 loss 0.0006 elapsed 29.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1700/1725 loss 0.0006 elapsed 30.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 3 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000001,0.999317]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.02313137 mean_pred_per_img=80.358 empty_frac=0.000000 TP=101747 FP=1842042 FN=5128 f1@0.2=0.099233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 val micro-f1 0.60110 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 100/1725 loss 0.0005 elapsed 31.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 200/1725 loss 0.0005 elapsed 32.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 300/1725 loss 0.0005 elapsed 33.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 400/1725 loss 0.0005 elapsed 33.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 500/1725 loss 0.0005 elapsed 34.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 600/1725 loss 0.0005 elapsed 34.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 700/1725 loss 0.0005 elapsed 35.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 800/1725 loss 0.0005 elapsed 35.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 900/1725 loss 0.0005 elapsed 36.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1000/1725 loss 0.0005 elapsed 36.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1100/1725 loss 0.0005 elapsed 37.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1200/1725 loss 0.0005 elapsed 37.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1300/1725 loss 0.0005 elapsed 38.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1400/1725 loss 0.0005 elapsed 38.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1500/1725 loss 0.0005 elapsed 39.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1600/1725 loss 0.0005 elapsed 39.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1700/1725 loss 0.0005 elapsed 40.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 4 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,0.999590]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01911626 mean_pred_per_img=66.410 empty_frac=0.000000 TP=100931 FP=1505458 FN=5944 f1@0.2=0.117823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 val micro-f1 0.60824 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 100/1725 loss 0.0004 elapsed 42.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 200/1725 loss 0.0004 elapsed 42.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 300/1725 loss 0.0004 elapsed 43.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 400/1725 loss 0.0004 elapsed 43.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 500/1725 loss 0.0004 elapsed 44.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 600/1725 loss 0.0004 elapsed 44.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 700/1725 loss 0.0004 elapsed 45.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 800/1725 loss 0.0004 elapsed 45.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 900/1725 loss 0.0004 elapsed 46.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1000/1725 loss nan elapsed 46.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1100/1725 loss nan elapsed 47.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1200/1725 loss nan elapsed 47.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1300/1725 loss nan elapsed 48.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1400/1725 loss nan elapsed 48.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1500/1725 loss nan elapsed 49.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1600/1725 loss nan elapsed 49.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1700/1725 loss nan elapsed 50.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 5 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,0.999794]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01620693 mean_pred_per_img=56.303 empty_frac=0.000000 TP=99852 FP=1262058 FN=7023 f1@0.2=0.135965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 val micro-f1 0.60904 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 100/1725 loss 0.0004 elapsed 52.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 200/1725 loss 0.0004 elapsed 52.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 300/1725 loss 0.0004 elapsed 53.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 400/1725 loss nan elapsed 53.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 500/1725 loss nan elapsed 54.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 600/1725 loss nan elapsed 54.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 700/1725 loss nan elapsed 55.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 800/1725 loss nan elapsed 55.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 900/1725 loss nan elapsed 56.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1000/1725 loss nan elapsed 56.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1100/1725 loss nan elapsed 57.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1200/1725 loss nan elapsed 58.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1300/1725 loss nan elapsed 58.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1400/1725 loss nan elapsed 59.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1500/1725 loss nan elapsed 59.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1600/1725 loss nan elapsed 60.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1700/1725 loss nan elapsed 60.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 6 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,0.999910]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01401615 mean_pred_per_img=48.692 empty_frac=0.000000 TP=98605 FP=1079208 FN=8270 f1@0.2=0.153508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 val micro-f1 0.60923 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 100/1725 loss 0.0003 elapsed 62.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 200/1725 loss 0.0003 elapsed 63.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 300/1725 loss 0.0003 elapsed 63.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 400/1725 loss 0.0003 elapsed 64.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 500/1725 loss 0.0003 elapsed 64.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 600/1725 loss 0.0003 elapsed 65.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 700/1725 loss 0.0003 elapsed 65.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 800/1725 loss nan elapsed 66.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 900/1725 loss nan elapsed 66.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1000/1725 loss nan elapsed 67.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1100/1725 loss nan elapsed 67.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1200/1725 loss nan elapsed 68.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1300/1725 loss nan elapsed 68.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1400/1725 loss nan elapsed 69.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1500/1725 loss nan elapsed 69.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1600/1725 loss nan elapsed 70.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1700/1725 loss nan elapsed 70.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 7 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,0.999974]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01242836 mean_pred_per_img=43.176 empty_frac=0.000000 TP=97470 FP=946917 FN=9405 f1@0.2=0.169327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 val micro-f1 0.60768 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 100/1725 loss 0.0003 elapsed 72.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 200/1725 loss 0.0003 elapsed 73.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 300/1725 loss 0.0003 elapsed 73.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 400/1725 loss 0.0003 elapsed 74.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 500/1725 loss 0.0003 elapsed 74.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 600/1725 loss nan elapsed 75.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 700/1725 loss nan elapsed 75.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 800/1725 loss nan elapsed 76.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 900/1725 loss nan elapsed 76.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1000/1725 loss nan elapsed 77.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1100/1725 loss nan elapsed 77.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1200/1725 loss nan elapsed 78.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1300/1725 loss nan elapsed 78.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1400/1725 loss nan elapsed 79.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1500/1725 loss nan elapsed 79.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1600/1725 loss nan elapsed 80.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1700/1725 loss nan elapsed 80.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 8 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,0.999985]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01206099 mean_pred_per_img=41.900 empty_frac=0.000000 TP=96866 FP=916650 FN=10009 f1@0.2=0.172915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 val micro-f1 0.60473 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 100/1725 loss 0.0003 elapsed 82.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 200/1725 loss 0.0003 elapsed 83.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 300/1725 loss 0.0003 elapsed 83.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 400/1725 loss nan elapsed 84.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 500/1725 loss nan elapsed 84.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 600/1725 loss nan elapsed 85.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 700/1725 loss nan elapsed 85.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 800/1725 loss nan elapsed 86.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 900/1725 loss nan elapsed 86.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1000/1725 loss nan elapsed 87.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1100/1725 loss nan elapsed 87.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1200/1725 loss nan elapsed 88.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1300/1725 loss nan elapsed 88.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1400/1725 loss nan elapsed 89.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1500/1725 loss nan elapsed 89.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1600/1725 loss nan elapsed 90.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1700/1725 loss nan elapsed 90.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 9 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,0.999991]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01194632 mean_pred_per_img=41.502 empty_frac=0.000000 TP=96597 FP=907283 FN=10278 f1@0.2=0.173930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 val micro-f1 0.60314 @ thr 0.500\nEarly stopping at epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Fold 0 done: best_f1 0.60923 thr 0.500 ====\n==== Fold 1 start ====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 100/1726 loss 0.0014 elapsed 0.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 200/1726 loss 0.0014 elapsed 1.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 300/1726 loss 0.0013 elapsed 1.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 400/1726 loss 0.0012 elapsed 2.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 500/1726 loss 0.0012 elapsed 2.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 600/1726 loss 0.0011 elapsed 3.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 700/1726 loss 0.0011 elapsed 3.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 800/1726 loss 0.0011 elapsed 4.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 900/1726 loss 0.0010 elapsed 4.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1000/1726 loss 0.0010 elapsed 5.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1100/1726 loss 0.0010 elapsed 5.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1200/1726 loss 0.0010 elapsed 6.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1300/1726 loss 0.0010 elapsed 6.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1400/1726 loss 0.0010 elapsed 7.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1500/1726 loss 0.0009 elapsed 7.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1600/1726 loss 0.0009 elapsed 8.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1700/1726 loss 0.0009 elapsed 8.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 1 epoch 1 ===\nval_size=24136 probs_shape=(24136, 3474) tgts_shape=(24136, 3474)\nprobs_range=[0.013146,0.974764]\ntgt_pos_rate=0.00127339 mean_pos_per_img=4.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.03784180 mean_pred_per_img=131.462 empty_frac=0.000000 TP=98410 FP=3074567 FN=8362 f1@0.2=0.060011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 val micro-f1 0.49859 @ thr 0.450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 100/1726 loss 0.0007 elapsed 10.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 200/1726 loss 0.0007 elapsed 11.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 300/1726 loss 0.0007 elapsed 11.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 400/1726 loss 0.0007 elapsed 12.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 500/1726 loss 0.0007 elapsed 12.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 600/1726 loss 0.0007 elapsed 13.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 700/1726 loss 0.0007 elapsed 13.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 800/1726 loss 0.0007 elapsed 14.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 900/1726 loss 0.0007 elapsed 15.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1000/1726 loss 0.0007 elapsed 15.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1100/1726 loss 0.0007 elapsed 16.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1200/1726 loss 0.0007 elapsed 16.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1300/1726 loss 0.0007 elapsed 17.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1400/1726 loss 0.0007 elapsed 17.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1500/1726 loss 0.0007 elapsed 18.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1600/1726 loss 0.0007 elapsed 18.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1700/1726 loss 0.0007 elapsed 19.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 1 epoch 2 ===\nval_size=24136 probs_shape=(24136, 3474) tgts_shape=(24136, 3474)\nprobs_range=[0.000502,0.997274]\ntgt_pos_rate=0.00127339 mean_pos_per_img=4.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.02895624 mean_pred_per_img=100.594 empty_frac=0.000000 TP=101933 FP=2326003 FN=4839 f1@0.2=0.080430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 val micro-f1 0.58535 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 100/1726 loss 0.0006 elapsed 21.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 200/1726 loss 0.0006 elapsed 21.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 300/1726 loss 0.0006 elapsed 22.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 400/1726 loss 0.0006 elapsed 22.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 500/1726 loss 0.0006 elapsed 23.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 600/1726 loss 0.0006 elapsed 23.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 700/1726 loss 0.0006 elapsed 24.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 800/1726 loss 0.0006 elapsed 24.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 900/1726 loss 0.0006 elapsed 25.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1000/1726 loss 0.0006 elapsed 25.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1100/1726 loss 0.0006 elapsed 26.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1200/1726 loss 0.0006 elapsed 26.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1300/1726 loss 0.0006 elapsed 27.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1400/1726 loss 0.0006 elapsed 27.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1500/1726 loss 0.0006 elapsed 28.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1600/1726 loss 0.0006 elapsed 28.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 1700/1726 loss 0.0006 elapsed 29.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 1 epoch 3 ===\nval_size=24136 probs_shape=(24136, 3474) tgts_shape=(24136, 3474)\nprobs_range=[0.000009,0.998926]\ntgt_pos_rate=0.00127339 mean_pos_per_img=4.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.02297969 mean_pred_per_img=79.831 empty_frac=0.000000 TP=101661 FP=1825151 FN=5111 f1@0.2=0.099982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 val micro-f1 0.60128 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 100/1726 loss 0.0005 elapsed 31.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 200/1726 loss 0.0005 elapsed 31.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 300/1726 loss 0.0005 elapsed 32.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 400/1726 loss 0.0005 elapsed 32.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 500/1726 loss 0.0005 elapsed 33.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 600/1726 loss 0.0005 elapsed 33.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 700/1726 loss 0.0005 elapsed 34.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 800/1726 loss 0.0005 elapsed 34.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 900/1726 loss 0.0005 elapsed 35.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1000/1726 loss 0.0005 elapsed 35.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1100/1726 loss 0.0005 elapsed 36.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1200/1726 loss 0.0005 elapsed 36.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1300/1726 loss 0.0005 elapsed 37.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1400/1726 loss 0.0005 elapsed 38.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1500/1726 loss 0.0005 elapsed 38.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1600/1726 loss 0.0005 elapsed 39.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 iter 1700/1726 loss 0.0005 elapsed 39.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 1 epoch 4 ===\nval_size=24136 probs_shape=(24136, 3474) tgts_shape=(24136, 3474)\nprobs_range=[0.000002,0.999317]\ntgt_pos_rate=0.00127339 mean_pos_per_img=4.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01898507 mean_pred_per_img=65.954 empty_frac=0.000000 TP=100748 FP=1491121 FN=6024 f1@0.2=0.118622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 4 val micro-f1 0.60766 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 100/1726 loss 0.0004 elapsed 41.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 200/1726 loss 0.0004 elapsed 42.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 300/1726 loss 0.0004 elapsed 42.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 400/1726 loss 0.0004 elapsed 43.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 500/1726 loss 0.0004 elapsed 43.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 600/1726 loss 0.0004 elapsed 44.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 700/1726 loss 0.0004 elapsed 44.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 800/1726 loss 0.0004 elapsed 45.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 900/1726 loss 0.0004 elapsed 45.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 1000/1726 loss nan elapsed 46.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 1100/1726 loss nan elapsed 46.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 1200/1726 loss nan elapsed 47.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 1300/1726 loss nan elapsed 47.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 1400/1726 loss nan elapsed 48.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 1500/1726 loss nan elapsed 48.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 1600/1726 loss nan elapsed 49.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 iter 1700/1726 loss nan elapsed 49.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 1 epoch 5 ===\nval_size=24136 probs_shape=(24136, 3474) tgts_shape=(24136, 3474)\nprobs_range=[0.000000,0.999402]\ntgt_pos_rate=0.00127339 mean_pos_per_img=4.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01601625 mean_pred_per_img=55.640 empty_frac=0.000000 TP=99639 FP=1243299 FN=7133 f1@0.2=0.137461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 5 val micro-f1 0.60960 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 100/1726 loss 0.0004 elapsed 51.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 200/1726 loss 0.0004 elapsed 52.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 300/1726 loss 0.0004 elapsed 52.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 400/1726 loss 0.0004 elapsed 53.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 500/1726 loss 0.0004 elapsed 53.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 600/1726 loss 0.0004 elapsed 54.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 700/1726 loss 0.0004 elapsed 54.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 800/1726 loss 0.0004 elapsed 55.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 900/1726 loss 0.0004 elapsed 55.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 1000/1726 loss 0.0004 elapsed 56.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 1100/1726 loss 0.0004 elapsed 56.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 1200/1726 loss 0.0004 elapsed 57.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 1300/1726 loss 0.0004 elapsed 57.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 1400/1726 loss 0.0004 elapsed 58.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 1500/1726 loss 0.0004 elapsed 58.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 1600/1726 loss 0.0004 elapsed 59.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 iter 1700/1726 loss nan elapsed 59.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 1 epoch 6 ===\nval_size=24136 probs_shape=(24136, 3474) tgts_shape=(24136, 3474)\nprobs_range=[0.000001,0.999750]\ntgt_pos_rate=0.00127339 mean_pos_per_img=4.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01383818 mean_pred_per_img=48.074 empty_frac=0.000000 TP=98359 FP=1061951 FN=8413 f1@0.2=0.155253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 6 val micro-f1 0.60846 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 100/1726 loss 0.0003 elapsed 61.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 200/1726 loss 0.0003 elapsed 62.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 300/1726 loss 0.0003 elapsed 62.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 400/1726 loss 0.0003 elapsed 63.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 500/1726 loss 0.0003 elapsed 63.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 600/1726 loss 0.0003 elapsed 64.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 700/1726 loss 0.0003 elapsed 64.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 800/1726 loss 0.0003 elapsed 65.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 900/1726 loss 0.0003 elapsed 66.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 1000/1726 loss 0.0003 elapsed 66.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 1100/1726 loss 0.0003 elapsed 67.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 1200/1726 loss 0.0003 elapsed 67.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 1300/1726 loss 0.0003 elapsed 68.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 1400/1726 loss 0.0003 elapsed 68.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 1500/1726 loss 0.0003 elapsed 69.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 1600/1726 loss 0.0003 elapsed 69.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 iter 1700/1726 loss 0.0003 elapsed 70.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 1 epoch 7 ===\nval_size=24136 probs_shape=(24136, 3474) tgts_shape=(24136, 3474)\nprobs_range=[0.000000,0.999760]\ntgt_pos_rate=0.00127339 mean_pos_per_img=4.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01225629 mean_pred_per_img=42.578 empty_frac=0.000000 TP=97203 FP=930468 FN=9569 f1@0.2=0.171367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 7 val micro-f1 0.60656 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 100/1726 loss 0.0003 elapsed 72.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 200/1726 loss 0.0003 elapsed 72.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 300/1726 loss 0.0003 elapsed 73.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 400/1726 loss 0.0003 elapsed 73.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 500/1726 loss 0.0003 elapsed 74.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 600/1726 loss 0.0003 elapsed 74.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 700/1726 loss 0.0003 elapsed 75.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 800/1726 loss 0.0003 elapsed 75.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 900/1726 loss 0.0003 elapsed 76.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 1000/1726 loss 0.0003 elapsed 76.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 1100/1726 loss nan elapsed 77.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 1200/1726 loss nan elapsed 77.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 1300/1726 loss nan elapsed 78.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 1400/1726 loss nan elapsed 78.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 1500/1726 loss nan elapsed 79.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 1600/1726 loss nan elapsed 79.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 iter 1700/1726 loss nan elapsed 80.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 1 epoch 8 ===\nval_size=24136 probs_shape=(24136, 3474) tgts_shape=(24136, 3474)\nprobs_range=[0.000000,0.999833]\ntgt_pos_rate=0.00127339 mean_pos_per_img=4.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01121518 mean_pred_per_img=38.962 empty_frac=0.000000 TP=96208 FP=844168 FN=10564 f1@0.2=0.183752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 8 val micro-f1 0.60493 @ thr 0.500\nEarly stopping at epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Fold 1 done: best_f1 0.60960 thr 0.500 ====\n==== Fold 2 start ====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 100/1725 loss 0.0014 elapsed 0.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 200/1725 loss 0.0013 elapsed 1.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 300/1725 loss 0.0013 elapsed 1.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 400/1725 loss 0.0012 elapsed 2.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 500/1725 loss 0.0012 elapsed 2.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 600/1725 loss 0.0011 elapsed 3.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 700/1725 loss 0.0011 elapsed 3.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 800/1725 loss 0.0011 elapsed 4.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 900/1725 loss 0.0010 elapsed 4.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 1000/1725 loss 0.0010 elapsed 5.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 1100/1725 loss 0.0010 elapsed 5.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 1200/1725 loss 0.0010 elapsed 6.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 1300/1725 loss 0.0010 elapsed 6.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 1400/1725 loss 0.0010 elapsed 7.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 1500/1725 loss 0.0009 elapsed 7.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 1600/1725 loss 0.0009 elapsed 8.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 iter 1700/1725 loss 0.0009 elapsed 8.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 2 epoch 1 ===\nval_size=24188 probs_shape=(24188, 3474) tgts_shape=(24188, 3474)\nprobs_range=[0.011247,0.979846]\ntgt_pos_rate=0.00127107 mean_pos_per_img=4.416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.03796108 mean_pred_per_img=131.877 empty_frac=0.000000 TP=98509 FP=3091327 FN=8298 f1@0.2=0.059763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 1 val micro-f1 0.49709 @ thr 0.450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 100/1725 loss 0.0007 elapsed 11.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 200/1725 loss 0.0007 elapsed 11.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 300/1725 loss 0.0007 elapsed 12.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 400/1725 loss 0.0007 elapsed 12.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 500/1725 loss 0.0007 elapsed 13.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 600/1725 loss 0.0007 elapsed 13.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 700/1725 loss 0.0007 elapsed 14.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 800/1725 loss 0.0007 elapsed 14.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 900/1725 loss 0.0007 elapsed 15.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 1000/1725 loss 0.0007 elapsed 15.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 1100/1725 loss 0.0007 elapsed 16.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 1200/1725 loss 0.0007 elapsed 16.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 1300/1725 loss 0.0007 elapsed 17.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 1400/1725 loss 0.0007 elapsed 17.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 1500/1725 loss nan elapsed 18.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 1600/1725 loss nan elapsed 18.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 iter 1700/1725 loss nan elapsed 19.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 2 epoch 2 ===\nval_size=24188 probs_shape=(24188, 3474) tgts_shape=(24188, 3474)\nprobs_range=[0.000130,0.999524]\ntgt_pos_rate=0.00127107 mean_pos_per_img=4.416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.02886725 mean_pred_per_img=100.285 empty_frac=0.000000 TP=102021 FP=2323668 FN=4786 f1@0.2=0.080570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 2 val micro-f1 0.58535 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 100/1725 loss 0.0006 elapsed 21.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 200/1725 loss 0.0006 elapsed 21.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 300/1725 loss 0.0006 elapsed 22.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 400/1725 loss 0.0006 elapsed 22.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 500/1725 loss 0.0006 elapsed 23.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 600/1725 loss 0.0006 elapsed 23.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 700/1725 loss 0.0006 elapsed 24.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 800/1725 loss 0.0006 elapsed 24.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 900/1725 loss 0.0006 elapsed 25.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 1000/1725 loss nan elapsed 25.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 1100/1725 loss nan elapsed 26.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 1200/1725 loss nan elapsed 26.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 1300/1725 loss nan elapsed 27.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 1400/1725 loss nan elapsed 27.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 1500/1725 loss nan elapsed 28.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 1600/1725 loss nan elapsed 28.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 iter 1700/1725 loss nan elapsed 29.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 2 epoch 3 ===\nval_size=24188 probs_shape=(24188, 3474) tgts_shape=(24188, 3474)\nprobs_range=[0.000006,0.999459]\ntgt_pos_rate=0.00127107 mean_pos_per_img=4.416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.02314043 mean_pred_per_img=80.390 empty_frac=0.000000 TP=101736 FP=1842734 FN=5071 f1@0.2=0.099193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 3 val micro-f1 0.60188 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 4 iter 100/1725 loss 0.0005 elapsed 31.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 4 iter 200/1725 loss 0.0005 elapsed 31.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 4 iter 300/1725 loss 0.0005 elapsed 32.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 4 iter 400/1725 loss 0.0005 elapsed 32.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 4 iter 500/1725 loss 0.0005 elapsed 33.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 4 iter 600/1725 loss 0.0005 elapsed 33.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 4 iter 700/1725 loss 0.0005 elapsed 34.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 4 iter 800/1725 loss 0.0005 elapsed 34.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 4 iter 900/1725 loss 0.0005 elapsed 35.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 4 iter 1000/1725 loss 0.0005 elapsed 36.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 4 iter 1100/1725 loss 0.0005 elapsed 36.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 2 epoch 4 iter 1200/1725 loss 0.0005 elapsed 37.0m\n"
          ]
        }
      ]
    },
    {
      "id": "e44c4ba1-c353-432e-bf1c-346f3310e741",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Queue next production run with stabilized train.py (FP32 ASL + ColorJitter) at 448\n",
        "import os, sys, time, shlex, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== Launch: b3@448 5-fold, EMA+TTA, using train_folds_top512.csv ===', flush=True)\n",
        "assert Path('train_folds_top512.csv').exists(), 'Missing train_folds_top512.csv'\n",
        "cmd = [\n",
        "    sys.executable, '-u', 'train.py',\n",
        "    '--model', 'tf_efficientnet_b3_ns',\n",
        "    '--img-size', '448',\n",
        "    '--epochs', '10',\n",
        "    '--batch-size', '48',\n",
        "    '--val-batch-size', '96',\n",
        "    '--num-workers', '10',\n",
        "    '--lr', '2e-4',\n",
        "    '--use-ema',\n",
        "    '--tta',\n",
        "    '--early-stop-patience', '3',\n",
        "    '--folds', '0,1,2,3,4',\n",
        "    '--folds-csv', 'train_folds_top512.csv',\n",
        "    '--out-dir', 'out_b3_448_top512',\n",
        "    '--pretrained'\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd), flush=True)\n",
        "t0 = time.time()\n",
        "env = dict(os.environ); env['PYTHONUNBUFFERED'] = '1'\n",
        "p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, env=env)\n",
        "try:\n",
        "    for line in p.stdout:\n",
        "        print(line, end='')\n",
        "finally:\n",
        "    rc = p.wait()\n",
        "print(f'Exit code: {rc}, elapsed {(time.time()-t0)/3600:.2f} h', flush=True)\n",
        "assert rc == 0, 'b3@448 production run failed'\n",
        "print('b3@448 production run completed.')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Launch: b3@448 5-fold, EMA+TTA, using train_folds_top512.csv ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running: /usr/bin/python3.11 -u train.py --model tf_efficientnet_b3_ns --img-size 448 --epochs 10 --batch-size 48 --val-batch-size 96 --num-workers 10 --lr 2e-4 --use-ema --tta --early-stop-patience 3 --folds 0,1,2,3,4 --folds-csv train_folds_top512.csv --out-dir out_b3_448_top512 --pretrained\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected image extension: .png\n==== Fold 0 start ====\n/app/.pip-target/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b3_ns to current tf_efficientnet_b3.ns_jft_in1k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:219: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:248: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 100/2012 loss 0.0014 elapsed 1.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 200/2012 loss 0.0014 elapsed 1.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 300/2012 loss 0.0013 elapsed 2.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 400/2012 loss 0.0013 elapsed 2.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 500/2012 loss 0.0012 elapsed 3.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 600/2012 loss 0.0012 elapsed 4.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 700/2012 loss 0.0011 elapsed 4.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 800/2012 loss 0.0011 elapsed 5.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 900/2012 loss 0.0011 elapsed 5.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1000/2012 loss 0.0010 elapsed 6.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1100/2012 loss 0.0010 elapsed 7.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1200/2012 loss 0.0010 elapsed 7.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1300/2012 loss 0.0010 elapsed 8.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1400/2012 loss 0.0010 elapsed 8.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1500/2012 loss 0.0010 elapsed 9.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1600/2012 loss 0.0009 elapsed 10.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1700/2012 loss 0.0009 elapsed 10.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1800/2012 loss 0.0009 elapsed 11.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1900/2012 loss 0.0009 elapsed 11.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2000/2012 loss 0.0009 elapsed 12.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 1 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.007554,0.988183]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.03720266 mean_pred_per_img=129.242 empty_frac=0.000000 TP=99421 FP=3026815 FN=7454 f1@0.2=0.061502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 val micro-f1 0.51705 @ thr 0.470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 100/2012 loss 0.0007 elapsed 15.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 200/2012 loss 0.0007 elapsed 16.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 300/2012 loss 0.0007 elapsed 16.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 400/2012 loss 0.0007 elapsed 17.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 500/2012 loss 0.0007 elapsed 17.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 600/2012 loss 0.0007 elapsed 18.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 700/2012 loss 0.0007 elapsed 19.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 800/2012 loss 0.0007 elapsed 19.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 900/2012 loss 0.0007 elapsed 20.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1000/2012 loss 0.0007 elapsed 20.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1100/2012 loss 0.0007 elapsed 21.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1200/2012 loss 0.0007 elapsed 22.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1300/2012 loss 0.0007 elapsed 22.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1400/2012 loss 0.0007 elapsed 23.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1500/2012 loss 0.0007 elapsed 23.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1600/2012 loss 0.0007 elapsed 24.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1700/2012 loss 0.0007 elapsed 25.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1800/2012 loss 0.0007 elapsed 25.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1900/2012 loss 0.0007 elapsed 26.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 2000/2012 loss 0.0007 elapsed 26.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 2 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000062,0.999027]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.02782863 mean_pred_per_img=96.677 empty_frac=0.000000 TP=102001 FP=2236511 FN=4874 f1@0.2=0.083423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 val micro-f1 0.58784 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 100/2012 loss 0.0006 elapsed 29.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 200/2012 loss 0.0006 elapsed 29.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 300/2012 loss 0.0006 elapsed 30.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 400/2012 loss 0.0006 elapsed 30.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 500/2012 loss 0.0006 elapsed 31.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 600/2012 loss 0.0006 elapsed 32.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 700/2012 loss 0.0006 elapsed 32.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 800/2012 loss 0.0006 elapsed 33.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 900/2012 loss 0.0006 elapsed 33.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1000/2012 loss 0.0006 elapsed 34.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1100/2012 loss 0.0006 elapsed 35.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1200/2012 loss 0.0006 elapsed 35.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1300/2012 loss 0.0006 elapsed 36.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1400/2012 loss 0.0006 elapsed 36.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1500/2012 loss 0.0006 elapsed 37.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1600/2012 loss 0.0006 elapsed 38.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1700/2012 loss 0.0006 elapsed 38.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1800/2012 loss 0.0006 elapsed 39.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1900/2012 loss 0.0006 elapsed 39.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 2000/2012 loss 0.0006 elapsed 40.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 3 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000003,0.999498]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.02209133 mean_pred_per_img=76.745 empty_frac=0.000000 TP=101618 FP=1754774 FN=5257 f1@0.2=0.103519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 val micro-f1 0.60401 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 100/2012 loss 0.0005 elapsed 42.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 200/2012 loss 0.0005 elapsed 43.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 300/2012 loss 0.0005 elapsed 44.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 400/2012 loss 0.0005 elapsed 44.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 500/2012 loss 0.0005 elapsed 45.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 600/2012 loss 0.0005 elapsed 45.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 700/2012 loss 0.0005 elapsed 46.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 800/2012 loss 0.0005 elapsed 47.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 900/2012 loss 0.0005 elapsed 47.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1000/2012 loss 0.0005 elapsed 48.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1100/2012 loss 0.0005 elapsed 48.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1200/2012 loss 0.0005 elapsed 49.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1300/2012 loss 0.0005 elapsed 50.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1400/2012 loss 0.0005 elapsed 50.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1500/2012 loss 0.0005 elapsed 51.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1600/2012 loss 0.0005 elapsed 51.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1700/2012 loss 0.0005 elapsed 52.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1800/2012 loss 0.0005 elapsed 53.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1900/2012 loss 0.0005 elapsed 53.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 2000/2012 loss 0.0005 elapsed 54.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 4 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,0.999646]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01831340 mean_pred_per_img=63.621 empty_frac=0.000000 TP=100738 FP=1438184 FN=6137 f1@0.2=0.122418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 val micro-f1 0.61032 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 100/2012 loss 0.0004 elapsed 56.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 200/2012 loss 0.0004 elapsed 57.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 300/2012 loss 0.0004 elapsed 57.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 400/2012 loss 0.0004 elapsed 58.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 500/2012 loss 0.0004 elapsed 58.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 600/2012 loss 0.0004 elapsed 59.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 700/2012 loss 0.0004 elapsed 60.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 800/2012 loss 0.0004 elapsed 60.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 900/2012 loss 0.0004 elapsed 61.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1000/2012 loss 0.0004 elapsed 61.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1100/2012 loss 0.0004 elapsed 62.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1200/2012 loss 0.0004 elapsed 63.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1300/2012 loss 0.0004 elapsed 63.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1400/2012 loss 0.0004 elapsed 64.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1500/2012 loss 0.0004 elapsed 64.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1600/2012 loss 0.0004 elapsed 65.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1700/2012 loss 0.0004 elapsed 66.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1800/2012 loss 0.0004 elapsed 66.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1900/2012 loss 0.0004 elapsed 67.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 2000/2012 loss 0.0004 elapsed 67.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 5 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,0.999792]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01557610 mean_pred_per_img=54.111 empty_frac=0.000000 TP=99671 FP=1209229 FN=7204 f1@0.2=0.140801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 val micro-f1 0.61259 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 100/2012 loss 0.0004 elapsed 70.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 200/2012 loss 0.0004 elapsed 70.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 300/2012 loss 0.0004 elapsed 71.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 400/2012 loss 0.0004 elapsed 72.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 500/2012 loss 0.0004 elapsed 72.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 600/2012 loss 0.0004 elapsed 73.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 700/2012 loss 0.0004 elapsed 73.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 800/2012 loss 0.0004 elapsed 74.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 900/2012 loss 0.0004 elapsed 75.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1000/2012 loss 0.0004 elapsed 75.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1100/2012 loss 0.0004 elapsed 76.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1200/2012 loss 0.0004 elapsed 76.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1300/2012 loss 0.0004 elapsed 77.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1400/2012 loss 0.0004 elapsed 78.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1500/2012 loss 0.0004 elapsed 78.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1600/2012 loss 0.0004 elapsed 79.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1700/2012 loss 0.0004 elapsed 79.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1800/2012 loss 0.0004 elapsed 80.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1900/2012 loss 0.0004 elapsed 81.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 2000/2012 loss 0.0004 elapsed 81.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 6 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,0.999938]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01347444 mean_pred_per_img=46.810 empty_frac=0.000000 TP=98498 FP=1033794 FN=8377 f1@0.2=0.158975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 val micro-f1 0.61262 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 100/2012 loss 0.0003 elapsed 83.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 200/2012 loss 0.0003 elapsed 84.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 300/2012 loss 0.0003 elapsed 85.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 400/2012 loss 0.0003 elapsed 85.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 500/2012 loss 0.0003 elapsed 86.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 600/2012 loss 0.0003 elapsed 86.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 700/2012 loss 0.0003 elapsed 87.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 800/2012 loss 0.0003 elapsed 88.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 900/2012 loss 0.0003 elapsed 88.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1000/2012 loss 0.0003 elapsed 89.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1100/2012 loss 0.0003 elapsed 89.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1200/2012 loss 0.0003 elapsed 90.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1300/2012 loss 0.0003 elapsed 91.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1400/2012 loss 0.0003 elapsed 91.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1500/2012 loss 0.0003 elapsed 92.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1600/2012 loss 0.0003 elapsed 92.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1700/2012 loss 0.0003 elapsed 93.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1800/2012 loss 0.0003 elapsed 94.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1900/2012 loss 0.0003 elapsed 94.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 2000/2012 loss 0.0003 elapsed 95.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 7 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,0.999929]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01193456 mean_pred_per_img=41.461 empty_frac=0.000000 TP=97377 FP=905515 FN=9498 f1@0.2=0.175491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 val micro-f1 0.61105 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 100/2012 loss 0.0003 elapsed 97.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 200/2012 loss 0.0003 elapsed 98.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 300/2012 loss 0.0003 elapsed 98.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 400/2012 loss 0.0003 elapsed 99.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 500/2012 loss 0.0003 elapsed 100.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 600/2012 loss 0.0003 elapsed 100.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 700/2012 loss 0.0003 elapsed 101.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 800/2012 loss 0.0003 elapsed 101.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 900/2012 loss 0.0003 elapsed 102.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1000/2012 loss 0.0003 elapsed 103.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1100/2012 loss 0.0003 elapsed 103.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1200/2012 loss 0.0003 elapsed 104.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1300/2012 loss 0.0003 elapsed 104.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1400/2012 loss 0.0003 elapsed 105.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1500/2012 loss 0.0003 elapsed 106.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1600/2012 loss 0.0003 elapsed 106.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1700/2012 loss 0.0003 elapsed 107.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1800/2012 loss 0.0003 elapsed 107.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1900/2012 loss 0.0003 elapsed 108.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 2000/2012 loss 0.0003 elapsed 109.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 8 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,0.999938]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01087303 mean_pred_per_img=37.773 empty_frac=0.000000 TP=96436 FP=817253 FN=10439 f1@0.2=0.188986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 val micro-f1 0.61021 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 100/2012 loss 0.0003 elapsed 111.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 200/2012 loss 0.0003 elapsed 111.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 300/2012 loss 0.0003 elapsed 112.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 400/2012 loss 0.0003 elapsed 113.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 500/2012 loss 0.0003 elapsed 113.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 600/2012 loss 0.0003 elapsed 114.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 700/2012 loss 0.0003 elapsed 114.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 800/2012 loss 0.0003 elapsed 115.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 900/2012 loss 0.0003 elapsed 116.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1000/2012 loss 0.0003 elapsed 116.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1100/2012 loss 0.0003 elapsed 117.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1200/2012 loss 0.0003 elapsed 117.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1300/2012 loss 0.0003 elapsed 118.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1400/2012 loss 0.0003 elapsed 119.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1500/2012 loss 0.0003 elapsed 119.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1600/2012 loss 0.0003 elapsed 120.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1700/2012 loss 0.0003 elapsed 120.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1800/2012 loss 0.0003 elapsed 121.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1900/2012 loss 0.0003 elapsed 122.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 2000/2012 loss 0.0003 elapsed 122.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 9 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,0.999965]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01032592 mean_pred_per_img=35.872 empty_frac=0.000000 TP=95906 FP=771808 FN=10969 f1@0.2=0.196813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 val micro-f1 0.60977 @ thr 0.500\nEarly stopping at epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Fold 0 done: best_f1 0.61262 thr 0.500 ====\n==== Fold 1 start ====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 100/2013 loss 0.0014 elapsed 0.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 200/2013 loss 0.0014 elapsed 1.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 300/2013 loss 0.0013 elapsed 1.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 400/2013 loss 0.0013 elapsed 2.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 500/2013 loss 0.0012 elapsed 3.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 600/2013 loss 0.0012 elapsed 3.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 700/2013 loss 0.0011 elapsed 4.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 800/2013 loss 0.0011 elapsed 4.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 900/2013 loss 0.0011 elapsed 5.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1000/2013 loss 0.0010 elapsed 6.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1100/2013 loss 0.0010 elapsed 6.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1200/2013 loss 0.0010 elapsed 7.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1300/2013 loss 0.0010 elapsed 7.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1400/2013 loss 0.0010 elapsed 8.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1500/2013 loss 0.0010 elapsed 9.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1600/2013 loss 0.0010 elapsed 9.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1700/2013 loss 0.0009 elapsed 10.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1800/2013 loss 0.0009 elapsed 10.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 1900/2013 loss 0.0009 elapsed 11.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 iter 2000/2013 loss 0.0009 elapsed 12.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 1 epoch 1 ===\nval_size=24136 probs_shape=(24136, 3474) tgts_shape=(24136, 3474)\nprobs_range=[0.009203,0.986488]\ntgt_pos_rate=0.00127339 mean_pos_per_img=4.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.03718645 mean_pred_per_img=129.186 empty_frac=0.000000 TP=99500 FP=3018527 FN=7272 f1@0.2=0.061709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 1 val micro-f1 0.51974 @ thr 0.470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 100/2013 loss 0.0007 elapsed 14.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 200/2013 loss 0.0007 elapsed 15.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 300/2013 loss 0.0007 elapsed 15.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 400/2013 loss 0.0007 elapsed 16.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 500/2013 loss 0.0007 elapsed 16.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 600/2013 loss 0.0007 elapsed 17.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 700/2013 loss 0.0007 elapsed 18.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 800/2013 loss 0.0007 elapsed 18.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 900/2013 loss 0.0007 elapsed 19.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1000/2013 loss 0.0007 elapsed 19.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1100/2013 loss 0.0007 elapsed 20.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1200/2013 loss 0.0007 elapsed 21.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1300/2013 loss 0.0007 elapsed 21.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1400/2013 loss 0.0007 elapsed 22.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1500/2013 loss 0.0007 elapsed 22.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1600/2013 loss 0.0007 elapsed 23.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1700/2013 loss 0.0007 elapsed 24.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1800/2013 loss 0.0007 elapsed 24.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 1900/2013 loss 0.0007 elapsed 25.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 iter 2000/2013 loss 0.0007 elapsed 25.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 1 epoch 2 ===\nval_size=24136 probs_shape=(24136, 3474) tgts_shape=(24136, 3474)\nprobs_range=[0.000067,0.998448]\ntgt_pos_rate=0.00127339 mean_pos_per_img=4.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.02769274 mean_pred_per_img=96.205 empty_frac=0.000000 TP=102042 FP=2219952 FN=4730 f1@0.2=0.084028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 2 val micro-f1 0.58936 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 100/2013 loss 0.0006 elapsed 28.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 1 epoch 3 iter 200/2013 loss 0.0006 elapsed 28.8m\n"
          ]
        }
      ]
    },
    {
      "id": "0df44a94-74d4-470f-99d7-76708da86dd1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, time, shlex, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== Launch: b4@448 5-fold, EMA+TTA, using train_folds_top512.csv ===', flush=True)\n",
        "assert Path('train_folds_top512.csv').exists(), 'Missing train_folds_top512.csv'\n",
        "cmd = [\n",
        "    sys.executable, '-u', 'train.py',\n",
        "    '--model', 'tf_efficientnet_b4_ns',\n",
        "    '--img-size', '448',\n",
        "    '--epochs', '12',\n",
        "    '--batch-size', '32',\n",
        "    '--val-batch-size', '64',\n",
        "    '--num-workers', '10',\n",
        "    '--lr', '5e-4',\n",
        "    '--use-ema',\n",
        "    '--tta',\n",
        "    '--early-stop-patience', '3',\n",
        "    '--folds', '0,1,2,3,4',\n",
        "    '--folds-csv', 'train_folds_top512.csv',\n",
        "    '--out-dir', 'out_b4_448_top512',\n",
        "    '--pretrained'\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd), flush=True)\n",
        "t0 = time.time()\n",
        "env = dict(os.environ); env['PYTHONUNBUFFERED'] = '1'\n",
        "p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, env=env)\n",
        "try:\n",
        "    for line in p.stdout:\n",
        "        print(line, end='')\n",
        "finally:\n",
        "    rc = p.wait()\n",
        "print(f'Exit code: {rc}, elapsed {(time.time()-t0)/3600:.2f} h', flush=True)\n",
        "assert rc == 0, 'b4@448 production run failed'\n",
        "print('b4@448 production run completed.')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Launch: b4@448 5-fold, EMA+TTA, using train_folds_top512.csv ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running: /usr/bin/python3.11 -u train.py --model tf_efficientnet_b4_ns --img-size 448 --epochs 12 --batch-size 32 --val-batch-size 64 --num-workers 10 --lr 5e-4 --use-ema --tta --early-stop-patience 3 --folds 0,1,2,3,4 --folds-csv train_folds_top512.csv --out-dir out_b4_448_top512 --pretrained\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected image extension: .png\n==== Fold 0 start ====\n/app/.pip-target/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b4_ns to current tf_efficientnet_b4.ns_jft_in1k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:220: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:249: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 100/3019 loss 0.0014 elapsed 0.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 200/3019 loss 0.0013 elapsed 1.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 300/3019 loss 0.0013 elapsed 1.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 400/3019 loss 0.0012 elapsed 2.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 500/3019 loss 0.0012 elapsed 3.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 600/3019 loss 0.0011 elapsed 3.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 700/3019 loss 0.0011 elapsed 4.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 800/3019 loss 0.0011 elapsed 4.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 900/3019 loss 0.0011 elapsed 5.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1000/3019 loss 0.0010 elapsed 5.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1100/3019 loss 0.0010 elapsed 6.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1200/3019 loss 0.0010 elapsed 6.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1300/3019 loss 0.0010 elapsed 7.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1400/3019 loss 0.0010 elapsed 7.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1500/3019 loss 0.0010 elapsed 8.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1600/3019 loss 0.0010 elapsed 8.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1700/3019 loss 0.0010 elapsed 9.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1800/3019 loss 0.0009 elapsed 9.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1900/3019 loss 0.0009 elapsed 10.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2000/3019 loss 0.0009 elapsed 10.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2100/3019 loss 0.0009 elapsed 11.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2200/3019 loss 0.0009 elapsed 12.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2300/3019 loss 0.0009 elapsed 12.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2400/3019 loss 0.0009 elapsed 13.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2500/3019 loss 0.0009 elapsed 13.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2600/3019 loss 0.0009 elapsed 14.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2700/3019 loss 0.0009 elapsed 14.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2800/3019 loss 0.0009 elapsed 15.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 2900/3019 loss 0.0009 elapsed 15.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 3000/3019 loss 0.0009 elapsed 16.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 1 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.020371,0.734199]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.04986531 mean_pred_per_img=173.232 empty_frac=0.000000 TP=94532 FP=4095779 FN=12343 f1@0.2=0.043997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 val micro-f1 0.40535 @ thr 0.420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 100/3019 loss 0.0007 elapsed 19.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 200/3019 loss 0.0007 elapsed 20.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 300/3019 loss 0.0007 elapsed 20.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 400/3019 loss 0.0007 elapsed 21.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 500/3019 loss 0.0007 elapsed 21.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 600/3019 loss 0.0007 elapsed 22.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 700/3019 loss 0.0007 elapsed 22.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 800/3019 loss 0.0007 elapsed 23.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 900/3019 loss 0.0007 elapsed 23.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1000/3019 loss 0.0007 elapsed 24.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1100/3019 loss 0.0007 elapsed 24.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1200/3019 loss 0.0007 elapsed 25.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1300/3019 loss 0.0007 elapsed 25.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1400/3019 loss 0.0007 elapsed 26.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1500/3019 loss 0.0007 elapsed 26.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1600/3019 loss 0.0007 elapsed 27.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1700/3019 loss 0.0007 elapsed 28.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1800/3019 loss 0.0007 elapsed 28.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1900/3019 loss 0.0007 elapsed 29.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 2000/3019 loss 0.0007 elapsed 29.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 2100/3019 loss 0.0007 elapsed 30.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 2200/3019 loss 0.0007 elapsed 30.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 2300/3019 loss 0.0007 elapsed 31.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 2400/3019 loss 0.0007 elapsed 31.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 2500/3019 loss 0.0007 elapsed 32.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 2600/3019 loss 0.0007 elapsed 32.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 2700/3019 loss 0.0007 elapsed 33.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 2800/3019 loss 0.0007 elapsed 33.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 2900/3019 loss 0.0007 elapsed 34.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 3000/3019 loss 0.0007 elapsed 34.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 2 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.005754,0.988331]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.06444909 mean_pred_per_img=223.896 empty_frac=0.000000 TP=103180 FP=5312644 FN=3695 f1@0.2=0.037366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 val micro-f1 0.55272 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 100/3019 loss 0.0006 elapsed 37.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 200/3019 loss 0.0006 elapsed 38.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 300/3019 loss 0.0006 elapsed 38.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 400/3019 loss 0.0006 elapsed 39.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 500/3019 loss 0.0006 elapsed 39.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 600/3019 loss 0.0006 elapsed 40.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 700/3019 loss 0.0006 elapsed 40.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 800/3019 loss 0.0006 elapsed 41.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 900/3019 loss 0.0006 elapsed 41.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1000/3019 loss 0.0006 elapsed 42.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1100/3019 loss 0.0006 elapsed 43.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1200/3019 loss 0.0006 elapsed 43.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1300/3019 loss 0.0006 elapsed 44.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1400/3019 loss 0.0006 elapsed 44.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1500/3019 loss 0.0006 elapsed 45.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1600/3019 loss 0.0006 elapsed 45.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1700/3019 loss 0.0006 elapsed 46.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1800/3019 loss 0.0006 elapsed 46.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 1900/3019 loss 0.0006 elapsed 47.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 2000/3019 loss 0.0006 elapsed 47.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 2100/3019 loss 0.0006 elapsed 48.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 2200/3019 loss 0.0006 elapsed 48.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 2300/3019 loss 0.0006 elapsed 49.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 2400/3019 loss 0.0006 elapsed 49.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 2500/3019 loss 0.0006 elapsed 50.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 2600/3019 loss 0.0006 elapsed 51.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 2700/3019 loss 0.0006 elapsed 51.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 2800/3019 loss 0.0006 elapsed 52.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 2900/3019 loss 0.0006 elapsed 52.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 3000/3019 loss 0.0006 elapsed 53.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 3 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000200,0.999089]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.05399261 mean_pred_per_img=187.570 empty_frac=0.000000 TP=104182 FP=4432957 FN=2693 f1@0.2=0.044867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 val micro-f1 0.57166 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 100/3019 loss 0.0005 elapsed 55.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 200/3019 loss 0.0005 elapsed 56.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 300/3019 loss 0.0005 elapsed 56.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 400/3019 loss 0.0005 elapsed 57.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 500/3019 loss 0.0005 elapsed 58.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 600/3019 loss 0.0005 elapsed 58.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 700/3019 loss 0.0005 elapsed 59.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 800/3019 loss 0.0005 elapsed 59.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 900/3019 loss 0.0005 elapsed 60.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1000/3019 loss 0.0005 elapsed 60.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1100/3019 loss 0.0005 elapsed 61.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1200/3019 loss 0.0005 elapsed 61.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1300/3019 loss 0.0005 elapsed 62.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1400/3019 loss 0.0005 elapsed 62.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1500/3019 loss 0.0005 elapsed 63.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1600/3019 loss 0.0005 elapsed 63.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1700/3019 loss 0.0005 elapsed 64.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1800/3019 loss 0.0005 elapsed 64.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 1900/3019 loss 0.0005 elapsed 65.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 2000/3019 loss 0.0005 elapsed 66.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 2100/3019 loss 0.0005 elapsed 66.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 2200/3019 loss 0.0005 elapsed 67.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 2300/3019 loss 0.0005 elapsed 67.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 2400/3019 loss 0.0005 elapsed 68.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 2500/3019 loss 0.0005 elapsed 68.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 2600/3019 loss 0.0005 elapsed 69.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 2700/3019 loss 0.0005 elapsed 69.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 2800/3019 loss 0.0005 elapsed 70.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 2900/3019 loss 0.0005 elapsed 70.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 iter 3000/3019 loss 0.0005 elapsed 71.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 4 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,0.999966]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.03541859 mean_pred_per_img=123.044 empty_frac=0.000000 TP=103456 FP=2872860 FN=3419 f1@0.2=0.067110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 4 val micro-f1 0.58290 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 100/3019 loss 0.0004 elapsed 74.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 200/3019 loss 0.0004 elapsed 74.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 300/3019 loss 0.0004 elapsed 75.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 400/3019 loss 0.0004 elapsed 75.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 500/3019 loss 0.0004 elapsed 76.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 600/3019 loss 0.0004 elapsed 76.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 700/3019 loss 0.0004 elapsed 77.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 800/3019 loss 0.0004 elapsed 77.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 900/3019 loss 0.0004 elapsed 78.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1000/3019 loss 0.0004 elapsed 78.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1100/3019 loss 0.0004 elapsed 79.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1200/3019 loss 0.0004 elapsed 79.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1300/3019 loss 0.0004 elapsed 80.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1400/3019 loss 0.0004 elapsed 81.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1500/3019 loss 0.0004 elapsed 81.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1600/3019 loss 0.0004 elapsed 82.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1700/3019 loss 0.0004 elapsed 82.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1800/3019 loss 0.0004 elapsed 83.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 1900/3019 loss 0.0004 elapsed 83.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 2000/3019 loss 0.0004 elapsed 84.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 2100/3019 loss 0.0004 elapsed 84.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 2200/3019 loss 0.0004 elapsed 85.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 2300/3019 loss 0.0004 elapsed 85.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 2400/3019 loss 0.0004 elapsed 86.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 2500/3019 loss 0.0004 elapsed 86.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 2600/3019 loss 0.0004 elapsed 87.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 2700/3019 loss 0.0004 elapsed 87.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 2800/3019 loss 0.0004 elapsed 88.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 2900/3019 loss 0.0004 elapsed 89.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 iter 3000/3019 loss 0.0004 elapsed 89.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 5 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,0.999999]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.02319197 mean_pred_per_img=80.569 empty_frac=0.000000 TP=102019 FP=1846862 FN=4856 f1@0.2=0.099252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 5 val micro-f1 0.59714 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 100/3019 loss 0.0004 elapsed 92.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 200/3019 loss 0.0004 elapsed 92.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 300/3019 loss 0.0004 elapsed 93.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 400/3019 loss 0.0004 elapsed 93.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 500/3019 loss 0.0004 elapsed 94.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 600/3019 loss 0.0004 elapsed 95.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 700/3019 loss 0.0004 elapsed 95.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 800/3019 loss 0.0004 elapsed 96.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 900/3019 loss 0.0004 elapsed 96.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1000/3019 loss 0.0004 elapsed 97.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1100/3019 loss 0.0004 elapsed 97.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1200/3019 loss 0.0004 elapsed 98.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1300/3019 loss 0.0004 elapsed 98.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1400/3019 loss 0.0004 elapsed 99.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1500/3019 loss 0.0004 elapsed 99.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1600/3019 loss 0.0004 elapsed 100.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1700/3019 loss 0.0004 elapsed 100.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1800/3019 loss 0.0004 elapsed 101.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 1900/3019 loss 0.0004 elapsed 101.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 2000/3019 loss 0.0004 elapsed 102.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 2100/3019 loss 0.0004 elapsed 103.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 2200/3019 loss 0.0004 elapsed 103.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 2300/3019 loss 0.0004 elapsed 104.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 2400/3019 loss 0.0004 elapsed 104.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 2500/3019 loss 0.0004 elapsed 105.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 2600/3019 loss 0.0004 elapsed 105.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 2700/3019 loss 0.0004 elapsed 106.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 2800/3019 loss 0.0004 elapsed 106.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 2900/3019 loss 0.0004 elapsed 107.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 iter 3000/3019 loss 0.0004 elapsed 107.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 6 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,1.000000]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01597677 mean_pred_per_img=55.503 empty_frac=0.000000 TP=100145 FP=1242424 FN=6730 f1@0.2=0.138184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 6 val micro-f1 0.60867 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 100/3019 loss 0.0003 elapsed 110.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 200/3019 loss 0.0003 elapsed 111.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 300/3019 loss 0.0003 elapsed 111.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 400/3019 loss 0.0003 elapsed 112.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 500/3019 loss 0.0003 elapsed 112.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 600/3019 loss 0.0003 elapsed 113.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 700/3019 loss 0.0003 elapsed 113.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 800/3019 loss 0.0003 elapsed 114.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 900/3019 loss 0.0003 elapsed 114.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1000/3019 loss 0.0003 elapsed 115.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1100/3019 loss 0.0003 elapsed 115.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1200/3019 loss 0.0003 elapsed 116.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1300/3019 loss 0.0003 elapsed 116.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1400/3019 loss 0.0003 elapsed 117.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1500/3019 loss 0.0003 elapsed 118.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1600/3019 loss 0.0003 elapsed 118.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1700/3019 loss 0.0003 elapsed 119.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1800/3019 loss 0.0003 elapsed 119.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 1900/3019 loss 0.0003 elapsed 120.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 2000/3019 loss 0.0003 elapsed 120.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 2100/3019 loss 0.0003 elapsed 121.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 2200/3019 loss 0.0003 elapsed 121.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 2300/3019 loss 0.0003 elapsed 122.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 2400/3019 loss 0.0003 elapsed 122.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 2500/3019 loss 0.0003 elapsed 123.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 2600/3019 loss 0.0003 elapsed 123.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 2700/3019 loss 0.0003 elapsed 124.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 2800/3019 loss 0.0003 elapsed 125.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 2900/3019 loss 0.0003 elapsed 125.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 iter 3000/3019 loss 0.0003 elapsed 126.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 7 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,1.000000]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.01203555 mean_pred_per_img=41.811 empty_frac=0.000000 TP=98170 FP=913208 FN=8705 f1@0.2=0.175577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 7 val micro-f1 0.61261 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 100/3019 loss 0.0002 elapsed 128.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 200/3019 loss 0.0002 elapsed 129.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 300/3019 loss 0.0002 elapsed 129.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 400/3019 loss 0.0002 elapsed 130.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 500/3019 loss 0.0002 elapsed 130.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 600/3019 loss 0.0002 elapsed 131.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 700/3019 loss 0.0002 elapsed 132.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 800/3019 loss 0.0002 elapsed 132.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 900/3019 loss 0.0002 elapsed 133.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1000/3019 loss 0.0002 elapsed 133.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1100/3019 loss 0.0002 elapsed 134.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1200/3019 loss 0.0002 elapsed 134.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1300/3019 loss 0.0002 elapsed 135.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1400/3019 loss 0.0002 elapsed 135.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1500/3019 loss 0.0002 elapsed 136.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1600/3019 loss 0.0002 elapsed 136.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1700/3019 loss 0.0002 elapsed 137.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1800/3019 loss 0.0002 elapsed 137.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 1900/3019 loss 0.0002 elapsed 138.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 2000/3019 loss 0.0002 elapsed 138.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 2100/3019 loss 0.0002 elapsed 139.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 2200/3019 loss 0.0002 elapsed 140.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 2300/3019 loss 0.0002 elapsed 140.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 2400/3019 loss 0.0002 elapsed 141.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 2500/3019 loss 0.0002 elapsed 141.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 2600/3019 loss 0.0002 elapsed 142.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 2700/3019 loss 0.0002 elapsed 142.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 2800/3019 loss 0.0002 elapsed 143.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 2900/3019 loss 0.0002 elapsed 143.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 iter 3000/3019 loss 0.0002 elapsed 144.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 8 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.000000,1.000000]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.00965440 mean_pred_per_img=33.539 empty_frac=0.000000 TP=96083 FP=715201 FN=10792 f1@0.2=0.209295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 8 val micro-f1 0.61213 @ thr 0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 100/3019 loss 0.0002 elapsed 147.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 200/3019 loss 0.0002 elapsed 147.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 300/3019 loss 0.0002 elapsed 148.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 400/3019 loss 0.0002 elapsed 148.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 500/3019 loss 0.0002 elapsed 149.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 600/3019 loss 0.0002 elapsed 149.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 700/3019 loss 0.0002 elapsed 150.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 800/3019 loss 0.0002 elapsed 150.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 900/3019 loss 0.0002 elapsed 151.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1000/3019 loss 0.0002 elapsed 151.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1100/3019 loss 0.0002 elapsed 152.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1200/3019 loss 0.0002 elapsed 152.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1300/3019 loss 0.0002 elapsed 153.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1400/3019 loss 0.0002 elapsed 153.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1500/3019 loss 0.0002 elapsed 154.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1600/3019 loss 0.0002 elapsed 155.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1700/3019 loss 0.0002 elapsed 155.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1800/3019 loss 0.0002 elapsed 156.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 1900/3019 loss 0.0002 elapsed 156.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 2000/3019 loss 0.0002 elapsed 157.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 2100/3019 loss 0.0002 elapsed 157.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 2200/3019 loss 0.0002 elapsed 158.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 2300/3019 loss 0.0002 elapsed 158.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 2400/3019 loss 0.0002 elapsed 159.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 2500/3019 loss 0.0002 elapsed 159.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 9 iter 2600/3019 loss 0.0002 elapsed 160.3m\n"
          ]
        }
      ]
    },
    {
      "id": "502e82cb-f376-4cc3-b1be-e84b1f1169f8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, time, shlex, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== Prepared: convnext_tiny@384 5-fold, EMA+TTA, using train_folds_top512.csv ===', flush=True)\n",
        "assert Path('train_folds_top512.csv').exists(), 'Missing train_folds_top512.csv'\n",
        "cmd = [\n",
        "    sys.executable, '-u', 'train.py',\n",
        "    '--model', 'convnext_tiny_in22k',\n",
        "    '--img-size', '384',\n",
        "    '--epochs', '8',\n",
        "    '--batch-size', '64',\n",
        "    '--val-batch-size', '96',\n",
        "    '--num-workers', '10',\n",
        "    '--lr', '1.5e-4',\n",
        "    '--use-ema',\n",
        "    '--tta',\n",
        "    '--early-stop-patience', '2',\n",
        "    '--folds', '0,1,2,3,4',\n",
        "    '--folds-csv', 'train_folds_top512.csv',\n",
        "    '--out-dir', 'out_convnext_tiny_384_top512',\n",
        "    '--pretrained'\n",
        "]\n",
        "print('Queued command (not running yet):', ' '.join(shlex.quote(x) for x in cmd), flush=True)\n",
        "\n",
        "# To launch after current GPU job completes, run this cell to start the convnext_tiny baseline for ensembling."
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Prepared: convnext_tiny@384 5-fold, EMA+TTA, using train_folds_top512.csv ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queued command (not running yet): /usr/bin/python3.11 -u train.py --model convnext_tiny_in22k --img-size 384 --epochs 8 --batch-size 64 --val-batch-size 96 --num-workers 10 --lr 1.5e-4 --use-ema --tta --early-stop-patience 2 --folds 0,1,2,3,4 --folds-csv train_folds_top512.csv --out-dir out_convnext_tiny_384_top512 --pretrained\n"
          ]
        }
      ]
    },
    {
      "id": "e64b5803-9933-4e7e-bc5e-b8829e1b91a7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, time, shlex, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== Launch: convnext_tiny@384 5-fold, EMA+TTA, using train_folds_top512.csv (v2 settings) ===', flush=True)\n",
        "assert Path('train_folds_top512.csv').exists(), 'Missing train_folds_top512.csv'\n",
        "cmd = [\n",
        "    sys.executable, '-u', 'train.py',\n",
        "    '--model', 'convnext_tiny_in22k',\n",
        "    '--img-size', '384',\n",
        "    '--epochs', '10',\n",
        "    '--batch-size', '64',\n",
        "    '--val-batch-size', '96',\n",
        "    '--num-workers', '10',\n",
        "    '--lr', '2e-4',\n",
        "    '--use-ema',\n",
        "    '--tta',\n",
        "    '--early-stop-patience', '3',\n",
        "    '--folds', '0,1,2,3,4',\n",
        "    '--folds-csv', 'train_folds_top512.csv',\n",
        "    '--out-dir', 'out_convnext_tiny_384_top512_v2',\n",
        "    '--pretrained'\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd), flush=True)\n",
        "t0 = time.time()\n",
        "env = dict(os.environ); env['PYTHONUNBUFFERED'] = '1'\n",
        "p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, env=env)\n",
        "try:\n",
        "    for line in p.stdout:\n",
        "        print(line, end='')\n",
        "finally:\n",
        "    rc = p.wait()\n",
        "print(f'Exit code: {rc}, elapsed {(time.time()-t0)/3600:.2f} h', flush=True)\n",
        "assert rc == 0, 'convnext_tiny@384 v2 production run failed'\n",
        "print('convnext_tiny@384 v2 production run completed.')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Launch: convnext_tiny@384 5-fold, EMA+TTA, using train_folds_top512.csv (v2 settings) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running: /usr/bin/python3.11 -u train.py --model convnext_tiny_in22k --img-size 384 --epochs 10 --batch-size 64 --val-batch-size 96 --num-workers 10 --lr 2e-4 --use-ema --tta --early-stop-patience 3 --folds 0,1,2,3,4 --folds-csv train_folds_top512.csv --out-dir out_convnext_tiny_384_top512_v2 --pretrained\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected image extension: .png\n==== Fold 0 start ====\n/app/.pip-target/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name convnext_tiny_in22k to current convnext_tiny.fb_in22k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:220: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:249: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 100/1509 loss 0.0014 elapsed 0.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 200/1509 loss 0.0013 elapsed 1.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 300/1509 loss 0.0012 elapsed 2.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 400/1509 loss 0.0012 elapsed 2.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 500/1509 loss 0.0011 elapsed 3.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 600/1509 loss 0.0011 elapsed 3.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 700/1509 loss 0.0011 elapsed 4.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 800/1509 loss 0.0010 elapsed 4.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 900/1509 loss 0.0010 elapsed 5.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1000/1509 loss 0.0010 elapsed 5.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1100/1509 loss 0.0010 elapsed 6.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1200/1509 loss 0.0010 elapsed 7.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1300/1509 loss 0.0009 elapsed 7.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1400/1509 loss 0.0009 elapsed 8.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1500/1509 loss 0.0009 elapsed 8.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 1 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.010003,0.604751]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.05146473 mean_pred_per_img=178.788 empty_frac=0.000000 TP=84819 FP=4239895 FN=22056 f1@0.2=0.038279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 val micro-f1 0.23136 @ thr 0.380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 100/1509 loss 0.0007 elapsed 10.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 200/1509 loss 0.0007 elapsed 11.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 300/1509 loss 0.0007 elapsed 12.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 400/1509 loss 0.0007 elapsed 12.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 500/1509 loss 0.0007 elapsed 13.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 600/1509 loss 0.0007 elapsed 13.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 700/1509 loss 0.0007 elapsed 14.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 800/1509 loss 0.0007 elapsed 14.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 900/1509 loss 0.0007 elapsed 15.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1000/1509 loss 0.0007 elapsed 15.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1100/1509 loss 0.0007 elapsed 16.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1200/1509 loss 0.0007 elapsed 17.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1300/1509 loss 0.0007 elapsed 17.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1400/1509 loss 0.0007 elapsed 18.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1500/1509 loss 0.0007 elapsed 18.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 2 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.008020,0.826771]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.04041846 mean_pred_per_img=140.414 empty_frac=0.000000 TP=91947 FP=3304521 FN=14928 f1@0.2=0.052491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 val micro-f1 0.38873 @ thr 0.410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 100/1509 loss 0.0005 elapsed 20.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 200/1509 loss 0.0005 elapsed 21.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 300/1509 loss 0.0005 elapsed 21.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 3 iter 400/1509 loss 0.0005 elapsed 22.3m\n"
          ]
        }
      ]
    },
    {
      "id": "d8cd569c-2663-4357-af6d-ff45a128212b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, time, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== Blend setup: weighted logit averaging with global thr sweep or cardinality match ===', flush=True)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def probs_to_logits(p, eps=1e-5):\n",
        "    p = np.clip(p, eps, 1.0 - eps)\n",
        "    return np.log(p / (1.0 - p))\n",
        "\n",
        "def micro_f1_from_probs(probs, targets, thr=0.2):\n",
        "    preds = (probs >= thr).astype(np.uint8)\n",
        "    t = targets.astype(np.uint8)\n",
        "    tp = np.logical_and(preds == 1, t == 1).sum(dtype=np.int64)\n",
        "    fp = np.logical_and(preds == 1, t == 0).sum(dtype=np.int64)\n",
        "    fn = np.logical_and(preds == 0, t == 1).sum(dtype=np.int64)\n",
        "    denom = 2 * tp + fp + fn\n",
        "    return float((2 * tp) / denom) if denom > 0 else 0.0\n",
        "\n",
        "def build_y_true(train_csv='train.csv', labels_csv='labels.csv'):\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    labels_df = pd.read_csv(labels_csv)\n",
        "    attr_ids = sorted(labels_df['attribute_id'].astype(int).unique().tolist())\n",
        "    attr_to_idx = {a:i for i,a in enumerate(attr_ids)}\n",
        "    y_true = np.zeros((len(train_df), len(attr_ids)), dtype=np.uint8)\n",
        "    for i, s in enumerate(train_df['attribute_ids'].fillna('').astype(str)):\n",
        "        if s:\n",
        "            for a in map(int, s.split()):\n",
        "                j = attr_to_idx.get(a, None)\n",
        "                if j is not None:\n",
        "                    y_true[i, j] = 1\n",
        "    return train_df, np.array(attr_ids, dtype=np.int32), y_true\n",
        "\n",
        "def load_model_artifacts(model_dir: Path):\n",
        "    model_dir = Path(model_dir)\n",
        "    oof_p = model_dir/'oof_probs.npy'\n",
        "    test_p = model_dir/'test_probs.npy'\n",
        "    meta_p = model_dir/'oof_meta.csv'\n",
        "    oof = np.load(oof_p) if oof_p.exists() else None\n",
        "    test = np.load(test_p) if test_p.exists() else None\n",
        "    meta = pd.read_csv(meta_p) if meta_p.exists() else None\n",
        "    return oof, test, meta\n",
        "\n",
        "def blend_equal_weight(model_dirs, write_submission=True, out_name='submission_blend.csv', default_thr=0.50, cardinality_target=None, weights=None):\n",
        "    model_dirs = [Path(d) for d in model_dirs]\n",
        "    train_df, idx_to_attr, y_true = build_y_true('train.csv', 'labels.csv')\n",
        "    train_mean_labels = float((y_true.sum(axis=1)).mean())\n",
        "    # Load all artifacts\n",
        "    models = []\n",
        "    for d in model_dirs:\n",
        "        oof, test, meta = load_model_artifacts(d)\n",
        "        if test is None:\n",
        "            print(f'[WARN] Missing test_probs.npy in {d}, skipping this model for test blend')\n",
        "        models.append({'dir': d, 'oof': oof, 'test': test, 'meta': meta})\n",
        "\n",
        "    have_test = [m for m in models if m['test'] is not None]\n",
        "    if len(have_test) == 0:\n",
        "        print('[INFO] No test outputs yet; cannot write submission.')\n",
        "        return None, None, None\n",
        "\n",
        "    # Prepare weights\n",
        "    if weights is not None:\n",
        "        if len(weights) != len(have_test):\n",
        "            print('[WARN] Provided weights length does not match number of test models; ignoring weights.')\n",
        "            weights_use = None\n",
        "        else:\n",
        "            weights_use = np.array(weights, dtype=np.float64)\n",
        "    else:\n",
        "        weights_use = None\n",
        "\n",
        "    # Blend test in logit space\n",
        "    test_logits_list = [probs_to_logits(m['test']) for m in have_test]\n",
        "    if weights_use is None:\n",
        "        Zt = np.mean(np.stack(test_logits_list, axis=0), axis=0)\n",
        "    else:\n",
        "        Zt = np.average(np.stack(test_logits_list, axis=0), axis=0, weights=weights_use)\n",
        "    Pt = sigmoid(Zt)\n",
        "\n",
        "    # Determine threshold\n",
        "    have_oof = [m for m in models if m['oof'] is not None and m['meta'] is not None]\n",
        "    oof_f1, best_thr = None, None\n",
        "    if len(have_oof) > 0:\n",
        "        oof_logits_list = [probs_to_logits(m['oof']) for m in have_oof]\n",
        "        if weights_use is None or len(have_oof) != len(have_test):\n",
        "            Zb = np.mean(np.stack(oof_logits_list, axis=0), axis=0)\n",
        "        else:\n",
        "            Zb = np.average(np.stack(oof_logits_list, axis=0), axis=0, weights=weights_use)\n",
        "        Pb = sigmoid(Zb)\n",
        "        thrs = np.arange(0.48, 0.5201, 0.002)\n",
        "        f1s = [micro_f1_from_probs(Pb, y_true, thr=t) for t in thrs]\n",
        "        bi = int(np.argmax(f1s))\n",
        "        best_thr = float(thrs[bi])\n",
        "        oof_f1 = float(f1s[bi])\n",
        "        print(f'Blended OOF micro-f1 {oof_f1:.5f} @ thr {best_thr:.3f}')\n",
        "    elif cardinality_target is not None:\n",
        "        thrs = np.arange(0.48, 0.5201, 0.002)\n",
        "        means = [float((Pt >= t).sum(axis=1).mean()) for t in thrs]\n",
        "        target = float(cardinality_target)\n",
        "        bi = int(np.argmin([abs(m - target) for m in means]))\n",
        "        best_thr = float(thrs[bi])\n",
        "        print(f'[CARD] Train mean labels/img={train_mean_labels:.3f}, target={target:.3f}, chosen thr={best_thr:.3f} (pred_mean={means[bi]:.3f})')\n",
        "    else:\n",
        "        best_thr = float(default_thr)\n",
        "        print(f'[INFO] No OOF available; using default threshold {best_thr:.3f}')\n",
        "\n",
        "    if write_submission and best_thr is not None:\n",
        "        sub = pd.read_csv('sample_submission.csv')\n",
        "        ids = sub['id'].values\n",
        "        rows = []\n",
        "        for i in range(len(ids)):\n",
        "            p = Pt[i]\n",
        "            pred_idx = np.where(p >= best_thr)[0].tolist()\n",
        "            if len(pred_idx) == 0:\n",
        "                pred_idx = [int(np.argmax(p))]\n",
        "            pred_attr = [int(idx_to_attr[j]) for j in sorted(set(pred_idx))]\n",
        "            rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(x) for x in pred_attr)})\n",
        "        sub_df = pd.DataFrame(rows)\n",
        "        sub_df.to_csv(out_name, index=False)\n",
        "        print(f'Wrote {out_name} with thr={best_thr:.3f} using {len(have_test)} models')\n",
        "    else:\n",
        "        print('[INFO] Skipping submission write (best_thr not available).')\n",
        "    return oof_f1, best_thr, Pt\n",
        "\n",
        "# Example usage (will run later when artifacts exist):\n",
        "MODEL_DIRS = [\n",
        "    'out_b3_384_top512',\n",
        "    'out_b3_448_top512',\n",
        "    'out_convnext_tiny_384_top512',\n",
        "]\n",
        "print('Ready. Call blend_equal_weight(MODEL_DIRS, weights=[2,1,1], cardinality_target=4.42) to create weighted submission.', flush=True)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Blend setup: weighted logit averaging with global thr sweep or cardinality match ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready. Call blend_equal_weight(MODEL_DIRS, weights=[2,1,1], cardinality_target=4.42) to create weighted submission.\n"
          ]
        }
      ]
    },
    {
      "id": "59c4ea71-4477-487f-9a7b-92a4f4f11fdb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, glob, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "import train as trn\n",
        "\n",
        "print('=== Inference-only helper: generate test_probs.npy from available fold weights ===', flush=True)\n",
        "\n",
        "def infer_available_folds(out_dir: str, model_name: str, img_size: int, val_batch_size: int = 96, num_workers: int = 10, use_tta: bool = True):\n",
        "    out = Path(out_dir)\n",
        "    pths = sorted(out.glob(f'{model_name.replace(\"/\",\"_\")}_fold*.pth'))\n",
        "    if len(pths) == 0:\n",
        "        print(f'[SKIP] No weights found in {out_dir}')\n",
        "        return None\n",
        "    # Build minimal cfg object expected by train.infer_test\n",
        "    class Cfg: pass\n",
        "    cfg = Cfg()\n",
        "    cfg.model = model_name\n",
        "    cfg.img_size = img_size\n",
        "    cfg.val_batch_size = val_batch_size\n",
        "    cfg.num_workers = num_workers\n",
        "    cfg.sample_sub = 'sample_submission.csv'\n",
        "    cfg.test_dir = Path('test')\n",
        "    cfg.use_ema = False\n",
        "    cfg.tta = bool(use_tta)\n",
        "\n",
        "    # Label mapping and image extension\n",
        "    labels_df = pd.read_csv('labels.csv')\n",
        "    attr_ids = sorted(labels_df['attribute_id'].astype(int).unique().tolist())\n",
        "    attr_to_idx = {a:i for i,a in enumerate(attr_ids)}\n",
        "    idx_to_attr = np.array(attr_ids, dtype=np.int32)\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    img_ext = trn.detect_ext(Path('train'), [train_df['id'].iloc[0]])\n",
        "\n",
        "    print(f'[INFO] {out_dir}: found {len(pths)} fold weights -> running test inference (tta={cfg.tta})')\n",
        "    ids, probs = trn.infer_test(cfg, [str(p) for p in pths], len(attr_ids), img_ext, attr_to_idx, idx_to_attr)\n",
        "    np.save(out / 'test_probs.npy', probs)\n",
        "    print(f'[DONE] Saved {out_dir}/test_probs.npy with shape {probs.shape}')\n",
        "    return probs\n",
        "\n",
        "# Example: run inference for partial dirs; re-run any time new weights appear\n",
        "TRY_JOBS = [\n",
        "    ('out_b3_384_top512', 'tf_efficientnet_b3_ns', 384),\n",
        "    ('out_b3_448_top512', 'tf_efficientnet_b3_ns', 448),\n",
        "    ('out_b3_384_card', 'tf_efficientnet_b3_ns', 384),\n",
        "    ('out_convnext_tiny_384_top512', 'convnext_tiny_in22k', 384),\n",
        "    ('out_convnext_tiny_384_top512_v2', 'convnext_tiny_in22k', 384),\n",
        "]\n",
        "for d, m, sz in TRY_JOBS:\n",
        "    if Path(d).exists():\n",
        "        try:\n",
        "            infer_available_folds(d, m, sz, val_batch_size=96, num_workers=10, use_tta=True)\n",
        "        except Exception as e:\n",
        "            print(f'[WARN] Inference failed for {d}: {e}')\n",
        "    else:\n",
        "        print(f'[SKIP] Missing dir {d}')\n",
        "\n",
        "print('Ready. You can re-run this cell any time new fold weights appear to refresh test_probs.npy for blending.', flush=True)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Inference-only helper: generate test_probs.npy from available fold weights ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] out_b3_384_top512: found 2 fold weights -> running test inference (tta=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:365: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(pth, map_location='cpu')\n/app/.pip-target/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b3_ns to current tf_efficientnet_b3.ns_jft_in1k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DONE] Saved out_b3_384_top512/test_probs.npy with shape (21318, 3474)\n[INFO] out_b3_448_top512: found 1 fold weights -> running test inference (tta=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:365: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(pth, map_location='cpu')\n/app/.pip-target/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b3_ns to current tf_efficientnet_b3.ns_jft_in1k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DONE] Saved out_b3_448_top512/test_probs.npy with shape (21318, 3474)\n[INFO] out_b3_384_card: found 2 fold weights -> running test inference (tta=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:365: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(pth, map_location='cpu')\n/app/.pip-target/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b3_ns to current tf_efficientnet_b3.ns_jft_in1k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DONE] Saved out_b3_384_card/test_probs.npy with shape (21318, 3474)\n[INFO] out_convnext_tiny_384_top512: found 1 fold weights -> running test inference (tta=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:365: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(pth, map_location='cpu')\n/app/.pip-target/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name convnext_tiny_in22k to current convnext_tiny.fb_in22k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DONE] Saved out_convnext_tiny_384_top512/test_probs.npy with shape (21318, 3474)\n[SKIP] Missing dir out_convnext_tiny_384_top512_v2\nReady. You can re-run this cell any time new fold weights appear to refresh test_probs.npy for blending.\n"
          ]
        }
      ]
    },
    {
      "id": "c106a632-c930-476a-8135-f52c12de855e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, time, shlex, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== Launch: convnext_small@384 5-fold, EMA+TTA, using train_folds_top512.csv ===', flush=True)\n",
        "assert Path('train_folds_top512.csv').exists(), 'Missing train_folds_top512.csv'\n",
        "cmd = [\n",
        "    sys.executable, '-u', 'train.py',\n",
        "    '--model', 'convnext_small_in22k',\n",
        "    '--img-size', '384',\n",
        "    '--epochs', '10',\n",
        "    '--batch-size', '56',\n",
        "    '--val-batch-size', '96',\n",
        "    '--num-workers', '10',\n",
        "    '--lr', '2e-4',\n",
        "    '--use-ema',\n",
        "    '--tta',\n",
        "    '--early-stop-patience', '3',\n",
        "    '--folds', '0,1,2,3,4',\n",
        "    '--folds-csv', 'train_folds_top512.csv',\n",
        "    '--out-dir', 'out_convnext_small_384_top512',\n",
        "    '--pretrained'\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd), flush=True)\n",
        "t0 = time.time()\n",
        "env = dict(os.environ); env['PYTHONUNBUFFERED'] = '1'\n",
        "p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, env=env)\n",
        "try:\n",
        "    for line in p.stdout:\n",
        "        print(line, end='')\n",
        "finally:\n",
        "    rc = p.wait()\n",
        "print(f'Exit code: {rc}, elapsed {(time.time()-t0)/3600:.2f} h', flush=True)\n",
        "assert rc == 0, 'convnext_small@384 production run failed'\n",
        "print('convnext_small@384 production run completed.')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Launch: convnext_small@384 5-fold, EMA+TTA, using train_folds_top512.csv ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running: /usr/bin/python3.11 -u train.py --model convnext_small_in22k --img-size 384 --epochs 10 --batch-size 56 --val-batch-size 96 --num-workers 10 --lr 2e-4 --use-ema --tta --early-stop-patience 3 --folds 0,1,2,3,4 --folds-csv train_folds_top512.csv --out-dir out_convnext_small_384_top512 --pretrained\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected image extension: .png\n==== Fold 0 start ====\n/app/.pip-target/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name convnext_small_in22k to current convnext_small.fb_in22k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:220: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:249: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 100/1725 loss 0.0014 elapsed 1.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 200/1725 loss 0.0013 elapsed 1.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 300/1725 loss 0.0013 elapsed 2.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 400/1725 loss 0.0012 elapsed 3.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 500/1725 loss 0.0012 elapsed 4.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 600/1725 loss 0.0011 elapsed 5.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 700/1725 loss 0.0011 elapsed 5.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 800/1725 loss 0.0011 elapsed 6.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 900/1725 loss 0.0010 elapsed 7.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1000/1725 loss 0.0010 elapsed 8.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1100/1725 loss 0.0010 elapsed 9.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1200/1725 loss 0.0010 elapsed 9.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1300/1725 loss 0.0010 elapsed 10.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1400/1725 loss 0.0009 elapsed 11.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1500/1725 loss 0.0009 elapsed 12.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1600/1725 loss 0.0009 elapsed 13.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 iter 1700/1725 loss 0.0009 elapsed 13.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== VAL DIAG fold 0 epoch 1 ===\nval_size=24189 probs_shape=(24189, 3474) tgts_shape=(24189, 3474)\nprobs_range=[0.005726,0.854392]\ntgt_pos_rate=0.00127183 mean_pos_per_img=4.418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thr=0.2 pred_pos_rate=0.05700069 mean_pred_per_img=198.020 empty_frac=0.000000 TP=86114 FP=4703801 FN=20761 f1@0.2=0.035172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 1 val micro-f1 0.26133 @ thr 0.410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 100/1725 loss 0.0006 elapsed 17.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 200/1725 loss 0.0006 elapsed 17.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 300/1725 loss 0.0006 elapsed 18.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 400/1725 loss 0.0006 elapsed 19.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 500/1725 loss 0.0006 elapsed 20.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 600/1725 loss 0.0006 elapsed 21.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 700/1725 loss 0.0006 elapsed 21.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 800/1725 loss 0.0006 elapsed 22.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 900/1725 loss 0.0006 elapsed 23.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1000/1725 loss 0.0006 elapsed 24.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1100/1725 loss 0.0006 elapsed 25.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1200/1725 loss 0.0006 elapsed 25.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1300/1725 loss 0.0006 elapsed 26.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0 epoch 2 iter 1400/1725 loss 0.0006 elapsed 27.4m\n"
          ]
        }
      ]
    },
    {
      "id": "a15ac617-94c6-471c-9c82-7595604ed7b5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('=== Blending now: b3@384_top512 + b3@448_top512 + b3@384_card + convnext_tiny + OOF(threshold)=out_smoke_b3_384 ===', flush=True)\n",
        "dirs = ['out_b3_384_top512','out_b3_448_top512','out_b3_384_card','out_convnext_tiny_384_top512','out_smoke_b3_384']\n",
        "oof_f1, best_thr, Pt = blend_equal_weight(dirs, write_submission=True, out_name='submission.csv', default_thr=0.50)\n",
        "print('Blend done. best_thr =', best_thr, 'OOF_f1 =', oof_f1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Blending now: b3@384_top512 + b3@448_top512 + b3@384_card + convnext_tiny + OOF(threshold)=out_smoke_b3_384 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] Missing test_probs.npy in out_smoke_b3_384, skipping this model for test blend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blended OOF micro-f1 0.08233 @ thr 0.050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with thr=0.050 using 4 models\nBlend done. best_thr = 0.05 OOF_f1 = 0.08233134629987264\n"
          ]
        }
      ]
    },
    {
      "id": "1cb7c7db-c46a-4fee-9afd-feb4e4fe346f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('=== Re-blend without OOF (use default thr=0.50) ===', flush=True)\n",
        "dirs = ['out_b3_384_top512','out_b3_448_top512','out_b3_384_card','out_convnext_tiny_384_top512']\n",
        "oof_f1, best_thr, Pt = blend_equal_weight(dirs, write_submission=True, out_name='submission.csv', default_thr=0.50)\n",
        "print('Reblend done. best_thr =', best_thr, 'OOF_f1 =', oof_f1)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Re-blend without OOF (use default thr=0.50) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] No complete OOF across models; using default threshold 0.500.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv with thr=0.500 using 4 models\nReblend done. best_thr = 0.5 OOF_f1 = None\n"
          ]
        }
      ]
    },
    {
      "id": "0d3053b7-8bcd-4a6e-b68b-347407f903b7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('=== Re-blend with cardinality matching to train mean (~4.42) ===', flush=True)\n",
        "dirs = ['out_b3_384_top512','out_b3_448_top512','out_b3_384_card','out_convnext_tiny_384_top512']\n",
        "oof_f1, best_thr, Pt = blend_equal_weight(dirs, write_submission=True, out_name='submission.csv', default_thr=0.50, cardinality_target=4.42)\n",
        "print('Reblend (cardinality) done. best_thr =', best_thr, 'OOF_f1 =', oof_f1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Re-blend with cardinality matching to train mean (~4.42) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.420, chosen thr=0.550 (pred_mean=4.559)\nWrote submission.csv with thr=0.550 using 4 models\nReblend (cardinality) done. best_thr = 0.5499999999999998 OOF_f1 = None\n"
          ]
        }
      ]
    },
    {
      "id": "e0defd41-43e2-43a0-98a9-938b4ffaabb3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('=== Safety blend: b3@384_top512 + b3@448_top512 + convnext_tiny | cardinality target=4.42 ===', flush=True)\n",
        "dirs = ['out_b3_384_top512','out_b3_448_top512','out_convnext_tiny_384_top512']\n",
        "oof_f1, best_thr, Pt = blend_equal_weight(dirs, write_submission=True, out_name='submission.csv', default_thr=0.50, cardinality_target=4.42)\n",
        "print('Safety blend done. thr=', best_thr, 'OOF_f1=', oof_f1)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Safety blend: b3@384_top512 + b3@448_top512 + convnext_tiny | cardinality target=4.42 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.420, chosen thr=0.495 (pred_mean=4.421)\nWrote submission.csv with thr=0.495 using 3 models\nSafety blend done. thr= 0.4949999999999998 OOF_f1= None\n"
          ]
        }
      ]
    },
    {
      "id": "c5ec7fd2-dd90-421f-801d-021ab4319869",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, time, shlex, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== Launch: b5@456 folds 0,1, EMA+TTA, using train_folds_top512.csv ===', flush=True)\n",
        "assert Path('train_folds_top512.csv').exists(), 'Missing train_folds_top512.csv'\n",
        "cmd = [\n",
        "    sys.executable, '-u', 'train.py',\n",
        "    '--model', 'tf_efficientnet_b5_ns',\n",
        "    '--img-size', '456',\n",
        "    '--epochs', '10',\n",
        "    '--batch-size', '24',\n",
        "    '--val-batch-size', '64',\n",
        "    '--num-workers', '10',\n",
        "    '--lr', '3e-4',\n",
        "    '--use-ema',\n",
        "    '--tta',\n",
        "    '--early-stop-patience', '3',\n",
        "    '--folds', '0,1',\n",
        "    '--folds-csv', 'train_folds_top512.csv',\n",
        "    '--out-dir', 'out_b5_456_top512',\n",
        "    '--pretrained'\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd), flush=True)\n",
        "t0 = time.time()\n",
        "env = dict(os.environ); env['PYTHONUNBUFFERED'] = '1'\n",
        "p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, env=env)\n",
        "try:\n",
        "    for line in p.stdout:\n",
        "        print(line, end='')\n",
        "finally:\n",
        "    rc = p.wait()\n",
        "print(f'Exit code: {rc}, elapsed {(time.time()-t0)/3600:.2f} h', flush=True)\n",
        "assert rc == 0, 'b5@456 run failed'\n",
        "print('b5@456 folds 0,1 completed.')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Launch: b5@456 folds 0,1, EMA+TTA, using train_folds_top512.csv ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running: /usr/bin/python3.11 -u train.py --model tf_efficientnet_b5_ns --img-size 456 --epochs 10 --batch-size 24 --val-batch-size 64 --num-workers 10 --lr 3e-4 --use-ema --tta --early-stop-patience 3 --folds 0,1 --folds-csv train_folds_top512.csv --out-dir out_b5_456_top512 --pretrained\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected image extension: .png\n==== Fold 0 start ====\n/app/.pip-target/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b5_ns to current tf_efficientnet_b5.ns_jft_in1k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:220: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:249: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n  File \"/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py\", line 484, in <module>\n    main()\n  File \"/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py\", line 445, in main\n    val_idx, oof_probs, thr, f1 = train_one_fold(cfg, f, train_df, num_classes, img_ext, attr_to_idx)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py\", line 250, in train_one_fold\n    logits = model(imgs)\n             ^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/timm/models/efficientnet.py\", line 268, in forward\n    x = self.forward_features(x)\n        ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/timm/models/efficientnet.py\", line 256, in forward_features\n    x = self.blocks(x)\n        ^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/container.py\", line 219, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/container.py\", line 219, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/timm/models/_efficientnet_blocks.py\", line 287, in forward\n    x = self.bn1(x)\n        ^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/timm/layers/norm_act.py\", line 115, in forward\n    x = F.batch_norm(\n        ^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/functional.py\", line 2512, in batch_norm\n    return torch.batch_norm(\n           ^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacity of 23.72 GiB of which 201.12 MiB is free. Process 11401 has 16.40 GiB memory in use. Process 283725 has 4.80 GiB memory in use. Of the allocated memory 4.48 GiB is allocated by PyTorch, and 50.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exit code: 1, elapsed 0.02 h\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "b5@456 run failed",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m     rc = p.wait()\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mExit code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, elapsed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time.time()-t0)/\u001b[32m3600\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m h\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m rc == \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mb5@456 run failed\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mb5@456 folds 0,1 completed.\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mAssertionError\u001b[39m: b5@456 run failed"
          ]
        }
      ]
    },
    {
      "id": "0caeebb9-ed6c-4a41-8f09-e25a6927071f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, time, shlex, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== Relaunch: b5@456 fold 0 ONLY (reduced BS to avoid OOM) ===', flush=True)\n",
        "assert Path('train_folds_top512.csv').exists(), 'Missing train_folds_top512.csv'\n",
        "cmd = [\n",
        "    sys.executable, '-u', 'train.py',\n",
        "    '--model', 'tf_efficientnet_b5_ns',\n",
        "    '--img-size', '456',\n",
        "    '--epochs', '10',\n",
        "    '--batch-size', '16',\n",
        "    '--val-batch-size', '48',\n",
        "    '--num-workers', '10',\n",
        "    '--lr', '3e-4',\n",
        "    '--use-ema',\n",
        "    '--tta',\n",
        "    '--early-stop-patience', '3',\n",
        "    '--folds', '0',\n",
        "    '--folds-csv', 'train_folds_top512.csv',\n",
        "    '--out-dir', 'out_b5_456_top512',\n",
        "    '--pretrained'\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd), flush=True)\n",
        "t0 = time.time()\n",
        "env = dict(os.environ); env['PYTHONUNBUFFERED'] = '1'; env['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, env=env)\n",
        "try:\n",
        "    for line in p.stdout:\n",
        "        print(line, end='')\n",
        "finally:\n",
        "    rc = p.wait()\n",
        "print(f'Exit code: {rc}, elapsed {(time.time()-t0)/3600:.2f} h', flush=True)\n",
        "assert rc == 0, 'b5@456 fold0 run failed'\n",
        "print('b5@456 fold 0 completed.')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Relaunch: b5@456 fold 0 ONLY (reduced BS to avoid OOM) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running: /usr/bin/python3.11 -u train.py --model tf_efficientnet_b5_ns --img-size 456 --epochs 10 --batch-size 16 --val-batch-size 48 --num-workers 10 --lr 3e-4 --use-ema --tta --early-stop-patience 3 --folds 0 --folds-csv train_folds_top512.csv --out-dir out_b5_456_top512 --pretrained\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected image extension: .png\n==== Fold 0 start ====\n/app/.pip-target/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b5_ns to current tf_efficientnet_b5.ns_jft_in1k.\n  model = create_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:220: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py:249: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n  File \"/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py\", line 484, in <module>\n    main()\n  File \"/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py\", line 445, in main\n    val_idx, oof_probs, thr, f1 = train_one_fold(cfg, f, train_df, num_classes, img_ext, attr_to_idx)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lib/simon/agent_run_states/imet-2020-fgvc7-20250927-162858/train.py\", line 250, in train_one_fold\n    logits = model(imgs)\n             ^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/timm/models/efficientnet.py\", line 268, in forward\n    x = self.forward_features(x)\n        ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/timm/models/efficientnet.py\", line 256, in forward_features\n    x = self.blocks(x)\n        ^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/container.py\", line 219, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/container.py\", line 219, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/timm/models/_efficientnet_blocks.py\", line 288, in forward\n    x = self.conv_dw(x)\n        ^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/conv.py\", line 458, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/.pip-target/torch/nn/modules/conv.py\", line 454, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.72 GiB of which 69.12 MiB is free. Process 11401 has 16.40 GiB memory in use. Process 284497 has 4.93 GiB memory in use. Of the allocated memory 4.61 GiB is allocated by PyTorch, and 46.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "id": "696f771f-8c02-4188-9768-eadf80c8bb6a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('=== Safety blend #2: b3@384_top512 + b3@448_top512 + convnext_tiny | cardinality target=4.33 ===', flush=True)\n",
        "dirs = ['out_b3_384_top512','out_b3_448_top512','out_convnext_tiny_384_top512']\n",
        "oof_f1, best_thr, Pt = blend_equal_weight(dirs, write_submission=True, out_name='submission_alt.csv', default_thr=0.50, cardinality_target=4.33)\n",
        "print('Safety blend #2 done. thr=', best_thr, 'OOF_f1=', oof_f1)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Safety blend #2: b3@384_top512 + b3@448_top512 + convnext_tiny | cardinality target=4.33 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.330, chosen thr=0.500 (pred_mean=4.332)\nWrote submission_alt.csv with thr=0.500 using 3 models\nSafety blend #2 done. thr= 0.4999999999999998 OOF_f1= None\n"
          ]
        }
      ]
    },
    {
      "id": "cf05140f-e297-47a3-b9c7-a25c7dfd93e0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('=== Weighted blends: 2:1:1 (b3_384:b3_448:convnext), prob clip->logits, narrow thr sweep via cardinality ===', flush=True)\n",
        "dirs_main = ['out_b3_384_top512','out_b3_448_top512','out_convnext_tiny_384_top512']\n",
        "\n",
        "# Main pick: match train mean ~4.42, overwrite submission.csv\n",
        "oof_f1, best_thr, Pt = blend_equal_weight(dirs_main, weights=[2,1,1], write_submission=True, out_name='submission.csv', cardinality_target=4.42)\n",
        "print('[MAIN] submission.csv thr=', best_thr, 'OOF_f1=', oof_f1)\n",
        "\n",
        "# Backup: slightly lower cardinality target ~4.36\n",
        "oof_f12, best_thr2, Pt2 = blend_equal_weight(dirs_main, weights=[2,1,1], write_submission=True, out_name='submission_weighted_alt.csv', cardinality_target=4.36)\n",
        "print('[ALT ] submission_weighted_alt.csv thr=', best_thr2, 'OOF_f1=', oof_f12)\n",
        "\n",
        "# Optional hedge: b3-only (drop convnext), 2:1 weight, at 4.42\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "oof_f1b3, thr_b3, _ = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=True, out_name='submission_b3_only.csv', cardinality_target=4.42)\n",
        "print('[B3  ] submission_b3_only.csv thr=', thr_b3, 'OOF_f1=', oof_f1b3)\n",
        "print('Weighted blending complete.')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Weighted blends: 2:1:1 (b3_384:b3_448:convnext), prob clip->logits, narrow thr sweep via cardinality ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.420, chosen thr=0.496 (pred_mean=4.403)\nWrote submission.csv with thr=0.496 using 3 models\n[MAIN] submission.csv thr= 0.496 OOF_f1= None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.360, chosen thr=0.498 (pred_mean=4.367)\nWrote submission_weighted_alt.csv with thr=0.498 using 3 models\n[ALT ] submission_weighted_alt.csv thr= 0.498 OOF_f1= None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.420, chosen thr=0.502 (pred_mean=4.406)\nWrote submission_b3_only.csv with thr=0.502 using 2 models\n[B3  ] submission_b3_only.csv thr= 0.502 OOF_f1= None\nWeighted blending complete.\n"
          ]
        }
      ]
    },
    {
      "id": "9a13d759-46a8-4972-8a37-41f83eaf92e8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== Post-process: per-group caps on weighted blend (2:1:1) ===', flush=True)\n",
        "\n",
        "# 1) Recompute weighted blend probs (do not write submission here)\n",
        "dirs_main = ['out_b3_384_top512','out_b3_448_top512','out_convnext_tiny_384_top512']\n",
        "oof_f1, best_thr, Pt = blend_equal_weight(dirs_main, weights=[2,1,1], write_submission=False, out_name='noop.csv', cardinality_target=4.42)\n",
        "print('[BLEND] thr chosen =', best_thr, 'OOF_f1=', oof_f1)\n",
        "\n",
        "# 2) Build group mapping from labels.csv prefixes\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "attr_ids = labels_df['attribute_id'].astype(int).tolist()\n",
        "attr_to_idx = {a:i for i,a in enumerate(sorted(attr_ids))}\n",
        "idx_to_attr = np.array(sorted(attr_ids), dtype=np.int32)\n",
        "idx_to_group = labels_df.set_index('attribute_id').loc[idx_to_attr, 'group'].values\n",
        "\n",
        "# Inspect available groups\n",
        "groups = pd.Series(idx_to_group).value_counts().index.tolist()\n",
        "print('[GROUPS]', groups)\n",
        "\n",
        "# 3) Define caps per group (fallback default=5)\n",
        "caps = {\n",
        "    'country': 1,\n",
        "    'culture': 1,\n",
        "    'century': 1,\n",
        "    'object_type': 1,\n",
        "    'technique': 2,\n",
        "    'material': 2,\n",
        "    'color': 1,\n",
        "    'tag': 5,\n",
        "    'tags': 5,\n",
        "    'subject': 5,\n",
        "}\n",
        "default_cap = 5\n",
        "\n",
        "# 4) Apply threshold then per-group caps\n",
        "sub = pd.read_csv('sample_submission.csv')\n",
        "ids = sub['id'].values\n",
        "rows = []\n",
        "thr = float(best_thr if best_thr is not None else 0.5)\n",
        "for i in range(len(ids)):\n",
        "    p = Pt[i]\n",
        "    cand = np.where(p >= thr)[0]\n",
        "    if cand.size == 0:\n",
        "        # ensure at least one label\n",
        "        top1 = int(np.argmax(p)); cand = np.array([top1], dtype=np.int64)\n",
        "    # group -> indices within cand\n",
        "    kept = []\n",
        "    # sort candidates by prob desc\n",
        "    cand_sorted = cand[np.argsort(-p[cand])]\n",
        "    used_per_group = {}\n",
        "    for j in cand_sorted:\n",
        "        g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "        kcap = caps.get(g, default_cap)\n",
        "        c = used_per_group.get(g, 0)\n",
        "        if c < kcap:\n",
        "            kept.append(j); used_per_group[g] = c + 1\n",
        "    if len(kept) == 0:\n",
        "        kept = [int(cand_sorted[0])]\n",
        "    pred_attr = [int(idx_to_attr[j]) for j in kept]\n",
        "    rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(x) for x in pred_attr)})\n",
        "\n",
        "sub_caps = pd.DataFrame(rows)\n",
        "sub_caps.to_csv('submission_caps.csv', index=False)\n",
        "print('Wrote submission_caps.csv with per-group caps. thr=', thr)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Post-process: per-group caps on weighted blend (2:1:1) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.420, chosen thr=0.496 (pred_mean=4.403)\n[INFO] Skipping submission write (best_thr not available).\n[BLEND] thr chosen = 0.496 OOF_f1= None\n[GROUPS] ['medium', 'tags', 'culture', 'country', 'dimension']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_caps.csv with per-group caps. thr= 0.496\n"
          ]
        }
      ]
    },
    {
      "id": "8df0ca19-91d2-4be1-ab03-cdb29ccbfebe",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shutil, os\n",
        "print('=== Set final submission to per-group capped blend ===', flush=True)\n",
        "src, dst = 'submission_caps.csv', 'submission.csv'\n",
        "assert os.path.exists(src), f'Missing {src}; run cell 26 first.'\n",
        "shutil.copyfile(src, dst)\n",
        "print(f'Copied {src} -> {dst}')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Set final submission to per-group capped blend ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied submission_caps.csv -> submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "826b7aee-6cfa-4ce1-a71c-964fc2b87198",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, os, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== Post-process: b3-only (2:1) weighted blend with per-group caps ===', flush=True)\n",
        "\n",
        "# 1) Recompute b3-only weighted blend probs (no write) with a slightly lower target (4.38)\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "oof_f1_b3, thr_b3, Pt_b3 = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=False, out_name='noop.csv', cardinality_target=4.38)\n",
        "print('[BLEND B3] thr chosen =', thr_b3, 'OOF_f1=', oof_f1_b3)\n",
        "\n",
        "# 2) Group mapping\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "attr_ids = labels_df['attribute_id'].astype(int).tolist()\n",
        "attr_to_idx = {a:i for i,a in enumerate(sorted(attr_ids))}\n",
        "idx_to_attr = np.array(sorted(attr_ids), dtype=np.int32)\n",
        "idx_to_group = labels_df.set_index('attribute_id').loc[idx_to_attr, 'group'].values\n",
        "\n",
        "# 3) Caps per group\n",
        "caps = {\n",
        "    'country': 1,\n",
        "    'culture': 1,\n",
        "    'century': 1,\n",
        "    'object_type': 1,\n",
        "    'technique': 2,\n",
        "    'material': 2,\n",
        "    'color': 1,\n",
        "    'tag': 5,\n",
        "    'tags': 5,\n",
        "    'subject': 5,\n",
        "}\n",
        "default_cap = 5\n",
        "\n",
        "# 4) Threshold then apply caps\n",
        "sub = pd.read_csv('sample_submission.csv')\n",
        "ids = sub['id'].values\n",
        "rows = []\n",
        "thr = float(thr_b3 if thr_b3 is not None else 0.5)\n",
        "for i in range(len(ids)):\n",
        "    p = Pt_b3[i]\n",
        "    cand = np.where(p >= thr)[0]\n",
        "    if cand.size == 0:\n",
        "        top1 = int(np.argmax(p)); cand = np.array([top1], dtype=np.int64)\n",
        "    cand_sorted = cand[np.argsort(-p[cand])]\n",
        "    kept, used_per_group = [], {}\n",
        "    for j in cand_sorted:\n",
        "        g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "        kcap = caps.get(g, default_cap)\n",
        "        c = used_per_group.get(g, 0)\n",
        "        if c < kcap:\n",
        "            kept.append(j); used_per_group[g] = c + 1\n",
        "    if len(kept) == 0:\n",
        "        kept = [int(cand_sorted[0])]\n",
        "    pred_attr = [int(idx_to_attr[j]) for j in kept]\n",
        "    rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(x) for x in pred_attr)})\n",
        "\n",
        "sub_b3_caps = pd.DataFrame(rows)\n",
        "sub_b3_caps.to_csv('submission_b3_caps.csv', index=False)\n",
        "print('Wrote submission_b3_caps.csv with per-group caps. thr=', thr)\n",
        "\n",
        "# 5) Make it the active submission for scoring\n",
        "shutil.copyfile('submission_b3_caps.csv', 'submission.csv')\n",
        "print('Copied submission_b3_caps.csv -> submission.csv')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Post-process: b3-only (2:1) weighted blend with per-group caps ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.380, chosen thr=0.504 (pred_mean=4.370)\n[INFO] Skipping submission write (best_thr not available).\n[BLEND B3] thr chosen = 0.504 OOF_f1= None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_b3_caps.csv with per-group caps. thr= 0.504\nCopied submission_b3_caps.csv -> submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "eafc78c0-e50c-4cd0-8d5c-a45430d8c29f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, shutil, os\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== Per-group caps v2 tuned to observed groups (medium, dimension, country, culture, tags) ===', flush=True)\n",
        "\n",
        "def make_caps_submission(model_dirs, weights, target_card, out_path):\n",
        "    # Blend to get probs\n",
        "    oof_f1, best_thr, Pt = blend_equal_weight(model_dirs, weights=weights, write_submission=False, out_name='noop.csv', cardinality_target=target_card)\n",
        "    print(f'[BLEND] thr={best_thr} target={target_card} models={model_dirs} weights={weights}')\n",
        "    # Groups\n",
        "    labels_df = pd.read_csv('labels.csv')\n",
        "    labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "    attr_ids = labels_df['attribute_id'].astype(int).tolist()\n",
        "    idx_to_attr = np.array(sorted(attr_ids), dtype=np.int32)\n",
        "    idx_to_group = labels_df.set_index('attribute_id').loc[idx_to_attr, 'group'].values\n",
        "    # Caps tuned to observed groups\n",
        "    # Seen: ['medium','tags','culture','country','dimension']\n",
        "    caps = {\n",
        "        'country': 1,\n",
        "        'culture': 1,\n",
        "        'medium': 2,        # limit materials/technique-like to 2\n",
        "        'dimension': 1,     # typically one dimension-related attr\n",
        "        'tags': 5,\n",
        "        'tag': 5,\n",
        "    }\n",
        "    default_cap = 3\n",
        "    # Threshold + caps\n",
        "    sub = pd.read_csv('sample_submission.csv')\n",
        "    ids = sub['id'].values\n",
        "    thr = float(best_thr if best_thr is not None else 0.5)\n",
        "    rows = []\n",
        "    for i in range(len(ids)):\n",
        "        p = Pt[i]\n",
        "        cand = np.where(p >= thr)[0]\n",
        "        if cand.size == 0:\n",
        "            cand = np.array([int(np.argmax(p))], dtype=np.int64)\n",
        "        cand_sorted = cand[np.argsort(-p[cand])]\n",
        "        used, kept = {}, []\n",
        "        for j in cand_sorted:\n",
        "            g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "            kcap = caps.get(g, default_cap)\n",
        "            c = used.get(g, 0)\n",
        "            if c < kcap:\n",
        "                kept.append(j); used[g] = c + 1\n",
        "        if len(kept) == 0:\n",
        "            kept = [int(cand_sorted[0])]\n",
        "        pred_attr = [int(idx_to_attr[j]) for j in kept]\n",
        "        rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(x) for x in pred_attr)})\n",
        "    sub_df = pd.DataFrame(rows)\n",
        "    sub_df.to_csv(out_path, index=False)\n",
        "    print('Wrote', out_path, 'thr=', thr)\n",
        "\n",
        "# Produce two variants quickly\n",
        "dirs_weighted = ['out_b3_384_top512','out_b3_448_top512','out_convnext_tiny_384_top512']\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "\n",
        "make_caps_submission(dirs_weighted, weights=[2,1,1], target_card=4.42, out_path='submission_caps_v2_weighted.csv')\n",
        "make_caps_submission(dirs_b3, weights=[2,1], target_card=4.38, out_path='submission_caps_v2_b3.csv')\n",
        "\n",
        "# Default active: weighted v2\n",
        "shutil.copyfile('submission_caps_v2_weighted.csv', 'submission.csv')\n",
        "print('Set submission.csv -> submission_caps_v2_weighted.csv')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Per-group caps v2 tuned to observed groups (medium, dimension, country, culture, tags) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.420, chosen thr=0.496 (pred_mean=4.403)\n[INFO] Skipping submission write (best_thr not available).\n[BLEND] thr=0.496 target=4.42 models=['out_b3_384_top512', 'out_b3_448_top512', 'out_convnext_tiny_384_top512'] weights=[2, 1, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_caps_v2_weighted.csv thr= 0.496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.380, chosen thr=0.504 (pred_mean=4.370)\n[INFO] Skipping submission write (best_thr not available).\n[BLEND] thr=0.504 target=4.38 models=['out_b3_384_top512', 'out_b3_448_top512'] weights=[2, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_caps_v2_b3.csv thr= 0.504\nSet submission.csv -> submission_caps_v2_weighted.csv\n"
          ]
        }
      ]
    },
    {
      "id": "849c2835-fe6f-4c66-875b-f78fed75c227",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== B3-only v2 caps @ target=4.42 with post-cap thr sweep to mean~4.40 ===', flush=True)\n",
        "\n",
        "# 1) Get b3-only blended probs (weights 2:1). We'll sweep thr post caps.\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "oof_f1_b3, thr_b3_base, Pt_b3 = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=False, out_name='noop.csv', cardinality_target=4.42)\n",
        "print('[BLEND B3] base thr=', thr_b3_base, 'OOF_f1=', oof_f1_b3)\n",
        "\n",
        "# 2) Mapping and v2 caps tuned to observed groups\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "attr_ids = labels_df['attribute_id'].astype(int).tolist()\n",
        "idx_to_attr = np.array(sorted(attr_ids), dtype=np.int32)\n",
        "idx_to_group = labels_df.set_index('attribute_id').loc[idx_to_attr, 'group'].values\n",
        "caps = {\n",
        "    'country': 1,\n",
        "    'culture': 1,\n",
        "    'medium': 2,\n",
        "    'dimension': 1,\n",
        "    'tags': 5,\n",
        "    'tag': 5,\n",
        "}\n",
        "default_cap = 3\n",
        "\n",
        "sub = pd.read_csv('sample_submission.csv')\n",
        "ids = sub['id'].values\n",
        "\n",
        "def apply_caps(Pt, thr):\n",
        "    rows = []\n",
        "    counts = []\n",
        "    for i in range(len(ids)):\n",
        "        p = Pt[i]\n",
        "        cand = np.where(p >= thr)[0]\n",
        "        if cand.size == 0:\n",
        "            cand = np.array([int(np.argmax(p))], dtype=np.int64)\n",
        "        cand_sorted = cand[np.argsort(-p[cand])]\n",
        "        used, kept = {}, []\n",
        "        for j in cand_sorted:\n",
        "            g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "            kcap = caps.get(g, default_cap)\n",
        "            c = used.get(g, 0)\n",
        "            if c < kcap:\n",
        "                kept.append(j); used[g] = c + 1\n",
        "        if len(kept) == 0:\n",
        "            kept = [int(cand_sorted[0])]\n",
        "        pred_attr = [int(idx_to_attr[j]) for j in kept]\n",
        "        rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(x) for x in pred_attr)})\n",
        "        counts.append(len(pred_attr))\n",
        "    return rows, float(np.mean(counts))\n",
        "\n",
        "# 3) Sweep global thr after caps to hit mean ~4.40\n",
        "thrs = np.arange(0.494, 0.5061, 0.002)\n",
        "target_mean = 4.40\n",
        "best = None\n",
        "best_rows = None\n",
        "for t in thrs:\n",
        "    rows_t, mean_t = apply_caps(Pt_b3, t)\n",
        "    delta = abs(mean_t - target_mean)\n",
        "    print(f'[SWEEP] thr={t:.3f} post-cap mean={mean_t:.3f} delta={delta:.3f}')\n",
        "    if (best is None) or (delta < best[0]):\n",
        "        best = (delta, t, mean_t)\n",
        "        best_rows = rows_t\n",
        "\n",
        "_, best_thr_cap, best_mean = best\n",
        "print(f'[CHOSEN] thr={best_thr_cap:.3f} post-cap mean={best_mean:.3f}')\n",
        "sub_df = pd.DataFrame(best_rows)\n",
        "out_path = 'submission_b3_caps_442.csv'\n",
        "sub_df.to_csv(out_path, index=False)\n",
        "shutil.copyfile(out_path, 'submission.csv')\n",
        "print('Wrote', out_path, 'and set as submission.csv')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== B3-only v2 caps @ target=4.42 with post-cap thr sweep to mean~4.40 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.420, chosen thr=0.502 (pred_mean=4.406)\n[INFO] Skipping submission write (best_thr not available).\n[BLEND B3] base thr= 0.502 OOF_f1= None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.494 post-cap mean=4.000 delta=0.400\n[SWEEP] thr=0.496 post-cap mean=3.975 delta=0.425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.498 post-cap mean=3.952 delta=0.448\n[SWEEP] thr=0.500 post-cap mean=3.929 delta=0.471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.502 post-cap mean=3.903 delta=0.497\n[SWEEP] thr=0.504 post-cap mean=3.880 delta=0.520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.506 post-cap mean=3.857 delta=0.543\n[CHOSEN] thr=0.494 post-cap mean=4.000\nWrote submission_b3_caps_442.csv and set as submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "2d3b3e17-0c07-4c07-b4b1-d10fa6aadc99",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== B3-only v2 caps @ target=4.42 with wider post-cap thr sweep to hit mean~4.40 ===', flush=True)\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "oof_f1_b3, thr_b3_base, Pt_b3 = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=False, out_name='noop.csv', cardinality_target=4.42)\n",
        "print('[BLEND B3] base thr=', thr_b3_base, 'OOF_f1=', oof_f1_b3)\n",
        "\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "attr_ids = labels_df['attribute_id'].astype(int).tolist()\n",
        "idx_to_attr = np.array(sorted(attr_ids), dtype=np.int32)\n",
        "idx_to_group = labels_df.set_index('attribute_id').loc[idx_to_attr, 'group'].values\n",
        "caps = {\n",
        "    'country': 1,\n",
        "    'culture': 1,\n",
        "    'medium': 2,\n",
        "    'dimension': 1,\n",
        "    'tags': 5,\n",
        "    'tag': 5,\n",
        "}\n",
        "default_cap = 3\n",
        "\n",
        "sub = pd.read_csv('sample_submission.csv')\n",
        "ids = sub['id'].values\n",
        "\n",
        "def apply_caps(Pt, thr):\n",
        "    rows = []; counts = []\n",
        "    for i in range(len(ids)):\n",
        "        p = Pt[i]\n",
        "        cand = np.where(p >= thr)[0]\n",
        "        if cand.size == 0:\n",
        "            cand = np.array([int(np.argmax(p))], dtype=np.int64)\n",
        "        cand_sorted = cand[np.argsort(-p[cand])]\n",
        "        used, kept = {}, []\n",
        "        for j in cand_sorted:\n",
        "            g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "            kcap = caps.get(g, default_cap)\n",
        "            c = used.get(g, 0)\n",
        "            if c < kcap:\n",
        "                kept.append(j); used[g] = c + 1\n",
        "        if len(kept) == 0:\n",
        "            kept = [int(cand_sorted[0])]\n",
        "        pred_attr = [int(idx_to_attr[j]) for j in kept]\n",
        "        rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(x) for x in pred_attr)})\n",
        "        counts.append(len(pred_attr))\n",
        "    return rows, float(np.mean(counts))\n",
        "\n",
        "# Wider sweep to reach post-cap mean ~4.40\n",
        "thrs = np.arange(0.460, 0.5201, 0.002)\n",
        "target_mean = 4.40\n",
        "best = None; best_rows = None\n",
        "for t in thrs:\n",
        "    rows_t, mean_t = apply_caps(Pt_b3, t)\n",
        "    delta = abs(mean_t - target_mean)\n",
        "    if (best is None) or (delta < best[0]):\n",
        "        best = (delta, t, mean_t); best_rows = rows_t\n",
        "    if int((t-0.460)/0.002) % 10 == 0:\n",
        "        print(f'[SWEEP] thr={t:.3f} post-cap mean={mean_t:.3f}')\n",
        "\n",
        "_, best_thr_cap, best_mean = best\n",
        "print(f'[CHOSEN] thr={best_thr_cap:.3f} post-cap mean={best_mean:.3f}')\n",
        "sub_df = pd.DataFrame(best_rows)\n",
        "out_path = 'submission_b3_caps_442_tuned.csv'\n",
        "sub_df.to_csv(out_path, index=False)\n",
        "shutil.copyfile(out_path, 'submission.csv')\n",
        "print('Wrote', out_path, 'and set as submission.csv')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== B3-only v2 caps @ target=4.42 with wider post-cap thr sweep to hit mean~4.40 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.420, chosen thr=0.502 (pred_mean=4.406)\n[INFO] Skipping submission write (best_thr not available).\n[BLEND B3] base thr= 0.502 OOF_f1= None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.460 post-cap mean=4.436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.480 post-cap mean=4.173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.502 post-cap mean=3.903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.520 post-cap mean=3.692\n[CHOSEN] thr=0.462 post-cap mean=4.409\nWrote submission_b3_caps_442_tuned.csv and set as submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "22fad772-108b-47de-9963-cf7e81046af0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== B3-only v2 caps @ target=4.38 with wider post-cap thr sweep to hit mean~4.40 ===', flush=True)\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "oof_f1_b3, thr_b3_base, Pt_b3 = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=False, out_name='noop.csv', cardinality_target=4.38)\n",
        "print('[BLEND B3] base thr=', thr_b3_base, 'OOF_f1=', oof_f1_b3)\n",
        "\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "attr_ids = labels_df['attribute_id'].astype(int).tolist()\n",
        "idx_to_attr = np.array(sorted(attr_ids), dtype=np.int32)\n",
        "idx_to_group = labels_df.set_index('attribute_id').loc[idx_to_attr, 'group'].values\n",
        "caps = {\n",
        "    'country': 1,\n",
        "    'culture': 1,\n",
        "    'medium': 2,\n",
        "    'dimension': 1,\n",
        "    'tags': 5,\n",
        "    'tag': 5,\n",
        "}\n",
        "default_cap = 3\n",
        "\n",
        "sub = pd.read_csv('sample_submission.csv')\n",
        "ids = sub['id'].values\n",
        "\n",
        "def apply_caps(Pt, thr):\n",
        "    rows = []; counts = []\n",
        "    for i in range(len(ids)):\n",
        "        p = Pt[i]\n",
        "        cand = np.where(p >= thr)[0]\n",
        "        if cand.size == 0:\n",
        "            cand = np.array([int(np.argmax(p))], dtype=np.int64)\n",
        "        cand_sorted = cand[np.argsort(-p[cand])]\n",
        "        used, kept = {}, []\n",
        "        for j in cand_sorted:\n",
        "            g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "            kcap = caps.get(g, default_cap)\n",
        "            c = used.get(g, 0)\n",
        "            if c < kcap:\n",
        "                kept.append(j); used[g] = c + 1\n",
        "        if len(kept) == 0:\n",
        "            kept = [int(cand_sorted[0])]\n",
        "        pred_attr = [int(idx_to_attr[j]) for j in kept]\n",
        "        rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(x) for x in pred_attr)})\n",
        "        counts.append(len(pred_attr))\n",
        "    return rows, float(np.mean(counts))\n",
        "\n",
        "# Wider sweep to reach post-cap mean ~4.40\n",
        "thrs = np.arange(0.460, 0.5201, 0.002)\n",
        "target_mean = 4.40\n",
        "best = None; best_rows = None\n",
        "for t in thrs:\n",
        "    rows_t, mean_t = apply_caps(Pt_b3, t)\n",
        "    delta = abs(mean_t - target_mean)\n",
        "    if (best is None) or (delta < best[0]):\n",
        "        best = (delta, t, mean_t); best_rows = rows_t\n",
        "    if int((t-0.460)/0.002) % 10 == 0:\n",
        "        print(f'[SWEEP] thr={t:.3f} post-cap mean={mean_t:.3f}')\n",
        "\n",
        "_, best_thr_cap, best_mean = best\n",
        "print(f'[CHOSEN] thr={best_thr_cap:.3f} post-cap mean={best_mean:.3f}')\n",
        "sub_df = pd.DataFrame(best_rows)\n",
        "out_path = 'submission_b3_caps_438_tuned.csv'\n",
        "sub_df.to_csv(out_path, index=False)\n",
        "shutil.copyfile(out_path, 'submission.csv')\n",
        "print('Wrote', out_path, 'and set as submission.csv')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== B3-only v2 caps @ target=4.38 with wider post-cap thr sweep to hit mean~4.40 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.380, chosen thr=0.504 (pred_mean=4.370)\n[INFO] Skipping submission write (best_thr not available).\n[BLEND B3] base thr= 0.504 OOF_f1= None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.460 post-cap mean=4.436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.480 post-cap mean=4.173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.502 post-cap mean=3.903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.520 post-cap mean=3.692\n[CHOSEN] thr=0.462 post-cap mean=4.409\nWrote submission_b3_caps_438_tuned.csv and set as submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "dfc8b82b-9a7c-432f-84ea-4fdab71cf2a3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== B3-only v2 caps @ target=4.35 with wider post-cap thr sweep to hit mean~4.40 ===', flush=True)\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "oof_f1_b3, thr_b3_base, Pt_b3 = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=False, out_name='noop.csv', cardinality_target=4.35)\n",
        "print('[BLEND B3] base thr=', thr_b3_base, 'OOF_f1=', oof_f1_b3)\n",
        "\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "attr_ids = labels_df['attribute_id'].astype(int).tolist()\n",
        "idx_to_attr = np.array(sorted(attr_ids), dtype=np.int32)\n",
        "idx_to_group = labels_df.set_index('attribute_id').loc[idx_to_attr, 'group'].values\n",
        "caps = {\n",
        "    'country': 1,\n",
        "    'culture': 1,\n",
        "    'medium': 2,\n",
        "    'dimension': 1,\n",
        "    'tags': 5,\n",
        "    'tag': 5,\n",
        "}\n",
        "default_cap = 3\n",
        "\n",
        "sub = pd.read_csv('sample_submission.csv')\n",
        "ids = sub['id'].values\n",
        "\n",
        "def apply_caps(Pt, thr):\n",
        "    rows = []; counts = []\n",
        "    for i in range(len(ids)):\n",
        "        p = Pt[i]\n",
        "        cand = np.where(p >= thr)[0]\n",
        "        if cand.size == 0:\n",
        "            cand = np.array([int(np.argmax(p))], dtype=np.int64)\n",
        "        cand_sorted = cand[np.argsort(-p[cand])]\n",
        "        used, kept = {}, []\n",
        "        for j in cand_sorted:\n",
        "            g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "            kcap = caps.get(g, default_cap)\n",
        "            c = used.get(g, 0)\n",
        "            if c < kcap:\n",
        "                kept.append(j); used[g] = c + 1\n",
        "        if len(kept) == 0:\n",
        "            kept = [int(cand_sorted[0])]\n",
        "        pred_attr = [int(idx_to_attr[j]) for j in kept]\n",
        "        rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(x) for x in pred_attr)})\n",
        "        counts.append(len(pred_attr))\n",
        "    return rows, float(np.mean(counts))\n",
        "\n",
        "# Wider sweep to reach post-cap mean ~4.40\n",
        "thrs = np.arange(0.460, 0.5201, 0.002)\n",
        "target_mean = 4.40\n",
        "best = None; best_rows = None\n",
        "for t in thrs:\n",
        "    rows_t, mean_t = apply_caps(Pt_b3, t)\n",
        "    delta = abs(mean_t - target_mean)\n",
        "    if (best is None) or (delta < best[0]):\n",
        "        best = (delta, t, mean_t); best_rows = rows_t\n",
        "    if int((t-0.460)/0.002) % 10 == 0:\n",
        "        print(f'[SWEEP] thr={t:.3f} post-cap mean={mean_t:.3f}')\n",
        "\n",
        "_, best_thr_cap, best_mean = best\n",
        "print(f'[CHOSEN] thr={best_thr_cap:.3f} post-cap mean={best_mean:.3f}')\n",
        "sub_df = pd.DataFrame(best_rows)\n",
        "out_path = 'submission_b3_caps_435_tuned.csv'\n",
        "sub_df.to_csv(out_path, index=False)\n",
        "shutil.copyfile(out_path, 'submission.csv')\n",
        "print('Wrote', out_path, 'and set as submission.csv')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== B3-only v2 caps @ target=4.35 with wider post-cap thr sweep to hit mean~4.40 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.350, chosen thr=0.506 (pred_mean=4.335)\n[INFO] Skipping submission write (best_thr not available).\n[BLEND B3] base thr= 0.506 OOF_f1= None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.460 post-cap mean=4.436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.502 post-cap mean=3.903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.520 post-cap mean=3.692\n[CHOSEN] thr=0.462 post-cap mean=4.409\nWrote submission_b3_caps_435_tuned.csv and set as submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "e59233b0-784e-4d28-b0f3-d905eb2a184c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys, subprocess, os, time, math, gc, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "print('=== Install and prepare CLIP kNN label transfer pipeline ===', flush=True)\n",
        "\n",
        "# 0) Ensure deps (open_clip_torch, faiss-cpu) are installed\n",
        "def pip(*args):\n",
        "    print('> pip', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "try:\n",
        "    import open_clip\n",
        "except Exception:\n",
        "    pip('install', 'open_clip_torch==2.26.1')\n",
        "    import open_clip\n",
        "try:\n",
        "    import faiss\n",
        "except Exception:\n",
        "    pip('install', 'faiss-cpu==1.8.0.post1')\n",
        "    import faiss\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# 1) Dataset for CLIP embedding extraction\n",
        "class ImgDataset(Dataset):\n",
        "    def __init__(self, ids, root: Path, ext: str, transform):\n",
        "        self.ids = ids\n",
        "        self.root = Path(root)\n",
        "        self.ext = ext\n",
        "        self.t = transform\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "    def __getitem__(self, i):\n",
        "        img_id = self.ids[i]\n",
        "        p = self.root / f'{img_id}{self.ext}'\n",
        "        with Image.open(p) as im:\n",
        "            im = im.convert('RGB')\n",
        "            x = self.t(im)\n",
        "        return x, i\n",
        "\n",
        "# 2) Extract CLIP embeddings for train/test and save\n",
        "def extract_clip_embeddings(model_name='ViT-B-32', pretrained='laion2b_s34b_b79k', image_size=224, batch_size=256, num_workers=8, device='cuda'):\n",
        "    import train as trn\n",
        "    base = Path('.')\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('sample_submission.csv')\n",
        "    # detect extension (png)\n",
        "    img_ext = trn.detect_ext(Path('train'), [train_df['id'].iloc[0]])\n",
        "    print('Detected ext:', img_ext, flush=True)\n",
        "\n",
        "    # Load CLIP\n",
        "    print('Loading CLIP:', model_name, pretrained, flush=True)\n",
        "    model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained, device=device)\n",
        "    model.eval()\n",
        "    # Replace preprocess with minimal equivalent to keep control over size/normalize\n",
        "    # open_clip preprocess already matches model's expected stats\n",
        "    transform = preprocess\n",
        "\n",
        "    def run_split(split_name, ids, img_root, out_path):\n",
        "        ds = ImgDataset(ids, img_root, img_ext, transform)\n",
        "        dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "        feats = np.zeros((len(ids), model.visual.output_dim), dtype=np.float32)\n",
        "        t0 = time.time()\n",
        "        seen = 0\n",
        "        with torch.no_grad():\n",
        "            for bi, (xb, idx) in enumerate(dl):\n",
        "                xb = xb.to(device, non_blocking=True)\n",
        "                with torch.cuda.amp.autocast(enabled=True):\n",
        "                    f = model.encode_image(xb)\n",
        "                f = f.float()\n",
        "                f = torch.nn.functional.normalize(f, dim=1)\n",
        "                feats[idx.numpy()] = f.cpu().numpy()\n",
        "                seen += xb.size(0)\n",
        "                if (bi+1) % 20 == 0:\n",
        "                    dt = time.time()-t0\n",
        "                    ips = seen / max(dt,1e-6)\n",
        "                    print(f'[{split_name}] {seen}/{len(ids)} in {dt/60:.1f}m ({ips:.1f} img/s)', flush=True)\n",
        "        np.save(out_path, feats)\n",
        "        print(f'[{split_name}] Saved {out_path} shape={feats.shape}', flush=True)\n",
        "        del feats; gc.collect()\n",
        "\n",
        "    # Train embeddings\n",
        "    train_ids = train_df['id'].tolist()\n",
        "    test_ids = test_df['id'].tolist()\n",
        "    if not Path('clip_train_emb.npy').exists():\n",
        "        run_split('train', train_ids, Path('train'), 'clip_train_emb.npy')\n",
        "    else:\n",
        "        print('[train] clip_train_emb.npy exists, skipping')\n",
        "    if not Path('clip_test_emb.npy').exists():\n",
        "        run_split('test', test_ids, Path('test'), 'clip_test_emb.npy')\n",
        "    else:\n",
        "        print('[test] clip_test_emb.npy exists, skipping')\n",
        "    print('CLIP embedding extraction done.')\n",
        "\n",
        "print('Ready: run this cell to install deps and define functions, then call extract_clip_embeddings() in the next cell.', flush=True)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Install and prepare CLIP kNN label transfer pipeline ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install open_clip_torch==2.26.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting open_clip_torch==2.26.1\n  Downloading open_clip_torch-2.26.1-py3-none-any.whl (1.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.5/1.5 MB 49.4 MB/s eta 0:00:00\nCollecting tqdm\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 416.6 MB/s eta 0:00:00\nCollecting ftfy\n  Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.8/44.8 KB 235.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface-hub\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 563.3/563.3 KB 280.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex\n  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 KB 133.2 MB/s eta 0:00:00\nCollecting torchvision\n  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8.6/8.6 MB 320.8 MB/s eta 0:00:00\nCollecting torch>=1.9.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 888.1/888.1 MB 320.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n  Downloading timm-1.0.20-py3-none-any.whl (2.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.5/2.5 MB 368.4 MB/s eta 0:00:00\nCollecting nvidia-cufile-cu12==1.13.1.3\n  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.2/1.2 MB 259.3 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.9.90\n  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 63.6/63.6 MB 272.3 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.5.8.93\n  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 288.2/288.2 MB 320.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.3.3.83\n  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 193.1/193.1 MB 331.3 MB/s eta 0:00:00\nCollecting typing-extensions>=4.10.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 421.1 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.8.90\n  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10.2/10.2 MB 301.5 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu12==2.27.3\n  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 322.4/322.4 MB 287.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 510.6 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.8.4.1\n  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 594.3/594.3 MB 299.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.7.3.90\n  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 267.5/267.5 MB 369.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparselt-cu12==0.7.1\n  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 287.2/287.2 MB 495.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.8.90\n  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 954.8/954.8 KB 417.0 MB/s eta 0:00:00\nCollecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 475.7 MB/s eta 0:00:00\nCollecting nvidia-nvjitlink-cu12==12.8.93\n  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.3/39.3 MB 323.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93\n  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 88.0/88.0 MB 407.8 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.10.2.21\n  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 706.8/706.8 MB 265.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.8.90\n  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 90.0/90.0 KB 350.9 MB/s eta 0:00:00\nCollecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 493.8 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting triton==3.4.0\n  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 155.5/155.5 MB 157.7 MB/s eta 0:00:00\nCollecting sympy>=1.13.3\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 487.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting setuptools>=40.8.0\n  Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.2/1.2 MB 536.6 MB/s eta 0:00:00\nCollecting wcwidth\n  Downloading wcwidth-0.2.14-py2.py3-none-any.whl (37 kB)\nCollecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 452.8 MB/s eta 0:00:00\nCollecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 563.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyyaml>=5.1\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 539.2 MB/s eta 0:00:00\nCollecting packaging>=20.9\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 431.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 541.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 565.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 317.7 MB/s eta 0:00:00\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 545.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\nCollecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 515.2 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 393.3 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 443.3 MB/s eta 0:00:00\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 505.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: Operation cancelled by user\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopen_clip\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'open_clip'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopen_clip\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[43mpip\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minstall\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mopen_clip_torch==2.26.1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopen_clip\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mpip\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpip\u001b[39m(*args):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m> pip\u001b[39m\u001b[33m'\u001b[39m, *args, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-m\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/subprocess.py:548\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    547\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    550\u001b[39m         process.kill()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/subprocess.py:1197\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1195\u001b[39m         stderr = \u001b[38;5;28mself\u001b[39m.stderr.read()\n\u001b[32m   1196\u001b[39m         \u001b[38;5;28mself\u001b[39m.stderr.close()\n\u001b[32m-> \u001b[39m\u001b[32m1197\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1199\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/subprocess.py:1260\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1258\u001b[39m     endtime = _time() + timeout\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1260\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1262\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1263\u001b[39m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[32m   1264\u001b[39m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[32m   1265\u001b[39m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[32m   1266\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/subprocess.py:1995\u001b[39m, in \u001b[36mPopen._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.returncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1994\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1995\u001b[39m (pid, sts) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1996\u001b[39m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[32m   1997\u001b[39m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[32m   1998\u001b[39m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[32m   1999\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pid == \u001b[38;5;28mself\u001b[39m.pid:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/subprocess.py:1953\u001b[39m, in \u001b[36mPopen._try_wait\u001b[39m\u001b[34m(self, wait_flags)\u001b[39m\n\u001b[32m   1951\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[32m   1952\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1953\u001b[39m     (pid, sts) = os.waitpid(\u001b[38;5;28mself\u001b[39m.pid, wait_flags)\n\u001b[32m   1954\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[32m   1955\u001b[39m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[32m   1956\u001b[39m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[32m   1957\u001b[39m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[32m   1958\u001b[39m     pid = \u001b[38;5;28mself\u001b[39m.pid\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "1b00ae05-c888-4b5e-abda-6019e6d9ac89",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys, subprocess, os, time, math, gc, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "print('=== Safe install for CLIP kNN (no torch upgrade) and define extract ===', flush=True)\n",
        "\n",
        "def pip(*args):\n",
        "    print('> pip', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "# 1) Install open_clip without deps to avoid torch override; then install minimal deps under constraints\n",
        "try:\n",
        "    import open_clip\n",
        "    print('open_clip already available')\n",
        "except Exception:\n",
        "    assert Path('constraints.txt').exists(), 'constraints.txt missing'\n",
        "    pip('install', '--no-deps', 'open_clip_torch==2.26.1')\n",
        "    # Minimal deps; honor torch constraints to prevent upgrades\n",
        "    pip('install', '-c', 'constraints.txt', 'ftfy==6.3.1', 'tqdm', 'huggingface-hub==0.35.1', 'regex', 'safetensors', 'pillow', '--upgrade-strategy', 'only-if-needed')\n",
        "    import open_clip  # noqa\n",
        "\n",
        "try:\n",
        "    import faiss  # noqa\n",
        "    print('faiss available')\n",
        "except Exception:\n",
        "    pip('install', 'faiss-cpu==1.8.0.post1')\n",
        "    import faiss  # noqa\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ImgDataset(Dataset):\n",
        "    def __init__(self, ids, root: Path, ext: str, transform):\n",
        "        self.ids = ids\n",
        "        self.root = Path(root)\n",
        "        self.ext = ext\n",
        "        self.t = transform\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "    def __getitem__(self, i):\n",
        "        img_id = self.ids[i]\n",
        "        p = self.root / f'{img_id}{self.ext}'\n",
        "        with Image.open(p) as im:\n",
        "            im = im.convert('RGB')\n",
        "            x = self.t(im)\n",
        "        return x, i\n",
        "\n",
        "def extract_clip_embeddings(model_name='ViT-B-32', pretrained='laion2b_s34b_b79k', batch_size=256, num_workers=8, device='cuda'):\n",
        "    import open_clip\n",
        "    import train as trn\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('sample_submission.csv')\n",
        "    img_ext = trn.detect_ext(Path('train'), [train_df['id'].iloc[0]])\n",
        "    print('Detected ext:', img_ext, flush=True)\n",
        "    print('Loading CLIP:', model_name, pretrained, flush=True)\n",
        "    model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained, device=device)\n",
        "    model.eval()\n",
        "    transform = preprocess\n",
        "\n",
        "    def run_split(split_name, ids, img_root, out_path):\n",
        "        dl = DataLoader(ImgDataset(ids, img_root, img_ext, transform), batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "        feats = np.zeros((len(ids), model.visual.output_dim), dtype=np.float32)\n",
        "        t0 = time.time(); seen = 0\n",
        "        with torch.no_grad():\n",
        "            for bi, (xb, idx) in enumerate(dl):\n",
        "                xb = xb.to(device, non_blocking=True)\n",
        "                with torch.cuda.amp.autocast(enabled=True):\n",
        "                    f = model.encode_image(xb)\n",
        "                f = torch.nn.functional.normalize(f.float(), dim=1)\n",
        "                feats[idx.numpy()] = f.cpu().numpy()\n",
        "                seen += xb.size(0)\n",
        "                if (bi+1) % 20 == 0:\n",
        "                    dt = time.time()-t0\n",
        "                    print(f'[{split_name}] {seen}/{len(ids)} ({seen/max(dt,1e-6):.1f} img/s) elapsed {dt/60:.1f}m', flush=True)\n",
        "        np.save(out_path, feats)\n",
        "        print(f'[{split_name}] Saved {out_path} shape={feats.shape}', flush=True)\n",
        "        del feats; gc.collect()\n",
        "\n",
        "    train_ids = train_df['id'].tolist()\n",
        "    test_ids = test_df['id'].tolist()\n",
        "    if not Path('clip_train_emb.npy').exists():\n",
        "        run_split('train', train_ids, Path('train'), 'clip_train_emb.npy')\n",
        "    else:\n",
        "        print('[train] clip_train_emb.npy exists, skipping')\n",
        "    if not Path('clip_test_emb.npy').exists():\n",
        "        run_split('test', test_ids, Path('test'), 'clip_test_emb.npy')\n",
        "    else:\n",
        "        print('[test] clip_test_emb.npy exists, skipping')\n",
        "    print('CLIP embedding extraction done.')\n",
        "\n",
        "print('Safe installer ready. Next: run extract_clip_embeddings() to generate CLIP embeddings, then build FAISS kNN and blend.', flush=True)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Safe install for CLIP kNN (no torch upgrade) and define extract ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install --no-deps open_clip_torch==2.26.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting open_clip_torch==2.26.1\n  Downloading open_clip_torch-2.26.1-py3-none-any.whl (1.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.5/1.5 MB 55.6 MB/s eta 0:00:00\nInstalling collected packages: open_clip_torch\nSuccessfully installed open_clip_torch-2.26.1\n> pip install -c constraints.txt ftfy==6.3.1 tqdm huggingface-hub==0.35.1 regex safetensors pillow --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy==6.3.1\n  Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.8/44.8 KB 3.3 MB/s eta 0:00:00\nCollecting tqdm\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 25.6 MB/s eta 0:00:00\nCollecting huggingface-hub==0.35.1\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 563.3/563.3 KB 82.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex\n  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 KB 450.7 MB/s eta 0:00:00\nCollecting safetensors\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 294.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 443.1 MB/s eta 0:00:00\nCollecting wcwidth\n  Downloading wcwidth-0.2.14-py2.py3-none-any.whl (37 kB)\nCollecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 478.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 414.0 MB/s eta 0:00:00\nCollecting pyyaml>=5.1\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 505.9 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting typing-extensions>=3.7.4.3\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 403.5 MB/s eta 0:00:00\nCollecting fsspec>=2023.5.0\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 488.5 MB/s eta 0:00:00\nCollecting packaging>=20.9\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 401.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 465.7 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 404.0 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 441.4 MB/s eta 0:00:00\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 469.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: wcwidth, urllib3, typing-extensions, tqdm, safetensors, regex, pyyaml, pillow, packaging, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, requests, ftfy, huggingface-hub\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed certifi-2025.8.3 charset_normalizer-3.4.3 filelock-3.19.1 fsspec-2025.9.0 ftfy-6.3.1 hf-xet-1.1.10 huggingface-hub-0.35.1 idna-3.10 packaging-25.0 pillow-11.3.0 pyyaml-6.0.3 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 tqdm-4.67.1 typing-extensions-4.15.0 urllib3-2.5.0 wcwidth-0.2.14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu==1.8.0.post1\n  Downloading faiss_cpu-1.8.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 27.0/27.0 MB 179.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<2.0,>=1.0\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 260.0 MB/s eta 0:00:00\nCollecting packaging\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 396.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: packaging, numpy, faiss-cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed faiss-cpu-1.8.0.post1 numpy-1.26.4 packaging-25.0\nSafe installer ready. Next: run extract_clip_embeddings() to generate CLIP embeddings, then build FAISS kNN and blend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging-25.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        }
      ]
    },
    {
      "id": "3f97cd0b-15a4-44be-9b18-cc4f3be0ae7b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('=== Extracting CLIP embeddings (ViT-B/32 @224, BS=256) for kNN label transfer ===', flush=True)\n",
        "try:\n",
        "    extract_clip_embeddings(model_name='ViT-B-32', pretrained='laion2b_s34b_b79k', batch_size=256, num_workers=8, device='cuda')\n",
        "except Exception as e:\n",
        "    print('CLIP extraction failed:', e)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Extracting CLIP embeddings (ViT-B/32 @224, BS=256) for kNN label transfer ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected ext: .png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CLIP: ViT-B-32 laion2b_s34b_b79k\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/open_clip/factory.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_107/1345938480.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 5120/120801 (1150.8 img/s) elapsed 0.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 10240/120801 (1457.1 img/s) elapsed 0.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 15360/120801 (1469.3 img/s) elapsed 0.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 20480/120801 (1562.4 img/s) elapsed 0.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 25600/120801 (1554.2 img/s) elapsed 0.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 30720/120801 (1601.4 img/s) elapsed 0.3m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 35840/120801 (1596.7 img/s) elapsed 0.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 40960/120801 (1626.7 img/s) elapsed 0.4m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 46080/120801 (1617.7 img/s) elapsed 0.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 51200/120801 (1641.7 img/s) elapsed 0.5m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 56320/120801 (1633.5 img/s) elapsed 0.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 61440/120801 (1651.5 img/s) elapsed 0.6m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 66560/120801 (1643.1 img/s) elapsed 0.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 71680/120801 (1658.2 img/s) elapsed 0.7m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 76800/120801 (1652.7 img/s) elapsed 0.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 81920/120801 (1668.3 img/s) elapsed 0.8m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 87040/120801 (1661.6 img/s) elapsed 0.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 92160/120801 (1674.6 img/s) elapsed 0.9m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 97280/120801 (1670.3 img/s) elapsed 1.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 102400/120801 (1679.1 img/s) elapsed 1.0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 107520/120801 (1677.2 img/s) elapsed 1.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 112640/120801 (1680.3 img/s) elapsed 1.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] 117760/120801 (1681.7 img/s) elapsed 1.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] Saved clip_train_emb.npy shape=(120801, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test] 5120/21318 (1135.0 img/s) elapsed 0.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test] 10240/21318 (1423.9 img/s) elapsed 0.1m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test] 15360/21318 (1487.9 img/s) elapsed 0.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test] 20480/21318 (1581.3 img/s) elapsed 0.2m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test] Saved clip_test_emb.npy shape=(21318, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP embedding extraction done.\n"
          ]
        }
      ]
    },
    {
      "id": "176ff616-b879-41b5-b515-3086cd6aafae",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, os, time, faiss, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== kNN label transfer with CLIP + b3-only logit blend + v2 caps + tuned threshold ===', flush=True)\n",
        "\n",
        "# 0) Preconditions: embeddings must exist\n",
        "train_emb_p = Path('clip_train_emb.npy')\n",
        "test_emb_p = Path('clip_test_emb.npy')\n",
        "if not (train_emb_p.exists() and test_emb_p.exists()):\n",
        "    print('[SKIP] Missing CLIP embeddings. Run extract_clip_embeddings() first.')\n",
        "else:\n",
        "    t0 = time.time()\n",
        "    Xtr = np.load(train_emb_p).astype(np.float32)\n",
        "    Xte = np.load(test_emb_p).astype(np.float32)\n",
        "    # Assumed already L2-normalized in extractor\n",
        "    print('Loaded embeddings:', Xtr.shape, Xte.shape, flush=True)\n",
        "\n",
        "    # 1) Build FAISS index (cosine via inner-product on normalized vectors)\n",
        "    d = Xtr.shape[1]\n",
        "    index = faiss.IndexFlatIP(d)\n",
        "    index.add(Xtr)\n",
        "    print('FAISS index built. nt=', index.ntotal, 'dim=', d, flush=True)\n",
        "\n",
        "    # 2) kNN search\n",
        "    k = 20\n",
        "    sims, nn_idx = index.search(Xte, k)  # sims in [-1,1] due to cosine\n",
        "    sims = np.maximum(sims, 0.0).astype(np.float32)  # clamp negatives to 0\n",
        "    print('kNN done. sims shape:', sims.shape, 'idx shape:', nn_idx.shape, flush=True)\n",
        "\n",
        "    # 3) Build train label matrix (CSR-like via manual aggregation)\n",
        "    labels_df = pd.read_csv('labels.csv')\n",
        "    attr_ids_sorted = np.array(sorted(labels_df['attribute_id'].astype(int).unique().tolist()), dtype=np.int32)\n",
        "    attr_to_col = {a:i for i,a in enumerate(attr_ids_sorted)}\n",
        "    C = len(attr_ids_sorted)\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    n = len(train_df)\n",
        "    # Prepare list of sets for memory efficiency during gather\n",
        "    lab_sets = []\n",
        "    for s in train_df['attribute_ids'].fillna('').astype(str).tolist():\n",
        "        if s:\n",
        "            lab_sets.append([attr_to_col[int(x)] for x in s.split() if x!='' and int(x) in attr_to_col])\n",
        "        else:\n",
        "            lab_sets.append([])\n",
        "    print('Built train label indices (lists).', flush=True)\n",
        "\n",
        "    # 4) Compute kNN label probabilities for test: weighted average over neighbors\n",
        "    ntest = Xte.shape[0]\n",
        "    probs_knn = np.zeros((ntest, C), dtype=np.float32)\n",
        "    wsum = sims.sum(axis=1, keepdims=True) + 1e-8\n",
        "    for i in range(ntest):\n",
        "        nn = nn_idx[i]\n",
        "        ws = sims[i] / wsum[i]\n",
        "        # accumulate weighted labels\n",
        "        for nbr, w in zip(nn, ws):\n",
        "            for c in lab_sets[nbr]:\n",
        "                probs_knn[i, c] += float(w)\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print(f'[kNN] processed {i+1}/{ntest}', flush=True)\n",
        "    print('kNN label transfer complete.', flush=True)\n",
        "\n",
        "    # 5) Get b3-only model probs on test via our blender\n",
        "    def sigmoid(x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "    def probs_to_logits(p, eps=1e-5):\n",
        "        p = np.clip(p, eps, 1.0 - eps)\n",
        "        return np.log(p / (1.0 - p))\n",
        "    from __main__ import blend_equal_weight  # defined earlier in this notebook\n",
        "    dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "    _, _, Pt_b3 = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=False, out_name='noop.csv', cardinality_target=4.40)\n",
        "    print('Blender produced model probs:', None if Pt_b3 is None else Pt_b3.shape, flush=True)\n",
        "\n",
        "    # 6) Logit blend: z_final = z_model + alpha * z_knn\n",
        "    alpha = 0.5\n",
        "    Zm = probs_to_logits(Pt_b3)\n",
        "    Zk = probs_to_logits(probs_knn)\n",
        "    Zf = Zm + alpha * Zk\n",
        "    Pf = sigmoid(Zf)\n",
        "    del Zm, Zk, Zf;\n",
        "\n",
        "    # 7) v2 caps + post-cap threshold sweep to target mean ~4.40\n",
        "    labels_df = pd.read_csv('labels.csv')\n",
        "    labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "    idx_to_group = labels_df.set_index('attribute_id').loc[attr_ids_sorted, 'group'].values\n",
        "    caps = {\n",
        "        'country': 1,\n",
        "        'culture': 1,\n",
        "        'medium': 2,\n",
        "        'dimension': 1,\n",
        "        'tags': 5,\n",
        "        'tag': 5,\n",
        "    }\n",
        "    default_cap = 3\n",
        "    sub = pd.read_csv('sample_submission.csv')\n",
        "    ids = sub['id'].values\n",
        "\n",
        "    def apply_caps(Pt, thr):\n",
        "        rows = []; counts = []\n",
        "        for i in range(Pt.shape[0]):\n",
        "            p = Pt[i]\n",
        "            cand = np.where(p >= thr)[0]\n",
        "            if cand.size == 0:\n",
        "                cand = np.array([int(np.argmax(p))], dtype=np.int64)\n",
        "            cand_sorted = cand[np.argsort(-p[cand])]\n",
        "            used, kept = {}, []\n",
        "            for j in cand_sorted:\n",
        "                g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "                kcap = caps.get(g, default_cap)\n",
        "                c = used.get(g, 0)\n",
        "                if c < kcap:\n",
        "                    kept.append(j); used[g] = c + 1\n",
        "            if len(kept) == 0:\n",
        "                kept = [int(cand_sorted[0])]\n",
        "            pred_attr = [int(attr_ids_sorted[j]) for j in kept]\n",
        "            rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(x) for x in pred_attr)})\n",
        "            counts.append(len(pred_attr))\n",
        "        return rows, float(np.mean(counts))\n",
        "\n",
        "    thrs = np.arange(0.460, 0.5201, 0.002)\n",
        "    target_mean = 4.40\n",
        "    best = None; best_rows = None\n",
        "    for t in thrs:\n",
        "        rows_t, mean_t = apply_caps(Pf, t)\n",
        "        delta = abs(mean_t - target_mean)\n",
        "        if (best is None) or (delta < best[0]):\n",
        "            best = (delta, t, mean_t); best_rows = rows_t\n",
        "        # periodic log\n",
        "        if int((t-0.460)/0.002) % 10 == 0:\n",
        "            print(f'[SWEEP] thr={t:.3f} post-cap mean={mean_t:.3f}')\n",
        "    _, best_thr_cap, best_mean = best\n",
        "    print(f'[CHOSEN] thr={best_thr_cap:.3f} post-cap mean={best_mean:.3f}')\n",
        "    sub_df = pd.DataFrame(best_rows)\n",
        "    out_path = 'submission_knn_blend_tuned.csv'\n",
        "    sub_df.to_csv(out_path, index=False)\n",
        "    shutil.copyfile(out_path, 'submission.csv')\n",
        "    print('Wrote', out_path, 'and set as submission.csv. Elapsed:', round((time.time()-t0)/60,1),'min', flush=True)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== kNN label transfer with CLIP + b3-only logit blend + v2 caps + tuned threshold ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded embeddings: (120801, 512) (21318, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index built. nt= 120801 dim= 512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kNN done. sims shape: (21318, 20) idx shape: (21318, 20)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built train label indices (lists).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] processed 2000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] processed 4000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] processed 6000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] processed 8000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] processed 10000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] processed 12000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] processed 14000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] processed 16000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] processed 18000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] processed 20000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kNN label transfer complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.400, chosen thr=0.502 (pred_mean=4.406)\n[INFO] Skipping submission write (best_thr not available).\nBlender produced model probs: (21318, 3474)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.460 post-cap mean=3.097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.480 post-cap mean=2.995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.502 post-cap mean=2.888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.520 post-cap mean=2.804\n[CHOSEN] thr=0.460 post-cap mean=3.097\nWrote submission_knn_blend_tuned.csv and set as submission.csv. Elapsed: 1.2 min\n"
          ]
        }
      ]
    },
    {
      "id": "f17a4287-0b21-43d9-82a4-bf01bccfba1f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== B3-only v2 caps @ target=4.38, weights 1:1, wide post-cap thr sweep to hit mean~4.40 ===', flush=True)\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "oof_f1_b3, thr_b3_base, Pt_b3 = blend_equal_weight(dirs_b3, weights=[1,1], write_submission=False, out_name='noop.csv', cardinality_target=4.38)\n",
        "print('[BLEND B3 1:1] base thr=', thr_b3_base, 'OOF_f1=', oof_f1_b3)\n",
        "\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "attr_ids = labels_df['attribute_id'].astype(int).tolist()\n",
        "idx_to_attr = np.array(sorted(attr_ids), dtype=np.int32)\n",
        "idx_to_group = labels_df.set_index('attribute_id').loc[idx_to_attr, 'group'].values\n",
        "caps = {\n",
        "    'country': 1,\n",
        "    'culture': 1,\n",
        "    'medium': 2,\n",
        "    'dimension': 1,\n",
        "    'tags': 5,\n",
        "    'tag': 5,\n",
        "}\n",
        "default_cap = 3\n",
        "\n",
        "sub = pd.read_csv('sample_submission.csv')\n",
        "ids = sub['id'].values\n",
        "\n",
        "def apply_caps(Pt, thr):\n",
        "    rows = []; counts = []\n",
        "    for i in range(len(ids)):\n",
        "        p = Pt[i]\n",
        "        cand = np.where(p >= thr)[0]\n",
        "        if cand.size == 0:\n",
        "            cand = np.array([int(np.argmax(p))], dtype=np.int64)\n",
        "        cand_sorted = cand[np.argsort(-p[cand])]\n",
        "        used, kept = {}, []\n",
        "        for j in cand_sorted:\n",
        "            g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "            kcap = caps.get(g, default_cap)\n",
        "            c = used.get(g, 0)\n",
        "            if c < kcap:\n",
        "                kept.append(j); used[g] = c + 1\n",
        "        if len(kept) == 0:\n",
        "            kept = [int(cand_sorted[0])]\n",
        "        pred_attr = [int(idx_to_attr[j]) for j in kept]\n",
        "        rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(x) for x in pred_attr)})\n",
        "        counts.append(len(pred_attr))\n",
        "    return rows, float(np.mean(counts))\n",
        "\n",
        "# Wide sweep to reach post-cap mean ~4.40\n",
        "thrs = np.arange(0.460, 0.5201, 0.002)\n",
        "target_mean = 4.40\n",
        "best = None; best_rows = None\n",
        "for t in thrs:\n",
        "    rows_t, mean_t = apply_caps(Pt_b3, t)\n",
        "    delta = abs(mean_t - target_mean)\n",
        "    if (best is None) or (delta < best[0]):\n",
        "        best = (delta, t, mean_t); best_rows = rows_t\n",
        "    if int((t-0.460)/0.002) % 10 == 0:\n",
        "        print(f'[SWEEP] thr={t:.3f} post-cap mean={mean_t:.3f}')\n",
        "\n",
        "_, best_thr_cap, best_mean = best\n",
        "print(f'[CHOSEN] thr={best_thr_cap:.3f} post-cap mean={best_mean:.3f}')\n",
        "sub_df = pd.DataFrame(best_rows)\n",
        "out_path = 'submission_b3_caps_438_w11_tuned.csv'\n",
        "sub_df.to_csv(out_path, index=False)\n",
        "shutil.copyfile(out_path, 'submission.csv')\n",
        "print('Wrote', out_path, 'and set as submission.csv')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== B3-only v2 caps @ target=4.38, weights 1:1, wide post-cap thr sweep to hit mean~4.40 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.380, chosen thr=0.504 (pred_mean=4.394)\n[INFO] Skipping submission write (best_thr not available).\n[BLEND B3 1:1] base thr= 0.504 OOF_f1= None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.460 post-cap mean=4.456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.480 post-cap mean=4.197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.502 post-cap mean=3.922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.520 post-cap mean=3.711\n[CHOSEN] thr=0.464 post-cap mean=4.400\nWrote submission_b3_caps_438_w11_tuned.csv and set as submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "80bfc1e5-1299-471f-9dfd-ce2471ffeccb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, os, time, faiss, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== kNN label transfer v2: k=50, alpha=0.3, b3-only blend, v2 caps, tuned threshold ===', flush=True)\n",
        "\n",
        "train_emb_p = Path('clip_train_emb.npy')\n",
        "test_emb_p = Path('clip_test_emb.npy')\n",
        "if not (train_emb_p.exists() and test_emb_p.exists()):\n",
        "    print('[SKIP] Missing CLIP embeddings.')\n",
        "else:\n",
        "    t0 = time.time()\n",
        "    Xtr = np.load(train_emb_p).astype(np.float32)\n",
        "    Xte = np.load(test_emb_p).astype(np.float32)\n",
        "    print('Loaded embeddings:', Xtr.shape, Xte.shape, flush=True)\n",
        "\n",
        "    d = Xtr.shape[1]\n",
        "    index = faiss.IndexFlatIP(d)\n",
        "    index.add(Xtr)\n",
        "    print('FAISS index built. nt=', index.ntotal, 'dim=', d, flush=True)\n",
        "\n",
        "    k = 50\n",
        "    sims, nn_idx = index.search(Xte, k)\n",
        "    sims = np.maximum(sims, 0.0).astype(np.float32)\n",
        "    print('kNN done. sims shape:', sims.shape, 'idx shape:', nn_idx.shape, flush=True)\n",
        "\n",
        "    labels_df = pd.read_csv('labels.csv')\n",
        "    attr_ids_sorted = np.array(sorted(labels_df['attribute_id'].astype(int).unique().tolist()), dtype=np.int32)\n",
        "    attr_to_col = {a:i for i,a in enumerate(attr_ids_sorted)}\n",
        "    C = len(attr_ids_sorted)\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    lab_lists = []\n",
        "    for s in train_df['attribute_ids'].fillna('').astype(str).tolist():\n",
        "        if s:\n",
        "            lab_lists.append([attr_to_col[int(x)] for x in s.split() if x!='' and int(x) in attr_to_col])\n",
        "        else:\n",
        "            lab_lists.append([])\n",
        "    print('Built train label lists.', flush=True)\n",
        "\n",
        "    ntest = Xte.shape[0]\n",
        "    probs_knn = np.zeros((ntest, C), dtype=np.float32)\n",
        "    wsum = sims.sum(axis=1, keepdims=True) + 1e-8\n",
        "    for i in range(ntest):\n",
        "        nn = nn_idx[i]\n",
        "        ws = sims[i] / wsum[i]\n",
        "        for nbr, w in zip(nn, ws):\n",
        "            for c in lab_lists[nbr]:\n",
        "                probs_knn[i, c] += float(w)\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print(f'[kNN] {i+1}/{ntest}', flush=True)\n",
        "\n",
        "    def sigmoid(x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "    def probs_to_logits(p, eps=1e-5):\n",
        "        p = np.clip(p, eps, 1.0 - eps)\n",
        "        return np.log(p / (1.0 - p))\n",
        "\n",
        "    from __main__ import blend_equal_weight\n",
        "    dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "    _, _, Pt_b3 = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=False, out_name='noop.csv', cardinality_target=4.40)\n",
        "    print('Model probs shape:', None if Pt_b3 is None else Pt_b3.shape, flush=True)\n",
        "\n",
        "    alpha = 0.3\n",
        "    Zm = probs_to_logits(Pt_b3)\n",
        "    Zk = probs_to_logits(probs_knn)\n",
        "    Pf = sigmoid(Zm + alpha * Zk)\n",
        "    del Zm, Zk\n",
        "\n",
        "    labels_df = pd.read_csv('labels.csv')\n",
        "    labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "    idx_to_group = labels_df.set_index('attribute_id').loc[attr_ids_sorted, 'group'].values\n",
        "    caps = {\n",
        "        'country': 1,\n",
        "        'culture': 1,\n",
        "        'medium': 2,\n",
        "        'dimension': 1,\n",
        "        'tags': 5,\n",
        "        'tag': 5,\n",
        "    }\n",
        "    default_cap = 3\n",
        "    sub = pd.read_csv('sample_submission.csv')\n",
        "    ids = sub['id'].values\n",
        "\n",
        "    def apply_caps(Pt, thr):\n",
        "        rows = []; counts = []\n",
        "        for i in range(Pt.shape[0]):\n",
        "            p = Pt[i]\n",
        "            cand = np.where(p >= thr)[0]\n",
        "            if cand.size == 0:\n",
        "                cand = np.array([int(np.argmax(p))], dtype=np.int64)\n",
        "            cand_sorted = cand[np.argsort(-p[cand])]\n",
        "            used, kept = {}, []\n",
        "            for j in cand_sorted:\n",
        "                g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "                kcap = caps.get(g, default_cap)\n",
        "                c = used.get(g, 0)\n",
        "                if c < kcap:\n",
        "                    kept.append(j); used[g] = c + 1\n",
        "            if len(kept) == 0:\n",
        "                kept = [int(cand_sorted[0])]\n",
        "            pred_attr = [int(attr_ids_sorted[j]) for j in kept]\n",
        "            rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(x) for x in pred_attr)})\n",
        "            counts.append(len(pred_attr))\n",
        "        return rows, float(np.mean(counts))\n",
        "\n",
        "    thrs = np.arange(0.440, 0.5401, 0.002)\n",
        "    target_mean = 4.40\n",
        "    best = None; best_rows = None\n",
        "    for t in thrs:\n",
        "        rows_t, mean_t = apply_caps(Pf, t)\n",
        "        delta = abs(mean_t - target_mean)\n",
        "        if (best is None) or (delta < best[0]):\n",
        "            best = (delta, t, mean_t); best_rows = rows_t\n",
        "        if int((t-0.440)/0.002) % 15 == 0:\n",
        "            print(f'[SWEEP] thr={t:.3f} post-cap mean={mean_t:.3f}')\n",
        "\n",
        "    _, best_thr_cap, best_mean = best\n",
        "    print(f'[CHOSEN] thr={best_thr_cap:.3f} post-cap mean={best_mean:.3f}')\n",
        "    sub_df = pd.DataFrame(best_rows)\n",
        "    out_path = 'submission_knn_blend_tuned_v2.csv'\n",
        "    sub_df.to_csv(out_path, index=False)\n",
        "    shutil.copyfile(out_path, 'submission.csv')\n",
        "    print('Wrote', out_path, 'and set as submission.csv. Elapsed:', round((time.time()-t0)/60,1), 'min', flush=True)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== kNN label transfer v2: k=50, alpha=0.3, b3-only blend, v2 caps, tuned threshold ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded embeddings: (120801, 512) (21318, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index built. nt= 120801 dim= 512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kNN done. sims shape: (21318, 50) idx shape: (21318, 50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built train label lists.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] 2000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] 4000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] 6000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] 8000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] 10000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] 12000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] 14000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] 16000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] 18000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN] 20000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.400, chosen thr=0.502 (pred_mean=4.406)\n[INFO] Skipping submission write (best_thr not available).\nModel probs shape: (21318, 3474)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.440 post-cap mean=3.431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.470 post-cap mean=3.231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.500 post-cap mean=3.040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.530 post-cap mean=2.860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CHOSEN] thr=0.440 post-cap mean=3.431\nWrote submission_knn_blend_tuned_v2.csv and set as submission.csv. Elapsed: 1.4 min\n"
          ]
        }
      ]
    },
    {
      "id": "8f572ec9-85de-4138-8200-de15406137e0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, os, time, faiss, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== kNN label transfer v3: softmax-weighted (tau=0.10) + IDF^0.5 + per-neighbor |L|^-1 + prob-blend beta=0.10 + caps + min-total ===', flush=True)\n",
        "\n",
        "t0 = time.time()\n",
        "train_emb_p = Path('clip_train_emb.npy')\n",
        "test_emb_p = Path('clip_test_emb.npy')\n",
        "assert train_emb_p.exists() and test_emb_p.exists(), 'Missing CLIP embeddings; run extraction first.'\n",
        "Xtr = np.load(train_emb_p).astype(np.float32)\n",
        "Xte = np.load(test_emb_p).astype(np.float32)\n",
        "print('Loaded embeddings:', Xtr.shape, Xte.shape, flush=True)\n",
        "\n",
        "# Build FAISS IP (cosine on L2-normalized feats)\n",
        "d = Xtr.shape[1]\n",
        "index = faiss.IndexFlatIP(d)\n",
        "index.add(Xtr)\n",
        "print('FAISS index built. nt=', index.ntotal, 'dim=', d, flush=True)\n",
        "\n",
        "# Parameters per expert\n",
        "k = 100\n",
        "tau = 0.10\n",
        "eta = 1.0  # per-neighbor normalization by |L_j|^eta\n",
        "gamma_idf = 0.5\n",
        "\n",
        "# kNN search\n",
        "sims, nn_idx = index.search(Xte, k)  # cosine similarities\n",
        "print('kNN done. sims shape:', sims.shape, 'idx shape:', nn_idx.shape, flush=True)\n",
        "\n",
        "# Labels and priors\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "attr_ids_sorted = np.array(sorted(labels_df['attribute_id'].astype(int).unique().tolist()), dtype=np.int32)\n",
        "attr_to_col = {a:i for i,a in enumerate(attr_ids_sorted)}\n",
        "C = len(attr_ids_sorted)\n",
        "train_df = pd.read_csv('train.csv')\n",
        "n_train = len(train_df)\n",
        "\n",
        "# Build train label lists and |L_j|\n",
        "lab_lists = []\n",
        "lab_sizes = np.zeros(n_train, dtype=np.int32)\n",
        "df_counts = np.zeros(C, dtype=np.int32)  # document frequency per class\n",
        "for i, s in enumerate(train_df['attribute_ids'].fillna('').astype(str).tolist()):\n",
        "    if s:\n",
        "        cols = [attr_to_col[int(x)] for x in s.split() if x!='' and int(x) in attr_to_col]\n",
        "        lab_lists.append(cols)\n",
        "        lab_sizes[i] = len(cols)\n",
        "        for c in set(cols):\n",
        "            df_counts[c] += 1\n",
        "    else:\n",
        "        lab_lists.append([])\n",
        "        lab_sizes[i] = 0\n",
        "\n",
        "# IDF weights\n",
        "idf = np.log(n_train / (df_counts.astype(np.float32) + 1.0))\n",
        "idf = np.clip(idf, 0.0, None) ** gamma_idf  # idf^gamma\n",
        "\n",
        "# Softmax weights over neighbors with temperature tau\n",
        "sims_sm = sims / max(tau, 1e-6)\n",
        "sims_sm = sims_sm - sims_sm.max(axis=1, keepdims=True)  # stabilize\n",
        "w = np.exp(sims_sm)\n",
        "w /= (w.sum(axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "# Optional prune by cumulative weight >= 0.95 (skip for speed/consistency)\n",
        "\n",
        "# Accumulate class scores with per-neighbor normalization by |L_j|^eta\n",
        "ntest = Xte.shape[0]\n",
        "probs_knn = np.zeros((ntest, C), dtype=np.float32)\n",
        "for i in range(ntest):\n",
        "    nn = nn_idx[i]\n",
        "    wi = w[i]\n",
        "    for rank, (nbr, wj) in enumerate(zip(nn, wi)):\n",
        "        Lj = lab_lists[nbr]\n",
        "        if not Lj:\n",
        "            continue\n",
        "        denom = (lab_sizes[nbr] ** eta) if eta > 0 else 1.0\n",
        "        add = float(wj) / float(max(denom, 1.0))\n",
        "        for c in Lj:\n",
        "            probs_knn[i, c] += add\n",
        "    if (i+1) % 2000 == 0:\n",
        "        print(f'[kNN-accum] {i+1}/{ntest}', flush=True)\n",
        "\n",
        "# Apply IDF and renormalize per image to sum 1\n",
        "probs_knn *= idf[None, :]\n",
        "row_sums = probs_knn.sum(axis=1, keepdims=True)\n",
        "probs_knn = np.divide(probs_knn, np.where(row_sums > 0, row_sums, 1.0), out=np.zeros_like(probs_knn), where=row_sums>0)\n",
        "\n",
        "# Load b3-only model probs and do probability-space blend\n",
        "from __main__ import blend_equal_weight\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "_, _, P_model = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=False, out_name='noop.csv', cardinality_target=4.40)\n",
        "assert P_model is not None and P_model.shape == probs_knn.shape, f'Shape mismatch: {None if P_model is None else P_model.shape} vs {probs_knn.shape}'\n",
        "beta = 0.10\n",
        "P_final = (1.0 - beta) * P_model + beta * probs_knn\n",
        "\n",
        "# Caps + min-total enforcement\n",
        "labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "idx_to_group = labels_df.set_index('attribute_id').loc[attr_ids_sorted, 'group'].values\n",
        "base_caps = {\n",
        "    'country': 1,\n",
        "    'culture': 1,\n",
        "    'medium': 2,\n",
        "    'dimension': 1,\n",
        "    'tags': 5,\n",
        "    'tag': 5,\n",
        "}\n",
        "default_cap = 3\n",
        "sub = pd.read_csv('sample_submission.csv')\n",
        "ids = sub['id'].values\n",
        "\n",
        "def apply_caps_with_min(Pt, thr, caps, min_total, fill_probs):\n",
        "    rows = []; counts = []\n",
        "    for i in range(Pt.shape[0]):\n",
        "        p = Pt[i]\n",
        "        cand = np.where(p >= thr)[0]\n",
        "        if cand.size == 0:\n",
        "            cand = np.array([int(np.argmax(p))], dtype=np.int64)\n",
        "        cand_sorted = cand[np.argsort(-p[cand])]\n",
        "        used, kept = {}, []\n",
        "        kept_set = set()\n",
        "        for j in cand_sorted:\n",
        "            g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "            kcap = caps.get(g, default_cap)\n",
        "            c = used.get(g, 0)\n",
        "            if c < kcap:\n",
        "                kept.append(j); kept_set.add(j); used[g] = c + 1\n",
        "        # Enforce min_total by filling from highest model probs not yet selected\n",
        "        if len(kept) < min_total:\n",
        "            order = np.argsort(-fill_probs[i])\n",
        "            for j in order:\n",
        "                if j in kept_set:\n",
        "                    continue\n",
        "                g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "                kcap = caps.get(g, default_cap)\n",
        "                c = used.get(g, 0)\n",
        "                if c < kcap:\n",
        "                    kept.append(j); kept_set.add(j); used[g] = c + 1\n",
        "                    if len(kept) >= min_total:\n",
        "                        break\n",
        "        if len(kept) == 0:\n",
        "            kept = [int(np.argmax(p))]\n",
        "        pred_attr = [int(attr_ids_sorted[j]) for j in kept]\n",
        "        rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(x) for x in pred_attr)})\n",
        "        counts.append(len(pred_attr))\n",
        "    return rows, float(np.mean(counts))\n",
        "\n",
        "# Sweep threshold to hit ~4.40 with min_total=3; if too low, relax tags cap and min_total=4\n",
        "def sweep_and_write(Pt, caps, min_total, out_path, thr_lo=0.46, thr_hi=0.54, thr_step=0.002):\n",
        "    thrs = np.arange(thr_lo, thr_hi + 1e-9, thr_step)\n",
        "    target_mean = 4.40\n",
        "    best = None; best_rows = None\n",
        "    for t in thrs:\n",
        "        rows_t, mean_t = apply_caps_with_min(Pt, t, caps, min_total, fill_probs=P_model)\n",
        "        delta = abs(mean_t - target_mean)\n",
        "        if (best is None) or (delta < best[0]):\n",
        "            best = (delta, t, mean_t); best_rows = rows_t\n",
        "    _, best_thr, best_mean = best\n",
        "    sub_df = pd.DataFrame(best_rows)\n",
        "    sub_df.to_csv(out_path, index=False)\n",
        "    shutil.copyfile(out_path, 'submission.csv')\n",
        "    print(f'[WRITE] {out_path} thr={best_thr:.3f} post-cap mean={best_mean:.3f} min_total={min_total} caps[tags]={caps.get(\"tags\", None)}')\n",
        "    return best_thr, best_mean\n",
        "\n",
        "# Attempt 1: base caps, min_total=3\n",
        "thr1, mean1 = sweep_and_write(P_final, base_caps.copy(), min_total=3, out_path='submission_knn_softmax_probblend.csv')\n",
        "\n",
        "# If still under ~4.35, try tags cap=6 and/or min_total=4\n",
        "if mean1 < 4.35:\n",
        "    caps2 = base_caps.copy(); caps2['tags'] = 6; caps2['tag'] = 6\n",
        "    thr2, mean2 = sweep_and_write(P_final, caps2, min_total=3, out_path='submission_knn_softmax_probblend_tags6.csv')\n",
        "    if mean2 < 4.35:\n",
        "        thr3, mean3 = sweep_and_write(P_final, caps2, min_total=4, out_path='submission_knn_softmax_probblend_tags6_min4.csv')\n",
        "\n",
        "print('Done. Total elapsed: {:.1f} min'.format((time.time()-t0)/60.0), flush=True)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== kNN label transfer v3: softmax-weighted (tau=0.10) + IDF^0.5 + per-neighbor |L|^-1 + prob-blend beta=0.10 + caps + min-total ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded embeddings: (120801, 512) (21318, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index built. nt= 120801 dim= 512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kNN done. sims shape: (21318, 100) idx shape: (21318, 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 2000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 4000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 6000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 8000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 10000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 12000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 14000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 16000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 18000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 20000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.400, chosen thr=0.502 (pred_mean=4.406)\n[INFO] Skipping submission write (best_thr not available).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WRITE] submission_knn_softmax_probblend.csv thr=0.460 post-cap mean=4.087 min_total=3 caps[tags]=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WRITE] submission_knn_softmax_probblend_tags6.csv thr=0.460 post-cap mean=4.093 min_total=3 caps[tags]=6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WRITE] submission_knn_softmax_probblend_tags6_min4.csv thr=0.482 post-cap mean=4.395 min_total=4 caps[tags]=6\nDone. Total elapsed: 5.0 min\n"
          ]
        }
      ]
    },
    {
      "id": "242f379c-9450-4b40-8107-60dbb3721d34",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== Post-process: enforce min-total=4 on kNN softmax blend (tags cap=6) using P_model for fills ===', flush=True)\n",
        "\n",
        "# Inputs\n",
        "sub_path = Path('submission_knn_softmax_probblend_tags6.csv')\n",
        "assert sub_path.exists(), 'Missing submission_knn_softmax_probblend_tags6.csv from kNN v3 cell'\n",
        "\n",
        "# Load mapping and model probabilities for fill ranking\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "attr_ids_sorted = np.array(sorted(labels_df['attribute_id'].astype(int).unique().tolist()), dtype=np.int32)\n",
        "attr_to_col = {a:i for i,a in enumerate(attr_ids_sorted)}\n",
        "idx_to_group = labels_df.set_index('attribute_id').loc[attr_ids_sorted, 'group'].values\n",
        "default_cap = 3\n",
        "caps = {'country':1,'culture':1,'medium':2,'dimension':1,'tags':6,'tag':6}\n",
        "\n",
        "# Get model-only probs for ranking fills\n",
        "from __main__ import blend_equal_weight\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "_, _, P_model = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=False, out_name='noop.csv', cardinality_target=4.40)\n",
        "assert P_model is not None, 'Failed to load model probabilities'\n",
        "\n",
        "# Load existing submission\n",
        "sub = pd.read_csv(sub_path)\n",
        "ids = sub['id'].values\n",
        "pred_lists = [str(x).strip().split() if isinstance(x, str) else [] for x in sub['attribute_ids'].values]\n",
        "\n",
        "rows = []; counts = []\n",
        "for i, (img_id, attrs) in enumerate(zip(ids, pred_lists)):\n",
        "    chosen_attr_ids = [int(a) for a in attrs if a!='']\n",
        "    chosen_idx = [attr_to_col[a] for a in chosen_attr_ids if a in attr_to_col]\n",
        "    used = {}\n",
        "    for j in chosen_idx:\n",
        "        g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "        used[g] = used.get(g, 0) + 1\n",
        "    # Fill until min-total=4 respecting caps\n",
        "    while len(chosen_idx) < 4:\n",
        "        # pick next best by P_model\n",
        "        order = np.argsort(-P_model[i])\n",
        "        picked = False\n",
        "        for j in order:\n",
        "            if j in chosen_idx:\n",
        "                continue\n",
        "            g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "            kcap = caps.get(g, default_cap)\n",
        "            c = used.get(g, 0)\n",
        "            if c < kcap:\n",
        "                chosen_idx.append(int(j)); used[g] = c + 1; picked = True\n",
        "                break\n",
        "        if not picked:\n",
        "            break  # no more slots available due to caps\n",
        "    pred_attr = [int(attr_ids_sorted[j]) for j in chosen_idx]\n",
        "    rows.append({'id': img_id, 'attribute_ids': ' '.join(str(x) for x in pred_attr)})\n",
        "    counts.append(len(pred_attr))\n",
        "\n",
        "out_path = 'submission_knn_softmax_probblend_tags6_min4_fill.csv'\n",
        "pd.DataFrame(rows).to_csv(out_path, index=False)\n",
        "shutil.copyfile(out_path, 'submission.csv')\n",
        "print(f'Wrote {out_path} (mean count={np.mean(counts):.3f}) and set as submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "f1854e65-eeb5-4246-b00b-1821da7af3d7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, time, shutil, faiss\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== kNN union hedge: add up to L=2 kNN labels (P_knn>=0.10) to model+caps; sweep thr to mean~4.40 ===', flush=True)\n",
        "t0 = time.time()\n",
        "\n",
        "# Load CLIP embeddings\n",
        "Xtr = np.load('clip_train_emb.npy').astype(np.float32)\n",
        "Xte = np.load('clip_test_emb.npy').astype(np.float32)\n",
        "d = Xtr.shape[1]\n",
        "index = faiss.IndexFlatIP(d); index.add(Xtr)\n",
        "\n",
        "# Params (per expert): k=150, tau=0.10, eta=1.0, idf^0.5\n",
        "k = 150; tau = 0.10; eta = 1.0; gamma_idf = 0.5\n",
        "sims, nn_idx = index.search(Xte, k)\n",
        "print('kNN search done:', sims.shape, flush=True)\n",
        "\n",
        "# Labels mapping\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "attr_ids_sorted = np.array(sorted(labels_df['attribute_id'].astype(int).unique().tolist()), dtype=np.int32)\n",
        "attr_to_col = {a:i for i,a in enumerate(attr_ids_sorted)}\n",
        "C = len(attr_ids_sorted)\n",
        "train_df = pd.read_csv('train.csv')\n",
        "n_train = len(train_df)\n",
        "\n",
        "# Train label lists and sizes + df counts\n",
        "lab_lists = []; lab_sizes = np.zeros(n_train, dtype=np.int32); df_counts = np.zeros(C, dtype=np.int32)\n",
        "for i, s in enumerate(train_df['attribute_ids'].fillna('').astype(str).tolist()):\n",
        "    if s:\n",
        "        cols = [attr_to_col[int(x)] for x in s.split() if x!='' and int(x) in attr_to_col]\n",
        "        lab_lists.append(cols); lab_sizes[i] = len(cols)\n",
        "        for c in set(cols): df_counts[c] += 1\n",
        "    else:\n",
        "        lab_lists.append([]); lab_sizes[i] = 0\n",
        "\n",
        "# Softmax weights with temperature\n",
        "sims_sm = sims / max(tau,1e-6)\n",
        "sims_sm -= sims_sm.max(axis=1, keepdims=True)\n",
        "w = np.exp(sims_sm); w /= (w.sum(axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "# Accumulate probs_knn with per-neighbor |L|^-eta\n",
        "ntest = Xte.shape[0]\n",
        "probs_knn = np.zeros((ntest, C), dtype=np.float32)\n",
        "for i in range(ntest):\n",
        "    nn = nn_idx[i]; wi = w[i]\n",
        "    for nbr, wj in zip(nn, wi):\n",
        "        Lj = lab_lists[nbr]\n",
        "        if not Lj: continue\n",
        "        denom = (lab_sizes[nbr] ** eta) if eta > 0 else 1.0\n",
        "        add = float(wj) / float(max(denom, 1.0))\n",
        "        for c in Lj: probs_knn[i, c] += add\n",
        "    if (i+1) % 2000 == 0: print(f'[kNN-accum] {i+1}/{ntest}', flush=True)\n",
        "\n",
        "# IDF^gamma and renormalize\n",
        "idf = np.log(n_train / (df_counts.astype(np.float32) + 1.0))\n",
        "idf = np.clip(idf, 0.0, None) ** gamma_idf\n",
        "probs_knn *= idf[None, :]\n",
        "row_sums = probs_knn.sum(axis=1, keepdims=True)\n",
        "probs_knn = np.divide(probs_knn, np.where(row_sums > 0, row_sums, 1.0), out=np.zeros_like(probs_knn), where=row_sums>0)\n",
        "\n",
        "# Load model probabilities (b3 2:1)\n",
        "from __main__ import blend_equal_weight\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "_, _, P_model = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=False, out_name='noop.csv', cardinality_target=4.40)\n",
        "assert P_model is not None and P_model.shape == probs_knn.shape\n",
        "\n",
        "# Groups and caps (tags cap=6 as per expert bump), default cap=3\n",
        "labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "idx_to_group = labels_df.set_index('attribute_id').loc[attr_ids_sorted, 'group'].values\n",
        "caps = {'country':1,'culture':1,'medium':2,'dimension':1,'tags':6,'tag':6}\n",
        "default_cap = 3\n",
        "ids = pd.read_csv('sample_submission.csv')['id'].values\n",
        "\n",
        "def build_union_rows(thr, L=2, gate=0.10, min_total=3):\n",
        "    rows = []; counts = []\n",
        "    for i in range(P_model.shape[0]):\n",
        "        p = P_model[i]; pk = probs_knn[i]\n",
        "        cand = np.where(p >= thr)[0]\n",
        "        if cand.size == 0: cand = np.array([int(np.argmax(p))], dtype=np.int64)\n",
        "        cand_sorted = cand[np.argsort(-p[cand])]\n",
        "        used = {}; kept = []; kept_set = set()\n",
        "        # apply caps on model-selected first\n",
        "        for j in cand_sorted:\n",
        "            g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "            kcap = caps.get(g, default_cap); c = used.get(g, 0)\n",
        "            if c < kcap: kept.append(j); kept_set.add(j); used[g] = c + 1\n",
        "        # union: add up to L labels from kNN by pk desc with gate\n",
        "        if L > 0:\n",
        "            order_knn = np.argsort(-pk)\n",
        "            added = 0\n",
        "            for j in order_knn:\n",
        "                if pk[j] < gate: break\n",
        "                if j in kept_set: continue\n",
        "                g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "                kcap = caps.get(g, default_cap); c = used.get(g, 0)\n",
        "                if c < kcap:\n",
        "                    kept.append(int(j)); kept_set.add(int(j)); used[g] = c + 1; added += 1\n",
        "                    if added >= L: break\n",
        "        # enforce min_total using model ranking\n",
        "        if len(kept) < min_total:\n",
        "            order = np.argsort(-p)\n",
        "            for j in order:\n",
        "                if j in kept_set: continue\n",
        "                g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "                kcap = caps.get(g, default_cap); c = used.get(g, 0)\n",
        "                if c < kcap:\n",
        "                    kept.append(int(j)); kept_set.add(int(j)); used[g] = c + 1\n",
        "                    if len(kept) >= min_total: break\n",
        "        if len(kept) == 0: kept = [int(np.argmax(p))]\n",
        "        rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(int(attr_ids_sorted[j])) for j in kept)})\n",
        "        counts.append(len(kept))\n",
        "    return rows, float(np.mean(counts))\n",
        "\n",
        "# Sweep thr to target mean ~4.40 after union; L=2, gate=0.10, min_total=3\n",
        "thrs = np.arange(0.460, 0.5401, 0.002)\n",
        "target_mean = 4.40\n",
        "best = None; best_rows = None\n",
        "for t in thrs:\n",
        "    rows_t, mean_t = build_union_rows(t, L=2, gate=0.10, min_total=3)\n",
        "    delta = abs(mean_t - target_mean)\n",
        "    if (best is None) or (delta < best[0]):\n",
        "        best = (delta, t, mean_t); best_rows = rows_t\n",
        "    if int((t-0.460)/0.002) % 10 == 0:\n",
        "        print(f'[SWEEP] thr={t:.3f} post-union mean={mean_t:.3f}', flush=True)\n",
        "\n",
        "_, best_thr, best_mean = best\n",
        "out_path = 'submission_union_knn_add2.csv'\n",
        "pd.DataFrame(best_rows).to_csv(out_path, index=False)\n",
        "shutil.copyfile(out_path, 'submission.csv')\n",
        "print(f'[WRITE] {out_path} thr={best_thr:.3f} post-union mean={best_mean:.3f} | Elapsed {(time.time()-t0)/60:.1f}m', flush=True)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== kNN union hedge: add up to L=2 kNN labels (P_knn>=0.10) to model+caps; sweep thr to mean~4.40 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kNN search done: (21318, 150)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 2000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 4000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 6000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 8000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 10000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 12000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 14000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 16000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 18000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 20000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.400, chosen thr=0.502 (pred_mean=4.406)\n[INFO] Skipping submission write (best_thr not available).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.460 post-union mean=4.696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.480 post-union mean=4.458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.502 post-union mean=4.234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.520 post-union mean=4.070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.540 post-union mean=3.919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WRITE] submission_union_knn_add2.csv thr=0.486 post-union mean=4.395 | Elapsed 2.7m\n"
          ]
        }
      ]
    },
    {
      "id": "fa99c2cb-855a-4d86-a2e6-f74d56b800be",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, time, shutil, faiss\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== kNN v3 variants: (A) tau=0.05, beta=0.10; (B) tau=0.10, beta=0.15, restrict to {country,culture,dimension} ===', flush=True)\n",
        "t0 = time.time()\n",
        "\n",
        "# Common: load embeddings and build FAISS once with k_max=150\n",
        "Xtr = np.load('clip_train_emb.npy').astype(np.float32)\n",
        "Xte = np.load('clip_test_emb.npy').astype(np.float32)\n",
        "d = Xtr.shape[1]\n",
        "index = faiss.IndexFlatIP(d); index.add(Xtr)\n",
        "k_max = 150\n",
        "sims_all, nn_idx_all = index.search(Xte, k_max)\n",
        "print('FAISS kNN ready:', sims_all.shape, flush=True)\n",
        "\n",
        "# Labels and train annotations\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "attr_ids_sorted = np.array(sorted(labels_df['attribute_id'].astype(int).unique().tolist()), dtype=np.int32)\n",
        "attr_to_col = {a:i for i,a in enumerate(attr_ids_sorted)}\n",
        "idx_to_group = labels_df.set_index('attribute_id').loc[attr_ids_sorted, 'group'].values\n",
        "C = len(attr_ids_sorted)\n",
        "train_df = pd.read_csv('train.csv')\n",
        "n_train = len(train_df)\n",
        "lab_lists = []; lab_sizes = np.zeros(n_train, dtype=np.int32); df_counts = np.zeros(C, dtype=np.int32)\n",
        "for i, s in enumerate(train_df['attribute_ids'].fillna('').astype(str).tolist()):\n",
        "    if s:\n",
        "        cols = [attr_to_col.get(int(x)) for x in s.split() if x!='' and (int(x) in attr_to_col)]\n",
        "        cols = [c for c in cols if c is not None]\n",
        "        lab_lists.append(cols); lab_sizes[i] = len(cols)\n",
        "        for c in set(cols): df_counts[c] += 1\n",
        "    else:\n",
        "        lab_lists.append([]); lab_sizes[i] = 0\n",
        "idf = np.log(n_train / (df_counts.astype(np.float32) + 1.0))\n",
        "idf = np.clip(idf, 0.0, None) ** 0.5  # gamma_idf=0.5\n",
        "\n",
        "# Load model probabilities (b3 2:1)\n",
        "from __main__ import blend_equal_weight\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "_, _, P_model = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=False, out_name='noop.csv', cardinality_target=4.40)\n",
        "assert P_model is not None and P_model.shape[1] == C\n",
        "\n",
        "sub = pd.read_csv('sample_submission.csv'); ids = sub['id'].values\n",
        "base_caps = {'country':1,'culture':1,'medium':2,'dimension':1,'tags':6,'tag':6}\n",
        "default_cap = 3\n",
        "\n",
        "def apply_caps_with_min(Pt, thr, caps, min_total, fill_probs):\n",
        "    rows = []; counts = []\n",
        "    for i in range(Pt.shape[0]):\n",
        "        p = Pt[i]\n",
        "        cand = np.where(p >= thr)[0]\n",
        "        if cand.size == 0:\n",
        "            cand = np.array([int(np.argmax(p))], dtype=np.int64)\n",
        "        cand_sorted = cand[np.argsort(-p[cand])]\n",
        "        used, kept = {}, []; kept_set = set()\n",
        "        for j in cand_sorted:\n",
        "            g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "            kcap = caps.get(g, default_cap); c = used.get(g, 0)\n",
        "            if c < kcap:\n",
        "                kept.append(j); kept_set.add(j); used[g] = c + 1\n",
        "        if len(kept) < min_total:\n",
        "            order = np.argsort(-fill_probs[i])\n",
        "            for j in order:\n",
        "                if j in kept_set: continue\n",
        "                g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "                kcap = caps.get(g, default_cap); c = used.get(g, 0)\n",
        "                if c < kcap:\n",
        "                    kept.append(j); kept_set.add(j); used[g] = c + 1\n",
        "                    if len(kept) >= min_total: break\n",
        "        if len(kept) == 0:\n",
        "            kept = [int(np.argmax(p))]\n",
        "        rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(int(attr_ids_sorted[j])) for j in kept)})\n",
        "        counts.append(len(kept))\n",
        "    return rows, float(np.mean(counts))\n",
        "\n",
        "def sweep_and_write(Pt, caps, min_total, out_path, thr_lo=0.46, thr_hi=0.54, thr_step=0.002):\n",
        "    thrs = np.arange(thr_lo, thr_hi + 1e-9, thr_step)\n",
        "    target_mean = 4.40\n",
        "    best = None; best_rows = None\n",
        "    for t in thrs:\n",
        "        rows_t, mean_t = apply_caps_with_min(Pt, t, caps, min_total, fill_probs=P_model)\n",
        "        delta = abs(mean_t - target_mean)\n",
        "        if (best is None) or (delta < best[0]):\n",
        "            best = (delta, t, mean_t); best_rows = rows_t\n",
        "    _, best_thr, best_mean = best\n",
        "    pd.DataFrame(best_rows).to_csv(out_path, index=False)\n",
        "    shutil.copyfile(out_path, 'submission.csv')\n",
        "    print(f'[WRITE] {out_path} thr={best_thr:.3f} post-cap mean={best_mean:.3f} min_total={min_total}')\n",
        "    return best_thr, best_mean\n",
        "\n",
        "def build_probs_knn(k_use, tau):\n",
        "    sims = sims_all[:, :k_use]\n",
        "    nn_idx = nn_idx_all[:, :k_use]\n",
        "    sims_sm = sims / max(tau, 1e-6)\n",
        "    sims_sm -= sims_sm.max(axis=1, keepdims=True)\n",
        "    w = np.exp(sims_sm); w /= (w.sum(axis=1, keepdims=True) + 1e-12)\n",
        "    ntest = sims.shape[0]\n",
        "    pk = np.zeros((ntest, C), dtype=np.float32)\n",
        "    for i in range(ntest):\n",
        "        wi = w[i]; nn = nn_idx[i]\n",
        "        for nbr, wj in zip(nn, wi):\n",
        "            Lj = lab_lists[nbr]\n",
        "            if not Lj: continue\n",
        "            denom = float(max(lab_sizes[nbr], 1))\n",
        "            add = float(wj) / denom\n",
        "            for c in Lj: pk[i, c] += add\n",
        "        if (i+1) % 2000 == 0: print(f'[accum] {i+1}/{ntest}', flush=True)\n",
        "    pk *= idf[None, :]\n",
        "    rs = pk.sum(axis=1, keepdims=True)\n",
        "    pk = np.divide(pk, np.where(rs>0, rs, 1.0), out=np.zeros_like(pk), where=rs>0)\n",
        "    return pk\n",
        "\n",
        "# Variant A: tau=0.05, k=150, beta=0.10, min_total=4, caps tags=6\n",
        "print('--- Variant A: tau=0.05, k=150, beta=0.10 ---', flush=True)\n",
        "Pk_A = build_probs_knn(150, 0.05)\n",
        "beta_A = 0.10\n",
        "P_final_A = (1.0 - beta_A) * P_model + beta_A * Pk_A\n",
        "sweep_and_write(P_final_A, base_caps, min_total=4, out_path='submission_knn_softmax_probblend_tau005.csv')\n",
        "\n",
        "# Variant B: tau=0.10, k=150, beta=0.15, restrict kNN to only country/culture/dimension\n",
        "print('--- Variant B: tau=0.10, k=150, beta=0.15, restrict groups ---', flush=True)\n",
        "Pk_B = build_probs_knn(150, 0.10)\n",
        "mask_groups = np.isin(idx_to_group, ['country','culture','dimension'])\n",
        "Pk_B_masked = Pk_B.copy(); Pk_B_masked[:, ~mask_groups] = 0.0\n",
        "beta_B = 0.15\n",
        "P_final_B = (1.0 - beta_B) * P_model + beta_B * Pk_B_masked\n",
        "sweep_and_write(P_final_B, base_caps, min_total=3, out_path='submission_knn_softmax_probblend_groupmask.csv')\n",
        "\n",
        "print('Done variants. Elapsed {:.1f}m'.format((time.time()-t0)/60.0), flush=True)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== kNN v3 variants: (A) tau=0.05, beta=0.10; (B) tau=0.10, beta=0.15, restrict to {country,culture,dimension} ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS kNN ready: (21318, 150)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.400, chosen thr=0.502 (pred_mean=4.406)\n[INFO] Skipping submission write (best_thr not available).\n--- Variant A: tau=0.05, k=150, beta=0.10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 2000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 4000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 6000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 8000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 10000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 12000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 14000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 16000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 18000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 20000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WRITE] submission_knn_softmax_probblend_tau005.csv thr=0.482 post-cap mean=4.395 min_total=4\n--- Variant B: tau=0.10, k=150, beta=0.15, restrict groups ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 2000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 4000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 6000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 8000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 10000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 12000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 14000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 16000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 18000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 20000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WRITE] submission_knn_softmax_probblend_groupmask.csv thr=0.460 post-cap mean=3.825 min_total=3\nDone variants. Elapsed 4.7m\n"
          ]
        }
      ]
    },
    {
      "id": "9d302c20-fbfe-4255-95cf-5f6d4c8c4b5e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shutil, pandas as pd, numpy as np\n",
        "print('=== Set submission to best kNN variant: tau=0.05 prob-blend (min_total=4) ===', flush=True)\n",
        "src = 'submission_knn_softmax_probblend_tau005.csv'\n",
        "df = pd.read_csv(src)\n",
        "mean_cnt = df['attribute_ids'].fillna('').apply(lambda s: 0 if s=='' else len(str(s).split())).mean()\n",
        "print(f'Source {src} mean count={mean_cnt:.3f}')\n",
        "shutil.copyfile(src, 'submission.csv')\n",
        "print('Copied', src, '-> submission.csv')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Set submission to best kNN variant: tau=0.05 prob-blend (min_total=4) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source submission_knn_softmax_probblend_tau005.csv mean count=4.395\nCopied submission_knn_softmax_probblend_tau005.csv -> submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "2939fb48-0d8d-44a5-a9be-421a65a905ea",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== Enforce per-group min-1 on {country,culture,dimension} for tau=0.05 blend (min_total=4) ===', flush=True)\n",
        "src = Path('submission_knn_softmax_probblend_tau005.csv')\n",
        "assert src.exists(), 'Missing base submission file submission_knn_softmax_probblend_tau005.csv'\n",
        "\n",
        "# Load labels and group mapping\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "attr_ids_sorted = np.array(sorted(labels_df['attribute_id'].astype(int).unique().tolist()), dtype=np.int32)\n",
        "attr_to_col = {a:i for i,a in enumerate(attr_ids_sorted)}\n",
        "idx_to_group = labels_df.set_index('attribute_id').loc[attr_ids_sorted, 'group'].values\n",
        "\n",
        "# Caps and required groups\n",
        "caps = {'country':1,'culture':1,'dimension':1,'medium':2,'tags':6,'tag':6}\n",
        "default_cap = 3\n",
        "required_groups = ['country','culture','dimension']\n",
        "\n",
        "# Load model probs for selecting best fills within a group\n",
        "from __main__ import blend_equal_weight\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "_, _, P_model = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=False, out_name='noop.csv', cardinality_target=4.40)\n",
        "assert P_model is not None, 'Failed to load P_model'\n",
        "\n",
        "# Build per-group column indices\n",
        "group_to_cols = {}\n",
        "for j, aid in enumerate(attr_ids_sorted):\n",
        "    g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "    group_to_cols.setdefault(g, []).append(j)\n",
        "\n",
        "# Load base submission and enforce per-group minimums\n",
        "sub = pd.read_csv(src)\n",
        "ids = sub['id'].values\n",
        "pred_lists = [str(x).strip().split() if isinstance(x, str) else [] for x in sub['attribute_ids'].values]\n",
        "\n",
        "rows = []; counts = []\n",
        "for i, (img_id, attrs) in enumerate(zip(ids, pred_lists)):\n",
        "    chosen_attr_ids = [int(a) for a in attrs if a!='']\n",
        "    chosen_idx = [attr_to_col[a] for a in chosen_attr_ids if a in attr_to_col]\n",
        "    used = {}\n",
        "    for j in chosen_idx:\n",
        "        g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "        used[g] = used.get(g, 0) + 1\n",
        "    chosen_set = set(chosen_idx)\n",
        "    # Enforce per-group min-1 for each required group if under its cap\n",
        "    for g in required_groups:\n",
        "        if used.get(g, 0) >= 1:\n",
        "            continue\n",
        "        cols = group_to_cols.get(g, [])\n",
        "        if not cols:\n",
        "            continue\n",
        "        kcap = caps.get(g, default_cap)\n",
        "        c = used.get(g, 0)\n",
        "        if c >= kcap:\n",
        "            continue\n",
        "        # select top label by model prob within this group not already chosen\n",
        "        probs_g = P_model[i, cols]\n",
        "        order = np.argsort(-probs_g)\n",
        "        picked = False\n",
        "        for oi in order:\n",
        "            j = int(cols[int(oi)])\n",
        "            if j in chosen_set:\n",
        "                continue\n",
        "            chosen_idx.append(j); chosen_set.add(j); used[g] = c + 1; picked = True\n",
        "            break\n",
        "        # if nothing picked (all already present), skip\n",
        "    pred_attr = [int(attr_ids_sorted[j]) for j in chosen_idx]\n",
        "    rows.append({'id': img_id, 'attribute_ids': ' '.join(str(x) for x in pred_attr)})\n",
        "    counts.append(len(pred_attr))\n",
        "\n",
        "out_path = 'submission_knn_tau005_minGroup.csv'\n",
        "pd.DataFrame(rows).to_csv(out_path, index=False)\n",
        "shutil.copyfile(out_path, 'submission.csv')\n",
        "print(f'Wrote {out_path} (mean count={np.mean(counts):.3f}) and set as submission.csv')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Enforce per-group min-1 on {country,culture,dimension} for tau=0.05 blend (min_total=4) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.400, chosen thr=0.502 (pred_mean=4.406)\n[INFO] Skipping submission write (best_thr not available).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission_knn_tau005_minGroup.csv (mean count=5.808) and set as submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "b5713c55-7264-4ae9-87fc-85423e6e413c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, shutil, faiss, time\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== kNN tau=0.05 prob-blend with per-group min-1 (country,culture,dimension) + thr sweep to mean~4.44 ===', flush=True)\n",
        "t0 = time.time()\n",
        "\n",
        "# Load embeddings and build FAISS (reuse config k=150, tau=0.05, IDF^0.5, |L|^-1)\n",
        "Xtr = np.load('clip_train_emb.npy').astype(np.float32)\n",
        "Xte = np.load('clip_test_emb.npy').astype(np.float32)\n",
        "d = Xtr.shape[1]\n",
        "index = faiss.IndexFlatIP(d); index.add(Xtr)\n",
        "k = 150; tau = 0.05; gamma_idf = 0.5\n",
        "sims, nn_idx = index.search(Xte, k)\n",
        "\n",
        "# Labels and train annotations\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "attr_ids_sorted = np.array(sorted(labels_df['attribute_id'].astype(int).unique().tolist()), dtype=np.int32)\n",
        "attr_to_col = {a:i for i,a in enumerate(attr_ids_sorted)}\n",
        "idx_to_group = labels_df.set_index('attribute_id').loc[attr_ids_sorted, 'group'].values\n",
        "C = len(attr_ids_sorted)\n",
        "train_df = pd.read_csv('train.csv')\n",
        "n_train = len(train_df)\n",
        "lab_lists = []; lab_sizes = np.zeros(n_train, dtype=np.int32); df_counts = np.zeros(C, dtype=np.int32)\n",
        "for i, s in enumerate(train_df['attribute_ids'].fillna('').astype(str).tolist()):\n",
        "    if s:\n",
        "        cols = [attr_to_col[int(x)] for x in s.split() if x!='' and int(x) in attr_to_col]\n",
        "        lab_lists.append(cols); lab_sizes[i] = len(cols)\n",
        "        for c in set(cols): df_counts[c] += 1\n",
        "    else:\n",
        "        lab_lists.append([]); lab_sizes[i] = 0\n",
        "idf = np.log(n_train / (df_counts.astype(np.float32) + 1.0))\n",
        "idf = np.clip(idf, 0.0, None) ** gamma_idf\n",
        "\n",
        "# Softmax weights\n",
        "sims_sm = sims / max(tau,1e-6); sims_sm -= sims_sm.max(axis=1, keepdims=True)\n",
        "w = np.exp(sims_sm); w /= (w.sum(axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "# Accumulate kNN probs with |L|^-1 and IDF^0.5 + renorm\n",
        "ntest = Xte.shape[0]\n",
        "Pk = np.zeros((ntest, C), dtype=np.float32)\n",
        "for i in range(ntest):\n",
        "    wi = w[i]; nn = nn_idx[i]\n",
        "    for nbr, wj in zip(nn, wi):\n",
        "        Lj = lab_lists[nbr]\n",
        "        if not Lj: continue\n",
        "        add = float(wj) / float(max(lab_sizes[nbr], 1))\n",
        "        for c in Lj: Pk[i, c] += add\n",
        "    if (i+1) % 2000 == 0: print(f'[accum] {i+1}/{ntest}', flush=True)\n",
        "Pk *= idf[None, :]\n",
        "rs = Pk.sum(axis=1, keepdims=True)\n",
        "Pk = np.divide(Pk, np.where(rs>0, rs, 1.0), out=np.zeros_like(Pk), where=rs>0)\n",
        "\n",
        "# Load model probs and prob-blend (beta=0.10)\n",
        "from __main__ import blend_equal_weight\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "_, _, P_model = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=False, out_name='noop.csv', cardinality_target=4.40)\n",
        "beta = 0.10\n",
        "P_final = (1.0 - beta) * P_model + beta * Pk\n",
        "\n",
        "# Caps, required groups, and per-group min rule\n",
        "caps = {'country':1,'culture':1,'dimension':1,'medium':2,'tags':6,'tag':6}\n",
        "default_cap = 3\n",
        "required_groups = ['country','culture','dimension']\n",
        "group_to_cols = {}\n",
        "for j, aid in enumerate(attr_ids_sorted):\n",
        "    g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "    group_to_cols.setdefault(g, []).append(j)\n",
        "\n",
        "ids = pd.read_csv('sample_submission.csv')['id'].values\n",
        "\n",
        "def apply_caps_min_total_and_groupmins(Pt, thr, min_total=4):\n",
        "    rows = []; counts = []\n",
        "    for i in range(Pt.shape[0]):\n",
        "        p = Pt[i]\n",
        "        cand = np.where(p >= thr)[0]\n",
        "        if cand.size == 0:\n",
        "            cand = np.array([int(np.argmax(p))], dtype=np.int64)\n",
        "        cand_sorted = cand[np.argsort(-p[cand])]\n",
        "        used, kept = {}, []; kept_set = set()\n",
        "        # Apply caps on thresholded set\n",
        "        for j in cand_sorted:\n",
        "            g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "            kcap = caps.get(g, default_cap); c = used.get(g, 0)\n",
        "            if c < kcap:\n",
        "                kept.append(j); kept_set.add(j); used[g] = c + 1\n",
        "        # Enforce per-group min-1 for required groups using P_model ranking within the group\n",
        "        for g in required_groups:\n",
        "            if used.get(g, 0) >= 1: continue\n",
        "            cols = group_to_cols.get(g, [])\n",
        "            if not cols: continue\n",
        "            kcap = caps.get(g, default_cap); c = used.get(g, 0)\n",
        "            if c >= kcap: continue\n",
        "            probs_g = P_model[i, cols]\n",
        "            order = np.argsort(-probs_g)\n",
        "            for oi in order:\n",
        "                j = int(cols[int(oi)])\n",
        "                if j in kept_set: continue\n",
        "                kept.append(j); kept_set.add(j); used[g] = c + 1\n",
        "                break\n",
        "        # Enforce min_total using P_model globally\n",
        "        if len(kept) < min_total:\n",
        "            order = np.argsort(-P_model[i])\n",
        "            for j in order:\n",
        "                if j in kept_set: continue\n",
        "                g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "                kcap = caps.get(g, default_cap); c = used.get(g, 0)\n",
        "                if c < kcap:\n",
        "                    kept.append(j); kept_set.add(j); used[g] = c + 1\n",
        "                    if len(kept) >= min_total: break\n",
        "        if len(kept) == 0:\n",
        "            kept = [int(np.argmax(p))]\n",
        "        rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(int(attr_ids_sorted[j])) for j in kept)})\n",
        "        counts.append(len(kept))\n",
        "    return rows, float(np.mean(counts))\n",
        "\n",
        "# Sweep global threshold to hit mean ~4.44 (recall-leaning per expert)\n",
        "thrs = np.arange(0.460, 0.5401, 0.002)\n",
        "target_mean = 4.44\n",
        "best = None; best_rows = None\n",
        "for t in thrs:\n",
        "    rows_t, mean_t = apply_caps_min_total_and_groupmins(P_final, t, min_total=4)\n",
        "    delta = abs(mean_t - target_mean)\n",
        "    if (best is None) or (delta < best[0]):\n",
        "        best = (delta, t, mean_t); best_rows = rows_t\n",
        "    if int((t-0.460)/0.002) % 10 == 0:\n",
        "        print(f'[SWEEP] thr={t:.3f} post-rule mean={mean_t:.3f}', flush=True)\n",
        "\n",
        "_, best_thr, best_mean = best\n",
        "out_path = 'submission_knn_tau005_minGroup_swept.csv'\n",
        "pd.DataFrame(best_rows).to_csv(out_path, index=False)\n",
        "shutil.copyfile(out_path, 'submission.csv')\n",
        "print(f'[WRITE] {out_path} thr={best_thr:.3f} post-rule mean={best_mean:.3f} | Elapsed {(time.time()-t0)/60:.1f}m', flush=True)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== kNN tau=0.05 prob-blend with per-group min-1 (country,culture,dimension) + thr sweep to mean~4.44 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 2000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 4000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 6000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 8000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 10000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 12000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 14000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 16000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 18000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 20000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.400, chosen thr=0.502 (pred_mean=4.406)\n[INFO] Skipping submission write (best_thr not available).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.460 post-rule mean=5.471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.480 post-rule mean=5.287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.502 post-rule mean=5.118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.520 post-rule mean=4.997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.540 post-rule mean=4.880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WRITE] submission_knn_tau005_minGroup_swept.csv thr=0.540 post-rule mean=4.880 | Elapsed 2.4m\n"
          ]
        }
      ]
    },
    {
      "id": "7b1e5710-019f-4478-bc6b-e5253e7208aa",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, shutil, faiss, time\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== Re-sweep: kNN tau=0.05 prob-blend + per-group min-1 with higher thr range to hit mean~4.44 ===', flush=True)\n",
        "t0 = time.time()\n",
        "\n",
        "# Recompute P_final (tau=0.05, k=150, IDF^0.5, |L|^-1) to avoid depending on cached variables\n",
        "Xtr = np.load('clip_train_emb.npy').astype(np.float32)\n",
        "Xte = np.load('clip_test_emb.npy').astype(np.float32)\n",
        "d = Xtr.shape[1]\n",
        "index = faiss.IndexFlatIP(d); index.add(Xtr)\n",
        "k = 150; tau = 0.05; gamma_idf = 0.5\n",
        "sims, nn_idx = index.search(Xte, k)\n",
        "\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "attr_ids_sorted = np.array(sorted(labels_df['attribute_id'].astype(int).unique().tolist()), dtype=np.int32)\n",
        "attr_to_col = {a:i for i,a in enumerate(attr_ids_sorted)}\n",
        "idx_to_group = labels_df.set_index('attribute_id').loc[attr_ids_sorted, 'group'].values\n",
        "C = len(attr_ids_sorted)\n",
        "train_df = pd.read_csv('train.csv')\n",
        "n_train = len(train_df)\n",
        "lab_lists = []; lab_sizes = np.zeros(n_train, dtype=np.int32); df_counts = np.zeros(C, dtype=np.int32)\n",
        "for i, s in enumerate(train_df['attribute_ids'].fillna('').astype(str).tolist()):\n",
        "    if s:\n",
        "        cols = [attr_to_col[int(x)] for x in s.split() if x!='' and int(x) in attr_to_col]\n",
        "        lab_lists.append(cols); lab_sizes[i] = len(cols)\n",
        "        for c in set(cols): df_counts[c] += 1\n",
        "    else:\n",
        "        lab_lists.append([]); lab_sizes[i] = 0\n",
        "idf = np.log(n_train / (df_counts.astype(np.float32) + 1.0))\n",
        "idf = np.clip(idf, 0.0, None) ** gamma_idf\n",
        "\n",
        "sims_sm = sims / max(tau,1e-6); sims_sm -= sims_sm.max(axis=1, keepdims=True)\n",
        "w = np.exp(sims_sm); w /= (w.sum(axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "ntest = Xte.shape[0]\n",
        "Pk = np.zeros((ntest, C), dtype=np.float32)\n",
        "for i in range(ntest):\n",
        "    wi = w[i]; nn = nn_idx[i]\n",
        "    for nbr, wj in zip(nn, wi):\n",
        "        Lj = lab_lists[nbr]\n",
        "        if not Lj: continue\n",
        "        add = float(wj) / float(max(lab_sizes[nbr], 1))\n",
        "        for c in Lj: Pk[i, c] += add\n",
        "    if (i+1) % 2000 == 0: print(f'[accum] {i+1}/{ntest}', flush=True)\n",
        "Pk *= idf[None, :]\n",
        "rs = Pk.sum(axis=1, keepdims=True)\n",
        "Pk = np.divide(Pk, np.where(rs>0, rs, 1.0), out=np.zeros_like(Pk), where=rs>0)\n",
        "\n",
        "from __main__ import blend_equal_weight\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "_, _, P_model = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=False, out_name='noop.csv', cardinality_target=4.40)\n",
        "beta = 0.10\n",
        "P_final = (1.0 - beta) * P_model + beta * Pk\n",
        "\n",
        "# Caps and per-group min rule\n",
        "caps = {'country':1,'culture':1,'dimension':1,'medium':2,'tags':6,'tag':6}\n",
        "default_cap = 3\n",
        "required_groups = ['country','culture','dimension']\n",
        "group_to_cols = {}\n",
        "for j, aid in enumerate(attr_ids_sorted):\n",
        "    g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "    group_to_cols.setdefault(g, []).append(j)\n",
        "\n",
        "ids = pd.read_csv('sample_submission.csv')['id'].values\n",
        "\n",
        "def apply_caps_min_total_and_groupmins(Pt, thr, min_total=4):\n",
        "    rows = []; counts = []\n",
        "    for i in range(Pt.shape[0]):\n",
        "        p = Pt[i]\n",
        "        cand = np.where(p >= thr)[0]\n",
        "        if cand.size == 0:\n",
        "            cand = np.array([int(np.argmax(p))], dtype=np.int64)\n",
        "        cand_sorted = cand[np.argsort(-p[cand])]\n",
        "        used, kept = {}, []; kept_set = set()\n",
        "        for j in cand_sorted:\n",
        "            g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "            kcap = caps.get(g, default_cap); c = used.get(g, 0)\n",
        "            if c < kcap:\n",
        "                kept.append(j); kept_set.add(j); used[g] = c + 1\n",
        "        # per-group min-1 for structural groups\n",
        "        for g in required_groups:\n",
        "            if used.get(g, 0) >= 1: continue\n",
        "            cols = group_to_cols.get(g, [])\n",
        "            if not cols: continue\n",
        "            kcap = caps.get(g, default_cap); c = used.get(g, 0)\n",
        "            if c >= kcap: continue\n",
        "            probs_g = P_model[i, cols]\n",
        "            order = np.argsort(-probs_g)\n",
        "            for oi in order:\n",
        "                j = int(cols[int(oi)])\n",
        "                if j in kept_set: continue\n",
        "                kept.append(j); kept_set.add(j); used[g] = c + 1\n",
        "                break\n",
        "        # enforce min_total using model ranking\n",
        "        if len(kept) < min_total:\n",
        "            order = np.argsort(-P_model[i])\n",
        "            for j in order:\n",
        "                if j in kept_set: continue\n",
        "                g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "                kcap = caps.get(g, default_cap); c = used.get(g, 0)\n",
        "                if c < kcap:\n",
        "                    kept.append(j); kept_set.add(j); used[g] = c + 1\n",
        "                    if len(kept) >= min_total: break\n",
        "        if len(kept) == 0:\n",
        "            kept = [int(np.argmax(p))]\n",
        "        rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(int(attr_ids_sorted[j])) for j in kept)})\n",
        "        counts.append(len(kept))\n",
        "    return rows, float(np.mean(counts))\n",
        "\n",
        "# Higher threshold sweep to bring mean down near 4.44\n",
        "thrs = np.arange(0.560, 0.7001, 0.002)\n",
        "target_mean = 4.44\n",
        "best = None; best_rows = None\n",
        "for t in thrs:\n",
        "    rows_t, mean_t = apply_caps_min_total_and_groupmins(P_final, t, min_total=4)\n",
        "    delta = abs(mean_t - target_mean)\n",
        "    if (best is None) or (delta < best[0]):\n",
        "        best = (delta, t, mean_t); best_rows = rows_t\n",
        "    if int((t-0.560)/0.002) % 20 == 0:\n",
        "        print(f'[SWEEP] thr={t:.3f} post-rule mean={mean_t:.3f}', flush=True)\n",
        "\n",
        "_, best_thr, best_mean = best\n",
        "out_path = 'submission_knn_tau005_minGroup_swept_hi.csv'\n",
        "pd.DataFrame(best_rows).to_csv(out_path, index=False)\n",
        "shutil.copyfile(out_path, 'submission.csv')\n",
        "print(f'[WRITE] {out_path} thr={best_thr:.3f} post-rule mean={best_mean:.3f} | Elapsed {(time.time()-t0)/60:.1f}m', flush=True)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Re-sweep: kNN tau=0.05 prob-blend + per-group min-1 with higher thr range to hit mean~4.44 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 2000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 4000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 6000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 8000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 10000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 12000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 14000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 16000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 18000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[accum] 20000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.400, chosen thr=0.502 (pred_mean=4.406)\n[INFO] Skipping submission write (best_thr not available).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.560 post-rule mean=4.774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.600 post-rule mean=4.606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.640 post-rule mean=4.480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.680 post-rule mean=4.380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WRITE] submission_knn_tau005_minGroup_swept_hi.csv thr=0.654 post-rule mean=4.441 | Elapsed 3.8m\n"
          ]
        }
      ]
    },
    {
      "id": "08683530-2e2f-4343-8e11-3b6ec1c713fc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, time, shutil, faiss\n",
        "from pathlib import Path\n",
        "\n",
        "print('=== kNN union (restricted): add up to L=3 kNN labels from {country,culture,dimension} with gate=0.08; sweep thr to mean~4.40 ===', flush=True)\n",
        "t0 = time.time()\n",
        "\n",
        "# Load CLIP embeddings\n",
        "Xtr = np.load('clip_train_emb.npy').astype(np.float32)\n",
        "Xte = np.load('clip_test_emb.npy').astype(np.float32)\n",
        "d = Xtr.shape[1]\n",
        "index = faiss.IndexFlatIP(d); index.add(Xtr)\n",
        "\n",
        "# Params\n",
        "k = 150; tau = 0.10; eta = 1.0; gamma_idf = 0.5\n",
        "sims, nn_idx = index.search(Xte, k)\n",
        "print('kNN search done:', sims.shape, flush=True)\n",
        "\n",
        "# Labels mapping\n",
        "labels_df = pd.read_csv('labels.csv')\n",
        "attr_ids_sorted = np.array(sorted(labels_df['attribute_id'].astype(int).unique().tolist()), dtype=np.int32)\n",
        "attr_to_col = {a:i for i,a in enumerate(attr_ids_sorted)}\n",
        "C = len(attr_ids_sorted)\n",
        "train_df = pd.read_csv('train.csv')\n",
        "n_train = len(train_df)\n",
        "\n",
        "# Train label lists and sizes + df counts\n",
        "lab_lists = []; lab_sizes = np.zeros(n_train, dtype=np.int32); df_counts = np.zeros(C, dtype=np.int32)\n",
        "for i, s in enumerate(train_df['attribute_ids'].fillna('').astype(str).tolist()):\n",
        "    if s:\n",
        "        cols = [attr_to_col[int(x)] for x in s.split() if x!='' and int(x) in attr_to_col]\n",
        "        lab_lists.append(cols); lab_sizes[i] = len(cols)\n",
        "        for c in set(cols): df_counts[c] += 1\n",
        "    else:\n",
        "        lab_lists.append([]); lab_sizes[i] = 0\n",
        "\n",
        "# Softmax weights with temperature\n",
        "sims_sm = sims / max(tau,1e-6)\n",
        "sims_sm -= sims_sm.max(axis=1, keepdims=True)\n",
        "w = np.exp(sims_sm); w /= (w.sum(axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "# Accumulate probs_knn with per-neighbor |L|^-eta\n",
        "ntest = Xte.shape[0]\n",
        "probs_knn = np.zeros((ntest, C), dtype=np.float32)\n",
        "for i in range(ntest):\n",
        "    nn = nn_idx[i]; wi = w[i]\n",
        "    for nbr, wj in zip(nn, wi):\n",
        "        Lj = lab_lists[nbr]\n",
        "        if not Lj: continue\n",
        "        denom = (lab_sizes[nbr] ** eta) if eta > 0 else 1.0\n",
        "        add = float(wj) / float(max(denom, 1.0))\n",
        "        for c in Lj: probs_knn[i, c] += add\n",
        "    if (i+1) % 2000 == 0: print(f'[kNN-accum] {i+1}/{ntest}', flush=True)\n",
        "\n",
        "# IDF^gamma and renormalize\n",
        "idf = np.log(n_train / (df_counts.astype(np.float32) + 1.0))\n",
        "idf = np.clip(idf, 0.0, None) ** gamma_idf\n",
        "probs_knn *= idf[None, :]\n",
        "row_sums = probs_knn.sum(axis=1, keepdims=True)\n",
        "probs_knn = np.divide(probs_knn, np.where(row_sums > 0, row_sums, 1.0), out=np.zeros_like(probs_knn), where=row_sums>0)\n",
        "\n",
        "# Load model probabilities (b3 2:1)\n",
        "from __main__ import blend_equal_weight\n",
        "dirs_b3 = ['out_b3_384_top512','out_b3_448_top512']\n",
        "_, _, P_model = blend_equal_weight(dirs_b3, weights=[2,1], write_submission=False, out_name='noop.csv', cardinality_target=4.40)\n",
        "assert P_model is not None and P_model.shape == probs_knn.shape\n",
        "\n",
        "# Groups and caps\n",
        "labels_df['group'] = labels_df['attribute_name'].astype(str).str.split('::').str[0].str.lower()\n",
        "idx_to_group = labels_df.set_index('attribute_id').loc[attr_ids_sorted, 'group'].values\n",
        "allowed_groups = {'country','culture','dimension'}\n",
        "caps = {'country':1,'culture':1,'dimension':1,'medium':2,'tags':6,'tag':6}\n",
        "default_cap = 3\n",
        "ids = pd.read_csv('sample_submission.csv')['id'].values\n",
        "\n",
        "def build_union_rows_restricted(thr, L=3, gate=0.08, min_total=3):\n",
        "    rows = []; counts = []\n",
        "    for i in range(P_model.shape[0]):\n",
        "        p = P_model[i]; pk = probs_knn[i]\n",
        "        cand = np.where(p >= thr)[0]\n",
        "        if cand.size == 0: cand = np.array([int(np.argmax(p))], dtype=np.int64)\n",
        "        cand_sorted = cand[np.argsort(-p[cand])]\n",
        "        used = {}; kept = []; kept_set = set()\n",
        "        # model thresholded + caps\n",
        "        for j in cand_sorted:\n",
        "            g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "            kcap = caps.get(g, default_cap); c = used.get(g, 0)\n",
        "            if c < kcap: kept.append(j); kept_set.add(j); used[g] = c + 1\n",
        "        # restricted union: only structural groups\n",
        "        if L > 0:\n",
        "            order_knn = np.argsort(-pk)\n",
        "            added = 0\n",
        "            for j in order_knn:\n",
        "                if pk[j] < gate: break\n",
        "                if j in kept_set: continue\n",
        "                g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "                if g not in allowed_groups: continue\n",
        "                kcap = caps.get(g, default_cap); c = used.get(g, 0)\n",
        "                if c < kcap:\n",
        "                    kept.append(int(j)); kept_set.add(int(j)); used[g] = c + 1; added += 1\n",
        "                    if added >= L: break\n",
        "        # enforce min_total using model ranking\n",
        "        if len(kept) < min_total:\n",
        "            order = np.argsort(-p)\n",
        "            for j in order:\n",
        "                if j in kept_set: continue\n",
        "                g = idx_to_group[j] if j < len(idx_to_group) else 'misc'\n",
        "                kcap = caps.get(g, default_cap); c = used.get(g, 0)\n",
        "                if c < kcap:\n",
        "                    kept.append(int(j)); kept_set.add(int(j)); used[g] = c + 1\n",
        "                    if len(kept) >= min_total: break\n",
        "        if len(kept) == 0: kept = [int(np.argmax(p))]\n",
        "        rows.append({'id': ids[i], 'attribute_ids': ' '.join(str(int(attr_ids_sorted[j])) for j in kept)})\n",
        "        counts.append(len(kept))\n",
        "    return rows, float(np.mean(counts))\n",
        "\n",
        "# Sweep thr to target mean ~4.40\n",
        "thrs = np.arange(0.460, 0.5401, 0.002)\n",
        "target_mean = 4.40\n",
        "best = None; best_rows = None\n",
        "for t in thrs:\n",
        "    rows_t, mean_t = build_union_rows_restricted(t, L=3, gate=0.08, min_total=3)\n",
        "    delta = abs(mean_t - target_mean)\n",
        "    if (best is None) or (delta < best[0]):\n",
        "        best = (delta, t, mean_t); best_rows = rows_t\n",
        "    if int((t-0.460)/0.002) % 10 == 0:\n",
        "        print(f'[SWEEP] thr={t:.3f} post-union mean={mean_t:.3f}', flush=True)\n",
        "\n",
        "_, best_thr, best_mean = best\n",
        "out_path = 'submission_union_knn_structL3.csv'\n",
        "pd.DataFrame(best_rows).to_csv(out_path, index=False)\n",
        "shutil.copyfile(out_path, 'submission.csv')\n",
        "print(f'[WRITE] {out_path} thr={best_thr:.3f} post-union mean={best_mean:.3f} | Elapsed {(time.time()-t0)/60:.1f}m', flush=True)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== kNN union (restricted): add up to L=3 kNN labels from {country,culture,dimension} with gate=0.08; sweep thr to mean~4.40 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kNN search done: (21318, 150)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 2000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 4000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 6000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 8000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 10000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 12000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 14000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 16000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 18000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[kNN-accum] 20000/21318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CARD] Train mean labels/img=4.421, target=4.400, chosen thr=0.502 (pred_mean=4.406)\n[INFO] Skipping submission write (best_thr not available).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.460 post-union mean=4.646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.480 post-union mean=4.404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.502 post-union mean=4.175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.520 post-union mean=4.009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SWEEP] thr=0.540 post-union mean=3.855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WRITE] submission_union_knn_structL3.csv thr=0.480 post-union mean=4.404 | Elapsed 2.8m\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}