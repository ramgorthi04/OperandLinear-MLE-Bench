{
  "cells": [
    {
      "id": "e22b3c3f-b20c-48c1-bc8e-32c551791217",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OOF Analysis and Threshold Optimization\n",
        "\n",
        "The goal of this notebook is to:\n",
        "1. Generate Out-of-Fold (OOF) predictions using the 3 models trained previously.\n",
        "2. Analyze the OOF predictions to find a more optimal F1 threshold than the one found during individual fold training.\n",
        "3. Potentially explore more advanced thresholding techniques (e.g., per-class thresholds).\n",
        "4. Generate a new submission file using the optimized threshold(s)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "da4263d9-3205-46f8-a684-975f10285a71",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import timm\n",
        "from torchvision import transforms as T\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "class CFG:\n",
        "    # General\n",
        "    seed = 42\n",
        "    num_workers = 0 \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Data paths\n",
        "    data_dir = './'\n",
        "    train_csv_path = os.path.join(data_dir, 'train.csv')\n",
        "    labels_csv_path = os.path.join(data_dir, 'labels.csv')\n",
        "    train_img_dir = os.path.join(data_dir, 'train')\n",
        "\n",
        "    # Model\n",
        "    model_name = 'tf_efficientnet_b4_ns'\n",
        "    img_size = 384\n",
        "    num_classes = None # Placeholder, will be set after class definition\n",
        "    model_paths = [\n",
        "        'models/tf_efficientnet_b4_ns_fold0_best.pth',\n",
        "        'models/tf_efficientnet_b4_ns_fold1_best.pth',\n",
        "        'models/tf_efficientnet_b4_ns_fold2_best.pth'\n",
        "    ]\n",
        "    n_folds = 3\n",
        "\n",
        "    # Inference\n",
        "    batch_size = 16 # OOM with 32, reducing to 16\n",
        "\n",
        "# Set num_classes correctly after CFG class is defined\n",
        "CFG.num_classes = len(pd.read_csv(CFG.labels_csv_path))\n",
        "\n",
        "# Clean up memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Sanity check model paths and num_classes\n",
        "print(\"--- Verifying Model Paths ---\")\n",
        "for path in CFG.model_paths:\n",
        "    if os.path.exists(path):\n",
        "        print(f\"OK: Found {path} (Size: {os.path.getsize(path) / 1e6:.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"ERROR: Missing {path}\")\n",
        "print(f\"Number of classes set in CFG: {CFG.num_classes}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "d8ba6f86-389a-4fd1-a461-2f3dadb0e628",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DEBUG: List directory contents to resolve path ambiguity using os module\n",
        "import os\n",
        "print(\"--- Listing CWD Contents ---\")\n",
        "try:\n",
        "    print(os.listdir('.'))\n",
        "except Exception as e:\n",
        "    print(f\"Error listing CWD: {e}\")\n",
        "\n",
        "print(\"\\n--- Listing 'models/' Contents ---\")\n",
        "try:\n",
        "    print(os.listdir('models'))\n",
        "except Exception as e:\n",
        "    print(f\"Error listing 'models/': {e}\")\n",
        "\n",
        "print(\"\\n--- Listing 'train/' Contents ---\")\n",
        "try:\n",
        "    print(os.listdir('train'))\n",
        "except Exception as e:\n",
        "    print(f\"Error listing 'train/': {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "02675af0-6f9a-4ea2-8979-5ea85d8812f8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Target Recreation (THE FINAL FIX)\n",
        "# After comparing with `01_training_pipeline.ipynb`, the root cause is identified.\n",
        "# The training script first AGGREGATES `train.csv` by `id` and then creates targets/folds.\n",
        "# My previous OOF script skipped the aggregation, leading to a structural mismatch.\n",
        "# This fix replicates the aggregation step exactly.\n",
        "\n",
        "print(\"--- Re-creating Dataframe with Aggregation and Correct Label Mapping ---\")\n",
        "\n",
        "# 1. Load base dataframes\n",
        "train_df_raw = pd.read_csv(CFG.train_csv_path)\n",
        "labels_df = pd.read_csv(CFG.labels_csv_path)\n",
        "folds_df = pd.read_feather('train_folds.feather')[['id', 'fold']] # Only use this for fold assignments\n",
        "\n",
        "# 2. Aggregate train_df_raw, exactly as in the training notebook.\n",
        "# This creates one row per image and sorts the dataframe by 'id' due to groupby's behavior.\n",
        "print(\"Aggregating train data by image ID...\")\n",
        "df_agg = train_df_raw.groupby('id')['attribute_ids'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "# 3. Merge the aggregated dataframe with the fold assignments.\n",
        "# Since both df_agg and folds_df are sorted by 'id', the merge is clean and preserves order.\n",
        "print(\"Merging aggregated data with folds...\")\n",
        "df = pd.merge(df_agg, folds_df, on='id')\n",
        "\n",
        "# 4. Create the correct attribute_id -> index mapping\n",
        "attr_ids_from_csv = labels_df['attribute_id'].values\n",
        "attr_id_to_idx = {attr_id: i for i, attr_id in enumerate(attr_ids_from_csv)}\n",
        "CFG.num_classes = len(labels_df)\n",
        "print(f\"Number of classes set from labels.csv: {CFG.num_classes}\")\n",
        "\n",
        "# 5. Create the one-hot encoded targets using the correct mapping on the correctly structured df\n",
        "targets = np.zeros((len(df), CFG.num_classes), dtype=np.int8)\n",
        "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Creating target vectors\"):\n",
        "    attr_ids_for_image = [int(attr_id) for attr_id in row['attribute_ids'].split()]\n",
        "    for attr_id in attr_ids_for_image:\n",
        "        if attr_id in attr_id_to_idx:\n",
        "            targets[i, attr_id_to_idx[attr_id]] = 1\n",
        "\n",
        "# 6. Add targets and filepaths to the dataframe\n",
        "df['targets'] = list(targets)\n",
        "df['filepath'] = df['id'].apply(lambda x: os.path.join(CFG.train_img_dir, f'{x}.png'))\n",
        "\n",
        "# 7. Final verification\n",
        "print(f\"Shape of the final dataframe: {df.shape}\")\n",
        "assert df.shape[0] == 120801, \"Dataframe has incorrect number of rows after aggregation!\"\n",
        "display(df.head())\n",
        "print(f\"Fold distribution:\\n{df['fold'].value_counts()}\")\n",
        "assert len(df['targets'].iloc[0]) == CFG.num_classes, \"Target vector length mismatch!\"\n",
        "print(\"Target creation and data preparation successful. The dataframe now matches the training structure.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1fe8e95f-6713-4dc4-a971-42116be4a4a7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_valid_transforms():\n",
        "    # CORRECTED: Based on 01_training_pipeline.ipynb, the validation transforms\n",
        "    # use aspect-ratio preserving resize, center crop, and ImageNet normalization.\n",
        "    # This was the critical bug.\n",
        "    print(\"--- Applying CORRECTED validation transforms (Resize+CenterCrop, ImageNet Norm) ---\")\n",
        "    return T.Compose([\n",
        "        T.Resize(CFG.img_size), # Preserves aspect ratio, resizes smaller edge to img_size\n",
        "        T.CenterCrop(CFG.img_size),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225],\n",
        "        ),\n",
        "    ])\n",
        "\n",
        "class iMetDataset(Dataset):\n",
        "    def __init__(self, df, transforms=None):\n",
        "        self.df = df\n",
        "        self.filepaths = df['filepath'].values\n",
        "        self.labels = df['targets'].values\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filepath = self.filepaths[idx]\n",
        "        image = Image.open(filepath).convert('RGB')\n",
        "        \n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "            \n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return image, label\n",
        "\n",
        "class iMetModel(nn.Module):\n",
        "    def __init__(self, model_name, pretrained=False): # Set pretrained=False as we load weights manually\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=CFG.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "29e99f8d-c43c-47a3-8bfd-3d8067f1665b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DEBUG: Verify the normalization constants being loaded\n",
        "\n",
        "print(\"--- Verifying data config from timm ---\")\n",
        "try:\n",
        "    # This is the function the expert suggested for a quick check\n",
        "    data_config = timm.data.resolve_model_data_config(CFG.model_name)\n",
        "    print(\"Result from timm.data.resolve_model_data_config:\")\n",
        "    print(f\"  Mean: {data_config['mean']}\")\n",
        "    print(f\"  Std: {data_config['std']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error with resolve_model_data_config: {e}\")\n",
        "\n",
        "print(\"\\nThis confirms that `timm` is resolving the config to standard ImageNet defaults, not TF-style defaults.\")\n",
        "print(\"The expert was right about the *type* of bug (normalization mismatch), but the suggested fix using `resolve_data_config` did not work as expected because of how timm handles this specific deprecated model name.\")\n",
        "print(\"The correct normalization for TF-trained models is mean=[0.5, 0.5, 0.5] and std=[0.5, 0.5, 0.5]. I will apply this manually.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "d9811207-2808-4271-8d0b-90806ebc81fb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# EXPERT ADVICE: Purge stale OOF artifacts before regenerating\n",
        "print(\"--- Purging stale OOF files ---\")\n",
        "stale_files = ['oof_preds.npy', 'oof_labels.npy']\n",
        "for f in stale_files:\n",
        "    if os.path.exists(f):\n",
        "        os.remove(f)\n",
        "        print(f\"Removed stale file: {f}\")\n",
        "    else:\n",
        "        print(f\"File not found (already clean): {f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "5d277675-13e6-4736-b29c-d1b1f855eef5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# EXPERT ADVICE: Quick single-image probe (sanity check)\n",
        "print(\"--- Running single-image sanity check ---\")\n",
        "sample_df = df.iloc[[0]].reset_index(drop=True)\n",
        "# Get the transforms defined in cell 4\n",
        "valid_transforms = get_valid_transforms()\n",
        "ds = iMetDataset(sample_df, transforms=valid_transforms)\n",
        "x, y = ds[0]\n",
        "\n",
        "# Load model correctly\n",
        "model = iMetModel(CFG.model_name, pretrained=False).to(CFG.device)\n",
        "state_dict = torch.load(CFG.model_paths[0], map_location=CFG.device)\n",
        "model.load_state_dict(state_dict, strict=True)\n",
        "model.eval()\n",
        "\n",
        "# Run inference on the single image\n",
        "with torch.no_grad():\n",
        "    p = model(x.unsqueeze(0).to(CFG.device)).sigmoid().cpu().numpy()\n",
        "\n",
        "print(f'Probs range for single image: min={p.min():.6f}, max={p.max():.6f}')\n",
        "max_prob_idx = np.argmax(p)\n",
        "print(f'Predicted class index with max prob: {max_prob_idx}')\n",
        "\n",
        "true_labels_indices = np.where(y.numpy() == 1)[0]\n",
        "print(f'True label indices: {true_labels_indices}')\n",
        "\n",
        "# Check if the max prob corresponds to a true label\n",
        "if max_prob_idx in true_labels_indices:\n",
        "    print(\"\\nSUCCESS: The highest probability prediction corresponds to a true label.\")\n",
        "    print(\"DIAGNOSIS: The basic pipeline (model, weights, transforms) is correct for a single item.\")\n",
        "    print(\"The bug is likely in the batching/looping logic of the full OOF generation.\")\n",
        "else:\n",
        "    print(\"\\nFAILURE: The highest probability prediction does NOT correspond to a true label.\")\n",
        "    print(\"DIAGNOSIS: The transforms or model weights are fundamentally wrong, despite the high probability value.\")\n",
        "\n",
        "del model, state_dict, ds, x, y, p\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "818f8b10-52f0-4acf-86f9-698ac829ce41",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## 3. Generate OOF Predictions (BREAKTHROUGH FIX APPLIED)\n",
        "\n",
        "# This version uses the corrected 'squishing' transform and a larger batch size.\n",
        "# Diagnostic prints have been removed for a clean, fast run.\n",
        "\n",
        "# Initialize lists to store predictions, labels, and indices from all folds\n",
        "all_preds_list = []\n",
        "all_labels_list = []\n",
        "all_indices_list = []\n",
        "\n",
        "for fold in range(CFG.n_folds):\n",
        "    print(f\"--- Generating OOF for Fold {fold} ---\")\n",
        "    \n",
        "    model_path = CFG.model_paths[fold]\n",
        "    \n",
        "    # FIX for OOM: Create model on CPU, load weights, THEN move to GPU.\n",
        "    model = iMetModel(CFG.model_name, pretrained=False) # 1. Create on CPU\n",
        "    state_dict = torch.load(model_path, map_location='cpu', weights_only=True) # 2. Load weights to CPU\n",
        "    model.load_state_dict(state_dict, strict=True)\n",
        "    model.to(CFG.device) # 3. Move fully loaded model to GPU\n",
        "    model.eval()\n",
        "    \n",
        "    valid_df_fold = df[df['fold'] == fold]\n",
        "    original_indices = valid_df_fold.index.values\n",
        "    \n",
        "    valid_dataset = iMetDataset(valid_df_fold.reset_index(drop=True), transforms=get_valid_transforms())\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)\n",
        "    \n",
        "    fold_preds = []\n",
        "    fold_labels = []\n",
        "    pbar = tqdm(valid_loader, desc=f\"Predicting Fold {fold}\")\n",
        "    with torch.no_grad():\n",
        "        for images, labels in pbar:\n",
        "            images = images.to(CFG.device)\n",
        "            logits = model(images)\n",
        "            preds = logits.sigmoid().cpu().numpy()\n",
        "            \n",
        "            fold_preds.append(preds)\n",
        "            fold_labels.append(labels.cpu().numpy())\n",
        "    \n",
        "    fold_preds = np.concatenate(fold_preds)\n",
        "    fold_labels = np.concatenate(fold_labels)\n",
        "    \n",
        "    all_preds_list.append(fold_preds)\n",
        "    all_labels_list.append(fold_labels)\n",
        "    all_indices_list.append(original_indices)\n",
        "    \n",
        "    del model, state_dict, valid_df_fold, valid_dataset, valid_loader, fold_preds, fold_labels\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nFinished prediction loops for all folds.\")\n",
        "\n",
        "# Re-assemble the OOF predictions in the original dataframe order\n",
        "all_preds = np.concatenate(all_preds_list)\n",
        "all_labels = np.concatenate(all_labels_list)\n",
        "all_indices = np.concatenate(all_indices_list)\n",
        "sort_order = np.argsort(all_indices)\n",
        "oof_preds = all_preds[sort_order]\n",
        "oof_labels = all_labels[sort_order]\n",
        "\n",
        "print(\"Verifying alignment...\")\n",
        "expected_labels = np.stack(df['targets'].values)\n",
        "assert np.array_equal(oof_labels, expected_labels), \"FATAL: OOF labels do not match!\"\n",
        "print(\"Verification successful. OOF arrays are correctly aligned.\")\n",
        "\n",
        "np.save('oof_preds.npy', oof_preds)\n",
        "np.save('oof_labels.npy', oof_labels)\n",
        "print(\"OOF predictions and labels saved to .npy files.\")\n",
        "\n",
        "# Final check of the overall OOF F1 score\n",
        "print(\"\\n--- Calculating Final OOF F1 Score ---\")\n",
        "best_f1 = 0\n",
        "best_threshold = 0\n",
        "for threshold in np.linspace(0.1, 0.4, 31):\n",
        "    f1 = f1_score(oof_labels, (oof_preds > threshold).astype(int), average='micro')\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_threshold = threshold\n",
        "print(f\"Best OOF F1 Score: {best_f1:.4f} at threshold {best_threshold:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "04f835cc-8866-46aa-b39a-a1192903be05",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4. Spearman Correlation Check\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "print(\"--- Performing Spearman Correlation Check ---\")\n",
        "\n",
        "# 1. Load the classifier bias from the trained model\n",
        "state = torch.load(CFG.model_paths[0], map_location='cpu')\n",
        "bias = state['model.classifier.bias'].cpu().numpy()\n",
        "print(f\"Loaded bias vector with shape: {bias.shape}\")\n",
        "\n",
        "# 2. Calculate class frequencies from the current dataframe\n",
        "# This assumes the 'targets' column in the current 'df' reflects the class order we are testing\n",
        "freq = np.array(np.stack(df['targets'].values)).mean(0)\n",
        "print(f\"Calculated frequency vector with shape: {freq.shape}\")\n",
        "\n",
        "# 3. Check for shape mismatch\n",
        "if bias.shape[0] != freq.shape[0]:\n",
        "    print(f\"FATAL: Shape mismatch! Bias shape {bias.shape} vs Freq shape {freq.shape}\")\n",
        "else:\n",
        "    # 4. Compute Spearman correlation\n",
        "    corr, pval = spearmanr(bias, freq)\n",
        "    print(f'Spearman corr(bias, freq): {corr:.4f}')\n",
        "    print(f'p-value: {pval}')\n",
        "\n",
        "    if np.abs(corr) < 0.1:\n",
        "        print(\"\\nWARNING: Correlation is near zero. This strongly suggests a class index mismatch between training and inference.\")\n",
        "    else:\n",
        "        print(\"\\nINFO: Correlation is significant. The class index mapping is likely correct.\")\n",
        "\n",
        "del state, bias, freq\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "14c5cdde-3f5f-4277-be7d-78fd1aa3f1f3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DEBUG: Verify file existence before loading\n",
        "import os\n",
        "print('--- CWD contents before loading OOF files ---')\n",
        "print(sorted(os.listdir('.')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "61721741-ebd6-4c35-ae08-242e3d07c439",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# This cell was modified based on expert advice to use sample-wise F2 and a wider threshold range.\n",
        "# Second run: Refining the search at higher thresholds as the score was still rising at 0.90.\n",
        "\n",
        "print(\"--- Finding Best Global Threshold (Sample-wise F2 - Refined High Range) ---\")\n",
        "\n",
        "# Reload OOF predictions and labels to ensure a clean state\n",
        "oof_preds = np.load('oof_preds.npy')\n",
        "oof_labels = np.load('oof_labels.npy')\n",
        "\n",
        "# Define a more granular range of thresholds at the high end\n",
        "thresholds = np.arange(0.85, 1.0, 0.01)\n",
        "scores = []\n",
        "avg_preds_per_image = []\n",
        "avg_trues_per_image = np.mean(oof_labels.sum(axis=1))\n",
        "\n",
        "# Calculate F-beta score for each threshold\n",
        "pbar = tqdm(thresholds, desc=\"Searching thresholds for Sample-wise F2 (High Range)\")\n",
        "for threshold in pbar:\n",
        "    y_pred_thresh = (oof_preds > threshold).astype(np.int8)\n",
        "    \n",
        "    # Apply fallback for empty predictions (vectorized)\n",
        "    empty_preds_mask = y_pred_thresh.sum(axis=1) == 0\n",
        "    if np.any(empty_preds_mask):\n",
        "        argmax_indices = oof_preds[empty_preds_mask].argmax(axis=1)\n",
        "        y_pred_thresh[empty_preds_mask, argmax_indices] = 1\n",
        "\n",
        "    # Calculate sample-wise F2 score\n",
        "    score = fbeta_score(oof_labels, y_pred_thresh, beta=2, average='samples', zero_division=0)\n",
        "    scores.append(score)\n",
        "    avg_preds_per_image.append(np.mean(y_pred_thresh.sum(axis=1)))\n",
        "\n",
        "# Find the best threshold and score\n",
        "best_score = max(scores)\n",
        "best_threshold_idx = np.argmax(scores)\n",
        "best_threshold = thresholds[best_threshold_idx]\n",
        "best_avg_preds = avg_preds_per_image[best_threshold_idx]\n",
        "\n",
        "print(f\"\\nBest Sample-wise F2 score on OOF: {best_score:.4f}\")\n",
        "print(f\"Best threshold: {best_threshold:.4f}\")\n",
        "print(f\"Avg predicted labels/image at best threshold: {best_avg_preds:.2f}\")\n",
        "print(f\"Avg true labels/image in OOF: {avg_trues_per_image:.2f}\")\n",
        "\n",
        "# Plot the scores\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot F2 score\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(thresholds, scores, marker='o')\n",
        "plt.title('Global Threshold vs. Sample-wise F2 Score (High Range)')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Sample-wise F2 Score')\n",
        "plt.grid(True)\n",
        "plt.axvline(best_threshold, color='r', linestyle='--', label=f'Best T={best_threshold:.2f}')\n",
        "plt.legend()\n",
        "\n",
        "# Plot avg labels\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(thresholds, avg_preds_per_image, marker='o', label='Avg. Predicted Labels')\n",
        "plt.axhline(avg_trues_per_image, color='g', linestyle='--', label=f'Avg. True Labels ({avg_trues_per_image:.2f})')\n",
        "plt.title('Avg. Predicted Labels vs. Threshold (High Range)')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Avg. Labels per Image')\n",
        "plt.grid(True)\n",
        "plt.axvline(best_threshold, color='r', linestyle='--', label=f'Best T={best_threshold:.2f}')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "e4e529c1-6c81-4499-ba93-71f96d06d022",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"--- Visualizing OOF Prediction Distribution ---\")\n",
        "\n",
        "# Plot a histogram of all predictions\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(oof_preds.flatten(), bins=100, log=True)\n",
        "plt.title('Distribution of OOF Predictions (Log Scale)')\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Frequency (Log Scale)')\n",
        "plt.axvline(best_threshold, color='r', linestyle='--', label=f'Global Best Threshold ({best_threshold:.4f})')\n",
        "plt.legend()\n",
        "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDiagnosis:\")\n",
        "print(\"The histogram shows the vast majority of predictions are very close to 0. \")\n",
        "print(\"This forces the optimizer to pick a very high threshold to separate the few confident positive predictions from the sea of negatives, leading to high precision but terrible recall.\")\n",
        "print(\"This strongly supports the per-class thresholding strategy.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a4106562-a856-46b4-81f6-1ad07e732ff5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"--- Analyzing Metrics at Best Global Threshold: {best_threshold:.4f} ---\")\n",
        "\n",
        "# Ensure oof_preds and oof_labels are loaded\n",
        "if 'oof_preds' not in locals():\n",
        "    print('Reloading OOF files...')\n",
        "    oof_preds = np.load('oof_preds.npy')\n",
        "    oof_labels = np.load('oof_labels.npy').astype(np.uint8)\n",
        "\n",
        "# Get predictions using the best global threshold\n",
        "y_pred = (oof_preds > best_threshold).astype(np.uint8)\n",
        "y_true = oof_labels\n",
        "\n",
        "# Manually calculate micro-averaged precision and recall for speed\n",
        "# This is much faster than sklearn's implementation for large arrays\n",
        "tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "print(f\"Micro Precision: {precision:.4f}\")\n",
        "print(f\"Micro Recall:    {recall:.4f}\")\n",
        "print(f\"Micro F1-Score:  {f1:.4f} (should match previous calculation)\")\n",
        "\n",
        "print(\"\\nDiagnosis:\")\n",
        "if recall < 0.3:\n",
        "    print(\"Confirmed: Recall is extremely low, as hypothesized. The high threshold is filtering out too many true positives.\")\n",
        "else:\n",
        "    print(\"Recall is not as low as expected. The issue might be more complex.\")\n",
        "\n",
        "# Also, let's see how many images have ZERO predictions with this threshold\n",
        "num_no_preds = np.sum(y_pred.sum(axis=1) == 0)\n",
        "print(f\"\\nNumber of images with ZERO predicted labels: {num_no_preds} / {len(y_pred)} ({num_no_preds/len(y_pred)*100:.2f}%)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8fda2e54-07b2-4818-ab5d-166c93ab1962",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# This cell is modified to implement a highly efficient, sorting-based per-class F2 thresholding.\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"--- Finding Optimal Thresholds Per Class (F2 Score, Efficient Sorting Method) ---\")\n",
        "\n",
        "# Ensure oof_preds and oof_labels are loaded\n",
        "if 'oof_preds' not in locals() or 'oof_labels' not in locals():\n",
        "    print('Reloading OOF files...')\n",
        "    oof_preds = np.load('oof_preds.npy')\n",
        "    oof_labels = np.load('oof_labels.npy')\n",
        "\n",
        "# Use the best global threshold as a default for regularization\n",
        "BEST_GLOBAL_THRESHOLD = 0.93\n",
        "MIN_SAMPLES_FOR_OPT = 5\n",
        "beta = 2\n",
        "beta2 = beta**2\n",
        "\n",
        "num_classes = oof_preds.shape[1]\n",
        "per_class_thresholds = np.zeros(num_classes)\n",
        "class_counts = oof_labels.sum(axis=0)\n",
        "low_sample_mask = class_counts < MIN_SAMPLES_FOR_OPT\n",
        "num_low_sample_classes = np.sum(low_sample_mask)\n",
        "\n",
        "print(f\"Found {num_low_sample_classes} classes with < {MIN_SAMPLES_FOR_OPT} samples. Using global threshold for them.\")\n",
        "per_class_thresholds[low_sample_mask] = BEST_GLOBAL_THRESHOLD\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Loop over classes that have enough samples\n",
        "optimizable_classes_indices = np.where(~low_sample_mask)[0]\n",
        "\n",
        "for i in tqdm(optimizable_classes_indices, desc=\"Optimizing thresholds\"):\n",
        "    y_true_class = oof_labels[:, i]\n",
        "    y_pred_probs_class = oof_preds[:, i]\n",
        "    \n",
        "    # Sort predictions and labels\n",
        "    desc_score_indices = np.argsort(y_pred_probs_class, kind=\"mergesort\")[::-1]\n",
        "    y_pred_probs_class = y_pred_probs_class[desc_score_indices]\n",
        "    y_true_class = y_true_class[desc_score_indices]\n",
        "    \n",
        "    # Calculate cumulative true positives and false positives\n",
        "    tps = np.cumsum(y_true_class)\n",
        "    fps = np.arange(1, len(y_true_class) + 1) - tps\n",
        "    \n",
        "    # Total number of positive samples for this class\n",
        "    total_positives = np.sum(y_true_class)\n",
        "    if total_positives == 0:\n",
        "        per_class_thresholds[i] = BEST_GLOBAL_THRESHOLD\n",
        "        continue\n",
        "        \n",
        "    # Calculate F2 score for each possible threshold\n",
        "    # (The thresholds are the prediction values themselves)\n",
        "    precision = tps / (tps + fps)\n",
        "    recall = tps / total_positives\n",
        "    \n",
        "    denominator = (beta2 * precision) + recall\n",
        "    f2_scores = np.divide((1 + beta2) * (precision * recall), denominator, out=np.zeros_like(denominator, dtype=float), where=denominator!=0)\n",
        "    \n",
        "    # Find the best score and corresponding threshold\n",
        "    best_idx = np.argmax(f2_scores)\n",
        "    best_threshold = y_pred_probs_class[best_idx]\n",
        "    per_class_thresholds[i] = best_threshold\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nFinished optimizing {len(optimizable_classes_indices)} thresholds in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "# Save the thresholds for inference\n",
        "np.save('per_class_thresholds.npy', per_class_thresholds)\n",
        "print(\"Per-class thresholds saved to 'per_class_thresholds.npy'\")\n",
        "\n",
        "# Display some stats about the found thresholds\n",
        "print(f\"\\nThreshold stats:\")\n",
        "print(f\"  Min: {np.min(per_class_thresholds):.4f}\")\n",
        "print(f\"  Max: {np.max(per_class_thresholds):.4f}\")\n",
        "print(f\"  Mean: {np.mean(per_class_thresholds):.4f}\")\n",
        "print(f\"  Median: {np.median(per_class_thresholds):.4f}\")\n",
        "print(f\"  Number of classes using global threshold: {num_low_sample_classes}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "eaa159d1-1a38-48ad-aed5-de17f50b40d7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"--- Evaluating OOF Score with Per-Class Thresholds ---\")\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "# Ensure oof_preds and oof_labels are loaded\n",
        "if 'oof_preds' not in locals() or 'oof_labels' not in locals():\n",
        "    print('Reloading OOF files...')\n",
        "    oof_preds = np.load('oof_preds.npy')\n",
        "    oof_labels = np.load('oof_labels.npy')\n",
        "\n",
        "# Load the per-class thresholds\n",
        "try:\n",
        "    per_class_thresholds = np.load('per_class_thresholds.npy')\n",
        "    print(\"Loaded per-class thresholds.\")\n",
        "\n",
        "    # Apply per-class thresholds\n",
        "    # Broadcasting: oof_preds (N, C) > per_class_thresholds (C,)\n",
        "    y_pred_thresh = (oof_preds > per_class_thresholds).astype(np.int8)\n",
        "\n",
        "    # Apply fallback for empty predictions\n",
        "    empty_preds_mask = y_pred_thresh.sum(axis=1) == 0\n",
        "    num_empty = np.sum(empty_preds_mask)\n",
        "    if num_empty > 0:\n",
        "        print(f\"Found {num_empty} images with no predictions. Applying fallback (argmax).\")\n",
        "        argmax_indices = oof_preds[empty_preds_mask].argmax(axis=1)\n",
        "        rows_to_update = np.where(empty_preds_mask)[0]\n",
        "        y_pred_thresh[rows_to_update, argmax_indices] = 1\n",
        "\n",
        "    # Calculate sample-wise F2 score\n",
        "    score = fbeta_score(oof_labels, y_pred_thresh, beta=2, average='samples', zero_division=0)\n",
        "    \n",
        "    print(f\"\\nOOF Sample-wise F2 score with Per-Class Thresholds: {score:.4f}\")\n",
        "    \n",
        "    if score > 0.40:\n",
        "        print(\"SUCCESS: Score is above the medal target! Proceeding to submission.\")\n",
        "    else:\n",
        "        print(\"INFO: Score is a significant improvement. This is the best strategy available.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: 'per_class_thresholds.npy' not found. Please run the cell above to generate it.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "3a7de1ae-1325-4297-9cf7-83be0192a69b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import fbeta_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"--- Tuning Blending Alpha for Final Submission ---\")\n",
        "\n",
        "# Load the components\n",
        "try:\n",
        "    per_class_thresholds = np.load('per_class_thresholds.npy')\n",
        "    global_threshold = 0.93 # The best global threshold found previously\n",
        "    oof_preds = np.load('oof_preds.npy')\n",
        "    oof_labels = np.load('oof_labels.npy')\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"ERROR: Missing a required .npy file: {e}. Please run prerequisite cells.\")\n",
        "    raise e\n",
        "\n",
        "alpha_values = [0.2, 0.15, 0.1, 0.05]\n",
        "best_score = -1\n",
        "best_alpha = -1\n",
        "best_thresholds = None\n",
        "\n",
        "for alpha in tqdm(alpha_values, desc=\"Tuning Alpha\"):\n",
        "    # Create blended thresholds\n",
        "    blended_thresholds = alpha * per_class_thresholds + (1 - alpha) * global_threshold\n",
        "    \n",
        "    # Apply blended thresholds\n",
        "    y_pred_thresh = (oof_preds > blended_thresholds).astype(np.int8)\n",
        "    \n",
        "    # Apply fallback for empty predictions\n",
        "    empty_preds_mask = y_pred_thresh.sum(axis=1) == 0\n",
        "    if np.any(empty_preds_mask):\n",
        "        argmax_indices = oof_preds[empty_preds_mask].argmax(axis=1)\n",
        "        rows_to_update = np.where(empty_preds_mask)[0]\n",
        "        y_pred_thresh[rows_to_update, argmax_indices] = 1\n",
        "        \n",
        "    # Calculate sample-wise F2 score\n",
        "    score = fbeta_score(oof_labels, y_pred_thresh, beta=2, average='samples', zero_division=0)\n",
        "    print(f\"Alpha = {alpha:.2f} -> OOF F2 Score: {score:.4f}\")\n",
        "    \n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_alpha = alpha\n",
        "        best_thresholds = blended_thresholds\n",
        "\n",
        "print(f\"\\nBest alpha found: {best_alpha:.2f} with score {best_score:.4f}\")\n",
        "np.save('thresholds_final.npy', best_thresholds)\n",
        "print(\"Saved best thresholds to 'thresholds_final.npy' for use with a non-TTA inference notebook.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tuning Blending Alpha for Final Submission ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTuning Alpha:   0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTuning Alpha:  25%|\u2588\u2588\u258c       | 1/4 [00:38<01:56, 38.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha = 0.20 -> OOF F2 Score: 0.3668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTuning Alpha:  50%|\u2588\u2588\u2588\u2588\u2588     | 2/4 [01:17<01:17, 38.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha = 0.15 -> OOF F2 Score: 0.3558\n"
          ]
        }
      ]
    },
    {
      "id": "997ba436-4587-423f-9f68-51e7e1c87d89",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"--- Generating Final Thresholds with alpha=0.15 ---\")\n",
        "\n",
        "# Based on the interrupted tuning run, alpha=0.15 gave an OOF F2 of 0.3558.\n",
        "# This is even more conservative and might generalize better. This is the final attempt.\n",
        "\n",
        "# Load the components\n",
        "per_class_thresholds = np.load('per_class_thresholds.npy')\n",
        "global_threshold = 0.93\n",
        "alpha = 0.15\n",
        "\n",
        "# Create blended thresholds\n",
        "blended_thresholds = alpha * per_class_thresholds + (1 - alpha) * global_threshold\n",
        "\n",
        "# Save the final thresholds for the non-TTA inference script\n",
        "np.save('thresholds_final.npy', blended_thresholds)\n",
        "print(\"Saved final thresholds with alpha=0.15 to 'thresholds_final.npy'\")\n",
        "print(f\"Threshold stats: min={blended_thresholds.min():.4f}, max={blended_thresholds.max():.4f}, mean={blended_thresholds.mean():.4f}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generating Final Thresholds with alpha=0.15 ---\nSaved final thresholds with alpha=0.15 to 'thresholds_final.npy'\nThreshold stats: min=0.8490, max=0.9405, mean=0.9351\n"
          ]
        }
      ]
    },
    {
      "id": "8d156e34-4ff0-43ac-83dd-5564b2984ac5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# FINAL ATTEMPT: Re-evaluating Blended Thresholds with Full Guard Logic\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import fbeta_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"--- Re-evaluating Blended Thresholds (alpha=0.25) with Max-K Guard ---\")\n",
        "\n",
        "# Load necessary data\n",
        "try:\n",
        "    oof_preds = np.load('oof_preds.npy')\n",
        "    oof_labels = np.load('oof_labels.npy')\n",
        "    # This file was created by cell 16 with alpha=0.25\n",
        "    blended_thresholds = np.load('blended_thresholds.npy')\n",
        "    print(\"Loaded OOF preds, labels, and blended thresholds (alpha=0.25).\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"ERROR: Missing a required .npy file: {e}. Please run previous cells.\")\n",
        "    raise e\n",
        "\n",
        "# Define the full guarded apply_thresholds function\n",
        "def apply_thresholds_guarded(probs, th):\n",
        "    y = (probs > th).astype(np.uint8)\n",
        "    # At-least-one fallback\n",
        "    empty = y.sum(axis=1) == 0\n",
        "    if np.any(empty):\n",
        "        idx = probs[empty].argmax(axis=1)\n",
        "        rows = np.where(empty)[0]\n",
        "        y[rows, idx] = 1\n",
        "    # Max-K cap\n",
        "    max_k = 3\n",
        "    row_sum = y.sum(axis=1)\n",
        "    too_many = np.where(row_sum > max_k)[0]\n",
        "    if too_many.size:\n",
        "        # Using tqdm to see progress as this loop can be slow\n",
        "        for r in tqdm(too_many, desc=\"Applying max-k cap\"):\n",
        "            topk = np.argpartition(-probs[r], max_k)[:max_k]\n",
        "            y[r] = 0\n",
        "            y[r, topk] = 1\n",
        "    return y\n",
        "\n",
        "# Evaluate with the guards\n",
        "print(\"Applying blended thresholds with full guards...\")\n",
        "y_pred_guarded = apply_thresholds_guarded(oof_preds, blended_thresholds)\n",
        "score = fbeta_score(oof_labels, y_pred_guarded, beta=2, average='samples', zero_division=0)\n",
        "\n",
        "print(f\"\\nOOF F2 Score with Blended (alpha=0.25) + Guards: {score:.4f}\")\n",
        "print(\"Original score without max-k guard was: 0.3775\")\n",
        "\n",
        "# This seems like a promising candidate. Let's save these thresholds for the final submission.\n",
        "np.save('thresholds_final.npy', blended_thresholds)\n",
        "print(\"Saved these blended thresholds as 'thresholds_final.npy' for the final submission.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Re-evaluating Blended Thresholds (alpha=0.25) with Max-K Guard ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded OOF preds, labels, and blended thresholds (alpha=0.25).\nApplying blended thresholds with full guards...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:   0%|          | 0/110498 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:   3%|\u258e         | 2848/110498 [00:00<00:03, 28468.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:   5%|\u258c         | 5705/110498 [00:00<00:03, 28522.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:   8%|\u258a         | 8562/110498 [00:00<00:03, 28543.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  10%|\u2588         | 11444/110498 [00:00<00:03, 28649.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  13%|\u2588\u258e        | 14332/110498 [00:00<00:03, 28730.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  16%|\u2588\u258c        | 17226/110498 [00:00<00:03, 28800.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  18%|\u2588\u258a        | 20122/110498 [00:00<00:03, 28851.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  21%|\u2588\u2588        | 23008/110498 [00:00<00:03, 28806.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  23%|\u2588\u2588\u258e       | 25889/110498 [00:00<00:02, 28800.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  26%|\u2588\u2588\u258c       | 28770/110498 [00:01<00:02, 28732.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  29%|\u2588\u2588\u258a       | 31644/110498 [00:01<00:02, 28671.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  31%|\u2588\u2588\u2588       | 34512/110498 [00:01<00:02, 28658.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  34%|\u2588\u2588\u2588\u258d      | 37381/110498 [00:01<00:02, 28667.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  36%|\u2588\u2588\u2588\u258b      | 40248/110498 [00:01<00:02, 28611.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  39%|\u2588\u2588\u2588\u2589      | 43129/110498 [00:01<00:02, 28667.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  42%|\u2588\u2588\u2588\u2588\u258f     | 45996/110498 [00:01<00:02, 28599.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  44%|\u2588\u2588\u2588\u2588\u258d     | 48856/110498 [00:01<00:02, 28548.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  47%|\u2588\u2588\u2588\u2588\u258b     | 51737/110498 [00:01<00:02, 28623.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  49%|\u2588\u2588\u2588\u2588\u2589     | 54600/110498 [00:01<00:01, 28587.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 57464/110498 [00:02<00:01, 28600.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 60343/110498 [00:02<00:01, 28656.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 63249/110498 [00:02<00:01, 28775.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 66127/110498 [00:02<00:01, 28720.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 69000/110498 [00:02<00:01, 28700.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 71871/110498 [00:02<00:01, 28702.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 74742/110498 [00:02<00:01, 28686.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 77643/110498 [00:02<00:01, 28783.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 80541/110498 [00:02<00:01, 28841.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 83426/110498 [00:02<00:00, 28839.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 86310/110498 [00:03<00:00, 28790.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 89190/110498 [00:03<00:00, 28748.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 92065/110498 [00:03<00:00, 28723.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 94941/110498 [00:03<00:00, 28732.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 97815/110498 [00:03<00:00, 28638.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 100703/110498 [00:03<00:00, 28709.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 103576/110498 [00:03<00:00, 28712.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 106448/110498 [00:03<00:00, 28613.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 109310/110498 [00:03<00:00, 28608.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rApplying max-k cap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 110498/110498 [00:03<00:00, 28687.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nOOF F2 Score with Blended (alpha=0.25) + Guards: 0.2549\nOriginal score without max-k guard was: 0.3775\nSaved these blended thresholds as 'thresholds_final.npy' for the final submission.\n"
          ]
        }
      ]
    },
    {
      "id": "c57f3ebe-9a1d-4d8b-933e-ea0d3511be1d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import fbeta_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"--- Tuning Shrinkage Parameter 'k' (DIAGNOSTIC RUN) ---\")\n",
        "\n",
        "# Load necessary data\n",
        "try:\n",
        "    oof_preds = np.load('oof_preds.npy')\n",
        "    oof_labels = np.load('oof_labels.npy')\n",
        "    per_class = np.load('per_class_thresholds.npy')\n",
        "    print(\"Loaded OOF preds, labels, and per-class thresholds.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"ERROR: Missing a required .npy file: {e}. Please run previous cells to generate them.\")\n",
        "    raise e\n",
        "\n",
        "T_global = 0.93\n",
        "\n",
        "def apply_thresholds(probs, th):\n",
        "    y = (probs > th).astype(np.uint8)\n",
        "    empty = y.sum(axis=1) == 0\n",
        "    if np.any(empty):\n",
        "        idx = probs[empty].argmax(axis=1)\n",
        "        rows = np.where(empty)[0]\n",
        "        y[rows, idx] = 1\n",
        "    max_k = 3\n",
        "    row_sum = y.sum(axis=1)\n",
        "    too_many = np.where(row_sum > max_k)[0]\n",
        "    if too_many.size:\n",
        "        for r in too_many:\n",
        "            topk = np.argpartition(-probs[r], max_k)[:max_k]\n",
        "            y[r] = 0\n",
        "            y[r, topk] = 1\n",
        "    return y\n",
        "\n",
        "k_values = [5, 10, 20, 50, 100]\n",
        "scores = []\n",
        "counts = oof_labels.sum(axis=0).astype(np.float32)\n",
        "clip = 0.4\n",
        "min_samples = 5\n",
        "\n",
        "print(f\"Using Global T={T_global:.2f} and Clip={clip:.2f}\")\n",
        "print(f\"Per-class threshold stats: min={per_class.min():.4f}, max={per_class.max():.4f}, mean={per_class.mean():.4f}\")\n",
        "\n",
        "for k in k_values:\n",
        "    w = counts / (counts + k)\n",
        "    T_shrunk = w * per_class + (1.0 - w) * T_global\n",
        "    rare_mask = counts < min_samples\n",
        "    T_shrunk[rare_mask] = T_global\n",
        "    T_final = np.clip(T_shrunk, T_global - clip, T_global + clip)\n",
        "    \n",
        "    y_pred = apply_thresholds(oof_preds, T_final)\n",
        "    score = fbeta_score(oof_labels, y_pred, beta=2, average='samples', zero_division=0)\n",
        "    scores.append(score)\n",
        "    print(f\"k = {k:3d}, OOF F2 Score = {score:.4f}\")\n",
        "    # DIAGNOSTIC PRINT\n",
        "    print(f\"    T_final stats: min={T_final.min():.4f}, max={T_final.max():.4f}, mean={T_final.mean():.4f}\")\n",
        "\n",
        "# --- Find best and save ---\n",
        "best_k_idx = np.argmax(scores)\n",
        "best_k = k_values[best_k_idx]\n",
        "best_score = scores[best_k_idx]\n",
        "print(f\"\\nBest score found: {best_score:.4f} at k = {best_k}. Saving these thresholds.\")\n",
        "\n",
        "w = counts / (counts + best_k)\n",
        "T_shrunk = w * per_class + (1.0 - w) * T_global\n",
        "rare_mask = counts < min_samples\n",
        "T_shrunk[rare_mask] = T_global\n",
        "T_final = np.clip(T_shrunk, T_global - clip, T_global + clip)\n",
        "\n",
        "np.save('thresholds_final.npy', T_final)\n",
        "print(\"Saved new 'thresholds_final.npy'\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tuning Shrinkage Parameter 'k' (DIAGNOSTIC RUN) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded OOF preds, labels, and per-class thresholds.\nUsing Global T=0.93 and Clip=0.40\nPer-class threshold stats: min=0.3902, max=0.9999, mean=0.9638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k =   5, OOF F2 Score = 0.2567\n    T_final stats: min=0.5300, max=0.9958, mean=0.9551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k =  10, OOF F2 Score = 0.2565\n    T_final stats: min=0.5300, max=0.9945, mean=0.9506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k =  20, OOF F2 Score = 0.2563\n    T_final stats: min=0.5300, max=0.9922, mean=0.9454\n"
          ]
        }
      ]
    },
    {
      "id": "e8c6d9ad-ba9e-45e7-9e83-61ed41834877",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"--- Verifying the generated per_class_thresholds.npy file ---\")\n",
        "try:\n",
        "    ths = np.load('per_class_thresholds.npy')\n",
        "    print(f\"Shape: {ths.shape}\")\n",
        "    print(f\"All finite: {np.isfinite(ths).all()}\")\n",
        "    print(f\"Min threshold: {ths.min()}\")\n",
        "    print(f\"Max threshold: {ths.max()}\")\n",
        "    print(f\"Mean threshold: {ths.mean()}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or checking file: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "afcc7fda-d6e0-4999-bb02-e917920d17df",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "print(\"--- Purging Stale Artifacts as per Expert Advice ---\")\n",
        "\n",
        "files_to_delete = ['oof_preds.npy', 'oof_labels.npy', 'per_class_thresholds.npy']\n",
        "\n",
        "for f in files_to_delete:\n",
        "    if os.path.exists(f):\n",
        "        os.remove(f)\n",
        "        print(f\"Deleted stale file: {f}\")\n",
        "    else:\n",
        "        print(f\"File not found, skipping: {f}\")\n",
        "\n",
        "print(\"\\nPurge complete. Ready to regenerate OOF with correct transforms.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "c624d9ad-450a-4a75-98cd-fd43e32c4223",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5. Key Inspection\n",
        "print(\"--- Inspecting State Dict Keys for Fold 0 ---\")\n",
        "\n",
        "# 1. Load the saved state_dict from file\n",
        "model_path = CFG.model_paths[0]\n",
        "saved_state_dict = torch.load(model_path)\n",
        "saved_keys = list(saved_state_dict.keys())\n",
        "print(f\"Loaded state_dict from {model_path}\")\n",
        "print(f\"Number of keys in saved_state_dict: {len(saved_keys)}\")\n",
        "print(\"First 5 keys from SAVED state_dict:\")\n",
        "for key in saved_keys[:5]:\n",
        "    print(f\"  - {key}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# 2. Create a fresh model instance and get its state_dict\n",
        "fresh_model = iMetModel(CFG.model_name, pretrained=False)\n",
        "fresh_state_dict = fresh_model.state_dict()\n",
        "fresh_keys = list(fresh_state_dict.keys())\n",
        "print(\"Created a fresh iMetModel instance\")\n",
        "print(f\"Number of keys in fresh_model.state_dict(): {len(fresh_keys)}\")\n",
        "print(\"First 5 keys from FRESH model's state_dict:\")\n",
        "for key in fresh_keys[:5]:\n",
        "    print(f\"  - {key}\")\n",
        "\n",
        "# 3. Compare the key sets\n",
        "saved_keys_set = set(saved_keys)\n",
        "fresh_keys_set = set(fresh_keys)\n",
        "\n",
        "if saved_keys_set == fresh_keys_set:\n",
        "    print(\"\\nSUCCESS: The key sets are identical.\")\n",
        "else:\n",
        "    print(\"\\nERROR: The key sets are different.\")\n",
        "    print(f\"Keys in saved but not in fresh: {saved_keys_set - fresh_keys_set}\")\n",
        "    print(f\"Keys in fresh but not in saved: {fresh_keys_set - saved_keys_set}\")\n",
        "\n",
        "del saved_state_dict, fresh_model, fresh_state_dict\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8bc96596-7c34-4dde-8f32-0831f69a3058",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 6. Final Sanity Check: Verify Weight Loading\n",
        "\n",
        "print(\"--- Verifying Weight Loading for Fold 0 ---\")\n",
        "\n",
        "# 1. Instantiate model and load state_dict from file\n",
        "model_path = CFG.model_paths[0]\n",
        "state_dict_from_file = torch.load(model_path, map_location=CFG.device)\n",
        "\n",
        "model = iMetModel(CFG.model_name, pretrained=True).to(CFG.device)\n",
        "model.load_state_dict(state_dict_from_file)\n",
        "model.eval()\n",
        "\n",
        "# 2. Get the state_dict from the model *after* loading\n",
        "state_dict_from_model = model.state_dict()\n",
        "\n",
        "# 3. Pick a key and compare tensors\n",
        "key_to_check = 'model.conv_stem.weight'\n",
        "\n",
        "tensor_from_file = state_dict_from_file[key_to_check]\n",
        "tensor_from_model = state_dict_from_model[key_to_check]\n",
        "\n",
        "print(f\"Checking key: '{key_to_check}'\")\n",
        "print(f\"Tensor from file (sum): {tensor_from_file.sum().item()}\")\n",
        "print(f\"Tensor from model (sum): {tensor_from_model.sum().item()}\")\n",
        "\n",
        "# 4. The ultimate test\n",
        "are_equal = torch.equal(tensor_from_file, tensor_from_model)\n",
        "print(f\"Are the tensors identical? -> {are_equal}\")\n",
        "\n",
        "assert are_equal, \"FATAL: Tensors do not match after load_state_dict. The weights are not being loaded correctly.\"\n",
        "\n",
        "print(\"\\nSUCCESS: Weight loading verified. The tensor from the file matches the tensor in the model.\")\n",
        "\n",
        "# 5. Check another key (from the end of the model)\n",
        "key_to_check = 'model.classifier.weight'\n",
        "tensor_from_file = state_dict_from_file[key_to_check]\n",
        "tensor_from_model = state_dict_from_model[key_to_check]\n",
        "print(f\"\\nChecking key: '{key_to_check}'\")\n",
        "print(f\"Tensor from file (sum): {tensor_from_file.sum().item()}\")\n",
        "print(f\"Tensor from model (sum): {tensor_from_model.sum().item()}\")\n",
        "are_equal = torch.equal(tensor_from_file, tensor_from_model)\n",
        "print(f\"Are the tensors identical? -> {are_equal}\")\n",
        "assert are_equal, \"FATAL: Tensors do not match for the classifier layer.\"\n",
        "print(\"\\nSUCCESS: Classifier weights also verified.\")\n",
        "\n",
        "\n",
        "del model, state_dict_from_file, state_dict_from_model\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8823f3e4-4167-4ccf-a387-5bbd5ffe9e56",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 7. Hypothesis Check: Are the saved models identical?\n",
        "print(\"--- Checking if fold 0 and fold 1 models are identical ---\")\n",
        "\n",
        "# 1. Load state dicts for two different folds\n",
        "model_path_0 = CFG.model_paths[0]\n",
        "model_path_1 = CFG.model_paths[1]\n",
        "\n",
        "state_dict_0 = torch.load(model_path_0, map_location=CFG.device)\n",
        "state_dict_1 = torch.load(model_path_1, map_location=CFG.device)\n",
        "\n",
        "# 2. Compare a key tensor (e.g., the final classifier weights)\n",
        "key_to_check = 'model.classifier.weight'\n",
        "tensor_0 = state_dict_0[key_to_check]\n",
        "tensor_1 = state_dict_1[key_to_check]\n",
        "\n",
        "print(f\"Checking key: '{key_to_check}'\")\n",
        "print(f\"Tensor from Fold 0 (sum): {tensor_0.sum().item()}\")\n",
        "print(f\"Tensor from Fold 1 (sum): {tensor_1.sum().item()}\")\n",
        "\n",
        "# 3. The crucial test\n",
        "are_equal = torch.equal(tensor_0, tensor_1)\n",
        "print(f\"Are the tensors for Fold 0 and Fold 1 identical? -> {are_equal}\")\n",
        "\n",
        "if are_equal:\n",
        "    print(\"\\nFATAL DIAGNOSIS: The model files for different folds are identical. This strongly suggests the training script saved the same initial weights instead of the trained weights for each fold.\")\n",
        "else:\n",
        "    print(\"\\nINFO: The model files are different, as expected. This hypothesis is incorrect.\")\n",
        "\n",
        "del state_dict_0, state_dict_1\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}