{
  "cells": [
    {
      "id": "64352935-6a6d-40d1-b053-b9c87819d7ce",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan to Medal: Text Normalization (Russian)\n",
        "\n",
        "## Goals\n",
        "- Achieve \u2265 bronze by building a strong, fast baseline, then iterate.\n",
        "\n",
        "## Data Understanding\n",
        "- Load ru_train.csv.zip, ru_test_2.csv.zip, ru_sample_submission_2.csv.zip.\n",
        "- Inspect columns, size, and whether data is token-level (common for TN challenges).\n",
        "\n",
        "## Baseline Approach\n",
        "1) Majority-class per source token memorization with smoothing:\n",
        "- For each source token/context key, memorize most frequent normalized target; backoff to lower-order keys and fallbacks.\n",
        "- Known to be very strong for TN competitions.\n",
        "\n",
        "2) Context-aware features:\n",
        "- Use (token, semiotic class if available, left/right neighbors, casing, punctuation patterns).\n",
        "- If class not provided, derive regex-based features (digits, dates, currency, roman numerals, abbreviations, etc.).\n",
        "\n",
        "3) Model\n",
        "- Start with frequency lexicon + backoff.\n",
        "- Add CatBoost/LightGBM classifier per token to choose between candidates when collisions occur.\n",
        "- Optionally train separate models per detected semiotic type (e.g., PLAIN, DATE, CARDINAL, etc.) if labels exist.\n",
        "\n",
        "## Evaluation\n",
        "- Create local CV split consistent with dataset structure (article/utt group-wise split if applicable).\n",
        "- Metric: accuracy over tokens/rows as per competition.\n",
        "- Log fold times and results.\n",
        "\n",
        "## Inference\n",
        "- Generate predictions for test using the lexicon+backoff; apply model only when ambiguous.\n",
        "- Save to submission.csv exactly matching sample format.\n",
        "\n",
        "## Timeline\n",
        "- T0: Load/EDA.\n",
        "- T1: Build frequency/backoff baseline; submit.\n",
        "- T2: Add regex feature extractor + CatBoost disambiguation; resubmit.\n",
        "- T3: Error analysis on mismatches; targeted rules (dates, times, money).\n",
        "\n",
        "## Checkpoints\n",
        "- Request expert review after plan, after EDA, after first baseline results, and before heavy training."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "1557b10d-ecfb-45ad-a917-bc3d6cf7685f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from time import time\n",
        "\n",
        "t0 = time()\n",
        "print(\"Loading data...\")\n",
        "train_path = \"ru_train.csv.zip\"\n",
        "test_path = \"ru_test_2.csv.zip\"\n",
        "sample_path = \"ru_sample_submission_2.csv.zip\"\n",
        "\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "sample = pd.read_csv(sample_path)\n",
        "print(f\"Loaded: train={train.shape}, test={test.shape}, sample={sample.shape}\")\n",
        "\n",
        "def info_df(name, df):\n",
        "    print(f\"\\n=== {name} columns ===\")\n",
        "    print(list(df.columns))\n",
        "    print(f\"dtypes:\\n{df.dtypes}\")\n",
        "    print(\"head:\")\n",
        "    print(df.head(10))\n",
        "\n",
        "info_df(\"train\", train)\n",
        "info_df(\"test\", test)\n",
        "info_df(\"sample\", sample)\n",
        "\n",
        "# Expect columns like: sentence_id, (token_id or id), before, after (train only), class/semiotic_class\n",
        "cols = set(train.columns)\n",
        "has_sentence = 'sentence_id' in cols or 'sentence' in cols\n",
        "sent_col = 'sentence_id' if 'sentence_id' in cols else ('sentence' if 'sentence' in cols else None)\n",
        "id_col = 'id' if 'id' in cols else ('token_id' if 'token_id' in cols else None)\n",
        "before_col = 'before' if 'before' in cols else None\n",
        "after_col = 'after' if 'after' in cols else None\n",
        "class_col = 'class' if 'class' in cols else ('semiotic_class' if 'semiotic_class' in cols else None)\n",
        "print(\"\\nDetected columns:\")\n",
        "print({\n",
        "    'sent_col': sent_col,\n",
        "    'id_col': id_col,\n",
        "    'before_col': before_col,\n",
        "    'after_col': after_col,\n",
        "    'class_col': class_col\n",
        "})\n",
        "\n",
        "if class_col:\n",
        "    classes = train[class_col].astype(str).fillna('NA').unique().tolist()\n",
        "    print(f\"\\nUnique classes in train ({len(classes)}):\", classes[:50])\n",
        "\n",
        "# Check if test has class\n",
        "test_has_class = class_col in test.columns if class_col else False\n",
        "print(f\"Test has class column: {test_has_class}\")\n",
        "\n",
        "# NBSP/NNBSP detection in before\n",
        "def count_nbsp(s):\n",
        "    if pd.isna(s):\n",
        "        return 0\n",
        "    return len(re.findall(r\"[\\u00A0\\u202F]\", str(s)))\n",
        "if before_col:\n",
        "    nb_train = train[before_col].astype(str).apply(lambda x: 1 if re.search(r\"[\\u00A0\\u202F]\", x) else 0).sum()\n",
        "    nb_test = test[before_col].astype(str).apply(lambda x: 1 if re.search(r\"[\\u00A0\\u202F]\", x) else 0).sum() if before_col in test.columns else np.nan\n",
        "    print(f\"NBSP present in train rows: {nb_train}\")\n",
        "    print(f\"NBSP present in test rows: {nb_test}\")\n",
        "\n",
        "# Tokenization identity checks\n",
        "common_cols = set(train.columns).intersection(set(test.columns))\n",
        "print(f\"Common columns train/test: {sorted(list(common_cols))}\")\n",
        "\n",
        "# Derive prev/next within sentence if available\n",
        "if sent_col and id_col and before_col:\n",
        "    print(\"\\nDeriving prev/next tokens within sentence on a small sample...\")\n",
        "    tmp = train[[sent_col, id_col, before_col]].copy().head(5000)\n",
        "    tmp = tmp.sort_values([sent_col, id_col])\n",
        "    tmp['prev_before'] = tmp.groupby(sent_col)[before_col].shift(1).fillna('<BOS>')\n",
        "    tmp['next_before'] = tmp.groupby(sent_col)[before_col].shift(-1).fillna('<EOS>')\n",
        "    print(tmp.head(10))\n",
        "else:\n",
        "    print(\"Skipping prev/next derivation (missing sent/id/before)\")\n",
        "\n",
        "# Basic frequency for (class,before) if class exists\n",
        "if class_col and before_col and after_col:\n",
        "    print(\"\\nBuilding quick freq table for (class, before) -> top after (preview)...\")\n",
        "    grp = train.groupby([class_col, before_col])[after_col].agg(lambda x: Counter(x).most_common(1)[0][0]).reset_index().head(10)\n",
        "    print(grp)\n",
        "\n",
        "print(f\"\\nDone in {time()-t0:.2f}s\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: train=(9515325, 5), test=(1059191, 3), sample=(1059191, 2)\n\n=== train columns ===\n['sentence_id', 'token_id', 'class', 'before', 'after']\ndtypes:\nsentence_id     int64\ntoken_id        int64\nclass          object\nbefore         object\nafter          object\ndtype: object\nhead:\n   sentence_id  token_id  class      before  \\\n0            0         0  PLAIN          \u041f\u043e   \n1            0         1  PLAIN   \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044e   \n2            0         2  PLAIN          \u043d\u0430   \n3            0         3   DATE    1862 \u0433\u043e\u0434   \n4            0         4  PUNCT           .   \n5            1         0  PLAIN  \u041e\u0441\u043d\u0430\u0449\u0430\u043b\u0438\u0441\u044c   \n6            1         1  PLAIN     \u043b\u0430\u0442\u043d\u044b\u043c\u0438   \n7            1         2  PLAIN  \u0440\u0443\u043a\u0430\u0432\u0438\u0446\u0430\u043c\u0438   \n8            1         3  PLAIN           \u0438   \n9            1         4  PLAIN  \u0441\u0430\u0431\u0430\u0442\u043e\u043d\u0430\u043c\u0438   \n\n                                    after  \n0                                      \u041f\u043e  \n1                               \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044e  \n2                                      \u043d\u0430  \n3  \u0442\u044b\u0441\u044f\u0447\u0430 \u0432\u043e\u0441\u0435\u043c\u044c\u0441\u043e\u0442 \u0448\u0435\u0441\u0442\u044c\u0434\u0435\u0441\u044f\u0442 \u0432\u0442\u043e\u0440\u043e\u0439 \u0433\u043e\u0434  \n4                                       .  \n5                              \u041e\u0441\u043d\u0430\u0449\u0430\u043b\u0438\u0441\u044c  \n6                                 \u043b\u0430\u0442\u043d\u044b\u043c\u0438  \n7                              \u0440\u0443\u043a\u0430\u0432\u0438\u0446\u0430\u043c\u0438  \n8                                       \u0438  \n9                              \u0441\u0430\u0431\u0430\u0442\u043e\u043d\u0430\u043c\u0438  \n\n=== test columns ===\n['sentence_id', 'token_id', 'before']\ndtypes:\nsentence_id     int64\ntoken_id        int64\nbefore         object\ndtype: object\nhead:\n   sentence_id  token_id       before\n0            0         0       \u0422\u0435\u043f\u0435\u0440\u044c\n1            0         1          \u0432\u0441\u0435\n2            0         2  \u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\n3            0         3        \u0437\u043e\u0432\u0443\u0442\n4            0         4      \u042f\u043c\u0430\u043c\u043e\u0442\u043e\n5            0         5        \u0410\u043d\u0438\u043a\u0438\n6            0         6            (\n7            0         7           \u044f\u043f\n8            0         8            .\n9            0         9            \u2014\n\n=== sample columns ===\n['id', 'after']\ndtypes:\nid       object\nafter    object\ndtype: object\nhead:\n    id        after\n0  0_0       \u0422\u0435\u043f\u0435\u0440\u044c\n1  0_1          \u0432\u0441\u0435\n2  0_2  \u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\n3  0_3        \u0437\u043e\u0432\u0443\u0442\n4  0_4      \u042f\u043c\u0430\u043c\u043e\u0442\u043e\n5  0_5        \u0410\u043d\u0438\u043a\u0438\n6  0_6            (\n7  0_7           \u044f\u043f\n8  0_8            .\n9  0_9            \u2014\n\nDetected columns:\n{'sent_col': 'sentence_id', 'id_col': 'token_id', 'before_col': 'before', 'after_col': 'after', 'class_col': 'class'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nUnique classes in train (15): ['PLAIN', 'DATE', 'PUNCT', 'ORDINAL', 'VERBATIM', 'LETTERS', 'CARDINAL', 'MEASURE', 'TELEPHONE', 'ELECTRONIC', 'DECIMAL', 'DIGIT', 'FRACTION', 'TIME', 'MONEY']\nTest has class column: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NBSP present in train rows: 0\nNBSP present in test rows: 0\nCommon columns train/test: ['before', 'sentence_id', 'token_id']\n\nDeriving prev/next tokens within sentence on a small sample...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sentence_id  token_id      before prev_before next_before\n0            0         0          \u041f\u043e       <BOS>   \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044e\n1            0         1   \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044e          \u041f\u043e          \u043d\u0430\n2            0         2          \u043d\u0430   \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044e    1862 \u0433\u043e\u0434\n3            0         3    1862 \u0433\u043e\u0434          \u043d\u0430           .\n4            0         4           .    1862 \u0433\u043e\u0434       <EOS>\n5            1         0  \u041e\u0441\u043d\u0430\u0449\u0430\u043b\u0438\u0441\u044c       <BOS>     \u043b\u0430\u0442\u043d\u044b\u043c\u0438\n6            1         1     \u043b\u0430\u0442\u043d\u044b\u043c\u0438  \u041e\u0441\u043d\u0430\u0449\u0430\u043b\u0438\u0441\u044c  \u0440\u0443\u043a\u0430\u0432\u0438\u0446\u0430\u043c\u0438\n7            1         2  \u0440\u0443\u043a\u0430\u0432\u0438\u0446\u0430\u043c\u0438     \u043b\u0430\u0442\u043d\u044b\u043c\u0438           \u0438\n8            1         3           \u0438  \u0440\u0443\u043a\u0430\u0432\u0438\u0446\u0430\u043c\u0438  \u0441\u0430\u0431\u0430\u0442\u043e\u043d\u0430\u043c\u0438\n9            1         4  \u0441\u0430\u0431\u0430\u0442\u043e\u043d\u0430\u043c\u0438           \u0438           \u0441\n\nBuilding quick freq table for (class, before) -> top after (preview)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      class   before                                after\n0  CARDINAL       -0                           \u043c\u0438\u043d\u0443\u0441 \u043d\u043e\u043b\u044c\n1  CARDINAL       -1                           \u043c\u0438\u043d\u0443\u0441 \u043e\u0434\u0438\u043d\n2  CARDINAL      -10                         \u043c\u0438\u043d\u0443\u0441 \u0434\u0435\u0441\u044f\u0442\u044c\n3  CARDINAL     -100                            \u043c\u0438\u043d\u0443\u0441 \u0441\u0442\u043e\n4  CARDINAL     -101                       \u043c\u0438\u043d\u0443\u0441 \u0441\u0442\u043e \u043e\u0434\u0438\u043d\n5  CARDINAL    -1011             \u043c\u0438\u043d\u0443\u0441 \u0442\u044b\u0441\u044f\u0447\u0430 \u043e\u0434\u0438\u043d\u043d\u0430\u0434\u0446\u0430\u0442\u044c\n6  CARDINAL  -101903  \u043c\u0438\u043d\u0443\u0441 \u0441\u0442\u043e \u043e\u0434\u043d\u0430 \u0442\u044b\u0441\u044f\u0447\u0430 \u0434\u0435\u0432\u044f\u0442\u044c\u0441\u043e\u0442 \u0442\u0440\u0438\n7  CARDINAL     -102                        \u043c\u0438\u043d\u0443\u0441 \u0441\u0442\u043e \u0434\u0432\u0430\n8  CARDINAL    -1024         \u043c\u0438\u043d\u0443\u0441 \u0442\u044b\u0441\u044f\u0447\u0430 \u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0447\u0435\u0442\u044b\u0440\u0435\n9  CARDINAL    -1028         \u043c\u0438\u043d\u0443\u0441 \u0442\u044b\u0441\u044f\u0447\u0430 \u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0432\u043e\u0441\u0435\u043c\u044c\n\nDone in 39.71s\n"
          ]
        }
      ]
    },
    {
      "id": "8e903574-ddfb-441a-bf54-dcef76f8e08b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "t0 = time()\n",
        "print(\"Deriving prev/next for train/test...\")\n",
        "# Minimize columns to reduce memory\n",
        "tr_cols = ['sentence_id', 'token_id', 'before', 'after']\n",
        "train_ctx = train[tr_cols].copy()\n",
        "train_ctx = train_ctx.sort_values(['sentence_id', 'token_id'])\n",
        "train_ctx['prev_before'] = train_ctx.groupby('sentence_id')['before'].shift(1).fillna('<BOS>')\n",
        "train_ctx['next_before'] = train_ctx.groupby('sentence_id')['before'].shift(-1).fillna('<EOS>')\n",
        "\n",
        "te_cols = ['sentence_id', 'token_id', 'before']\n",
        "test_ctx = test[te_cols].copy()\n",
        "test_ctx = test_ctx.sort_values(['sentence_id', 'token_id'])\n",
        "test_ctx['prev_before'] = test_ctx.groupby('sentence_id')['before'].shift(1).fillna('<BOS>')\n",
        "test_ctx['next_before'] = test_ctx.groupby('sentence_id')['before'].shift(-1).fillna('<EOS>')\n",
        "\n",
        "print(f\"Context derived in {time()-t0:.2f}s\")\n",
        "\n",
        "def build_top_map(df, key_cols, value_col='after', logname=''):\n",
        "    t = time()\n",
        "    cols = key_cols + [value_col]\n",
        "    tmp = df[cols].copy()\n",
        "    # group by key+value, count, keep top value per key\n",
        "    cnt = tmp.groupby(cols, observed=True).size().reset_index(name='cnt')\n",
        "    cnt = cnt.sort_values(key_cols + ['cnt'], ascending=[True]*len(key_cols) + [False])\n",
        "    top = cnt.drop_duplicates(subset=key_cols, keep='first')\n",
        "    print(f\"Built map {logname or key_cols} with {top.shape[0]} keys in {time()-t:.2f}s\")\n",
        "    return top[key_cols + [value_col]]\n",
        "\n",
        "maps = []\n",
        "print(\"Building backoff maps...\")\n",
        "# K1: (before, prev, next)\n",
        "maps.append(build_top_map(train_ctx, ['before', 'prev_before', 'next_before'], 'after', 'K1'))\n",
        "# K2: (before, prev)\n",
        "maps.append(build_top_map(train_ctx, ['before', 'prev_before'], 'after', 'K2'))\n",
        "# K3: (before, next)\n",
        "maps.append(build_top_map(train_ctx, ['before', 'next_before'], 'after', 'K3'))\n",
        "# K4: (before)\n",
        "maps.append(build_top_map(train_ctx, ['before'], 'after', 'K4'))\n",
        "# K5: (lower(before))\n",
        "train_ctx['before_lower'] = train_ctx['before'].str.lower()\n",
        "maps.append(build_top_map(train_ctx, ['before_lower'], 'after', 'K5'))\n",
        "\n",
        "print(f\"Maps built in total {time()-t0:.2f}s\")\n",
        "\n",
        "print(\"Applying backoff to test...\")\n",
        "t1 = time()\n",
        "pred = test_ctx.copy()\n",
        "\n",
        "# Stepwise fill using merges\n",
        "pred['after'] = np.nan\n",
        "\n",
        "# K1\n",
        "pred = pred.merge(maps[0].rename(columns={'after':'after_k1'}), on=['before','prev_before','next_before'], how='left')\n",
        "pred['after'] = pred['after'].fillna(pred['after_k1'])\n",
        "pred.drop(columns=['after_k1'], inplace=True)\n",
        "print(f\"After K1 filled: {pred['after'].notna().mean():.4f}\")\n",
        "\n",
        "# K2\n",
        "pred = pred.merge(maps[1].rename(columns={'after':'after_k2'}), on=['before','prev_before'], how='left')\n",
        "pred['after'] = pred['after'].fillna(pred['after_k2'])\n",
        "pred.drop(columns=['after_k2'], inplace=True)\n",
        "print(f\"After K2 filled: {pred['after'].notna().mean():.4f}\")\n",
        "\n",
        "# K3\n",
        "pred = pred.merge(maps[2].rename(columns={'after':'after_k3'}), on=['before','next_before'], how='left')\n",
        "pred['after'] = pred['after'].fillna(pred['after_k3'])\n",
        "pred.drop(columns=['after_k3'], inplace=True)\n",
        "print(f\"After K3 filled: {pred['after'].notna().mean():.4f}\")\n",
        "\n",
        "# K4\n",
        "pred = pred.merge(maps[3].rename(columns={'after':'after_k4'}), on=['before'], how='left')\n",
        "pred['after'] = pred['after'].fillna(pred['after_k4'])\n",
        "pred.drop(columns=['after_k4'], inplace=True)\n",
        "print(f\"After K4 filled: {pred['after'].notna().mean():.4f}\")\n",
        "\n",
        "# K5 lower\n",
        "pred['before_lower'] = pred['before'].str.lower()\n",
        "pred = pred.merge(maps[4].rename(columns={'after':'after_k5'}), on=['before_lower'], how='left')\n",
        "pred['after'] = pred['after'].fillna(pred['after_k5'])\n",
        "pred.drop(columns=['after_k5','before_lower'], inplace=True)\n",
        "print(f\"After K5 filled: {pred['after'].notna().mean():.4f}\")\n",
        "\n",
        "# Identity fallback\n",
        "miss = pred['after'].isna().sum()\n",
        "if miss:\n",
        "    print(f\"Falling back to identity for {miss} rows\")\n",
        "pred['after'] = pred['after'].fillna(pred['before'])\n",
        "\n",
        "print(f\"Backoff applied in {time()-t1:.2f}s; coverage 100.00%\")\n",
        "\n",
        "print(\"Building submission.csv ...\")\n",
        "sub = pred[['sentence_id','token_id','after']].copy()\n",
        "sub['id'] = sub['sentence_id'].astype(str) + '_' + sub['token_id'].astype(str)\n",
        "sub = sub[['id','after']]\n",
        "\n",
        "# Align to sample order to be safe\n",
        "submission = sample[['id']].merge(sub, on='id', how='left')\n",
        "na = submission['after'].isna().sum()\n",
        "if na:\n",
        "    print(f\"Warning: {na} missing after filled via before from test\")\n",
        "    # Fill from test order if any missing (shouldn't happen)\n",
        "    tmp_fix = test_ctx.copy()\n",
        "    tmp_fix['id'] = tmp_fix['sentence_id'].astype(str) + '_' + tmp_fix['token_id'].astype(str)\n",
        "    submission = submission.merge(tmp_fix[['id','before']], on='id', how='left')\n",
        "    submission['after'] = submission['after'].fillna(submission['before'])\n",
        "    submission.drop(columns=['before'], inplace=True)\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Saved submission.csv with shape\", submission.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deriving prev/next for train/test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context derived in 5.41s\nBuilding backoff maps...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built map K1 with 6626125 keys in 40.73s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built map K2 with 4034677 keys in 21.21s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built map K3 with 3930113 keys in 20.21s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built map K4 with 751569 keys in 8.47s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built map K5 with 679023 keys in 9.76s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maps built in total 111.17s\nApplying backoff to test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After K1 filled: 0.3674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After K2 filled: 0.6564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After K3 filled: 0.8159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After K4 filled: 0.9516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After K5 filled: 0.9566\nFalling back to identity for 45921 rows\nBackoff applied in 19.42s; coverage 100.00%\nBuilding submission.csv ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: 3 missing after filled via before from test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with shape (1059191, 2)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}