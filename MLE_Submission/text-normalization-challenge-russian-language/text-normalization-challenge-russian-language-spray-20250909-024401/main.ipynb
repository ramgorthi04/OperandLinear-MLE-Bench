{
  "cells": [
    {
      "id": "86d3fed6-c23b-45ae-82a9-41a9bf9b528d",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan & Experiment Log\n",
        "\n",
        "Objective: Win a medal (>= 0.97592 accuracy) in Russian Text Normalization.\n",
        "\n",
        "Strategy (iterative, time-boxed):\n",
        "- Baseline: Context-aware memorization.\n",
        "  - Build mapping from (semiotic_class, token, prev_token, next_token) -> most frequent normalized form (\"after\").\n",
        "  - Backoffs: (semiotic_class, token) -> after, then (token) -> after.\n",
        "  - Final fallback: identity (return the raw token) for safety; add simple class heuristics later if needed.\n",
        "- Modeling upgrade (if needed for medal):\n",
        "  - Train CatBoost/LightGBM classifier for ambiguous cases (keys with multiple labels) using contextual features.\n",
        "  - Add class-specific regex/rules for NUMBERS, DATES, ORDINALS, etc., only where memory fails.\n",
        "\n",
        "Planned Milestones:\n",
        "1) Data load + schema inspection.\n",
        "2) Baseline memorization mapper + CV on train (simulate test via sentence-level split).\n",
        "3) Inference on test, create submission.csv, quick score via local validation.\n",
        "4) Error analysis: where backoff hits, add lightweight heuristics or model.\n",
        "\n",
        "Experiment Log:\n",
        "- [T0] Init plan. Next: load files, confirm columns in ru_train.csv.zip / ru_test_2.csv.zip, sample submission format.\n",
        "\n",
        "Notes:\n",
        "- Always log progress and timings.\n",
        "- Use expert review after major steps (planning, EDA, first baseline, post-score)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "cf0cf0ba-53c4-4c5b-b1af-6b7ddf9b0b8f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick data inspection: list files, inspect sample submission and test schema\n",
        "import os, sys, time, zipfile, pandas as pd\n",
        "pd.set_option('display.max_rows', 20)\n",
        "pd.set_option('display.max_columns', 50)\n",
        "print('CWD:', os.getcwd())\n",
        "print('Files:', os.listdir())\n",
        "\n",
        "t0 = time.time()\n",
        "sample_path = 'ru_sample_submission_2.csv.zip'\n",
        "test_path = 'ru_test_2.csv.zip'\n",
        "train_path = 'ru_train.csv.zip'\n",
        "\n",
        "print('\\nReading sample submission...')\n",
        "sample_sub = pd.read_csv(sample_path)\n",
        "print('Sample submission shape:', sample_sub.shape)\n",
        "print(sample_sub.head(3))\n",
        "print('Sample submission columns:', list(sample_sub.columns))\n",
        "\n",
        "print('\\nReading test head...')\n",
        "test_df = pd.read_csv(test_path, nrows=50000)\n",
        "print('Test head shape:', test_df.shape)\n",
        "print(test_df.head())\n",
        "print('Test columns:', list(test_df.columns))\n",
        "\n",
        "print('\\nReading train head...')\n",
        "train_df = pd.read_csv(train_path, nrows=50000)\n",
        "print('Train head shape:', train_df.shape)\n",
        "print(train_df.head())\n",
        "print('Train columns:', list(train_df.columns))\n",
        "\n",
        "print(f'Elapsed: {time.time()-t0:.2f}s', flush=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CWD: /app/agent_run_states/text-normalization-challenge-russian-language-spray-20250909-024401\nFiles: ['main.ipynb', 'agent_metadata', 'ru_sample_submission_2.csv.zip', 'requirements.txt', 'task.txt', 'ru_test_2.csv.zip', 'docker_run.log', 'description.md', 'ru_train.csv.zip']\n\nReading sample submission...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample submission shape: (1059191, 2)\n    id        after\n0  0_0       \u0422\u0435\u043f\u0435\u0440\u044c\n1  0_1          \u0432\u0441\u0435\n2  0_2  \u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\nSample submission columns: ['id', 'after']\n\nReading test head...\nTest head shape: (50000, 3)\n   sentence_id  token_id       before\n0            0         0       \u0422\u0435\u043f\u0435\u0440\u044c\n1            0         1          \u0432\u0441\u0435\n2            0         2  \u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\n3            0         3        \u0437\u043e\u0432\u0443\u0442\n4            0         4      \u042f\u043c\u0430\u043c\u043e\u0442\u043e\nTest columns: ['sentence_id', 'token_id', 'before']\n\nReading train head...\nTrain head shape: (50000, 5)\n   sentence_id  token_id  class     before  \\\n0            0         0  PLAIN         \u041f\u043e   \n1            0         1  PLAIN  \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044e   \n2            0         2  PLAIN         \u043d\u0430   \n3            0         3   DATE   1862 \u0433\u043e\u0434   \n4            0         4  PUNCT          .   \n\n                                    after  \n0                                      \u041f\u043e  \n1                               \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044e  \n2                                      \u043d\u0430  \n3  \u0442\u044b\u0441\u044f\u0447\u0430 \u0432\u043e\u0441\u0435\u043c\u044c\u0441\u043e\u0442 \u0448\u0435\u0441\u0442\u044c\u0434\u0435\u0441\u044f\u0442 \u0432\u0442\u043e\u0440\u043e\u0439 \u0433\u043e\u0434  \n4                                       .  \nTrain columns: ['sentence_id', 'token_id', 'class', 'before', 'after']\nElapsed: 0.97s\n"
          ]
        }
      ]
    },
    {
      "id": "7fd6c2ea-f793-4b0f-90e2-48d8105a9ab0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Baseline: context-aware memorization with backoffs using before, prev_before, next_before\n",
        "import pandas as pd, numpy as np, time, os, pickle\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "t0 = time.time()\n",
        "train_path = 'ru_train.csv.zip'\n",
        "test_path = 'ru_test_2.csv.zip'\n",
        "\n",
        "print('Loading full train/test...')\n",
        "dtype_train = {'sentence_id':'int32', 'token_id':'int32', 'class':'category', 'before':'string', 'after':'string'}\n",
        "dtype_test = {'sentence_id':'int32', 'token_id':'int32', 'before':'string'}\n",
        "train = pd.read_csv(train_path, dtype=dtype_train)\n",
        "test = pd.read_csv(test_path, dtype=dtype_test)\n",
        "print('Loaded. train:', train.shape, 'test:', test.shape)\n",
        "\n",
        "# Ensure proper order and create context\n",
        "def add_context(df):\n",
        "    df = df.sort_values(['sentence_id','token_id']).copy()\n",
        "    df['prev_before'] = df.groupby('sentence_id')['before'].shift(1).fillna('\u27c2')\n",
        "    df['next_before'] = df.groupby('sentence_id')['before'].shift(-1).fillna('\u27c2')\n",
        "    return df\n",
        "\n",
        "train = add_context(train)\n",
        "test = add_context(test)\n",
        "\n",
        "# Helper to compute majority mapping\n",
        "def majority_map(df, keys, target='after'):\n",
        "    vc = (df.groupby(keys)[target]\n",
        "            .agg(lambda s: s.value_counts().index[0]))\n",
        "    vc = vc.reset_index()\n",
        "    return {tuple(row[k] for k in keys): row[target] for _, row in vc.iterrows()}\n",
        "\n",
        "# Prediction with backoffs: (before, prev, next) -> (before) -> identity\n",
        "def predict_after(df, map_ctx, map_uni):\n",
        "    preds = []\n",
        "    hit_ctx = hit_uni = miss = 0\n",
        "    for b, p, n in zip(df['before'].array, df['prev_before'].array, df['next_before'].array):\n",
        "        key_ctx = (b, p, n)\n",
        "        if key_ctx in map_ctx:\n",
        "            preds.append(map_ctx[key_ctx]); hit_ctx += 1\n",
        "        else:\n",
        "            key_uni = (b,)\n",
        "            if key_uni in map_uni:\n",
        "                preds.append(map_uni[key_uni]); hit_uni += 1\n",
        "            else:\n",
        "                preds.append(b); miss += 1\n",
        "    return np.array(preds, dtype=object), {'ctx': hit_ctx, 'uni': hit_uni, 'miss': miss}\n",
        "\n",
        "# Cross-validation\n",
        "print('\\nStarting GroupKFold CV...')\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train['sentence_id'].to_numpy()\n",
        "accs = []\n",
        "fold_logs = []\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    t_fold = time.time()\n",
        "    tr = train.iloc[tr_idx]\n",
        "    va = train.iloc[va_idx]\n",
        "    # Build maps on training fold\n",
        "    map_ctx = majority_map(tr, keys=['before','prev_before','next_before'])\n",
        "    map_uni = majority_map(tr, keys=['before'])\n",
        "    # Predict on validation fold\n",
        "    va_preds, stats = predict_after(va, map_ctx, map_uni)\n",
        "    acc = (va_preds == va['after'].to_numpy()).mean()\n",
        "    accs.append(acc)\n",
        "    fold_logs.append((fold, acc, stats))\n",
        "    print(f'Fold {fold}: acc={acc:.6f}, hits ctx={stats[\"ctx\"]}, uni={stats[\"uni\"]}, miss={stats[\"miss\"]}, elapsed={time.time()-t_fold:.1f}s', flush=True)\n",
        "\n",
        "print(f'CV mean acc: {np.mean(accs):.6f} +- {np.std(accs):.6f}')\n",
        "\n",
        "# Fit on full train and predict test\n",
        "t_fit = time.time()\n",
        "map_ctx_full = majority_map(train, keys=['before','prev_before','next_before'])\n",
        "map_uni_full = majority_map(train, keys=['before'])\n",
        "test_preds, stats_test = predict_after(test, map_ctx_full, map_uni_full)\n",
        "print('Test backoff stats:', stats_test)\n",
        "\n",
        "# Build submission\n",
        "sub = pd.DataFrame({\n",
        "    'id': test['sentence_id'].astype('string') + '_' + test['token_id'].astype('string'),\n",
        "    'after': test_preds\n",
        "})\n",
        "out_path = 'submission.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "print('Saved submission to', out_path, 'with shape', sub.shape)\n",
        "print(sub.head(5))\n",
        "print(f'Total elapsed: {time.time()-t0:.1f}s')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "id": "b9bf017d-9c31-4b1e-96af-662fd239b41e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fast submission: efficient majority mapping without CV\n",
        "import pandas as pd, numpy as np, time\n",
        "\n",
        "t0 = time.time()\n",
        "train_path = 'ru_train.csv.zip'\n",
        "test_path = 'ru_test_2.csv.zip'\n",
        "\n",
        "def add_context(df):\n",
        "    df = df.sort_values(['sentence_id','token_id']).copy()\n",
        "    df['prev_before'] = df.groupby('sentence_id')['before'].shift(1).fillna('\u27c2')\n",
        "    df['next_before'] = df.groupby('sentence_id')['before'].shift(-1).fillna('\u27c2')\n",
        "    return df\n",
        "\n",
        "print('Loading data (if needed)...')\n",
        "need_load = False\n",
        "try:\n",
        "    _ = train.shape\n",
        "    _ = test.shape\n",
        "except NameError:\n",
        "    need_load = True\n",
        "if need_load:\n",
        "    dtype_train = {'sentence_id':'int32', 'token_id':'int32', 'class':'category', 'before':'string', 'after':'string'}\n",
        "    dtype_test = {'sentence_id':'int32', 'token_id':'int32', 'before':'string'}\n",
        "    train = pd.read_csv(train_path, dtype=dtype_train)\n",
        "    test = pd.read_csv(test_path, dtype=dtype_test)\n",
        "    print('Loaded from disk.')\n",
        "else:\n",
        "    print('Using in-memory dataframes.')\n",
        "\n",
        "train = add_context(train)\n",
        "test = add_context(test)\n",
        "print('Train/Test ready:', train.shape, test.shape, flush=True)\n",
        "\n",
        "def top_freq_map(df, keys, target='after'):\n",
        "    # Count occurrences per (keys + target), pick top target per keys\n",
        "    cols = list(keys) + [target]\n",
        "    cnt = df.groupby(cols, observed=True).size().reset_index(name='cnt')\n",
        "    cnt.sort_values(keys + ['cnt'], ascending=[True]*len(keys) + [False], inplace=True)\n",
        "    top = cnt.drop_duplicates(subset=keys, keep='first')\n",
        "    # Build dict\n",
        "    return {tuple(row[k] for k in keys): row[target] for _, row in top.iterrows()}\n",
        "\n",
        "print('Building maps...')\n",
        "t_build = time.time()\n",
        "map_ctx = top_freq_map(train, keys=['before','prev_before','next_before'])\n",
        "map_uni = top_freq_map(train, keys=['before'])\n",
        "print(f'Maps built in {time.time()-t_build:.1f}s | ctx keys={len(map_ctx):,} uni keys={len(map_uni):,}', flush=True)\n",
        "\n",
        "def predict_after(df, map_ctx, map_uni):\n",
        "    preds = []\n",
        "    hit_ctx = hit_uni = miss = 0\n",
        "    b_arr = df['before'].array\n",
        "    p_arr = df['prev_before'].array\n",
        "    n_arr = df['next_before'].array\n",
        "    for b, p, n in zip(b_arr, p_arr, n_arr):\n",
        "        t = map_ctx.get((b, p, n))\n",
        "        if t is not None:\n",
        "            preds.append(t); hit_ctx += 1\n",
        "        else:\n",
        "            t = map_uni.get((b,))\n",
        "            if t is not None:\n",
        "                preds.append(t); hit_uni += 1\n",
        "            else:\n",
        "                preds.append(b); miss += 1\n",
        "    return np.array(preds, dtype=object), {'ctx': hit_ctx, 'uni': hit_uni, 'miss': miss}\n",
        "\n",
        "print('Predicting test...')\n",
        "t_pred = time.time()\n",
        "test_preds, stats_test = predict_after(test, map_ctx, map_uni)\n",
        "print(f'Backoff stats on test: {stats_test}, pred time {time.time()-t_pred:.1f}s')\n",
        "\n",
        "sub = pd.DataFrame({\n",
        "    'id': test['sentence_id'].astype('string') + '_' + test['token_id'].astype('string'),\n",
        "    'after': test_preds\n",
        "})\n",
        "out_path = 'submission.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "print('Saved submission:', out_path, sub.shape, 'Elapsed total:', f'{time.time()-t0:.1f}s')\n",
        "sub.head()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "id": "2406b64f-a844-438d-99f4-a7d7d46f1865",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ultra-fast baseline: unigram mapping (before -> most frequent after), identity fallback\n",
        "import pandas as pd, numpy as np, time\n",
        "\n",
        "t0 = time.time()\n",
        "train_path = 'ru_train.csv.zip'\n",
        "test_path = 'ru_test_2.csv.zip'\n",
        "\n",
        "print('Loading data (if needed)...')\n",
        "need_load = False\n",
        "try:\n",
        "    _ = train.shape; _ = test.shape\n",
        "except NameError:\n",
        "    need_load = True\n",
        "if need_load:\n",
        "    dtype_train = {'sentence_id':'int32', 'token_id':'int32', 'class':'category', 'before':'string', 'after':'string'}\n",
        "    dtype_test = {'sentence_id':'int32', 'token_id':'int32', 'before':'string'}\n",
        "    train = pd.read_csv(train_path, dtype=dtype_train)\n",
        "    test = pd.read_csv(test_path, dtype=dtype_test)\n",
        "    print('Loaded from disk.')\n",
        "else:\n",
        "    print('Using in-memory dataframes.')\n",
        "\n",
        "print('Building unigram map (before -> most frequent after)...')\n",
        "t_build = time.time()\n",
        "cnt = (train.groupby(['before','after'], observed=True).size().reset_index(name='cnt'))\n",
        "cnt.sort_values(['before','cnt'], ascending=[True, False], inplace=True)\n",
        "top = cnt.drop_duplicates(subset=['before'], keep='first')\n",
        "map_uni = dict(zip(top['before'].array, top['after'].array))\n",
        "print(f'Unigram map size: {len(map_uni):,} built in {time.time()-t_build:.1f}s')\n",
        "\n",
        "print('Predicting on test with identity fallback...')\n",
        "t_pred = time.time()\n",
        "b = test['before'].array\n",
        "preds = np.fromiter((map_uni.get(x, x) for x in b), dtype=object, count=len(b))\n",
        "print(f'Predicted {len(preds):,} tokens in {time.time()-t_pred:.1f}s')\n",
        "\n",
        "sub = pd.DataFrame({\n",
        "    'id': test['sentence_id'].astype('string') + '_' + test['token_id'].astype('string'),\n",
        "    'after': preds\n",
        "})\n",
        "out_path = 'submission.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "print('Saved submission:', out_path, sub.shape, 'Total elapsed:', f'{time.time()-t0:.1f}s')\n",
        "sub.head()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "id": "e47c89da-04d3-4e7f-8e08-3616dc6d37ba",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optimized unigram mapper using categorical codes + numpy (fast on 9.5M rows)\n",
        "import pandas as pd, numpy as np, time\n",
        "\n",
        "t0 = time.time()\n",
        "train_path = 'ru_train.csv.zip'\n",
        "test_path = 'ru_test_2.csv.zip'\n",
        "\n",
        "print('Loading data (if needed)...')\n",
        "need_load = False\n",
        "try:\n",
        "    _ = train.shape; _ = test.shape\n",
        "except NameError:\n",
        "    need_load = True\n",
        "if need_load:\n",
        "    dtype_train = {'sentence_id':'int32', 'token_id':'int32', 'class':'category', 'before':'string', 'after':'string'}\n",
        "    dtype_test = {'sentence_id':'int32', 'token_id':'int32', 'before':'string'}\n",
        "    train = pd.read_csv(train_path, dtype=dtype_train)\n",
        "    test = pd.read_csv(test_path, dtype=dtype_test)\n",
        "    print('Loaded from disk.')\n",
        "else:\n",
        "    print('Using in-memory dataframes.')\n",
        "\n",
        "print('Building categorical encodings...')\n",
        "t1 = time.time()\n",
        "before_cat = pd.Categorical(train['before'])\n",
        "after_cat = pd.Categorical(train['after'])\n",
        "bc = before_cat.codes.astype(np.int32)\n",
        "ac = after_cat.codes.astype(np.int32)\n",
        "print(f'Unique before: {len(before_cat.categories):,}, unique after: {len(after_cat.categories):,}, took {time.time()-t1:.1f}s')\n",
        "\n",
        "print('Counting (before, after) pairs via numpy unique...')\n",
        "t2 = time.time()\n",
        "key = (bc.astype(np.int64) << 32) | ac.astype(np.int64)\n",
        "uniq_key, counts = np.unique(key, return_counts=True)\n",
        "bc_u = (uniq_key >> 32).astype(np.int32)\n",
        "ac_u = (uniq_key & np.int64(0xFFFFFFFF)).astype(np.int32)\n",
        "print(f'Pairs: {len(uniq_key):,}, counting took {time.time()-t2:.1f}s')\n",
        "\n",
        "print('Selecting top after per before...')\n",
        "t3 = time.time()\n",
        "order = np.argsort(bc_u, kind='mergesort')\n",
        "bc_s = bc_u[order]\n",
        "ac_s = ac_u[order]\n",
        "cnt_s = counts[order]\n",
        "diff = np.empty_like(bc_s, dtype=bool)\n",
        "diff[0] = True\n",
        "diff[1:] = bc_s[1:] != bc_s[:-1]\n",
        "starts = np.flatnonzero(diff)\n",
        "ends = np.append(starts[1:], len(bc_s))\n",
        "best_ac = np.full(len(before_cat.categories), -1, dtype=np.int32)\n",
        "for s, e in zip(starts, ends):\n",
        "    arg = s + np.argmax(cnt_s[s:e])\n",
        "    best_ac[bc_s[s]] = ac_s[arg]\n",
        "print(f'Selected best labels for {len(starts):,} befores in {time.time()-t3:.1f}s')\n",
        "\n",
        "print('Predicting on test...')\n",
        "t4 = time.time()\n",
        "test_b_codes = pd.Categorical(test['before'], categories=before_cat.categories).codes.astype(np.int32)\n",
        "seen_mask = test_b_codes != -1\n",
        "pred_after_codes = np.full(len(test_b_codes), -1, dtype=np.int32)\n",
        "pred_after_codes[seen_mask] = best_ac[test_b_codes[seen_mask]]\n",
        "# Map codes to strings; unseen -> identity\n",
        "after_strings = np.asarray(after_cat.categories, dtype=object)\n",
        "test_before_arr = test['before'].to_numpy(dtype=object)\n",
        "preds = test_before_arr.copy()\n",
        "seen_and_have = seen_mask & (pred_after_codes != -1)\n",
        "preds[seen_and_have] = after_strings[pred_after_codes[seen_and_have]]\n",
        "print(f'Predicted {seen_and_have.sum():,} via map, {len(preds)-seen_and_have.sum():,} via identity; time {time.time()-t4:.1f}s')\n",
        "\n",
        "sub = pd.DataFrame({\n",
        "    'id': test['sentence_id'].astype('string') + '_' + test['token_id'].astype('string'),\n",
        "    'after': preds\n",
        "})\n",
        "out_path = 'submission.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "print('Saved submission:', out_path, sub.shape, 'Total elapsed:', f'{time.time()-t0:.1f}s')\n",
        "sub.head()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "id": "5689fc43-8e53-4006-a815-14774aa8a4ff",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Chunked mapper restricted to test tokens: fast and memory-efficient\n",
        "import pandas as pd, numpy as np, time, gc\n",
        "\n",
        "t0 = time.time()\n",
        "train_path = 'ru_train.csv.zip'\n",
        "test_path = 'ru_test_2.csv.zip'\n",
        "\n",
        "print('Loading test...')\n",
        "dtype_test = {'sentence_id':'int32', 'token_id':'int32', 'before':'string'}\n",
        "test = pd.read_csv(test_path, dtype=dtype_test)\n",
        "print('Test shape:', test.shape, flush=True)\n",
        "\n",
        "# Collect unique test befores\n",
        "test_befores = pd.unique(test['before']).tolist()\n",
        "test_before_set = set(test_befores)\n",
        "print('Unique test befores:', len(test_before_set))\n",
        "\n",
        "print('Counting train pairs (before, after) restricted to test befores)...')\n",
        "t1 = time.time()\n",
        "chunk_size = 500_000\n",
        "dtype_train = {'sentence_id':'int32', 'token_id':'int32', 'class':'category', 'before':'string', 'after':'string'}\n",
        "counts = {}  # dict of before -> dict(after->count)\n",
        "total_rows = 0\n",
        "for i, chunk in enumerate(pd.read_csv(train_path, dtype=dtype_train, chunksize=chunk_size)):\n",
        "    total_rows += len(chunk)\n",
        "    sub = chunk[chunk['before'].isin(test_before_set)][['before','after']]\n",
        "    if not sub.empty:\n",
        "        grp = sub.groupby(['before','after'], observed=True).size().reset_index(name='cnt')\n",
        "        for b, a, c in grp.itertuples(index=False):\n",
        "            d = counts.get(b)\n",
        "            if d is None:\n",
        "                d = {}; counts[b] = d\n",
        "            d[a] = d.get(a, 0) + int(c)\n",
        "    if (i+1) % 10 == 0:\n",
        "        print(f'  processed ~{total_rows:,} rows, current keys={len(counts):,}', flush=True)\n",
        "    del chunk, sub\n",
        "    gc.collect()\n",
        "print(f'Finished counting in {time.time()-t1:.1f}s; total rows {total_rows:,}, keys {len(counts):,}')\n",
        "\n",
        "print('Selecting top after per before...')\n",
        "t2 = time.time()\n",
        "map_uni = {}\n",
        "for b, d in counts.items():\n",
        "    # pick after with max count; ties resolved by first encountered\n",
        "    best_a = max(d.items(), key=lambda kv: kv[1])[0]\n",
        "    map_uni[b] = best_a\n",
        "counts = None\n",
        "gc.collect()\n",
        "print(f'Map size: {len(map_uni):,} built in {time.time()-t2:.1f}s')\n",
        "\n",
        "print('Predicting test with identity fallback...')\n",
        "t3 = time.time()\n",
        "b_arr = test['before'].to_numpy(dtype=object)\n",
        "preds = np.fromiter((map_uni.get(x, x) for x in b_arr), dtype=object, count=len(b_arr))\n",
        "print(f'Predicted {len(preds):,} tokens in {time.time()-t3:.1f}s')\n",
        "\n",
        "sub = pd.DataFrame({\n",
        "    'id': test['sentence_id'].astype('string') + '_' + test['token_id'].astype('string'),\n",
        "    'after': preds\n",
        "})\n",
        "out_path = 'submission.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "print('Saved submission:', out_path, sub.shape, 'Total elapsed:', f'{time.time()-t0:.1f}s')\n",
        "sub.head()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "id": "34178163-e2e8-41e0-97d6-56bc77e7c710",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Emergency identity submission to get LB baseline fast\n",
        "import pandas as pd, time\n",
        "t0 = time.time()\n",
        "test_path = 'ru_test_2.csv.zip'\n",
        "dtype_test = {'sentence_id':'int32', 'token_id':'int32', 'before':'string'}\n",
        "print('Loading test...')\n",
        "test = pd.read_csv(test_path, dtype=dtype_test)\n",
        "print('Test shape:', test.shape)\n",
        "sub = pd.DataFrame({\n",
        "    'id': test['sentence_id'].astype('string') + '_' + test['token_id'].astype('string'),\n",
        "    'after': test['before']\n",
        "})\n",
        "out_path = 'submission.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "print('Saved identity submission:', out_path, sub.shape, 'Elapsed:', f'{time.time()-t0:.1f}s')\n",
        "sub.head()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "id": "73b61d9d-f0cd-45ed-8d6b-6b57c359df19",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fastest path: copy sample submission from zip to submission.csv without parsing\n",
        "import zipfile, os, time\n",
        "t0 = time.time()\n",
        "zip_path = 'ru_sample_submission_2.csv.zip'\n",
        "out_path = 'submission.csv'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "    names = zf.namelist()\n",
        "    if not names:\n",
        "        raise RuntimeError('Sample submission zip has no files')\n",
        "    member = names[0]\n",
        "    with zf.open(member, 'r') as src, open(out_path, 'wb') as dst:\n",
        "        # stream copy\n",
        "        while True:\n",
        "            chunk = src.read(1024 * 1024)\n",
        "            if not chunk:\n",
        "                break\n",
        "            dst.write(chunk)\n",
        "print('Wrote submission.csv from', zip_path, 'member:', member, 'size:', os.path.getsize(out_path), 'bytes', 'Elapsed:', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote submission.csv from ru_sample_submission_2.csv.zip member: ru_sample_submission_2.csv size: 23892801 bytes Elapsed: 0.0s\n"
          ]
        }
      ]
    },
    {
      "id": "b105a8f6-7948-41f4-a82f-56783d5d66bd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Efficient unigram mapper: unzip train once, chunked counting restricted to test tokens\n",
        "import os, zipfile, time, gc\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "t0 = time.time()\n",
        "train_zip = 'ru_train.csv.zip'\n",
        "test_zip = 'ru_test_2.csv.zip'\n",
        "train_csv = 'ru_train.csv'\n",
        "\n",
        "# 1) Ensure uncompressed train CSV exists (pandas chunks work only on uncompressed CSV reliably)\n",
        "if not os.path.exists(train_csv):\n",
        "    print('Extracting train CSV from zip...')\n",
        "    t_ex = time.time()\n",
        "    with zipfile.ZipFile(train_zip, 'r') as zf:\n",
        "        names = zf.namelist()\n",
        "        member = names[0]\n",
        "        with zf.open(member, 'r') as src, open(train_csv, 'wb') as dst:\n",
        "            while True:\n",
        "                chunk = src.read(1024 * 1024 * 8)\n",
        "                if not chunk:\n",
        "                    break\n",
        "                dst.write(chunk)\n",
        "    print(f'Extracted to {train_csv} in {time.time()-t_ex:.1f}s, size={os.path.getsize(train_csv):,} bytes')\n",
        "else:\n",
        "    print('Found existing uncompressed train CSV:', train_csv, 'size=', os.path.getsize(train_csv))\n",
        "\n",
        "# 2) Load test (small enough to read directly from zip)\n",
        "print('Loading test...')\n",
        "dtype_test = {'sentence_id':'int32', 'token_id':'int32', 'before':'string'}\n",
        "test = pd.read_csv(test_zip, dtype=dtype_test)\n",
        "print('Test shape:', test.shape, flush=True)\n",
        "\n",
        "# 3) Collect unique test befores\n",
        "test_before_set = set(pd.unique(test['before']))\n",
        "print('Unique test befores:', len(test_before_set))\n",
        "\n",
        "# 4) Chunked counting over uncompressed train CSV\n",
        "print('Counting (before, after) pairs restricted to test befores...')\n",
        "t1 = time.time()\n",
        "chunk_size = 1_000_000\n",
        "dtype_train = {'sentence_id':'int32', 'token_id':'int32', 'class':'category', 'before':'string', 'after':'string'}\n",
        "counts = {}  # before -> dict(after->count)\n",
        "total_rows = 0\n",
        "reader = pd.read_csv(train_csv, dtype=dtype_train, usecols=['before','after'], chunksize=chunk_size)\n",
        "for i, chunk in enumerate(reader, 1):\n",
        "    total_rows += len(chunk)\n",
        "    sub = chunk[chunk['before'].isin(test_before_set)]\n",
        "    if not sub.empty:\n",
        "        grp = sub.groupby(['before','after'], observed=True).size().reset_index(name='cnt')\n",
        "        for b, a, c in grp.itertuples(index=False):\n",
        "            d = counts.get(b)\n",
        "            if d is None:\n",
        "                d = {}; counts[b] = d\n",
        "            d[a] = d.get(a, 0) + int(c)\n",
        "    if i % 5 == 0:\n",
        "        print(f'  processed ~{total_rows:,} rows, keys so far={len(counts):,}', flush=True)\n",
        "    del chunk, sub\n",
        "    gc.collect()\n",
        "print(f'Finished counting in {time.time()-t1:.1f}s; total rows {total_rows:,}, keys {len(counts):,}')\n",
        "\n",
        "# 5) Select top after per before\n",
        "print('Selecting top after per before...')\n",
        "t2 = time.time()\n",
        "map_uni = {}\n",
        "for b, d in counts.items():\n",
        "    best_a = max(d.items(), key=lambda kv: kv[1])[0]\n",
        "    map_uni[b] = best_a\n",
        "counts = None\n",
        "gc.collect()\n",
        "print(f'Map size: {len(map_uni):,} built in {time.time()-t2:.1f}s')\n",
        "\n",
        "# 6) Predict on test with identity fallback\n",
        "print('Predicting test...')\n",
        "t3 = time.time()\n",
        "b_arr = test['before'].to_numpy(dtype=object)\n",
        "preds = np.fromiter((map_uni.get(x, x) for x in b_arr), dtype=object, count=len(b_arr))\n",
        "print(f'Predicted {len(preds):,} tokens in {time.time()-t3:.1f}s')\n",
        "\n",
        "# 7) Write submission\n",
        "sub = pd.DataFrame({\n",
        "    'id': test['sentence_id'].astype('string') + '_' + test['token_id'].astype('string'),\n",
        "    'after': preds\n",
        "})\n",
        "out_path = 'submission.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "print('Saved submission:', out_path, sub.shape, 'Total elapsed:', f'{time.time()-t0:.1f}s')\n",
        "sub.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting train CSV from zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted to ru_train.csv in 0.7s, size=443,635,601 bytes\nLoading test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test shape: (1059191, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique test befores: 185535\nCounting (before, after) pairs restricted to test befores...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed ~5,000,000 rows, keys so far=121,605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed ~9,515,325 rows, keys so far=135,873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished counting in 20.3s; total rows 9,515,325, keys 135,873\nSelecting top after per before...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map size: 135,873 built in 0.3s\nPredicting test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted 1,059,191 tokens in 0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission: submission.csv (1059191, 2) Total elapsed: 24.3s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "    id        after\n0  0_0       \u0422\u0435\u043f\u0435\u0440\u044c\n1  0_1          \u0432\u0441\u0435\n2  0_2  \u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\n3  0_3        \u0437\u043e\u0432\u0443\u0442\n4  0_4      \u042f\u043c\u0430\u043c\u043e\u0442\u043e",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>after</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0_0</td>\n      <td>\u0422\u0435\u043f\u0435\u0440\u044c</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0_1</td>\n      <td>\u0432\u0441\u0435</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0_2</td>\n      <td>\u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0_3</td>\n      <td>\u0437\u043e\u0432\u0443\u0442</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0_4</td>\n      <td>\u042f\u043c\u0430\u043c\u043e\u0442\u043e</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "10d95c4a-abd3-4223-8331-cf44c4356e2f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Contextual memorization with backoffs: (before, prev, next) -> (before, prev) -> (before, next) -> unigram -> identity\n",
        "import pandas as pd, numpy as np, time, os\n",
        "\n",
        "t0 = time.time()\n",
        "train_csv = 'ru_train.csv'  # uncompressed from previous step\n",
        "test_zip = 'ru_test_2.csv.zip'\n",
        "\n",
        "def add_context(df):\n",
        "    df = df.sort_values(['sentence_id','token_id']).copy()\n",
        "    df['prev_before'] = df.groupby('sentence_id', observed=True)['before'].shift(1).fillna('\u27c2')\n",
        "    df['next_before'] = df.groupby('sentence_id', observed=True)['before'].shift(-1).fillna('\u27c2')\n",
        "    return df\n",
        "\n",
        "print('Loading test and adding context...')\n",
        "dtype_test = {'sentence_id':'int32','token_id':'int32','before':'string'}\n",
        "test = pd.read_csv(test_zip, dtype=dtype_test)\n",
        "test = add_context(test)\n",
        "print('Test ready:', test.shape)\n",
        "\n",
        "# Build key sets from test\n",
        "print('Building test key sets...')\n",
        "test['k_tri'] = (test['before'] + '\\t' + test['prev_before'] + '\\t' + test['next_before']).astype('string')\n",
        "test['k_bp']  = (test['before'] + '\\t' + test['prev_before']).astype('string')\n",
        "test['k_bn']  = (test['before'] + '\\t' + test['next_before']).astype('string')\n",
        "tri_set = set(pd.unique(test['k_tri']))\n",
        "bp_set  = set(pd.unique(test['k_bp']))\n",
        "bn_set  = set(pd.unique(test['k_bn']))\n",
        "print('Unique test keys -> tri:', len(tri_set), 'bp:', len(bp_set), 'bn:', len(bn_set))\n",
        "\n",
        "print('Loading full train (uncompressed) minimal cols and adding context...')\n",
        "dtype_train = {'sentence_id':'int32','token_id':'int32','before':'string','after':'string'}\n",
        "train = pd.read_csv(train_csv, dtype=dtype_train, usecols=['sentence_id','token_id','before','after'])\n",
        "train = add_context(train)\n",
        "print('Train ready:', train.shape, 'Elapsed:', f'{time.time()-t0:.1f}s')\n",
        "\n",
        "# Compose keys in train\n",
        "print('Composing train keys...')\n",
        "t1 = time.time()\n",
        "train['k_tri'] = (train['before'] + '\\t' + train['prev_before'] + '\\t' + train['next_before']).astype('string')\n",
        "train['k_bp']  = (train['before'] + '\\t' + train['prev_before']).astype('string')\n",
        "train['k_bn']  = (train['before'] + '\\t' + train['next_before']).astype('string')\n",
        "print('Keys composed in', f'{time.time()-t1:.1f}s')\n",
        "\n",
        "# Filter rows that can hit any test key\n",
        "print('Filtering candidate rows...')\n",
        "t2 = time.time()\n",
        "mask = train['k_tri'].isin(tri_set) | train['k_bp'].isin(bp_set) | train['k_bn'].isin(bn_set)\n",
        "cand = train.loc[mask, ['after','k_tri','k_bp','k_bn']]\n",
        "print('Candidates:', cand.shape, 'filter time:', f'{time.time()-t2:.1f}s')\n",
        "\n",
        "def build_top_map(df, key_col):\n",
        "    if df.empty: return {}\n",
        "    grp = df.groupby([key_col, 'after'], observed=True).size().reset_index(name='cnt')\n",
        "    grp.sort_values([key_col, 'cnt'], ascending=[True, False], inplace=True)\n",
        "    top = grp.drop_duplicates(subset=[key_col], keep='first')\n",
        "    return dict(zip(top[key_col].array, top['after'].array))\n",
        "\n",
        "print('Building maps...')\n",
        "t3 = time.time()\n",
        "map_tri = build_top_map(cand[['k_tri','after']].rename(columns={'k_tri':'key'}), 'key') if not cand.empty else {}\n",
        "map_bp  = build_top_map(cand[['k_bp','after']].rename(columns={'k_bp':'key'}), 'key') if not cand.empty else {}\n",
        "map_bn  = build_top_map(cand[['k_bn','after']].rename(columns={'k_bn':'key'}), 'key') if not cand.empty else {}\n",
        "print(f'Maps built in {time.time()-t3:.1f}s | tri={len(map_tri):,} bp={len(map_bp):,} bn={len(map_bn):,}')\n",
        "\n",
        "# Optional: unigram map (fallback). Use fast value_counts style on full train.\n",
        "print('Building unigram fallback map...')\n",
        "t4 = time.time()\n",
        "u = train.groupby(['before','after'], observed=True).size().reset_index(name='cnt')\n",
        "u.sort_values(['before','cnt'], ascending=[True, False], inplace=True)\n",
        "u_top = u.drop_duplicates(subset=['before'], keep='first')\n",
        "map_uni = dict(zip(u_top['before'].array, u_top['after'].array))\n",
        "print('Unigram size:', len(map_uni), 'built in', f'{time.time()-t4:.1f}s')\n",
        "\n",
        "# Predict with backoffs\n",
        "print('Predicting with backoffs...')\n",
        "t5 = time.time()\n",
        "preds = []\n",
        "hit_tri = hit_bp = hit_bn = hit_uni = miss = 0\n",
        "for b, p, n in zip(test['before'].array, test['prev_before'].array, test['next_before'].array):\n",
        "    kt = f'{b}\\t{p}\\t{n}'\n",
        "    r = map_tri.get(kt)\n",
        "    if r is not None:\n",
        "        preds.append(r); hit_tri += 1; continue\n",
        "    kb = f'{b}\\t{p}'\n",
        "    r = map_bp.get(kb)\n",
        "    if r is not None:\n",
        "        preds.append(r); hit_bp += 1; continue\n",
        "    kn = f'{b}\\t{n}'\n",
        "    r = map_bn.get(kn)\n",
        "    if r is not None:\n",
        "        preds.append(r); hit_bn += 1; continue\n",
        "    r = map_uni.get(b)\n",
        "    if r is not None:\n",
        "        preds.append(r); hit_uni += 1; continue\n",
        "    preds.append(b); miss += 1\n",
        "preds = np.array(preds, dtype=object)\n",
        "print(f'Hits tri={hit_tri:,} bp={hit_bp:,} bn={hit_bn:,} uni={hit_uni:,} miss={miss:,} | Elapsed {time.time()-t5:.1f}s')\n",
        "\n",
        "sub = pd.DataFrame({\n",
        "    'id': test['sentence_id'].astype('string') + '_' + test['token_id'].astype('string'),\n",
        "    'after': preds\n",
        "})\n",
        "out_path = 'submission.csv'\n",
        "sub.to_csv(out_path, index=False)\n",
        "print('Saved submission:', out_path, sub.shape, '| Total elapsed:', f'{time.time()-t0:.1f}s')\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading test and adding context...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test ready: (1059191, 5)\nBuilding test key sets...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}