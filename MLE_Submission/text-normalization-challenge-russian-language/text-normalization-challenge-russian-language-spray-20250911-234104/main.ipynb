{
  "cells": [
    {
      "id": "64ff1bad-8194-49fc-9add-f035afe68c2d",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan to Medal: Text Normalization Challenge (Russian)\n",
        "\n",
        "Objectives:\n",
        "- Build a strong, fast baseline with reliable CV; iterate to reach \u2265 bronze.\n",
        "\n",
        "High-level Strategy:\n",
        "1) Data understanding:\n",
        "   - Inspect train/test schemas, sizes, target definition, and sample submission.\n",
        "   - Verify label distribution and common semiotic classes (e.g., PLAIN, PUNCT, DATE, CARDINAL, etc.).\n",
        "2) CV protocol:\n",
        "   - Mirror test distribution by grouping/stratifying on semiotic class if available.\n",
        "   - Deterministic KFold with fixed seed; save folds to disk.\n",
        "3) Baseline modeling:\n",
        "   - Token-level classifier to predict the normalized form from the written token.\n",
        "   - Start with class-conditional rules + ML fallback:\n",
        "     a) Heuristics/rules for PLAIN/PUNCT/LETTERS-to-lowercase passthrough.\n",
        "     b) ML model(s) for challenging classes (NUMBER/CARDINAL/ORDINAL/DATE/TIME/MONEY/MEASURE).\n",
        "   - Baseline ML: CatBoost/XGBoost with character n-gram features and simple regex-derived features.\n",
        "4) Feature engineering:\n",
        "   - Regex flags (has_digits, has_letters, cyrillic_only, latin_only, contains_currency, contains_colon, etc.).\n",
        "   - Character n-gram TF-IDF blocks; cache matrices.\n",
        "   - Context features: previous/next token/symbols.\n",
        "5) Inference pipeline:\n",
        "   - Semiotic class detection (model or rules) \u2192 route to per-class normalizer (rule or model).\n",
        "   - Fallback to identity where appropriate.\n",
        "6) Iteration & validation discipline:\n",
        "   - Log OOF accuracy by class; mine top error buckets and add rules.\n",
        "   - Save OOF/test predictions and per-class confusion matrices.\n",
        "7) Ensembling:\n",
        "   - Blend per-class specialized models if beneficial.\n",
        "\n",
        "Immediate Next Steps:\n",
        "- Verify GPU, list files, load train/test heads, parse schema, inspect sample submission.\n",
        "- Identify target column(s) and class labels; count semiotic classes.\n",
        "- Establish a simple deterministic CV and a minimal baseline (identity/rule-based) for a quick score.\n",
        "\n",
        "Questions for Experts (next cell will request review):\n",
        "- What medal-winning architectures/pipelines won here? Rule-based cascades + ML? Best per-class strategies?\n",
        "- Key regex/rule patterns that cover most errors? Common pitfalls/leakage traps in this dataset?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "e81ad6b8-2dba-482b-82a3-0dd6d9ba06b6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment and data inspection\n",
        "import os, sys, zipfile, json, re, time, textwrap, subprocess, io\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "print('CWD:', os.getcwd())\n",
        "print('Files:', os.listdir())\n",
        "\n",
        "# Quick CUDA/NVIDIA presence check without heavy installs\n",
        "print('\\nCUDA paths present:', os.path.exists('/usr/local/cuda'), os.path.exists('/usr/local/nvidia'))\n",
        "try:\n",
        "    out = subprocess.run(['bash','-lc','nvcc --version'], capture_output=True, text=True, timeout=5)\n",
        "    print('nvcc:', out.stdout.splitlines()[-1] if out.returncode==0 else out.stderr.strip())\n",
        "except Exception as e:\n",
        "    print('nvcc check skipped:', e)\n",
        "\n",
        "def zip_head(path, n=5):\n",
        "    df = pd.read_csv(path, compression='zip', nrows=n)\n",
        "    print(f'Head of {path}:')\n",
        "    print(df.head())\n",
        "    return df\n",
        "\n",
        "def zip_columns(path):\n",
        "    df = pd.read_csv(path, compression='zip', nrows=1)\n",
        "    return list(df.columns)\n",
        "\n",
        "def zip_row_count(path, chunksize=1_000_000):\n",
        "    cnt = 0\n",
        "    for chunk in pd.read_csv(path, compression='zip', chunksize=chunksize):\n",
        "        cnt += len(chunk)\n",
        "    return cnt\n",
        "\n",
        "train_path = 'ru_train.csv.zip'\n",
        "test_path = 'ru_test_2.csv.zip'\n",
        "sub_path = 'ru_sample_submission_2.csv.zip'\n",
        "\n",
        "print('\\nColumns:')\n",
        "for p in [train_path, test_path, sub_path]:\n",
        "    try:\n",
        "        print(p, '->', zip_columns(p))\n",
        "    except Exception as e:\n",
        "        print(p, '-> error:', e)\n",
        "\n",
        "print('\\nPreview:')\n",
        "train_head = zip_head(train_path, 5)\n",
        "test_head = zip_head(test_path, 5)\n",
        "sub_head = zip_head(sub_path, 5)\n",
        "\n",
        "print('\\nCounting rows (this may take a moment, logs per file):')\n",
        "t0=time.time()\n",
        "for p in [train_path, test_path, sub_path]:\n",
        "    t1=time.time()\n",
        "    n = zip_row_count(p, chunksize=500_000)\n",
        "    print(f'{p}: rows={n:,} (elapsed {time.time()-t1:.1f}s)')\n",
        "print('Total elapsed:', time.time()-t0, 's')\n",
        "\n",
        "# Heuristic guess of key columns\n",
        "train_cols = list(train_head.columns)\n",
        "print('\\nGuessed columns in train:', train_cols)\n",
        "likely_id = [c for c in train_cols if c.lower() in ('id','index') or c.endswith('_id')]\n",
        "likely_before = [c for c in train_cols if c.lower()=='before']\n",
        "likely_after = [c for c in train_cols if c.lower()=='after']\n",
        "likely_class = [c for c in train_cols if c.lower() in ('class','semiotic_class')]\n",
        "likely_sent = [c for c in train_cols if 'sent' in c.lower()]\n",
        "print('id:', likely_id, 'before:', likely_before, 'after:', likely_after, 'class:', likely_class, 'sentence:', likely_sent)\n",
        "\n",
        "# Show basic value_counts if columns exist\n",
        "if likely_class:\n",
        "    vc = pd.read_csv(train_path, compression='zip', usecols=[likely_class[0]], nrows=1_000_000)[likely_class[0]].value_counts().head(20)\n",
        "    print('\\nClass distribution (first 1M rows):')\n",
        "    print(vc)\n",
        "\n",
        "# Sample identity-rate estimate on small sample\n",
        "if likely_before and likely_after:\n",
        "    sample = pd.read_csv(train_path, compression='zip', nrows=200_000, usecols=[likely_before[0], likely_after[0]])\n",
        "    ident_rate = (sample[likely_before[0]] == sample[likely_after[0]]).mean()\n",
        "    print(f'\\nApprox identity rate on 200k sample: {ident_rate:.4f}')\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CWD: /app/agent_run_states/text-normalization-challenge-russian-language-spray-20250911-234104\nFiles: ['main.ipynb', 'agent_metadata', 'ru_sample_submission_2.csv.zip', 'requirements.txt', 'task.txt', 'vm_info.json', 'ru_test_2.csv.zip', 'docker_run.log', 'description.md', 'ru_train.csv.zip']\n\nCUDA paths present: True True\nnvcc: bash: line 1: nvcc: command not found\n\nColumns:\nru_train.csv.zip -> ['sentence_id', 'token_id', 'class', 'before', 'after']\nru_test_2.csv.zip -> ['sentence_id', 'token_id', 'before']\nru_sample_submission_2.csv.zip -> ['id', 'after']\n\nPreview:\nHead of ru_train.csv.zip:\n   sentence_id  token_id  class     before  \\\n0            0         0  PLAIN         \u041f\u043e   \n1            0         1  PLAIN  \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044e   \n2            0         2  PLAIN         \u043d\u0430   \n3            0         3   DATE   1862 \u0433\u043e\u0434   \n4            0         4  PUNCT          .   \n\n                                    after  \n0                                      \u041f\u043e  \n1                               \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044e  \n2                                      \u043d\u0430  \n3  \u0442\u044b\u0441\u044f\u0447\u0430 \u0432\u043e\u0441\u0435\u043c\u044c\u0441\u043e\u0442 \u0448\u0435\u0441\u0442\u044c\u0434\u0435\u0441\u044f\u0442 \u0432\u0442\u043e\u0440\u043e\u0439 \u0433\u043e\u0434  \n4                                       .  \nHead of ru_test_2.csv.zip:\n   sentence_id  token_id       before\n0            0         0       \u0422\u0435\u043f\u0435\u0440\u044c\n1            0         1          \u0432\u0441\u0435\n2            0         2  \u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\n3            0         3        \u0437\u043e\u0432\u0443\u0442\n4            0         4      \u042f\u043c\u0430\u043c\u043e\u0442\u043e\nHead of ru_sample_submission_2.csv.zip:\n    id        after\n0  0_0       \u0422\u0435\u043f\u0435\u0440\u044c\n1  0_1          \u0432\u0441\u0435\n2  0_2  \u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\n3  0_3        \u0437\u043e\u0432\u0443\u0442\n4  0_4      \u042f\u043c\u0430\u043c\u043e\u0442\u043e\n\nCounting rows (this may take a moment, logs per file):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ru_train.csv.zip: rows=9,515,325 (elapsed 9.5s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ru_test_2.csv.zip: rows=1,059,191 (elapsed 0.5s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ru_sample_submission_2.csv.zip: rows=1,059,191 (elapsed 0.8s)\nTotal elapsed: 10.918184041976929 s\n\nGuessed columns in train: ['sentence_id', 'token_id', 'class', 'before', 'after']\nid: ['sentence_id', 'token_id'] before: ['before'] after: ['after'] class: ['class'] sentence: ['sentence_id']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nClass distribution (first 1M rows):\nclass\nPLAIN         694962\nPUNCT         217498\nCARDINAL       25691\nLETTERS        18609\nDATE           17593\nVERBATIM       14572\nORDINAL         4311\nMEASURE         3818\nTELEPHONE        966\nDECIMAL          657\nELECTRONIC       549\nMONEY            219\nFRACTION         214\nDIGIT            189\nTIME             152\nName: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nApprox identity rate on 200k sample: 0.8787\n\nDone.\n"
          ]
        }
      ]
    },
    {
      "id": "2ab096d7-7967-481e-80b3-fb9ddaf042bb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Baseline submission: memorize (before -> most frequent after) and fallback to identity\n",
        "import pandas as pd\n",
        "import time\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "train_path = 'ru_train.csv.zip'\n",
        "test_path = 'ru_test_2.csv.zip'\n",
        "\n",
        "t0 = time.time()\n",
        "print('Building memorization map from train (before -> most frequent after) ...')\n",
        "counts = defaultdict(Counter)\n",
        "rows = 0\n",
        "chunk_idx = 0\n",
        "for chunk in pd.read_csv(train_path, compression='zip', usecols=['before','after'], chunksize=500_000):\n",
        "    chunk_idx += 1\n",
        "    for b, a in zip(chunk['before'].values, chunk['after'].values):\n",
        "        counts[b][a] += 1\n",
        "    rows += len(chunk)\n",
        "    if chunk_idx % 5 == 0:\n",
        "        print(f'  processed {rows:,} rows (chunks {chunk_idx}), unique befores: {len(counts):,}')\n",
        "\n",
        "memorized = {}\n",
        "for b, ctr in counts.items():\n",
        "    memorized[b] = ctr.most_common(1)[0][0]\n",
        "print(f'Map size: {len(memorized):,}. Build time: {time.time()-t0:.1f}s')\n",
        "\n",
        "print('Loading test and generating predictions ...')\n",
        "test = pd.read_csv(test_path, compression='zip')\n",
        "test['id'] = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)\n",
        "t1 = time.time()\n",
        "pred = test['before'].map(memorized)\n",
        "missing = pred.isna().sum()\n",
        "print(f'Mapped: {len(pred)-missing:,}, missing: {missing:,}')\n",
        "pred = pred.fillna(test['before'])\n",
        "\n",
        "submission = pd.DataFrame({'id': test['id'], 'after': pred})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with', len(submission), 'rows. Total elapsed:', f'{time.time()-t0:.1f}s')\n",
        "\n",
        "# Preview\n",
        "print(submission.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building memorization map from train (before -> most frequent after) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 2,500,000 rows (chunks 5), unique befores: 324,298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 5,000,000 rows (chunks 10), unique befores: 504,028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 7,500,000 rows (chunks 15), unique befores: 649,053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 9,515,325 rows (chunks 20), unique befores: 751,570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map size: 751,570. Build time: 18.6s\nLoading test and generating predictions ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapped: 1,007,917, missing: 51,274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with 1059191 rows. Total elapsed: 21.5s\n    id        after\n0  0_0       \u0422\u0435\u043f\u0435\u0440\u044c\n1  0_1          \u0432\u0441\u0435\n2  0_2  \u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\n3  0_3        \u0437\u043e\u0432\u0443\u0442\n4  0_4      \u042f\u043c\u0430\u043c\u043e\u0442\u043e\n"
          ]
        }
      ]
    },
    {
      "id": "7c325057-2b15-4fcb-b89a-11e8690c22e4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# OOF CV for memorization baseline with <self> fix and unicode normalization\n",
        "import pandas as pd, numpy as np, time, unicodedata\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "def normalize_before(x: str) -> str:\n",
        "    if not isinstance(x, str):\n",
        "        return x\n",
        "    x = unicodedata.normalize('NFKC', x)\n",
        "    x = x.replace('\\u00A0', ' ').replace('\\u2009', ' ')\n",
        "    return x\n",
        "\n",
        "t0 = time.time()\n",
        "print('Loading full train for OOF...')\n",
        "train = pd.read_csv('ru_train.csv.zip', compression='zip', usecols=['sentence_id','token_id','class','before','after'])\n",
        "print('Train shape:', train.shape)\n",
        "\n",
        "print('Normalizing before...')\n",
        "train['before_norm'] = train['before'].map(normalize_before)\n",
        "\n",
        "groups = train['sentence_id'].values\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "\n",
        "oof_pred = pd.Series(index=train.index, dtype=object)\n",
        "fold_acc = []\n",
        "t_start = time.time()\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    t_fold = time.time()\n",
        "    print(f'Fold {fold}: train {len(tr_idx):,}, valid {len(va_idx):,}')\n",
        "    tr = train.loc[tr_idx, ['before_norm','after']]\n",
        "    # Build mapping: for each before_norm, pick most frequent after\n",
        "    cnt = tr.value_counts(['before_norm','after']).reset_index(name='cnt')\n",
        "    # For speed, sort and drop_duplicates to keep top by cnt\n",
        "    cnt = cnt.sort_values(['before_norm','cnt'], ascending=[True, False])\n",
        "    top = cnt.drop_duplicates('before_norm').set_index('before_norm')['after']\n",
        "    va_before_norm = train.loc[va_idx, 'before_norm']\n",
        "    mapped = va_before_norm.map(top)\n",
        "    # <self> fix: when mapped is literal '<self>', replace by original before\n",
        "    before_orig = train.loc[va_idx, 'before']\n",
        "    mapped = np.where(mapped == '<self>', before_orig.values, mapped)\n",
        "    # Fallback: if unmapped (NaN), use original before\n",
        "    mapped = pd.Series(mapped, index=va_idx)\n",
        "    na_mask = mapped.isna()\n",
        "    if na_mask.any():\n",
        "        mapped.loc[na_mask] = before_orig.loc[na_mask]\n",
        "    oof_pred.loc[va_idx] = mapped\n",
        "    # Accuracy\n",
        "    y_true = train.loc[va_idx, 'after']\n",
        "    acc = (mapped.values == y_true.values).mean()\n",
        "    fold_acc.append(acc)\n",
        "    print(f'  fold {fold} acc: {acc:.6f} | elapsed {(time.time()-t_fold):.1f}s')\n",
        "    \n",
        "overall_acc = (oof_pred.values == train['after'].values).mean()\n",
        "print(f'OOF overall accuracy: {overall_acc:.6f}')\n",
        "print('Per-class OOF (top 15 classes):')\n",
        "cls_acc = train.assign(pred=oof_pred).groupby('class').apply(lambda df: (df['pred'].values == df['after'].values).mean()).sort_values(ascending=False)\n",
        "print(cls_acc.head(15))\n",
        "print('OOF done. Total elapsed:', f'{time.time()-t0:.1f}s')\n",
        "\n",
        "# Optional: rebuild full mapping on all train for improved final baseline, with normalization and <self> fix\n",
        "print('\\nRebuilding full mapping with normalization for final baseline (no save here) ...')\n",
        "cnt_all = train.value_counts(['before_norm','after']).reset_index(name='cnt')\n",
        "cnt_all = cnt_all.sort_values(['before_norm','cnt'], ascending=[True, False])\n",
        "full_map = cnt_all.drop_duplicates('before_norm').set_index('before_norm')['after']\n",
        "print('Full map size:', len(full_map))\n",
        "print('Example lookups:')\n",
        "for ex in ['1', '12', '1999', '2000', '\u0433.']:\n",
        "    key = normalize_before(ex)\n",
        "    val = full_map.get(key, None)\n",
        "    print(ex, '->', val)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading full train for OOF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (9515325, 5)\nNormalizing before...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 1 acc: 0.972497 | elapsed 11.7s\nFold 2: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 2 acc: 0.972404 | elapsed 11.6s\nFold 3: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 3 acc: 0.972433 | elapsed 11.2s\nFold 4: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 4 acc: 0.972655 | elapsed 11.0s\nFold 5: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 5 acc: 0.972408 | elapsed 11.3s\nOOF overall accuracy: 0.972479\nPer-class OOF (top 15 classes):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/31782437.py:55: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  cls_acc = train.assign(pred=oof_pred).groupby('class').apply(lambda df: (df['pred'].values == df['after'].values).mean()).sort_values(ascending=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class\nPUNCT         1.000000\nVERBATIM      0.992550\nPLAIN         0.984675\nLETTERS       0.939792\nDIGIT         0.900336\nDATE          0.773572\nCARDINAL      0.768191\nTIME          0.700396\nORDINAL       0.550440\nMEASURE       0.534407\nFRACTION      0.517162\nELECTRONIC    0.418627\nDECIMAL       0.360833\nTELEPHONE     0.197771\nMONEY         0.148087\ndtype: float64\nOOF done. Total elapsed: 78.2s\n\nRebuilding full mapping with normalization for final baseline (no save here) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full map size: 751508\nExample lookups:\n1 -> \u043e\u0434\u0438\u043d\n12 -> \u0434\u0432\u0435\u043d\u0430\u0434\u0446\u0430\u0442\u044c\n1999 -> \u0442\u044b\u0441\u044f\u0447\u0430 \u0434\u0435\u0432\u044f\u0442\u044c\u0441\u043e\u0442 \u0434\u0435\u0432\u044f\u043d\u043e\u0441\u0442\u043e \u0434\u0435\u0432\u044f\u0442\u044c\n2000 -> \u0434\u0432\u0435 \u0442\u044b\u0441\u044f\u0447\u0438\n\u0433. -> None\n"
          ]
        }
      ]
    },
    {
      "id": "9feef31b-6efc-47c0-a83c-bce2c027cc2b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Rebuild submission with unicode-normalized mapping and <self> fix\n",
        "import pandas as pd, time, unicodedata\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def normalize_before(x: str) -> str:\n",
        "    if not isinstance(x, str):\n",
        "        return x\n",
        "    x = unicodedata.normalize('NFKC', x)\n",
        "    x = x.replace('\\u00A0', ' ').replace('\\u2009', ' ')\n",
        "    return x\n",
        "\n",
        "train_path = 'ru_train.csv.zip'\n",
        "test_path = 'ru_test_2.csv.zip'\n",
        "\n",
        "t0 = time.time()\n",
        "print('Building normalized memorization map with <self> fix ...')\n",
        "counts = defaultdict(Counter)\n",
        "rows = 0\n",
        "chunk_idx = 0\n",
        "for chunk in pd.read_csv(train_path, compression='zip', usecols=['before','after'], chunksize=500_000):\n",
        "    chunk_idx += 1\n",
        "    bnorm = chunk['before'].map(normalize_before).values\n",
        "    after = chunk['after'].values\n",
        "    for b, a in zip(bnorm, after):\n",
        "        counts[b][a] += 1\n",
        "    rows += len(chunk)\n",
        "    if chunk_idx % 5 == 0:\n",
        "        print(f'  processed {rows:,} rows (chunks {chunk_idx}), unique normalized befores: {len(counts):,}')\n",
        "\n",
        "memorized_norm = {}\n",
        "for b, ctr in counts.items():\n",
        "    memorized_norm[b] = ctr.most_common(1)[0][0]\n",
        "print(f'Normalized map size: {len(memorized_norm):,}. Build time: {time.time()-t0:.1f}s')\n",
        "\n",
        "print('Loading test and generating predictions with <self> fix ...')\n",
        "test = pd.read_csv(test_path, compression='zip')\n",
        "test['id'] = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)\n",
        "bn = test['before'].map(normalize_before)\n",
        "pred = bn.map(memorized_norm)\n",
        "missing = pred.isna().sum()\n",
        "print(f'Mapped: {len(pred)-missing:,}, missing: {missing:,}')\n",
        "# <self> -> original token\n",
        "pred = pred.mask(pred == '<self>', test['before'])\n",
        "# Fallback to identity\n",
        "pred = pred.fillna(test['before'])\n",
        "\n",
        "submission = pd.DataFrame({'id': test['id'], 'after': pred})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with', len(submission), 'rows. Total elapsed:', f'{time.time()-t0:.1f}s')\n",
        "print(submission.head())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building normalized memorization map with <self> fix ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 2,500,000 rows (chunks 5), unique normalized befores: 324,280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 5,000,000 rows (chunks 10), unique normalized befores: 503,997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 7,500,000 rows (chunks 15), unique normalized befores: 648,998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 9,515,325 rows (chunks 20), unique normalized befores: 751,509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized map size: 751,509. Build time: 22.8s\nLoading test and generating predictions with <self> fix ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapped: 1,007,927, missing: 51,264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with 1059191 rows. Total elapsed: 26.4s\n    id        after\n0  0_0       \u0422\u0435\u043f\u0435\u0440\u044c\n1  0_1          \u0432\u0441\u0435\n2  0_2  \u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\n3  0_3        \u0437\u043e\u0432\u0443\u0442\n4  0_4      \u042f\u043c\u0430\u043c\u043e\u0442\u043e\n"
          ]
        }
      ]
    },
    {
      "id": "4d9177a2-cac7-48a8-a688-5c7aae68978d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Improved generator: regex router + CARDINAL/DECIMAL using num2words(ru); fallback to normalized memorization\n",
        "import re, time, unicodedata, subprocess, sys\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Install num2words if missing\n",
        "try:\n",
        "    import num2words\n",
        "except ImportError:\n",
        "    print('Installing num2words...')\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'num2words'], check=True)\n",
        "    import num2words\n",
        "\n",
        "from num2words import num2words as n2w\n",
        "\n",
        "def normalize_before(x: str) -> str:\n",
        "    if not isinstance(x, str):\n",
        "        return x\n",
        "    x = unicodedata.normalize('NFKC', x)\n",
        "    x = x.replace('\\u00A0', ' ').replace('\\u2009', ' ')\n",
        "    return x\n",
        "\n",
        "digit_words = ['\u043d\u043e\u043b\u044c','\u043e\u0434\u0438\u043d','\u0434\u0432\u0430','\u0442\u0440\u0438','\u0447\u0435\u0442\u044b\u0440\u0435','\u043f\u044f\u0442\u044c','\u0448\u0435\u0441\u0442\u044c','\u0441\u0435\u043c\u044c','\u0432\u043e\u0441\u0435\u043c\u044c','\u0434\u0435\u0432\u044f\u0442\u044c']\n",
        "\n",
        "int_re = re.compile(r'^-?\\d{1,3}([ \\u00A0]?\\d{3})*$|^-?\\d+$')\n",
        "dec_re = re.compile(r'^-?\\d+[\\.,]\\d+$')\n",
        "\n",
        "def to_int_cardinal_ru(s: str) -> str | None:\n",
        "    try:\n",
        "        neg = s.strip().startswith('-')\n",
        "        cleaned = s.replace(' ', '').replace('\\u00A0','').lstrip('+')\n",
        "        cleaned = cleaned[1:] if cleaned.startswith('-') else cleaned\n",
        "        num = int(cleaned)\n",
        "        spoken = n2w(num, lang='ru')\n",
        "        if neg:\n",
        "            spoken = '\u043c\u0438\u043d\u0443\u0441 ' + spoken\n",
        "        return spoken\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def to_decimal_ru(s: str) -> str | None:\n",
        "    try:\n",
        "        neg = s.strip().startswith('-')\n",
        "        t = s.strip().lstrip('+')\n",
        "        t = t[1:] if t.startswith('-') else t\n",
        "        if ',' in t:\n",
        "            ip, fp = t.split(',', 1)\n",
        "        else:\n",
        "            ip, fp = t.split('.', 1)\n",
        "        ip_spoken = to_int_cardinal_ru(ip) if ip else '\u043d\u043e\u043b\u044c'\n",
        "        if ip_spoken is None:\n",
        "            return None\n",
        "        fp_digits = [digit_words[int(ch)] if ch.isdigit() else ch for ch in fp]\n",
        "        frac_spoken = ' '.join(fp_digits)\n",
        "        out = f\"{ip_spoken} \u0437\u0430\u043f\u044f\u0442\u0430\u044f {frac_spoken}\"\n",
        "        if neg:\n",
        "            out = '\u043c\u0438\u043d\u0443\u0441 ' + out if not out.startswith('\u043c\u0438\u043d\u0443\u0441') else out\n",
        "        return out\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def build_normalized_memorizer(train_path: str, chunksize: int = 500_000):\n",
        "    counts = defaultdict(Counter)\n",
        "    rows = 0\n",
        "    chunk_idx = 0\n",
        "    t0 = time.time()\n",
        "    for chunk in pd.read_csv(train_path, compression='zip', usecols=['before','after'], chunksize=chunksize):\n",
        "        chunk_idx += 1\n",
        "        bnorm = chunk['before'].map(normalize_before).values\n",
        "        after = chunk['after'].values\n",
        "        for b, a in zip(bnorm, after):\n",
        "            counts[b][a] += 1\n",
        "        rows += len(chunk)\n",
        "        if chunk_idx % 5 == 0:\n",
        "            print(f'  memorizer processed {rows:,} rows (chunks {chunk_idx}), unique keys: {len(counts):,}')\n",
        "    mem = {b: ctr.most_common(1)[0][0] for b, ctr in counts.items()}\n",
        "    print(f'  memorizer size: {len(mem):,}. build {time.time()-t0:.1f}s')\n",
        "    return mem\n",
        "\n",
        "train_path = 'ru_train.csv.zip'\n",
        "test_path = 'ru_test_2.csv.zip'\n",
        "\n",
        "print('Building normalized memorizer (for fallback) ...')\n",
        "memorized_norm = build_normalized_memorizer(train_path)\n",
        "\n",
        "print('Loading test ...')\n",
        "test = pd.read_csv(test_path, compression='zip')\n",
        "test['id'] = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)\n",
        "bn = test['before'].map(normalize_before)\n",
        "\n",
        "pred = []\n",
        "t0 = time.time()\n",
        "for i, (b_raw, b) in enumerate(zip(test['before'].values, bn.values)):\n",
        "    out = None\n",
        "    if isinstance(b, str):\n",
        "        if int_re.match(b):\n",
        "            out = to_int_cardinal_ru(b)\n",
        "        elif dec_re.match(b):\n",
        "            out = to_decimal_ru(b)\n",
        "    # Fallback to memorized normalized map\n",
        "    if out is None:\n",
        "        out = memorized_norm.get(b, None)\n",
        "        if out == '<self>' or out is None:\n",
        "            out = b_raw\n",
        "    pred.append(out)\n",
        "    if (i+1) % 200000 == 0:\n",
        "        print(f'  processed {i+1:,}/{len(test)} tokens...')\n",
        "\n",
        "submission = pd.DataFrame({'id': test['id'], 'after': pred})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with', len(submission), 'rows. Elapsed:', f'{time.time()-t0:.1f}s')\n",
        "print(submission.head())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing num2words...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building normalized memorizer (for fallback) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer processed 2,500,000 rows (chunks 5), unique keys: 324,280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer processed 5,000,000 rows (chunks 10), unique keys: 503,997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer processed 7,500,000 rows (chunks 15), unique keys: 648,998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer processed 9,515,325 rows (chunks 20), unique keys: 751,509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer size: 751,509. build 20.4s\nLoading test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 200,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 400,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 600,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 800,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 1,000,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with 1059191 rows. Elapsed: 2.9s\n    id        after\n0  0_0       \u0422\u0435\u043f\u0435\u0440\u044c\n1  0_1          \u0432\u0441\u0435\n2  0_2  \u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\n3  0_3        \u0437\u043e\u0432\u0443\u0442\n4  0_4      \u042f\u043c\u0430\u043c\u043e\u0442\u043e\n"
          ]
        }
      ]
    },
    {
      "id": "0d1eaab7-9b2b-4329-bba8-0485d641f19e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# OOF CV for enhanced pipeline: integer/decimal generators + fold-built memorizer\n",
        "import pandas as pd, numpy as np, time, unicodedata\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Reuse helpers from previous cells: normalize_before, int_re, dec_re, to_int_cardinal_ru, to_decimal_ru\n",
        "\n",
        "def build_memorizer_from_df(df: pd.DataFrame) -> dict:\n",
        "    cnt = df.value_counts(['before_norm','after']).reset_index(name='cnt')\n",
        "    cnt = cnt.sort_values(['before_norm','cnt'], ascending=[True, False])\n",
        "    return cnt.drop_duplicates('before_norm').set_index('before_norm')['after'].to_dict()\n",
        "\n",
        "t0 = time.time()\n",
        "print('Loading full train...')\n",
        "train = pd.read_csv('ru_train.csv.zip', compression='zip', usecols=['sentence_id','token_id','class','before','after'])\n",
        "train['before_norm'] = train['before'].map(normalize_before)\n",
        "print('Train shape:', train.shape)\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train['sentence_id'].values\n",
        "oof_pred = pd.Series(index=train.index, dtype=object)\n",
        "fold_acc = []\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    t_fold = time.time()\n",
        "    print(f'Fold {fold}: train {len(tr_idx):,}, valid {len(va_idx):,}')\n",
        "    tr = train.loc[tr_idx, ['before_norm','after']]\n",
        "    mem = build_memorizer_from_df(tr)\n",
        "    va_bn = train.loc[va_idx, 'before_norm'].values\n",
        "    va_braw = train.loc[va_idx, 'before'].values\n",
        "\n",
        "    preds = []\n",
        "    for b_raw, b in zip(va_braw, va_bn):\n",
        "        out = None\n",
        "        if isinstance(b, str):\n",
        "            if int_re.match(b):\n",
        "                out = to_int_cardinal_ru(b)\n",
        "            elif dec_re.match(b):\n",
        "                out = to_decimal_ru(b)\n",
        "        if out is None:\n",
        "            out = mem.get(b, None)\n",
        "            if out == '<self>' or out is None:\n",
        "                out = b_raw\n",
        "        preds.append(out)\n",
        "    preds = pd.Series(preds, index=va_idx)\n",
        "    oof_pred.loc[va_idx] = preds\n",
        "    acc = (preds.values == train.loc[va_idx, 'after'].values).mean()\n",
        "    fold_acc.append(acc)\n",
        "    print(f'  fold {fold} acc: {acc:.6f} | elapsed {(time.time()-t_fold):.1f}s')\n",
        "\n",
        "overall = (oof_pred.values == train['after'].values).mean()\n",
        "print(f'Enhanced OOF overall accuracy: {overall:.6f}')\n",
        "print('Per-class OOF (top 15):')\n",
        "cls_acc = train.assign(pred=oof_pred).groupby('class').apply(lambda df: (df['pred'].values == df['after'].values).mean()).sort_values(ascending=False)\n",
        "print(cls_acc.head(15))\n",
        "print('Done. Total elapsed:', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading full train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (9515325, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 1 acc: 0.966581 | elapsed 14.5s\nFold 2: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 2 acc: 0.966732 | elapsed 14.3s\nFold 3: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 3 acc: 0.966559 | elapsed 14.1s\nFold 4: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 4 acc: 0.966855 | elapsed 14.2s\nFold 5: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 5 acc: 0.966559 | elapsed 14.2s\nEnhanced OOF overall accuracy: 0.966657\nPer-class OOF (top 15):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class\nPUNCT         1.000000\nVERBATIM      0.992550\nPLAIN         0.984675\nLETTERS       0.939792\nDATE          0.773572\nTIME          0.677219\nCARDINAL      0.555602\nORDINAL       0.543798\nMEASURE       0.534407\nFRACTION      0.517162\nELECTRONIC    0.418627\nTELEPHONE     0.195232\nDECIMAL       0.173200\nMONEY         0.148087\nDIGIT         0.000000\ndtype: float64\nDone. Total elapsed: 93.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/2083575202.py:54: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  cls_acc = train.assign(pred=oof_pred).groupby('class').apply(lambda df: (df['pred'].values == df['after'].values).mean()).sort_values(ascending=False)\n"
          ]
        }
      ]
    },
    {
      "id": "00983b7b-c334-4f15-acdd-86087cf9def2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# OOF CV: correct backoff order (memorizer first, then generators) for INT/DEC\n",
        "import pandas as pd, numpy as np, time\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "# Requires helpers from earlier cells: normalize_before, int_re, dec_re, to_int_cardinal_ru, to_decimal_ru\n",
        "\n",
        "def build_memorizer_from_df(df: pd.DataFrame) -> dict:\n",
        "    cnt = df.value_counts(['before_norm','after']).reset_index(name='cnt')\n",
        "    cnt = cnt.sort_values(['before_norm','cnt'], ascending=[True, False])\n",
        "    return cnt.drop_duplicates('before_norm').set_index('before_norm')['after'].to_dict()\n",
        "\n",
        "t0 = time.time()\n",
        "print('Loading full train...')\n",
        "train = pd.read_csv('ru_train.csv.zip', compression='zip', usecols=['sentence_id','token_id','class','before','after'])\n",
        "train['before_norm'] = train['before'].map(normalize_before)\n",
        "print('Train shape:', train.shape)\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train['sentence_id'].values\n",
        "oof_pred = pd.Series(index=train.index, dtype=object)\n",
        "fold_acc = []\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    t_fold = time.time()\n",
        "    print(f'Fold {fold}: train {len(tr_idx):,}, valid {len(va_idx):,}')\n",
        "    tr = train.loc[tr_idx, ['before_norm','after']]\n",
        "    mem = build_memorizer_from_df(tr)\n",
        "    va_bn = train.loc[va_idx, 'before_norm'].values\n",
        "    va_braw = train.loc[va_idx, 'before'].values\n",
        "\n",
        "    preds = []\n",
        "    for b_raw, b in zip(va_braw, va_bn):\n",
        "        # 1) Memorizer first\n",
        "        out = mem.get(b, None)\n",
        "        if out == '<self>':\n",
        "            out = None\n",
        "        # 2) Generators if no confident memorized mapping\n",
        "        if out is None and isinstance(b, str):\n",
        "            if int_re.match(b):\n",
        "                out = to_int_cardinal_ru(b)\n",
        "            elif dec_re.match(b):\n",
        "                out = to_decimal_ru(b)\n",
        "        # 3) Fallbacks\n",
        "        if out is None:\n",
        "            out = b_raw\n",
        "        preds.append(out)\n",
        "    preds = pd.Series(preds, index=va_idx)\n",
        "    oof_pred.loc[va_idx] = preds\n",
        "    acc = (preds.values == train.loc[va_idx, 'after'].values).mean()\n",
        "    fold_acc.append(acc)\n",
        "    print(f'  fold {fold} acc: {acc:.6f} | elapsed {(time.time()-t_fold):.1f}s')\n",
        "\n",
        "overall = (oof_pred.values == train['after'].values).mean()\n",
        "print(f'Correct-backoff OOF overall accuracy: {overall:.6f}')\n",
        "print('Per-class OOF (top 15):')\n",
        "cls_acc = train.assign(pred=oof_pred).groupby('class').apply(lambda df: (df['pred'].values == df['after'].values).mean()).sort_values(ascending=False)\n",
        "print(cls_acc.head(15))\n",
        "print('Done. Total elapsed:', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading full train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (9515325, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 1 acc: 0.973045 | elapsed 14.3s\nFold 2: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 2 acc: 0.972953 | elapsed 12.9s\nFold 3: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 3 acc: 0.972978 | elapsed 13.1s\nFold 4: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 4 acc: 0.973220 | elapsed 12.8s\nFold 5: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 5 acc: 0.972938 | elapsed 13.2s\nCorrect-backoff OOF overall accuracy: 0.973027\nPer-class OOF (top 15):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class\nPUNCT         1.000000\nVERBATIM      0.992550\nPLAIN         0.984675\nLETTERS       0.939792\nDIGIT         0.900336\nCARDINAL      0.789590\nDATE          0.773572\nTIME          0.700396\nORDINAL       0.550440\nMEASURE       0.534407\nFRACTION      0.517162\nELECTRONIC    0.418627\nDECIMAL       0.353692\nTELEPHONE     0.197771\nMONEY         0.148087\ndtype: float64\nDone. Total elapsed: 88.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/1149771031.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  cls_acc = train.assign(pred=oof_pred).groupby('class').apply(lambda df: (df['pred'].values == df['after'].values).mean()).sort_values(ascending=False)\n"
          ]
        }
      ]
    },
    {
      "id": "1dc0bfcf-2d29-4b63-90d4-1417a4a1ca15",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inference v2: memorizer-first + regex router for DATE/TIME/DECIMAL + CARDINAL\n",
        "import re, time, unicodedata, subprocess, sys\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Ensure num2words installed\n",
        "try:\n",
        "    from num2words import num2words as n2w\n",
        "except Exception:\n",
        "    print('Installing num2words...')\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'num2words'], check=True)\n",
        "    from num2words import num2words as n2w\n",
        "\n",
        "def normalize_before(x: str) -> str:\n",
        "    if not isinstance(x, str):\n",
        "        return x\n",
        "    x = unicodedata.normalize('NFKC', x)\n",
        "    x = x.replace('\\u00A0', ' ').replace('\\u2009', ' ')\n",
        "    return x\n",
        "\n",
        "digit_words = ['\u043d\u043e\u043b\u044c','\u043e\u0434\u0438\u043d','\u0434\u0432\u0430','\u0442\u0440\u0438','\u0447\u0435\u0442\u044b\u0440\u0435','\u043f\u044f\u0442\u044c','\u0448\u0435\u0441\u0442\u044c','\u0441\u0435\u043c\u044c','\u0432\u043e\u0441\u0435\u043c\u044c','\u0434\u0435\u0432\u044f\u0442\u044c']\n",
        "\n",
        "def plural_form(n, one, two_four, many):\n",
        "    try:\n",
        "        n = abs(int(n))\n",
        "    except Exception:\n",
        "        return many\n",
        "    if 11 <= n % 100 <= 14:\n",
        "        return many\n",
        "    if n % 10 == 1:\n",
        "        return one\n",
        "    if n % 10 in (2,3,4):\n",
        "        return two_four\n",
        "    return many\n",
        "\n",
        "# Regexes\n",
        "int_re = re.compile(r'^-?\\d{1,3}(?:[ \\u00A0]?\\d{3})*$|^-?\\d+$')\n",
        "dec_re = re.compile(r'^-?\\d+[\\.,]\\d+$')\n",
        "date_ddmmyyyy_re = re.compile(r'^(0[1-9]|[12]\\d|3[01])\\.(0[1-9]|1[0-2])\\.(\\d{4})$')\n",
        "date_d_month_yyyy_re = re.compile(r'^([1-9]|[12]\\d|3[01])\\s+(\u044f\u043d\u0432\u0430\u0440\u044f|\u0444\u0435\u0432\u0440\u0430\u043b\u044f|\u043c\u0430\u0440\u0442\u0430|\u0430\u043f\u0440\u0435\u043b\u044f|\u043c\u0430\u044f|\u0438\u044e\u043d\u044f|\u0438\u044e\u043b\u044f|\u0430\u0432\u0433\u0443\u0441\u0442\u0430|\u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044f|\u043e\u043a\u0442\u044f\u0431\u0440\u044f|\u043d\u043e\u044f\u0431\u0440\u044f|\u0434\u0435\u043a\u0430\u0431\u0440\u044f)\\s+(\\d{4})$', re.I)\n",
        "date_year_word_re = re.compile(r'^(\\d{4})\\s*(\u0433\\.|\u0433\u043e\u0434)$', re.I)\n",
        "time_hhmm_re = re.compile(r'^(?:[01]?\\d|2[0-3]):[0-5]\\d$')\n",
        "ip_or_version_re = re.compile(r'^\\d+(?:\\.\\d+){2,}$')\n",
        "has_letter_re = re.compile(r'[A-Za-z\u0410-\u042f\u0430-\u044f]')\n",
        "url_email_re = re.compile(r'^[a-z0-9._%+\\-]+@[a-z0-9.\\-]+\\.[a-z]{2,}$|^https?://', re.I)\n",
        "\n",
        "months_gen = ['\u044f\u043d\u0432\u0430\u0440\u044f','\u0444\u0435\u0432\u0440\u0430\u043b\u044f','\u043c\u0430\u0440\u0442\u0430','\u0430\u043f\u0440\u0435\u043b\u044f','\u043c\u0430\u044f','\u0438\u044e\u043d\u044f','\u0438\u044e\u043b\u044f','\u0430\u0432\u0433\u0443\u0441\u0442\u0430','\u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044f','\u043e\u043a\u0442\u044f\u0431\u0440\u044f','\u043d\u043e\u044f\u0431\u0440\u044f','\u0434\u0435\u043a\u0430\u0431\u0440\u044f']\n",
        "day_ord_gen = {\n",
        " 1:'\u043f\u0435\u0440\u0432\u043e\u0433\u043e',2:'\u0432\u0442\u043e\u0440\u043e\u0433\u043e',3:'\u0442\u0440\u0435\u0442\u044c\u0435\u0433\u043e',4:'\u0447\u0435\u0442\u0432\u0435\u0440\u0442\u043e\u0433\u043e',5:'\u043f\u044f\u0442\u043e\u0433\u043e',6:'\u0448\u0435\u0441\u0442\u043e\u0433\u043e',7:'\u0441\u0435\u0434\u044c\u043c\u043e\u0433\u043e',8:'\u0432\u043e\u0441\u044c\u043c\u043e\u0433\u043e',9:'\u0434\u0435\u0432\u044f\u0442\u043e\u0433\u043e',10:'\u0434\u0435\u0441\u044f\u0442\u043e\u0433\u043e',\n",
        " 11:'\u043e\u0434\u0438\u043d\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',12:'\u0434\u0432\u0435\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',13:'\u0442\u0440\u0438\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',14:'\u0447\u0435\u0442\u044b\u0440\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',15:'\u043f\u044f\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',16:'\u0448\u0435\u0441\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',17:'\u0441\u0435\u043c\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',18:'\u0432\u043e\u0441\u0435\u043c\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',19:'\u0434\u0435\u0432\u044f\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',\n",
        " 20:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',21:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u043f\u0435\u0440\u0432\u043e\u0433\u043e',22:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0432\u0442\u043e\u0440\u043e\u0433\u043e',23:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0442\u0440\u0435\u0442\u044c\u0435\u0433\u043e',24:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u043e\u0433\u043e',25:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u043f\u044f\u0442\u043e\u0433\u043e',26:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0448\u0435\u0441\u0442\u043e\u0433\u043e',27:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0441\u0435\u0434\u044c\u043c\u043e\u0433\u043e',28:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0432\u043e\u0441\u044c\u043c\u043e\u0433\u043e',29:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0434\u0435\u0432\u044f\u0442\u043e\u0433\u043e',\n",
        " 30:'\u0442\u0440\u0438\u0434\u0446\u0430\u0442\u043e\u0433\u043e',31:'\u0442\u0440\u0438\u0434\u0446\u0430\u0442\u044c \u043f\u0435\u0440\u0432\u043e\u0433\u043e'\n",
        "}\n",
        "\n",
        "def to_int_cardinal_ru(s: str) -> str | None:\n",
        "    try:\n",
        "        neg = s.strip().startswith('-')\n",
        "        cleaned = s.replace(' ', '').replace('\\u00A0','').lstrip('+')\n",
        "        cleaned = cleaned[1:] if cleaned.startswith('-') else cleaned\n",
        "        num = int(cleaned)\n",
        "        spoken = n2w(num, lang='ru')\n",
        "        if neg:\n",
        "            spoken = '\u043c\u0438\u043d\u0443\u0441 ' + spoken\n",
        "        return spoken\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def to_decimal_ru(s: str) -> str | None:\n",
        "    try:\n",
        "        if url_email_re.search(s) or ip_or_version_re.match(s) or has_letter_re.search(s):\n",
        "            return None\n",
        "        # Avoid interpreting dd.mm.yyyy as decimal\n",
        "        if date_ddmmyyyy_re.match(s):\n",
        "            return None\n",
        "        neg = s.strip().startswith('-')\n",
        "        t = s.strip().lstrip('+')\n",
        "        t = t[1:] if t.startswith('-') else t\n",
        "        if ',' in t:\n",
        "            ip, fp = t.split(',', 1)\n",
        "        else:\n",
        "            ip, fp = t.split('.', 1)\n",
        "        ip_spoken = to_int_cardinal_ru(ip) if ip else '\u043d\u043e\u043b\u044c'\n",
        "        if ip_spoken is None:\n",
        "            return None\n",
        "        fp_digits = [digit_words[int(ch)] if ch.isdigit() else ch for ch in fp if ch.isdigit()]\n",
        "        if not fp_digits:\n",
        "            return None\n",
        "        frac_spoken = ' '.join(fp_digits)\n",
        "        out = f\"{ip_spoken} \u0437\u0430\u043f\u044f\u0442\u0430\u044f {frac_spoken}\"\n",
        "        if neg:\n",
        "            out = '\u043c\u0438\u043d\u0443\u0441 ' + out if not out.startswith('\u043c\u0438\u043d\u0443\u0441') else out\n",
        "        return out\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def masc_nom_to_gen(word: str) -> str:\n",
        "    # crude but effective for year ordinals: replace last adjective ending\n",
        "    if word.endswith('\u044b\u0439') or word.endswith('\u0438\u0439') or word.endswith('\u043e\u0439'):\n",
        "        return word[:-2] + '\u043e\u0433\u043e'\n",
        "    return word\n",
        "\n",
        "def ordinal_phrase_nom_to_gen(phrase: str) -> str:\n",
        "    # convert last token's ending to genitive masculine\n",
        "    parts = phrase.split()\n",
        "    if not parts:\n",
        "        return phrase\n",
        "    parts[-1] = masc_nom_to_gen(parts[-1])\n",
        "    return ' '.join(parts)\n",
        "\n",
        "def year_to_ord_gen(y: int) -> str | None:\n",
        "    try:\n",
        "        if 1000 <= y <= 1999:\n",
        "            rest = y - 1000\n",
        "            if rest == 0:\n",
        "                base = '\u0442\u044b\u0441\u044f\u0447\u0430'\n",
        "                # '\u0442\u044b\u0441\u044f\u0447\u043d\u043e\u0433\u043e' uncommon; prefer memorizer if exists; return None\n",
        "                return None\n",
        "            rest_ord = n2w(rest, lang='ru', to='ordinal')\n",
        "            rest_ord_gen = ordinal_phrase_nom_to_gen(rest_ord)\n",
        "            return f'\u0442\u044b\u0441\u044f\u0447\u0430 {rest_ord_gen}'\n",
        "        if y == 2000:\n",
        "            return '\u0434\u0432\u0443\u0445\u0442\u044b\u0441\u044f\u0447\u043d\u043e\u0433\u043e'\n",
        "        if 2001 <= y <= 2099:\n",
        "            rest = y - 2000\n",
        "            rest_ord = n2w(rest, lang='ru', to='ordinal')\n",
        "            rest_ord_gen = ordinal_phrase_nom_to_gen(rest_ord)\n",
        "            return f'\u0434\u0432\u0435 \u0442\u044b\u0441\u044f\u0447\u0438 {rest_ord_gen}'\n",
        "        # outside safe zone: skip to memorizer\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def to_date_ru(s: str) -> str | None:\n",
        "    m = date_ddmmyyyy_re.match(s)\n",
        "    if m:\n",
        "        d = int(m.group(1))\n",
        "        mo = int(m.group(2))\n",
        "        y = int(m.group(3))\n",
        "        d_sp = day_ord_gen.get(d)\n",
        "        if d_sp is None:\n",
        "            return None\n",
        "        month_sp = months_gen[mo-1]\n",
        "        y_sp = year_to_ord_gen(y)\n",
        "        if y_sp is None:\n",
        "            return None\n",
        "        return f\"{d_sp} {month_sp} {y_sp} \u0433\u043e\u0434\u0430\"\n",
        "    m = date_d_month_yyyy_re.match(s)\n",
        "    if m:\n",
        "        d = int(m.group(1))\n",
        "        month_word = m.group(2).lower()\n",
        "        y = int(m.group(3))\n",
        "        d_sp = day_ord_gen.get(d)\n",
        "        if d_sp is None or month_word not in months_gen:\n",
        "            return None\n",
        "        y_sp = year_to_ord_gen(y)\n",
        "        if y_sp is None:\n",
        "            return None\n",
        "        return f\"{d_sp} {month_word} {y_sp} \u0433\u043e\u0434\u0430\"\n",
        "    m = date_year_word_re.match(s)\n",
        "    if m:\n",
        "        y = int(m.group(1))\n",
        "        y_sp = year_to_ord_gen(y)\n",
        "        if y_sp is None:\n",
        "            return None\n",
        "        # Prefer '\u0433\u043e\u0434\u0430' per train style\n",
        "        return f\"{y_sp} \u0433\u043e\u0434\u0430\"\n",
        "    return None\n",
        "\n",
        "def to_time_ru(s: str) -> str | None:\n",
        "    m = time_hhmm_re.match(s)\n",
        "    if not m:\n",
        "        return None\n",
        "    try:\n",
        "        hh, mm = s.split(':')\n",
        "        h = int(hh)\n",
        "        m_ = int(mm)\n",
        "        h_sp = n2w(h, lang='ru')\n",
        "        h_unit = plural_form(h, '\u0447\u0430\u0441', '\u0447\u0430\u0441\u0430', '\u0447\u0430\u0441\u043e\u0432')\n",
        "        if m_ == 0:\n",
        "            m_sp = '\u043d\u043e\u043b\u044c'\n",
        "            m_unit = '\u043c\u0438\u043d\u0443\u0442'\n",
        "        else:\n",
        "            m_sp = n2w(m_, lang='ru')\n",
        "            m_unit = plural_form(m_, '\u043c\u0438\u043d\u0443\u0442\u0430', '\u043c\u0438\u043d\u0443\u0442\u044b', '\u043c\u0438\u043d\u0443\u0442')\n",
        "        return f\"{h_sp} {h_unit} {m_sp} {m_unit}\"\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def build_normalized_memorizer(train_path: str, chunksize: int = 500_000):\n",
        "    counts = defaultdict(Counter)\n",
        "    rows = 0\n",
        "    chunk_idx = 0\n",
        "    t0 = time.time()\n",
        "    for chunk in pd.read_csv(train_path, compression='zip', usecols=['before','after'], chunksize=chunksize):\n",
        "        chunk_idx += 1\n",
        "        bnorm = chunk['before'].map(normalize_before).values\n",
        "        after = chunk['after'].values\n",
        "        for b, a in zip(bnorm, after):\n",
        "            counts[b][a] += 1\n",
        "        rows += len(chunk)\n",
        "        if chunk_idx % 5 == 0:\n",
        "            print(f'  memorizer processed {rows:,} rows (chunks {chunk_idx}), unique keys: {len(counts):,}')\n",
        "    mem = {b: ctr.most_common(1)[0][0] for b, ctr in counts.items()}\n",
        "    print(f'  memorizer size: {len(mem):,}. build {time.time()-t0:.1f}s')\n",
        "    return mem\n",
        "\n",
        "train_path = 'ru_train.csv.zip'\n",
        "test_path = 'ru_test_2.csv.zip'\n",
        "\n",
        "print('Building normalized memorizer (for first backoff) ...')\n",
        "memorized_norm = build_normalized_memorizer(train_path)\n",
        "\n",
        "print('Loading test ...')\n",
        "test = pd.read_csv(test_path, compression='zip')\n",
        "test['id'] = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)\n",
        "bn = test['before'].map(normalize_before)\n",
        "\n",
        "pred = []\n",
        "t0 = time.time()\n",
        "for i, (b_raw, b) in enumerate(zip(test['before'].values, bn.values)):\n",
        "    out = None\n",
        "    # 0) memorizer first\n",
        "    if isinstance(b, str):\n",
        "        out = memorized_norm.get(b, None)\n",
        "        if out == '<self>':\n",
        "            out = None\n",
        "    # 1) DATE\n",
        "    if out is None and isinstance(b, str):\n",
        "        out = to_date_ru(b)\n",
        "    # 2) TIME\n",
        "    if out is None and isinstance(b, str):\n",
        "        out = to_time_ru(b)\n",
        "    # 3) DECIMAL with guards\n",
        "    if out is None and isinstance(b, str):\n",
        "        if dec_re.match(b) and not ip_or_version_re.match(b) and not has_letter_re.search(b) and not url_email_re.search(b):\n",
        "            out = to_decimal_ru(b)\n",
        "    # 4) CARDINAL\n",
        "    if out is None and isinstance(b, str) and int_re.match(b) and not url_email_re.search(b):\n",
        "        out = to_int_cardinal_ru(b)\n",
        "    # 5) fallback identity if memorizer had nothing or returned None\n",
        "    if out is None:\n",
        "        # try memorizer again (if it wasn't <self>)\n",
        "        if isinstance(b, str):\n",
        "            tmp = memorized_norm.get(b, None)\n",
        "            if tmp is not None and tmp != '<self>':\n",
        "                out = tmp\n",
        "    if out is None:\n",
        "        out = b_raw\n",
        "    pred.append(out)\n",
        "    if (i+1) % 200000 == 0:\n",
        "        print(f'  processed {i+1:,}/{len(test)} tokens...')\n",
        "\n",
        "submission = pd.DataFrame({'id': test['id'], 'after': pred})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with', len(submission), 'rows. Elapsed:', f'{time.time()-t0:.1f}s')\n",
        "print(submission.head())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building normalized memorizer (for first backoff) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer processed 2,500,000 rows (chunks 5), unique keys: 324,280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer processed 5,000,000 rows (chunks 10), unique keys: 503,997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer processed 7,500,000 rows (chunks 15), unique keys: 648,998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer processed 9,515,325 rows (chunks 20), unique keys: 751,509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer size: 751,509. build 21.3s\nLoading test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 200,000/1059191 tokens...\n  processed 400,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 600,000/1059191 tokens...\n  processed 800,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 1,000,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with 1059191 rows. Elapsed: 2.2s\n    id        after\n0  0_0       \u0422\u0435\u043f\u0435\u0440\u044c\n1  0_1          \u0432\u0441\u0435\n2  0_2  \u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\n3  0_3        \u0437\u043e\u0432\u0443\u0442\n4  0_4      \u042f\u043c\u0430\u043c\u043e\u0442\u043e\n"
          ]
        }
      ]
    },
    {
      "id": "1303379c-1b1e-4e25-8fea-959a576da474",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# OOF CV for v2 router: memorizer-first + DATE/TIME/DECIMAL/CARDINAL\n",
        "import pandas as pd, numpy as np, time\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Reuse helpers from cell 8: normalize_before, regexes, and converters\n",
        "\n",
        "def build_memorizer_from_df(df: pd.DataFrame) -> dict:\n",
        "    cnt = df.value_counts(['before_norm','after']).reset_index(name='cnt')\n",
        "    cnt = cnt.sort_values(['before_norm','cnt'], ascending=[True, False])\n",
        "    return cnt.drop_duplicates('before_norm').set_index('before_norm')['after'].to_dict()\n",
        "\n",
        "t0 = time.time()\n",
        "print('Loading full train...')\n",
        "train = pd.read_csv('ru_train.csv.zip', compression='zip', usecols=['sentence_id','token_id','class','before','after'])\n",
        "train['before_norm'] = train['before'].map(normalize_before)\n",
        "print('Train shape:', train.shape)\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train['sentence_id'].values\n",
        "oof_pred = pd.Series(index=train.index, dtype=object)\n",
        "fold_acc = []\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    t_fold = time.time()\n",
        "    print(f'Fold {fold}: train {len(tr_idx):,}, valid {len(va_idx):,}')\n",
        "    tr = train.loc[tr_idx, ['before_norm','after']]\n",
        "    mem = build_memorizer_from_df(tr)\n",
        "    va_bn = train.loc[va_idx, 'before_norm'].values\n",
        "    va_braw = train.loc[va_idx, 'before'].values\n",
        "\n",
        "    preds = []\n",
        "    for b_raw, b in zip(va_braw, va_bn):\n",
        "        out = None\n",
        "        # 0) memorizer first\n",
        "        if isinstance(b, str):\n",
        "            out = mem.get(b, None)\n",
        "            if out == '<self>':\n",
        "                out = None\n",
        "        # 1) DATE\n",
        "        if out is None and isinstance(b, str):\n",
        "            out = to_date_ru(b)\n",
        "        # 2) TIME\n",
        "        if out is None and isinstance(b, str):\n",
        "            out = to_time_ru(b)\n",
        "        # 3) DECIMAL (with guards, same as inference)\n",
        "        if out is None and isinstance(b, str):\n",
        "            if dec_re.match(b) and not ip_or_version_re.match(b) and not has_letter_re.search(b) and not url_email_re.search(b):\n",
        "                out = to_decimal_ru(b)\n",
        "        # 4) CARDINAL\n",
        "        if out is None and isinstance(b, str) and int_re.match(b) and not url_email_re.search(b):\n",
        "            out = to_int_cardinal_ru(b)\n",
        "        # 5) fallback to memorizer if exists and not <self>, else original\n",
        "        if out is None and isinstance(b, str):\n",
        "            tmp = mem.get(b, None)\n",
        "            if tmp is not None and tmp != '<self>':\n",
        "                out = tmp\n",
        "        if out is None:\n",
        "            out = b_raw\n",
        "        preds.append(out)\n",
        "    preds = pd.Series(preds, index=va_idx)\n",
        "    oof_pred.loc[va_idx] = preds\n",
        "    acc = (preds.values == train.loc[va_idx, 'after'].values).mean()\n",
        "    fold_acc.append(acc)\n",
        "    print(f'  fold {fold} acc: {acc:.6f} | elapsed {(time.time()-t_fold):.1f}s')\n",
        "\n",
        "overall = (oof_pred.values == train['after'].values).mean()\n",
        "print(f'v2 router OOF overall accuracy: {overall:.6f}')\n",
        "print('Per-class OOF (top 20):')\n",
        "cls_acc = train.assign(pred=oof_pred).groupby('class').apply(lambda df: (df['pred'].values == df['after'].values).mean()).sort_values(ascending=False)\n",
        "print(cls_acc.head(20))\n",
        "print('Done. Total elapsed:', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading full train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (9515325, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 1 acc: 0.973343 | elapsed 14.1s\nFold 2: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 2 acc: 0.973258 | elapsed 13.4s\nFold 3: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 3 acc: 0.973276 | elapsed 13.1s\nFold 4: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 4 acc: 0.973546 | elapsed 13.1s\nFold 5: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 5 acc: 0.973244 | elapsed 13.2s\nv2 router OOF overall accuracy: 0.973333\nPer-class OOF (top 20):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class\nPUNCT         1.000000\nVERBATIM      0.992550\nPLAIN         0.984675\nLETTERS       0.939792\nDIGIT         0.900336\nTIME          0.872244\nCARDINAL      0.789590\nDATE          0.789204\nORDINAL       0.550440\nMEASURE       0.534407\nFRACTION      0.517162\nELECTRONIC    0.418627\nDECIMAL       0.353692\nTELEPHONE     0.197771\nMONEY         0.148087\ndtype: float64\nDone. Total elapsed: 89.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/77273722.py:70: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  cls_acc = train.assign(pred=oof_pred).groupby('class').apply(lambda df: (df['pred'].values == df['after'].values).mean()).sort_values(ascending=False)\n"
          ]
        }
      ]
    },
    {
      "id": "de913821-f99f-4a39-a676-c789c0d8633f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inference v3: add MONEY and MEASURE handlers; memorizer-first routing\n",
        "import re, time, unicodedata, subprocess, sys\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Reuse helpers from previous cells (installed already)\n",
        "from num2words import num2words as n2w\n",
        "\n",
        "def normalize_before(x: str) -> str:\n",
        "    if not isinstance(x, str):\n",
        "        return x\n",
        "    x = unicodedata.normalize('NFKC', x)\n",
        "    x = x.replace('\\u00A0', ' ').replace('\\u2009', ' ')\n",
        "    return x\n",
        "\n",
        "digit_words = ['\\u043d\\u043e\\u043b\\u044c','\\u043e\\u0434\\u0438\\u043d','\\u0434\\u0432\\u0430','\\u0442\\u0440\\u0438','\\u0447\\u0435\\u0442\\u044b\\u0440\\u0435','\\u043f\\u044f\\u0442\\u044c','\\u0448\\u0435\\u0441\\u0442\\u044c','\\u0441\\u0435\\u043c\\u044c','\\u0432\\u043e\\u0441\\u0435\\u043c\\u044c','\\u0434\\u0435\\u0432\\u044f\\u0442\\u044c']\n",
        "\n",
        "def plural_form(n, one, two_four, many):\n",
        "    try:\n",
        "        n = abs(int(float(n)))\n",
        "    except Exception:\n",
        "        return many\n",
        "    if 11 <= n % 100 <= 14:\n",
        "        return many\n",
        "    if n % 10 == 1:\n",
        "        return one\n",
        "    if n % 10 in (2,3,4):\n",
        "        return two_four\n",
        "    return many\n",
        "\n",
        "# Regexes (reuse from v2 where applicable)\n",
        "int_re = re.compile(r'^-?\\d{1,3}(?:[ \\u00A0]?\\d{3})*$|^-?\\d+$')\n",
        "dec_re = re.compile(r'^-?\\d+[\\.,]\\d+$')\n",
        "date_ddmmyyyy_re = re.compile(r'^(0[1-9]|[12]\\d|3[01])\\.(0[1-9]|1[0-2])\\.(\\d{4})$')\n",
        "date_d_month_yyyy_re = re.compile(r'^([1-9]|[12]\\d|3[01])\\s+(\\u044f\\u043d\\u0432\\u0430\\u0440\\u044f|\\u0444\\u0435\\u0432\\u0440\\u0430\\u043b\\u044f|\\u043c\\u0430\\u0440\\u0442\\u0430|\\u0430\\u043f\\u0440\\u0435\\u043b\\u044f|\\u043c\\u0430\\u044f|\\u0438\\u044e\\u043d\\u044f|\\u0438\\u044e\\u043b\\u044f|\\u0430\\u0432\\u0433\\u0443\\u0441\\u0442\\u0430|\\u0441\\u0435\\u043d\\u0442\\u044f\\u0431\\u0440\\u044f|\\u043e\\u043a\\u0442\\u044f\\u0431\\u0440\\u044f|\\u043d\\u043e\\u044f\\u0431\\u0440\\u044f|\\u0434\\u0435\\u043a\\u0430\\u0431\\u0440\\u044f)\\s+(\\d{4})$', re.I)\n",
        "date_year_word_re = re.compile(r'^(\\d{4})\\s*(\\u0433\\.|\\u0433\\u043e\\u0434)$', re.I)\n",
        "time_hhmm_re = re.compile(r'^(?:[01]?\\d|2[0-3]):[0-5]\\d$')\n",
        "ip_or_version_re = re.compile(r'^\\d+(?:\\.\\d+){2,}$')\n",
        "has_letter_re = re.compile(r'[A-Za-z\\u0410-\\u042f\\u0430-\\u044f]')\n",
        "url_email_re = re.compile(r'^[a-z0-9._%+\\-]+@[a-z0-9.\\-]+\\.[a-z]{2,}$|^https?://', re.I)\n",
        "# MONEY and MEASURE\n",
        "money_re = re.compile(r'^-?\\d+(?:[ \\u00A0]?\\d{3})*(?:[\\.,]\\d{1,2})?\\s*(\u20bd|\\u20BD|\u0440\u0443\u0431\\.?|\u0440\\.?|RUB|\\$|USD|\u20ac|EUR)$', re.I)\n",
        "measure_re = re.compile(r'^-?\\d+(?:[ \\u00A0]?\\d{3})*(?:[\\.,]\\d+)?\\s*(\u043a\u043c|\u043a\u0433|\u043c|%|\u00b0C|\u00b0\u0421)$', re.I)\n",
        "\n",
        "months_gen = ['\\u044f\\u043d\\u0432\\u0430\\u0440\\u044f','\\u0444\\u0435\\u0432\\u0440\\u0430\\u043b\\u044f','\\u043c\\u0430\\u0440\\u0442\\u0430','\\u0430\\u043f\\u0440\\u0435\\u043b\\u044f','\\u043c\\u0430\\u044f','\\u0438\\u044e\\u043d\\u044f','\\u0438\\u044e\\u043b\\u044f','\\u0430\\u0432\\u0433\\u0443\\u0441\\u0442\\u0430','\\u0441\\u0435\\u043d\\u0442\\u044f\\u0431\\u0440\\u044f','\\u043e\\u043a\\u0442\\u044f\\u0431\\u0440\\u044f','\\u043d\\u043e\\u044f\\u0431\\u0440\\u044f','\\u0434\\u0435\\u043a\\u0430\\u0431\\u0440\\u044f']\n",
        "day_ord_gen = {1:'\\u043f\\u0435\\u0440\\u0432\\u043e\\u0433\\u043e',2:'\\u0432\\u0442\\u043e\\u0440\\u043e\\u0433\\u043e',3:'\\u0442\\u0440\\u0435\\u0442\\u044c\\u0435\\u0433\\u043e',4:'\\u0447\\u0435\\u0442\\u0432\\u0435\\u0440\\u0442\\u043e\\u0433\\u043e',5:'\\u043f\\u044f\\u0442\\u043e\\u0433\\u043e',6:'\\u0448\\u0435\\u0441\\u0442\\u043e\\u0433\\u043e',7:'\\u0441\\u0435\\u0434\\u044c\\u043c\\u043e\\u0433\\u043e',8:'\\u0432\\u043e\\u0441\\u044c\\u043c\\u043e\\u0433\\u043e',9:'\\u0434\\u0435\\u0432\\u044f\\u0442\\u043e\\u0433\\u043e',10:'\\u0434\\u0435\\u0441\\u044f\\u0442\\u043e\\u0433\\u043e',11:'\\u043e\\u0434\\u0438\\u043d\\u043d\\u0430\\u0434\\u0446\\u0430\\u0442\\u043e\\u0433\\u043e',12:'\\u0434\\u0432\\u0435\\u043d\\u0430\\u0434\\u0446\\u0430\\u0442\\u043e\\u0433\\u043e',13:'\\u0442\\u0440\\u0438\\u043d\\u0430\\u0434\\u0446\\u0430\\u0442\\u043e\\u0433\\u043e',14:'\\u0447\\u0435\\u0442\\u044b\\u0440\\u043d\\u0430\\u0434\\u0446\\u0430\\u0442\\u043e\\u0433\\u043e',15:'\\u043f\\u044f\\u0442\\u043d\\u0430\\u0434\\u0446\\u0430\\u0442\\u043e\\u0433\\u043e',16:'\\u0448\\u0435\\u0441\\u0442\\u043d\\u0430\\u0434\\u0446\\u0430\\u0442\\u043e\\u0433\\u043e',17:'\\u0441\\u0435\\u043c\\u043d\\u0430\\u0434\\u0446\\u0430\\u0442\\u043e\\u0433\\u043e',18:'\\u0432\\u043e\\u0441\\u0435\\u043c\\u043d\\u0430\\u0434\\u0446\\u0430\\u0442\\u043e\\u0433\\u043e',19:'\\u0434\\u0435\\u0432\\u044f\\u0442\\u043d\\u0430\\u0434\\u0446\\u0430\\u0442\\u043e\\u0433\\u043e',20:'\\u0434\\u0432\\u0430\\u0434\\u0446\\u0430\\u0442\\u043e\\u0433\\u043e',21:'\\u0434\\u0432\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c \\u043f\\u0435\\u0440\\u0432\\u043e\\u0433\\u043e',22:'\\u0434\\u0432\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c \\u0432\\u0442\\u043e\\u0440\\u043e\\u0433\\u043e',23:'\\u0434\\u0432\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c \\u0442\\u0440\\u0435\\u0442\\u044c\\u0435\\u0433\\u043e',24:'\\u0434\\u0432\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c \\u0447\\u0435\\u0442\\u0432\\u0435\\u0440\\u0442\\u043e\\u0433\\u043e',25:'\\u0434\\u0432\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c \\u043f\\u044f\\u0442\\u043e\\u0433\\u043e',26:'\\u0434\\u0432\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c \\u0448\\u0435\\u0441\\u0442\\u043e\\u0433\\u043e',27:'\\u0434\\u0432\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c \\u0441\\u0435\\u0434\\u044c\\u043c\\u043e\\u0433\\u043e',28:'\\u0434\\u0432\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c \\u0432\\u043e\\u0441\\u044c\\u043c\\u043e\\u0433\\u043e',29:'\\u0434\\u0432\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c \\u0434\\u0435\\u0432\\u044f\\u0442\\u043e\\u0433\\u043e',30:'\\u0442\\u0440\\u0438\\u0434\\u0446\\u0430\\u0442\\u043e\\u0433\\u043e',31:'\\u0442\\u0440\\u0438\\u0434\\u0446\\u0430\\u0442\\u044c \\u043f\\u0435\\u0440\\u0432\\u043e\\u0433\\u043e'}\n",
        "\n",
        "def to_int_cardinal_ru(s: str) -> str | None:\n",
        "    try:\n",
        "        neg = s.strip().startswith('-')\n",
        "        cleaned = s.replace(' ', '').replace('\\u00A0','').lstrip('+')\n",
        "        cleaned = cleaned[1:] if cleaned.startswith('-') else cleaned\n",
        "        num = int(cleaned)\n",
        "        spoken = n2w(num, lang='ru')\n",
        "        if neg:\n",
        "            spoken = '\\u043c\\u0438\\u043d\\u0443\\u0441 ' + spoken\n",
        "        return spoken\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def to_decimal_ru(s: str) -> str | None:\n",
        "    try:\n",
        "        if url_email_re.search(s) or ip_or_version_re.match(s) or has_letter_re.search(s):\n",
        "            return None\n",
        "        if date_ddmmyyyy_re.match(s):\n",
        "            return None\n",
        "        neg = s.strip().startswith('-')\n",
        "        t = s.strip().lstrip('+')\n",
        "        t = t[1:] if t.startswith('-') else t\n",
        "        if ',' in t:\n",
        "            ip, fp = t.split(',', 1)\n",
        "        else:\n",
        "            ip, fp = t.split('.', 1)\n",
        "        ip_spoken = to_int_cardinal_ru(ip) if ip else '\\u043d\\u043e\\u043b\\u044c'\n",
        "        if ip_spoken is None:\n",
        "            return None\n",
        "        fp_digits = [digit_words[int(ch)] for ch in fp if ch.isdigit()]\n",
        "        if not fp_digits:\n",
        "            return None\n",
        "        frac_spoken = ' '.join(fp_digits)\n",
        "        out = f\"{ip_spoken} \\u0437\\u0430\\u043f\\u044f\\u0442\\u0430\\u044f {frac_spoken}\"\n",
        "        if neg:\n",
        "            out = '\\u043c\\u0438\\u043d\\u0443\\u0441 ' + out if not out.startswith('\\u043c\\u0438\\u043d\\u0443\\u0441') else out\n",
        "        return out\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def masc_nom_to_gen(word: str) -> str:\n",
        "    if word.endswith('\\u044b\\u0439') or word.endswith('\\u0438\\u0439') or word.endswith('\\u043e\\u0439'):\n",
        "        return word[:-2] + '\\u043e\\u0433\\u043e'\n",
        "    return word\n",
        "\n",
        "def ordinal_phrase_nom_to_gen(phrase: str) -> str:\n",
        "    parts = phrase.split()\n",
        "    if not parts:\n",
        "        return phrase\n",
        "    parts[-1] = masc_nom_to_gen(parts[-1])\n",
        "    return ' '.join(parts)\n",
        "\n",
        "def year_to_ord_gen(y: int) -> str | None:\n",
        "    try:\n",
        "        if 1000 <= y <= 1999:\n",
        "            rest = y - 1000\n",
        "            if rest == 0:\n",
        "                return None\n",
        "            rest_ord = n2w(rest, lang='ru', to='ordinal')\n",
        "            rest_ord_gen = ordinal_phrase_nom_to_gen(rest_ord)\n",
        "            return f'\\u0442\\u044b\\u0441\\u044f\\u0447\\u0430 {rest_ord_gen}'\n",
        "        if y == 2000:\n",
        "            return '\\u0434\\u0432\\u0443\\u0445\\u0442\\u044b\\u0441\\u044f\\u0447\\u043d\\u043e\\u0433\\u043e'\n",
        "        if 2001 <= y <= 2099:\n",
        "            rest = y - 2000\n",
        "            rest_ord = n2w(rest, lang='ru', to='ordinal')\n",
        "            rest_ord_gen = ordinal_phrase_nom_to_gen(rest_ord)\n",
        "            return f'\\u0434\\u0432\\u0435 \\u0442\\u044b\\u0441\\u044f\\u0447\\u0438 {rest_ord_gen}'\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def to_date_ru(s: str) -> str | None:\n",
        "    m = date_ddmmyyyy_re.match(s)\n",
        "    if m:\n",
        "        d = int(m.group(1)); mo = int(m.group(2)); y = int(m.group(3))\n",
        "        d_sp = day_ord_gen.get(d); month_sp = months_gen[mo-1]; y_sp = year_to_ord_gen(y)\n",
        "        if d_sp and y_sp:\n",
        "            return f\"{d_sp} {month_sp} {y_sp} \\u0433\\u043e\\u0434\\u0430\"\n",
        "        return None\n",
        "    m = date_d_month_yyyy_re.match(s)\n",
        "    if m:\n",
        "        d = int(m.group(1)); month_word = m.group(2).lower(); y = int(m.group(3))\n",
        "        d_sp = day_ord_gen.get(d); y_sp = year_to_ord_gen(y)\n",
        "        if d_sp and y_sp and month_word in months_gen:\n",
        "            return f\"{d_sp} {month_word} {y_sp} \\u0433\\u043e\\u0434\\u0430\"\n",
        "        return None\n",
        "    m = date_year_word_re.match(s)\n",
        "    if m:\n",
        "        y = int(m.group(1)); suf = m.group(2).lower(); y_sp = year_to_ord_gen(y)\n",
        "        if y_sp:\n",
        "            # If original contains explicit '\u0433\u043e\u0434', use nominative '\u0433\u043e\u0434'; for '\u0433.' prefer '\u0433\u043e\u0434' as well to match train example style\n",
        "            if '\u0433\u043e\u0434' in suf:\n",
        "                return f\"{y_sp} \\u0433\\u043e\\u0434\"\n",
        "            else:\n",
        "                return f\"{y_sp} \\u0433\\u043e\\u0434\"\n",
        "    return None\n",
        "\n",
        "def to_time_ru(s: str) -> str | None:\n",
        "    m = time_hhmm_re.match(s)\n",
        "    if not m:\n",
        "        return None\n",
        "    try:\n",
        "        hh, mm = s.split(':')\n",
        "        h = int(hh); m_ = int(mm)\n",
        "        h_sp = n2w(h, lang='ru')\n",
        "        h_unit = plural_form(h, '\\u0447\\u0430\\u0441', '\\u0447\\u0430\\u0441\\u0430', '\\u0447\\u0430\\u0441\\u043e\\u0432')\n",
        "        if m_ == 0:\n",
        "            m_sp = '\\u043d\\u043e\\u043b\\u044c'; m_unit = '\\u043c\\u0438\\u043d\\u0443\\u0442'\n",
        "        else:\n",
        "            m_sp = n2w(m_, lang='ru'); m_unit = plural_form(m_, '\\u043c\\u0438\\u043d\\u0443\\u0442\\u0430', '\\u043c\\u0438\\u043d\\u0443\\u0442\\u044b', '\\u043c\\u0438\\u043d\\u0443\\u0442')\n",
        "        return f\"{h_sp} {h_unit} {m_sp} {m_unit}\"\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def to_money_ru(s: str) -> str | None:\n",
        "    m = money_re.match(s)\n",
        "    if not m:\n",
        "        return None\n",
        "    try:\n",
        "        cur = m.group(1).upper() if m.group(1) else ''\n",
        "        # extract numeric part (everything before currency)\n",
        "        num_part = s[:m.start(1)].strip() if m.lastindex else s.strip()\n",
        "        t = num_part.replace('\\u00A0',' ').replace(' ', '').replace('+','')\n",
        "        neg = t.startswith('-'); t = t[1:] if neg else t\n",
        "        if ',' in t: ip, fp = t.split(',', 1)\n",
        "        elif '.' in t: ip, fp = t.split('.', 1)\n",
        "        else: ip, fp = t, ''\n",
        "        r = int(ip) if ip else 0\n",
        "        r_sp = n2w(r, lang='ru') if r != 0 else '\\u043d\\u043e\\u043b\\u044c'\n",
        "        if cur in {'\u20bd','\\u20BD','\u0420\u0423\u0411.','\u0420\u0423\u0411','\u0420.','RUB'}:\n",
        "            rub_unit = plural_form(r, '\\u0440\\u0443\\u0431\\u043b\\u044c','\\u0440\\u0443\\u0431\\u043b\\u044f','\\u0440\\u0443\\u0431\\u043b\\u0435\\u0439')\n",
        "            out = f\"{r_sp} {rub_unit}\"\n",
        "            k = int(fp[:2]) if fp else 0\n",
        "            if fp and len(fp) == 1:\n",
        "                k *= 10\n",
        "            if k > 0:\n",
        "                k_sp = n2w(k, lang='ru')\n",
        "                kop_unit = plural_form(k, '\\u043a\\u043e\\u043f\\u0435\\u0439\\u043a\\u0430','\\u043a\\u043e\\u043f\\u0435\\u0439\\u043a\\u0438','\\u043a\\u043e\\u043f\\u0435\\u0435\\u043a')\n",
        "                out = f\"{out} {k_sp} {kop_unit}\"\n",
        "            if neg:\n",
        "                out = '\\u043c\\u0438\\u043d\\u0443\\u0441 ' + out\n",
        "            return out\n",
        "        if cur in {'$','USD'}:\n",
        "            unit = ('\\u0434\\u043e\\u043b\\u043b\\u0430\\u0440','\\u0434\\u043e\\u043b\\u043b\\u0430\\u0440\\u0430','\\u0434\\u043e\\u043b\\u043b\\u0430\\u0440\\u043e\\u0432')\n",
        "            out = f\"{r_sp} {plural_form(r,*unit)}\"\n",
        "            c = int(fp[:2]) if fp else 0\n",
        "            if fp and len(fp) == 1:\n",
        "                c *= 10\n",
        "            if c > 0:\n",
        "                c_sp = n2w(c, lang='ru')\n",
        "                cent_unit = plural_form(c, '\\u0446\\u0435\\u043d\\u0442','\\u0446\\u0435\\u043d\\u0442\\u0430','\\u0446\\u0435\\u043d\\u0442\\u043e\\u0432')\n",
        "                out = f\"{out} {c_sp} {cent_unit}\"\n",
        "            if neg: out = '\\u043c\\u0438\\u043d\\u0443\\u0441 ' + out\n",
        "            return out\n",
        "        if cur in {'\u20ac','EUR'}:\n",
        "            # \u0435\u0432\u0440\u043e invariable; cents as above\n",
        "            out = f\"{r_sp} \\u0435\\u0432\\u0440\\u043e\"\n",
        "            c = int(fp[:2]) if fp else 0\n",
        "            if fp and len(fp) == 1: c *= 10\n",
        "            if c > 0:\n",
        "                c_sp = n2w(c, lang='ru')\n",
        "                cent_unit = plural_form(c, '\\u0446\\u0435\\u043d\\u0442','\\u0446\\u0435\\u043d\\u0442\\u0430','\\u0446\\u0435\\u043d\\u0442\\u043e\\u0432')\n",
        "                out = f\"{out} {c_sp} {cent_unit}\"\n",
        "            if neg: out = '\\u043c\\u0438\\u043d\\u0443\\u0441 ' + out\n",
        "            return out\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "unit_forms = {\n",
        "    '\\u043a\\u043c': ('\\u043a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440','\\u043a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440\\u0430','\\u043a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440\\u043e\\u0432'),\n",
        "    '\\u043a\\u0433': ('\\u043a\\u0438\\u043b\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c','\\u043a\\u0438\\u043b\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0430','\\u043a\\u0438\\u043b\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u043e\\u0432'),\n",
        "    '\\u043c': ('\\u043c\\u0435\\u0442\\u0440','\\u043c\\u0435\\u0442\\u0440\\u0430','\\u043c\\u0435\\u0442\\u0440\\u043e\\u0432'),\n",
        "    '%': ('\\u043f\\u0440\\u043e\\u0446\\u0435\\u043d\\u0442','\\u043f\\u0440\\u043e\\u0446\\u0435\\u043d\\u0442\\u0430','\\u043f\\u0440\\u043e\\u0446\\u0435\\u043d\\u0442\\u043e\\u0432'),\n",
        "    '\\u00b0C': ('\\u0433\\u0440\\u0430\\u0434\\u0443\\u0441 \\u0426\\u0435\\u043b\\u044c\\u0441\\u0438\\u044f','\\u0433\\u0440\\u0430\\u0434\\u0443\\u0441\\u0430 \\u0426\\u0435\\u043b\\u044c\\u0441\\u0438\\u044f','\\u0433\\u0440\\u0430\\u0434\\u0443\\u0441\\u043e\\u0432 \\u0426\\u0435\\u043b\\u044c\\u0441\\u0438\\u044f'),\n",
        "    '\\u00b0\\u0421': ('\\u0433\\u0440\\u0430\\u0434\\u0443\\u0441 \\u0426\\u0435\\u043b\\u044c\\u0441\\u0438\\u044f','\\u0433\\u0440\\u0430\\u0434\\u0443\\u0441\\u0430 \\u0426\\u0435\\u043b\\u044c\\u0441\\u0438\\u044f','\\u0433\\u0440\\u0430\\u0434\\u0443\\u0441\\u043e\\u0432 \\u0426\\u0435\\u043b\\u044c\\u0441\\u0438\\u044f')\n",
        "}\n",
        "\n",
        "def to_measure_ru(s: str) -> str | None:\n",
        "    m = measure_re.match(s)\n",
        "    if not m:\n",
        "        return None\n",
        "    try:\n",
        "        unit_raw = m.group(1)\n",
        "        # extract numeric part\n",
        "        num_part = s[:m.start(1)].strip()\n",
        "        t = num_part.replace('\\u00A0',' ').replace(' ', '').replace('+','')\n",
        "        neg = t.startswith('-'); t = t[1:] if neg else t\n",
        "        is_decimal = (',' in t) or ('.' in t)\n",
        "        if ',' in t: ip, fp = t.split(',', 1)\n",
        "        elif '.' in t: ip, fp = t.split('.', 1)\n",
        "        else: ip, fp = t, ''\n",
        "        val_int = int(ip) if ip else 0\n",
        "        num_sp = n2w(val_int, lang='ru') if not is_decimal else n2w(int(ip or '0'), lang='ru')\n",
        "        unit_key = unit_raw\n",
        "        if unit_key not in unit_forms:\n",
        "            return None\n",
        "        forms = unit_forms[unit_key]\n",
        "        # plural by integer part only\n",
        "        unit_word = plural_form(val_int, forms[0], forms[1], forms[2])\n",
        "        out = f\"{num_sp} {unit_word}\"\n",
        "        if neg:\n",
        "            out = '\\u043c\\u0438\\u043d\\u0443\\u0441 ' + out\n",
        "        return out\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def build_normalized_memorizer(train_path: str, chunksize: int = 500_000):\n",
        "    counts = defaultdict(Counter)\n",
        "    rows = 0; chunk_idx = 0; t0 = time.time()\n",
        "    for chunk in pd.read_csv(train_path, compression='zip', usecols=['before','after'], chunksize=chunksize):\n",
        "        chunk_idx += 1\n",
        "        bnorm = chunk['before'].map(normalize_before).values\n",
        "        after = chunk['after'].values\n",
        "        for b, a in zip(bnorm, after):\n",
        "            counts[b][a] += 1\n",
        "        rows += len(chunk)\n",
        "        if chunk_idx % 5 == 0:\n",
        "            print(f'  memorizer processed {rows:,} rows (chunks {chunk_idx}), unique keys: {len(counts):,}')\n",
        "    mem = {b: ctr.most_common(1)[0][0] for b, ctr in counts.items()}\n",
        "    print(f'  memorizer size: {len(mem):,}. build {time.time()-t0:.1f}s')\n",
        "    return mem\n",
        "\n",
        "train_path = 'ru_train.csv.zip'\n",
        "test_path = 'ru_test_2.csv.zip'\n",
        "\n",
        "print('Building normalized memorizer (for first backoff) ...')\n",
        "memorized_norm = build_normalized_memorizer(train_path)\n",
        "\n",
        "print('Loading test ...')\n",
        "test = pd.read_csv(test_path, compression='zip')\n",
        "test['id'] = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)\n",
        "bn = test['before'].map(normalize_before)\n",
        "\n",
        "pred = []\n",
        "t0 = time.time()\n",
        "for i, (b_raw, b) in enumerate(zip(test['before'].values, bn.values)):\n",
        "    out = None\n",
        "    # 0) memorizer first\n",
        "    if isinstance(b, str):\n",
        "        out = memorized_norm.get(b, None)\n",
        "        if out == '<self>':\n",
        "            out = None\n",
        "    # 1) DATE\n",
        "    if out is None and isinstance(b, str):\n",
        "        out = to_date_ru(b)\n",
        "    # 2) TIME\n",
        "    if out is None and isinstance(b, str):\n",
        "        out = to_time_ru(b)\n",
        "    # 3) MONEY\n",
        "    if out is None and isinstance(b, str):\n",
        "        out = to_money_ru(b)\n",
        "    # 4) DECIMAL (with guards)\n",
        "    if out is None and isinstance(b, str):\n",
        "        if dec_re.match(b) and not ip_or_version_re.match(b) and not has_letter_re.search(b) and not url_email_re.search(b):\n",
        "            out = to_decimal_ru(b)\n",
        "    # 5) CARDINAL\n",
        "    if out is None and isinstance(b, str) and int_re.match(b) and not url_email_re.search(b):\n",
        "        out = to_int_cardinal_ru(b)\n",
        "    # 6) MEASURE\n",
        "    if out is None and isinstance(b, str):\n",
        "        out = to_measure_ru(b)\n",
        "    # 7) fallback to memorizer (if exists and not <self>), else identity\n",
        "    if out is None and isinstance(b, str):\n",
        "        tmp = memorized_norm.get(b, None)\n",
        "        if tmp is not None and tmp != '<self>':\n",
        "            out = tmp\n",
        "    if out is None:\n",
        "        out = b_raw\n",
        "    pred.append(out)\n",
        "    if (i+1) % 200000 == 0:\n",
        "        print(f'  processed {i+1:,}/{len(test)} tokens...')\n",
        "\n",
        "submission = pd.DataFrame({'id': test['id'], 'after': pred})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with', len(submission), 'rows. Elapsed:', f'{time.time()-t0:.1f}s')\n",
        "print(submission.head())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building normalized memorizer (for first backoff) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer processed 2,500,000 rows (chunks 5), unique keys: 324,280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer processed 5,000,000 rows (chunks 10), unique keys: 503,997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer processed 7,500,000 rows (chunks 15), unique keys: 648,998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer processed 9,515,325 rows (chunks 20), unique keys: 751,509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer size: 751,509. build 21.9s\nLoading test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 200,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 400,000/1059191 tokens...\n  processed 600,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 800,000/1059191 tokens...\n  processed 1,000,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with 1059191 rows. Elapsed: 2.4s\n    id        after\n0  0_0       \u0422\u0435\u043f\u0435\u0440\u044c\n1  0_1          \u0432\u0441\u0435\n2  0_2  \u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\n3  0_3        \u0437\u043e\u0432\u0443\u0442\n4  0_4      \u042f\u043c\u0430\u043c\u043e\u0442\u043e\n"
          ]
        }
      ]
    },
    {
      "id": "a1685812-7959-4f9d-a11e-eacea2445434",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# OOF CV for v3 router: memorizer-first + DATE/TIME/MONEY/DECIMAL/CARDINAL/MEASURE\n",
        "import pandas as pd, numpy as np, time\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "# Reuse helpers from cells 8/10: normalize_before, regexes, and converters: to_date_ru, to_time_ru, to_money_ru, to_decimal_ru, to_int_cardinal_ru, to_measure_ru\n",
        "\n",
        "def build_memorizer_from_df(df: pd.DataFrame) -> dict:\n",
        "    cnt = df.value_counts(['before_norm','after']).reset_index(name='cnt')\n",
        "    cnt = cnt.sort_values(['before_norm','cnt'], ascending=[True, False])\n",
        "    return cnt.drop_duplicates('before_norm').set_index('before_norm')['after'].to_dict()\n",
        "\n",
        "t0 = time.time()\n",
        "print('Loading full train...')\n",
        "train = pd.read_csv('ru_train.csv.zip', compression='zip', usecols=['sentence_id','token_id','class','before','after'])\n",
        "train['before_norm'] = train['before'].map(normalize_before)\n",
        "print('Train shape:', train.shape)\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train['sentence_id'].values\n",
        "oof_pred = pd.Series(index=train.index, dtype=object)\n",
        "fold_acc = []\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    t_fold = time.time()\n",
        "    print(f'Fold {fold}: train {len(tr_idx):,}, valid {len(va_idx):,}')\n",
        "    tr = train.loc[tr_idx, ['before_norm','after']]\n",
        "    mem = build_memorizer_from_df(tr)\n",
        "    va_bn = train.loc[va_idx, 'before_norm'].values\n",
        "    va_braw = train.loc[va_idx, 'before'].values\n",
        "\n",
        "    preds = []\n",
        "    for b_raw, b in zip(va_braw, va_bn):\n",
        "        out = None\n",
        "        # 0) memorizer first\n",
        "        if isinstance(b, str):\n",
        "            out = mem.get(b, None)\n",
        "            if out == '<self>':\n",
        "                out = None\n",
        "        # 1) DATE\n",
        "        if out is None and isinstance(b, str):\n",
        "            out = to_date_ru(b)\n",
        "        # 2) TIME\n",
        "        if out is None and isinstance(b, str):\n",
        "            out = to_time_ru(b)\n",
        "        # 3) MONEY\n",
        "        if out is None and isinstance(b, str):\n",
        "            out = to_money_ru(b)\n",
        "        # 4) DECIMAL with guards\n",
        "        if out is None and isinstance(b, str):\n",
        "            if dec_re.match(b) and not ip_or_version_re.match(b) and not has_letter_re.search(b) and not url_email_re.search(b):\n",
        "                out = to_decimal_ru(b)\n",
        "        # 5) CARDINAL\n",
        "        if out is None and isinstance(b, str) and int_re.match(b) and not url_email_re.search(b):\n",
        "            out = to_int_cardinal_ru(b)\n",
        "        # 6) MEASURE\n",
        "        if out is None and isinstance(b, str):\n",
        "            out = to_measure_ru(b)\n",
        "        # 7) fallback to memorizer (if exists and not <self>), else original\n",
        "        if out is None and isinstance(b, str):\n",
        "            tmp = mem.get(b, None)\n",
        "            if tmp is not None and tmp != '<self>':\n",
        "                out = tmp\n",
        "        if out is None:\n",
        "            out = b_raw\n",
        "        preds.append(out)\n",
        "    preds = pd.Series(preds, index=va_idx)\n",
        "    oof_pred.loc[va_idx] = preds\n",
        "    acc = (preds.values == train.loc[va_idx, 'after'].values).mean()\n",
        "    fold_acc.append(acc)\n",
        "    print(f'  fold {fold} acc: {acc:.6f} | elapsed {(time.time()-t_fold):.1f}s')\n",
        "\n",
        "overall = (oof_pred.values == train['after'].values).mean()\n",
        "print(f'v3 router OOF overall accuracy: {overall:.6f}')\n",
        "print('Per-class OOF (top 20):')\n",
        "cls_acc = train.assign(pred=oof_pred).groupby('class').apply(lambda df: (df['pred'].values == df['after'].values).mean()).sort_values(ascending=False)\n",
        "print(cls_acc.head(20))\n",
        "print('Done. Total elapsed:', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading full train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (9515325, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 1 acc: 0.973381 | elapsed 14.6s\nFold 2: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 2 acc: 0.973305 | elapsed 13.7s\nFold 3: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 3 acc: 0.973310 | elapsed 13.4s\nFold 4: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 4 acc: 0.973584 | elapsed 13.7s\nFold 5: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 5 acc: 0.973287 | elapsed 13.5s\nv3 router OOF overall accuracy: 0.973373\nPer-class OOF (top 20):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class\nPUNCT         1.000000\nVERBATIM      0.992550\nPLAIN         0.984675\nLETTERS       0.939792\nDIGIT         0.900336\nTIME          0.872244\nCARDINAL      0.789590\nDATE          0.789204\nORDINAL       0.550440\nMEASURE       0.543861\nFRACTION      0.517162\nELECTRONIC    0.418627\nDECIMAL       0.353692\nTELEPHONE     0.197771\nMONEY         0.163062\ndtype: float64\nDone. Total elapsed: 91.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/3235721086.py:75: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  cls_acc = train.assign(pred=oof_pred).groupby('class').apply(lambda df: (df['pred'].values == df['after'].values).mean()).sort_values(ascending=False)\n"
          ]
        }
      ]
    },
    {
      "id": "dc8c3b3d-6897-406b-b462-a0af1b7f211c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# v4 router: add ORDINAL (safe hyphen trigger) + TELEPHONE gates; improved routing order + FRACTION + PERCENT\n",
        "import re, time\n",
        "import pandas as pd, numpy as np\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "# Reuse helpers from previous cells (normalization, num2words, date/time/money/decimal/measure)\n",
        "from num2words import num2words as n2w\n",
        "\n",
        "# Gates from earlier cells (reuse if defined); redefine here for isolation\n",
        "has_letter_re = re.compile(r'[A-Za-z\\u0410-\\u042f\\u0430-\\u044f]')\n",
        "url_email_re = re.compile(r'^[a-z0-9._%+\\-]+@[a-z0-9.\\-]+\\.[a-z]{2,}$|^https?://|^www\\.', re.I)\n",
        "ip_or_version_re = re.compile(r'^\\d+(?:\\.\\d+){2,}$')\n",
        "int_re = re.compile(r'^-?\\d{1,3}(?:[ \\u00A0]?\\d{3})*$|^-?\\d+$')\n",
        "dec_re = re.compile(r'^-?\\d+[\\.,]\\d+$')\n",
        "date_ddmmyyyy_re = re.compile(r'^(0[1-9]|[12]\\d|3[01])\\.(0[1-9]|1[0-2])\\.(\\d{4})$')\n",
        "fraction_re = re.compile(r'^(\\d+)/(\\d+)$')\n",
        "percent_re = re.compile(r'^-?\\d+(?:[ \\u00A0]?\\d{3})*(?:[\\.,]\\d+)?\\s*%$')\n",
        "\n",
        "# TELEPHONE gates (conservative) before numeric generators\n",
        "tel_re_generic = re.compile(r'^\\+?\\d[\\d\\-\\(\\) \\u00A0]{6,}\\d$')\n",
        "tel_re_ru_1 = re.compile(r'^\\+7\\d{10}$')\n",
        "tel_re_ru_2 = re.compile(r'^8\\d{10}$')\n",
        "\n",
        "digit_words = ['\\u043d\\u043e\\u043b\\u044c','\\u043e\\u0434\\u0438\\u043d','\\u0434\\u0432\\u0430','\\u0442\\u0440\\u0438','\\u0447\\u0435\\u0442\\u044b\\u0440\\u0435','\\u043f\\u044f\\u0442\\u044c','\\u0448\\u0435\\u0441\\u0442\\u044c','\\u0441\\u0435\\u043c\\u044c','\\u0432\\u043e\\u0441\\u0435\\u043c\\u044c','\\u0434\\u0435\\u0432\\u044f\\u0442\\u044c']\n",
        "def speak_digits_ru(s: str) -> str:\n",
        "    out = []\n",
        "    lead_plus = s.strip().startswith('+')\n",
        "    for ch in s:\n",
        "        if ch.isdigit():\n",
        "            out.append(digit_words[int(ch)])\n",
        "    if lead_plus:\n",
        "        return '\\u043f\\u043b\\u044e\\u0441 ' + ' '.join(out)\n",
        "    return ' '.join(out)\n",
        "\n",
        "# ORDINAL safe trigger: hyphen/dotted suffixes only\n",
        "ordinal_re = re.compile(r'^(\\d+)-?(\\u0439|\\u044b\\u0439|\\u0438\\u0439|\\u044f|\\u0435|\\u043e\\u0435|\\u0430\\u044f|\\u0433\\u043e|\\u043e\\u0433\\u043e|\\u0435\\u043c\\u0443|\\u043e\\u043c\\u0443|\\u044b\\u043c|\\u0438\\u043c|\\u043e\\u043c|\\u043e\\u0439|\\u0443\\u044e|\\u043c\\u0438|\\u044b\\u043c\\u0438|\\u044b\\u0445|\\u0438\\u0445)\\.?$', re.I)\n",
        "\n",
        "def morph_ordinal_phrase(base_nom_masc: str, suffix: str) -> str:\n",
        "    # Convert last token according to Russian case/gender implied by suffix\n",
        "    parts = base_nom_masc.split()\n",
        "    if not parts:\n",
        "        return base_nom_masc\n",
        "    last = parts[-1]\n",
        "    suf = suffix.lower()\n",
        "    def to_gen(word):\n",
        "        if word.endswith('\\u044b\\u0439') or word.endswith('\\u0438\\u0439') or word.endswith('\\u043e\\u0439'):\n",
        "            return word[:-2] + '\\u043e\\u0433\\u043e'\n",
        "        return word\n",
        "    def to_prepdat(word):\n",
        "        if word.endswith('\\u044b\\u0439') or word.endswith('\\u0438\\u0439') or word.endswith('\\u043e\\u0439'):\n",
        "            return word[:-2] + '\\u043e\\u043c\\u0443'\n",
        "        return word\n",
        "    def to_instr(word):\n",
        "        if word.endswith('\\u044b\\u0439') or word.endswith('\\u0438\\u0439') or word.endswith('\\u043e\\u0439'):\n",
        "            return word[:-2] + '\\u044b\\u043c'\n",
        "        return word\n",
        "    def to_fem_nom(word):\n",
        "        if word.endswith('\\u044b\\u0439') or word.endswith('\\u0438\\u0439') or word.endswith('\\u043e\\u0439'):\n",
        "            return word[:-2] + '\\u0430\\u044f'\n",
        "        return word\n",
        "    def to_neut_nom(word):\n",
        "        if word.endswith('\\u044b\\u0439') or word.endswith('\\u0438\\u0439') or word.endswith('\\u043e\\u0439'):\n",
        "            return word[:-2] + '\\u043e\\u0435'\n",
        "        return word\n",
        "    # Map suffix to target transformation\n",
        "    if suf in ('\\u0439','\\u044b\\u0439','\\u0438\\u0439'):\n",
        "        parts[-1] = last\n",
        "    elif suf in ('\\u0433\\u043e','\\u043e\\u0433\\u043e','\\u0435\\u0433\\u043e'):\n",
        "        parts[-1] = to_gen(last)\n",
        "    elif suf in ('\\u043e\\u043c\\u0443','\\u0435\\u043c\\u0443'):\n",
        "        parts[-1] = to_prepdat(last)\n",
        "    elif suf in ('\\u044b\\u043c','\\u0438\\u043c'):\n",
        "        parts[-1] = to_instr(last)\n",
        "    elif suf in ('\\u044f','\\u0430\\u044f'):\n",
        "        parts[-1] = to_fem_nom(last)\n",
        "    elif suf in ('\\u0435','\\u043e\\u0435','\\u0435\\u0435'):\n",
        "        parts[-1] = to_neut_nom(last)\n",
        "    elif suf in ('\\u043e\\u0439','\\u0435\\u0439'):\n",
        "        parts[-1] = to_gen(last)  # gen/prep merged simplification\n",
        "    elif suf == '\\u0443\\u044e':\n",
        "        parts[-1] = to_fem_nom(last)  # accusative fem; approx\n",
        "    # Plural/other endings: leave base (simplified, safe)\n",
        "    return ' '.join(parts)\n",
        "\n",
        "def to_ordinal_ru(token: str) -> str | None:\n",
        "    m = ordinal_re.match(token)\n",
        "    if not m:\n",
        "        return None\n",
        "    try:\n",
        "        n = int(m.group(1))\n",
        "        suf = m.group(2)\n",
        "        base = n2w(n, lang='ru', to='ordinal')  # masculine nominative phrase\n",
        "        phr = morph_ordinal_phrase(base, suf)\n",
        "        return phr\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# FRACTION handler\n",
        "def plural_form(n, one, two_four, many):\n",
        "    try:\n",
        "        n = abs(int(n))\n",
        "    except Exception:\n",
        "        return many\n",
        "    if 11 <= n % 100 <= 14:\n",
        "        return many\n",
        "    if n % 10 == 1:\n",
        "        return one\n",
        "    if n % 10 in (2,3,4):\n",
        "        return two_four\n",
        "    return many\n",
        "\n",
        "denom_nom_fem = {2:'\\u0432\\u0442\\u043e\\u0440\\u0430\\u044f',3:'\\u0442\\u0440\\u0435\\u0442\\u044c\\u044f',4:'\\u0447\\u0435\\u0442\\u0432\\u0435\\u0440\\u0442\\u0430\\u044f',5:'\\u043f\\u044f\\u0442\\u0430\\u044f',6:'\\u0448\\u0435\\u0441\\u0442\\u0430\\u044f',7:'\\u0441\\u0435\\u0434\\u044c\\u043c\\u0430\\u044f',8:'\\u0432\\u043e\\u0441\\u044c\\u043c\\u0430\\u044f',9:'\\u0434\\u0435\\u0432\\u044f\\u0442\\u0430\\u044f',10:'\\u0434\\u0435\\u0441\\u044f\\u0442\\u0430\\u044f'}\n",
        "denom_gen_pl = {2:'\\u0432\\u0442\\u043e\\u0440\\u044b\\u0445',3:'\\u0442\\u0440\\u0435\\u0442\\u044c\\u0438\\u0445',4:'\\u0447\\u0435\\u0442\\u0432\\u0435\\u0440\\u0442\\u044b\\u0445',5:'\\u043f\\u044f\\u0442\\u044b\\u0445',6:'\\u0448\\u0435\\u0441\\u0442\\u044b\\u0445',7:'\\u0441\\u0435\\u0434\\u044c\\u043c\\u044b\\u0445',8:'\\u0432\\u043e\\u0441\\u044c\\u043c\\u044b\\u0445',9:'\\u0434\\u0435\\u0432\\u044f\\u0442\\u044b\\u0445',10:'\\u0434\\u0435\\u0441\\u044f\\u0442\\u044b\\u0445'}\n",
        "\n",
        "def to_fraction_ru(token: str) -> str | None:\n",
        "    m = fraction_re.match(token)\n",
        "    if not m:\n",
        "        return None\n",
        "    a = int(m.group(1)); b = int(m.group(2))\n",
        "    if b == 0:\n",
        "        return None\n",
        "    try:\n",
        "        if a == 1:\n",
        "            if b in denom_nom_fem:\n",
        "                num_sp = '\\u043e\\u0434\\u043d\\u0430'\n",
        "                return f\"{num_sp} {denom_nom_fem[b]}\"\n",
        "            # fallback generic\n",
        "            return f\"\\u043e\\u0434\\u043d\\u0430 {n2w(b, lang='ru', to='ordinal')}\"\n",
        "        # plural genitive pattern: '\\u0434\\u0432\\u0435/\\u0442\\u0440\\u0438/\\u0447\\u0435\\u0442\\u044b\\u0440\\u0435/\\u043f\\u044f\\u0442\\u044c ...' + denom gen pl\n",
        "        a_sp = n2w(a, lang='ru')\n",
        "        if a % 10 == 2 and a % 100 != 12:\n",
        "            a_sp = '\\u0434\\u0432\\u0435' if a == 2 else a_sp\n",
        "        denom = denom_gen_pl.get(b, None)\n",
        "        if denom is None:\n",
        "            denom = n2w(b, lang='ru', to='ordinal')\n",
        "            # crude genitive plural by replacing masculine endings\n",
        "            if denom.endswith('\\u044b\\u0439') or denom.endswith('\\u0438\\u0439') or denom.endswith('\\u043e\\u0439'):\n",
        "                denom = denom[:-2] + '\\u044b\\u0445'\n",
        "        return f\"{a_sp} {denom}\"\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# PERCENT handler (before DECIMAL/MEASURE)\n",
        "def to_percent_ru(token: str) -> str | None:\n",
        "    if not percent_re.match(token):\n",
        "        return None\n",
        "    try:\n",
        "        t = token.replace('\\u00A0',' ').strip()\n",
        "        t = t[:-1].strip()  # remove %\n",
        "        neg = t.startswith('-')\n",
        "        t = t.lstrip('+')\n",
        "        if t.startswith('-'):\n",
        "            t = t[1:]\n",
        "        # split integer and fractional by comma/dot\n",
        "        if ',' in t:\n",
        "            ip, fp = t.split(',', 1)\n",
        "        elif '.' in t:\n",
        "            ip, fp = t.split('.', 1)\n",
        "        else:\n",
        "            ip, fp = t, ''\n",
        "        # integer part spoken\n",
        "        ip_sp = n2w(int(ip) if ip else 0, lang='ru')\n",
        "        if fp:\n",
        "            frac_digits = ' '.join(digit_words[int(ch)] for ch in fp if ch.isdigit())\n",
        "            core = f\"{ip_sp} \\u0437\\u0430\\u043f\\u044f\\u0442\\u0430\\u044f {frac_digits}\"\n",
        "        else:\n",
        "            core = ip_sp\n",
        "        # plural for '\u043f\u0440\u043e\u0446\u0435\u043d\u0442'\n",
        "        try:\n",
        "            ip_val = abs(int(ip)) if ip else 0\n",
        "        except Exception:\n",
        "            ip_val = 0\n",
        "        unit = plural_form(ip_val, '\\u043f\\u0440\\u043e\\u0446\\u0435\\u043d\\u0442', '\\u043f\\u0440\\u043e\\u0446\\u0435\\u043d\\u0442\\u0430', '\\u043f\\u0440\\u043e\\u0446\\u0435\\u043d\\u0442\\u043e\\u0432')\n",
        "        out = f\"{core} {unit}\"\n",
        "        if neg:\n",
        "            out = '\\u043c\\u0438\\u043d\\u0443\\u0441 ' + out\n",
        "        return out\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def build_memorizer_from_df(df: pd.DataFrame) -> dict:\n",
        "    cnt = df.value_counts(['before_norm','after']).reset_index(name='cnt')\n",
        "    cnt = cnt.sort_values(['before_norm','cnt'], ascending=[True, False])\n",
        "    return cnt.drop_duplicates('before_norm').set_index('before_norm')['after'].to_dict()\n",
        "\n",
        "print('Loading full train...')\n",
        "train = pd.read_csv('ru_train.csv.zip', compression='zip', usecols=['sentence_id','token_id','class','before','after'])\n",
        "train['before_norm'] = train['before'].map(normalize_before)\n",
        "print('Train shape:', train.shape)\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train['sentence_id'].values\n",
        "oof_pred = pd.Series(index=train.index, dtype=object)\n",
        "fold_acc = []\n",
        "t0 = time.time()\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    tf = time.time()\n",
        "    print(f'Fold {fold}: train {len(tr_idx):,}, valid {len(va_idx):,}')\n",
        "    tr = train.loc[tr_idx, ['before_norm','after']]\n",
        "    mem = build_memorizer_from_df(tr)\n",
        "    va_bn = train.loc[va_idx, 'before_norm'].values\n",
        "    va_braw = train.loc[va_idx, 'before'].values\n",
        "\n",
        "    preds = []\n",
        "    for b_raw, b in zip(va_braw, va_bn):\n",
        "        out = None\n",
        "        if isinstance(b, str):\n",
        "            # 0) Memorizer first (skip '<self>')\n",
        "            out = mem.get(b, None)\n",
        "            if out == '<self>':\n",
        "                out = None\n",
        "            # 1) ELECTRONIC gate: if URL/email, leave to memorizer/identity; prevents numeric misfires\n",
        "            if out is None and url_email_re.search(b):\n",
        "                # try mem again\n",
        "                tmp = mem.get(b, None)\n",
        "                out = tmp if (tmp is not None and tmp != '<self>') else b_raw\n",
        "            # 2) TELEPHONE\n",
        "            if out is None and (tel_re_generic.match(b) or tel_re_ru_1.match(b) or tel_re_ru_2.match(b)):\n",
        "                out = speak_digits_ru(b)\n",
        "            # 3) DATE\n",
        "            if out is None:\n",
        "                out = to_date_ru(b)\n",
        "            # 4) ORDINAL (safe)\n",
        "            if out is None:\n",
        "                out = to_ordinal_ru(b)\n",
        "            # 5) TIME\n",
        "            if out is None:\n",
        "                out = to_time_ru(b)\n",
        "            # 6) MONEY\n",
        "            if out is None:\n",
        "                out = to_money_ru(b)\n",
        "            # 7) FRACTION\n",
        "            if out is None:\n",
        "                out = to_fraction_ru(b)\n",
        "            # 8) PERCENT\n",
        "            if out is None and percent_re.match(b):\n",
        "                out = to_percent_ru(b)\n",
        "            # 9) DECIMAL with guards\n",
        "            if out is None and dec_re.match(b) and not ip_or_version_re.match(b) and not has_letter_re.search(b) and not url_email_re.search(b) and not date_ddmmyyyy_re.match(b):\n",
        "                out = to_decimal_ru(b)\n",
        "            # 10) MEASURE\n",
        "            if out is None:\n",
        "                out = to_measure_ru(b)\n",
        "            # 11) CARDINAL\n",
        "            if out is None and int_re.match(b) and not url_email_re.search(b):\n",
        "                out = to_int_cardinal_ru(b)\n",
        "            # 12) fallback memorizer, else identity\n",
        "            if out is None:\n",
        "                tmp = mem.get(b, None)\n",
        "                if tmp is not None and tmp != '<self>':\n",
        "                    out = tmp\n",
        "        if out is None:\n",
        "            out = b_raw\n",
        "        preds.append(out)\n",
        "    preds = pd.Series(preds, index=va_idx)\n",
        "    oof_pred.loc[va_idx] = preds\n",
        "    acc = (preds.values == train.loc[va_idx, 'after'].values).mean()\n",
        "    fold_acc.append(acc)\n",
        "    print(f'  fold {fold} acc: {acc:.6f} | elapsed {time.time()-tf:.1f}s')\n",
        "\n",
        "overall = (oof_pred.values == train['after'].values).mean()\n",
        "print(f'v4 router OOF overall accuracy: {overall:.6f}')\n",
        "print('Per-class OOF (top 20):')\n",
        "cls_acc = train.assign(pred=oof_pred).groupby('class').apply(lambda df: (df['pred'].values == df['after'].values).mean()).sort_values(ascending=False)\n",
        "print(cls_acc.head(20))\n",
        "print('Done. Total elapsed:', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading full train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (9515325, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 1 acc: 0.973432 | elapsed 15.6s\nFold 2: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 2 acc: 0.973348 | elapsed 14.7s\nFold 3: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 3 acc: 0.973371 | elapsed 14.4s\nFold 4: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 4 acc: 0.973593 | elapsed 14.5s\nFold 5: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 5 acc: 0.973323 | elapsed 14.5s\nv4 router OOF overall accuracy: 0.973414\nPer-class OOF (top 20):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class\nPUNCT         1.000000\nVERBATIM      0.992550\nPLAIN         0.984675\nLETTERS       0.939792\nDIGIT         0.900336\nTIME          0.872244\nDATE          0.789049\nCARDINAL      0.787920\nFRACTION      0.631121\nORDINAL       0.563201\nMEASURE       0.542981\nELECTRONIC    0.418627\nDECIMAL       0.353692\nTELEPHONE     0.204724\nMONEY         0.163062\ndtype: float64\nDone. Total elapsed: 83.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/90430906.py:264: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  cls_acc = train.assign(pred=oof_pred).groupby('class').apply(lambda df: (df['pred'].values == df['after'].values).mean()).sort_values(ascending=False)\n"
          ]
        }
      ]
    },
    {
      "id": "650fedc1-bac1-476a-a667-95c8a2e25628",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inference v4: add ORDINAL + TELEPHONE gates; improved routing order; memorizer-first (+FRACTION + PERCENT)\n",
        "import re, time, pandas as pd\n",
        "from collections import defaultdict, Counter\n",
        "from num2words import num2words as n2w\n",
        "\n",
        "# Gates/regex (match v4 OOF cell)\n",
        "has_letter_re = re.compile(r'[A-Za-z\\u0410-\\u042f\\u0430-\\u044f]')\n",
        "url_email_re = re.compile(r'^[a-z0-9._%+\\-]+@[a-z0-9.\\-]+\\.[a-z]{2,}$|^https?://|^www\\.', re.I)\n",
        "ip_or_version_re = re.compile(r'^\\d+(?:\\.\\d+){2,}$')\n",
        "int_re = re.compile(r'^-?\\d{1,3}(?:[ \\u00A0]?\\d{3})*$|^-?\\d+$')\n",
        "dec_re = re.compile(r'^-?\\d+[\\.,]\\d+$')\n",
        "date_ddmmyyyy_re = re.compile(r'^(0[1-9]|[12]\\d|3[01])\\.(0[1-9]|1[0-2])\\.(\\d{4})$')\n",
        "fraction_re = re.compile(r'^(\\d+)/(\\d+)$')\n",
        "percent_re = re.compile(r'^-?\\d+(?:[ \\u00A0]?\\d{3})*(?:[\\.,]\\d+)?\\s*%$')\n",
        "\n",
        "# TELEPHONE gates\n",
        "tel_re_generic = re.compile(r'^\\+?\\d[\\d\\-\\(\\) \\u00A0]{6,}\\d$')\n",
        "tel_re_ru_1 = re.compile(r'^\\+7\\d{10}$')\n",
        "tel_re_ru_2 = re.compile(r'^8\\d{10}$')\n",
        "\n",
        "digit_words = ['\\u043d\\u043e\\u043b\\u044c','\\u043e\\u0434\\u0438\\u043d','\\u0434\\u0432\\u0430','\\u0442\\u0440\\u0438','\\u0447\\u0435\\u0442\\u044b\\u0440\\u0435','\\u043f\\u044f\\u0442\\u044c','\\u0448\\u0435\\u0441\\u0442\\u044c','\\u0441\\u0435\\u043c\\u044c','\\u0432\\u043e\\u0441\\u0435\\u043c\\u044c','\\u0434\\u0435\\u0432\\u044f\\u0442\\u044c']\n",
        "def speak_digits_ru(s: str) -> str:\n",
        "    out = []\n",
        "    lead_plus = s.strip().startswith('+')\n",
        "    for ch in s:\n",
        "        if ch.isdigit():\n",
        "            out.append(digit_words[int(ch)])\n",
        "    if lead_plus:\n",
        "        return '\\u043f\\u043b\\u044e\\u0441 ' + ' '.join(out)\n",
        "    return ' '.join(out)\n",
        "\n",
        "# ORDINAL safe trigger\n",
        "ordinal_re = re.compile(r'^(\\d+)-?(\\u0439|\\u044b\\u0439|\\u0438\\u0439|\\u044f|\\u0435|\\u043e\\u0435|\\u0430\\u044f|\\u0433\\u043e|\\u043e\\u0433\\u043e|\\u0435\\u043c\\u0443|\\u043e\\u043c\\u0443|\\u044b\\u043c|\\u0438\\u043c|\\u043e\\u043c|\\u043e\\u0439|\\u0443\\u044e|\\u043c\\u0438|\\u044b\\u043c\\u0438|\\u044b\\u0445|\\u0438\\u0445)\\.?$', re.I)\n",
        "def morph_ordinal_phrase(base_nom_masc: str, suffix: str) -> str:\n",
        "    parts = base_nom_masc.split()\n",
        "    if not parts:\n",
        "        return base_nom_masc\n",
        "    last = parts[-1]\n",
        "    suf = suffix.lower()\n",
        "    def to_gen(word):\n",
        "        if word.endswith('\\u044b\\u0439') or word.endswith('\\u0438\\u0439') or word.endswith('\\u043e\\u0439'):\n",
        "            return word[:-2] + '\\u043e\\u0433\\u043e'\n",
        "        return word\n",
        "    def to_prepdat(word):\n",
        "        if word.endswith('\\u044b\\u0439') or word.endswith('\\u0438\\u0439') or word.endswith('\\u043e\\u0439'):\n",
        "            return word[:-2] + '\\u043e\\u043c\\u0443'\n",
        "        return word\n",
        "    def to_instr(word):\n",
        "        if word.endswith('\\u044b\\u0439') or word.endswith('\\u0438\\u0439') or word.endswith('\\u043e\\u0439'):\n",
        "            return word[:-2] + '\\u044b\\u043c'\n",
        "        return word\n",
        "    def to_fem_nom(word):\n",
        "        if word.endswith('\\u044b\\u0439') or word.endswith('\\u0438\\u0439') or word.endswith('\\u043e\\u0439'):\n",
        "            return word[:-2] + '\\u0430\\u044f'\n",
        "        return word\n",
        "    def to_neut_nom(word):\n",
        "        if word.endswith('\\u044b\\u0439') or word.endswith('\\u0438\\u0439') or word.endswith('\\u043e\\u0439'):\n",
        "            return word[:-2] + '\\u043e\\u0435'\n",
        "        return word\n",
        "    if suf in ('\\u0439','\\u044b\\u0439','\\u0438\\u0439'):\n",
        "        parts[-1] = last\n",
        "    elif suf in ('\\u0433\\u043e','\\u043e\\u0433\\u043e','\\u0435\\u0433\\u043e'):\n",
        "        parts[-1] = to_gen(last)\n",
        "    elif suf in ('\\u043e\\u043c\\u0443','\\u0435\\u043c\\u0443'):\n",
        "        parts[-1] = to_prepdat(last)\n",
        "    elif suf in ('\\u044b\\u043c','\\u0438\\u043c'):\n",
        "        parts[-1] = to_instr(last)\n",
        "    elif suf in ('\\u044f','\\u0430\\u044f'):\n",
        "        parts[-1] = to_fem_nom(last)\n",
        "    elif suf in ('\\u0435','\\u043e\\u0435','\\u0435\\u0435'):\n",
        "        parts[-1] = to_neut_nom(last)\n",
        "    elif suf in ('\\u043e\\u0439','\\u0435\\u0439'):\n",
        "        parts[-1] = to_gen(last)\n",
        "    elif suf == '\\u0443\\u044e':\n",
        "        parts[-1] = to_fem_nom(last)\n",
        "    return ' '.join(parts)\n",
        "\n",
        "def to_ordinal_ru(token: str) -> str | None:\n",
        "    m = ordinal_re.match(token)\n",
        "    if not m:\n",
        "        return None\n",
        "    try:\n",
        "        n = int(m.group(1)); suf = m.group(2)\n",
        "        base = n2w(n, lang='ru', to='ordinal')\n",
        "        return morph_ordinal_phrase(base, suf)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# FRACTION and PERCENT helpers\n",
        "def plural_form(n, one, two_four, many):\n",
        "    try:\n",
        "        n = abs(int(n))\n",
        "    except Exception:\n",
        "        return many\n",
        "    if 11 <= n % 100 <= 14:\n",
        "        return many\n",
        "    if n % 10 == 1:\n",
        "        return one\n",
        "    if n % 10 in (2,3,4):\n",
        "        return two_four\n",
        "    return many\n",
        "\n",
        "denom_nom_fem = {2:'\\u0432\\u0442\\u043e\\u0440\\u0430\\u044f',3:'\\u0442\\u0440\\u0435\\u0442\\u044c\\u044f',4:'\\u0447\\u0435\\u0442\\u0432\\u0435\\u0440\\u0442\\u0430\\u044f',5:'\\u043f\\u044f\\u0442\\u0430\\u044f',6:'\\u0448\\u0435\\u0441\\u0442\\u0430\\u044f',7:'\\u0441\\u0435\\u0434\\u044c\\u043c\\u0430\\u044f',8:'\\u0432\\u043e\\u0441\\u044c\\u043c\\u0430\\u044f',9:'\\u0434\\u0435\\u0432\\u044f\\u0442\\u0430\\u044f',10:'\\u0434\\u0435\\u0441\\u044f\\u0442\\u0430\\u044f'}\n",
        "denom_gen_pl = {2:'\\u0432\\u0442\\u043e\\u0440\\u044b\\u0445',3:'\\u0442\\u0440\\u0435\\u0442\\u044c\\u0438\\u0445',4:'\\u0447\\u0435\\u0442\\u0432\\u0435\\u0440\\u0442\\u044b\\u0445',5:'\\u043f\\u044f\\u0442\\u044b\\u0445',6:'\\u0448\\u0435\\u0441\\u0442\\u044b\\u0445',7:'\\u0441\\u0435\\u0434\\u044c\\u043c\\u044b\\u0445',8:'\\u0432\\u043e\\u0441\\u044c\\u043c\\u044b\\u0445',9:'\\u0434\\u0435\\u0432\\u044f\\u0442\\u044b\\u0445',10:'\\u0434\\u0435\\u0441\\u044f\\u0442\\u044b\\u0445'}\n",
        "\n",
        "def to_fraction_ru(token: str) -> str | None:\n",
        "    m = fraction_re.match(token)\n",
        "    if not m:\n",
        "        return None\n",
        "    a = int(m.group(1)); b = int(m.group(2))\n",
        "    if b == 0:\n",
        "        return None\n",
        "    try:\n",
        "        if a == 1:\n",
        "            if b in denom_nom_fem:\n",
        "                num_sp = '\\u043e\\u0434\\u043d\\u0430'\n",
        "                return f\"{num_sp} {denom_nom_fem[b]}\"\n",
        "            return f\"\\u043e\\u0434\\u043d\\u0430 {n2w(b, lang='ru', to='ordinal')}\"\n",
        "        a_sp = n2w(a, lang='ru')\n",
        "        if a % 10 == 2 and a % 100 != 12:\n",
        "            a_sp = '\\u0434\\u0432\\u0435' if a == 2 else a_sp\n",
        "        denom = denom_gen_pl.get(b, None)\n",
        "        if denom is None:\n",
        "            denom = n2w(b, lang='ru', to='ordinal')\n",
        "            if denom.endswith('\\u044b\\u0439') or denom.endswith('\\u0438\\u0439') or denom.endswith('\\u043e\\u0439'):\n",
        "                denom = denom[:-2] + '\\u044b\\u0445'\n",
        "        return f\"{a_sp} {denom}\"\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def to_percent_ru(token: str) -> str | None:\n",
        "    if not percent_re.match(token):\n",
        "        return None\n",
        "    try:\n",
        "        t = token.replace('\\u00A0',' ').strip()\n",
        "        t = t[:-1].strip()\n",
        "        neg = t.startswith('-')\n",
        "        t = t.lstrip('+')\n",
        "        if t.startswith('-'):\n",
        "            t = t[1:]\n",
        "        if ',' in t:\n",
        "            ip, fp = t.split(',', 1)\n",
        "        elif '.' in t:\n",
        "            ip, fp = t.split('.', 1)\n",
        "        else:\n",
        "            ip, fp = t, ''\n",
        "        ip_sp = n2w(int(ip) if ip else 0, lang='ru')\n",
        "        if fp:\n",
        "            frac_digits = ' '.join(digit_words[int(ch)] for ch in fp if ch.isdigit())\n",
        "            core = f\"{ip_sp} \\u0437\\u0430\\u043f\\u044f\\u0442\\u0430\\u044f {frac_digits}\"\n",
        "        else:\n",
        "            core = ip_sp\n",
        "        try:\n",
        "            ip_val = abs(int(ip)) if ip else 0\n",
        "        except Exception:\n",
        "            ip_val = 0\n",
        "        unit = plural_form(ip_val, '\\u043f\\u0440\\u043e\\u0446\\u0435\\u043d\\u0442', '\\u043f\\u0440\\u043e\\u0446\\u0435\\u043d\\u0442\\u0430', '\\u043f\\u0440\\u043e\\u0446\\u0435\\u043d\\u0442\\u043e\\u0432')\n",
        "        out = f\"{core} {unit}\"\n",
        "        if neg:\n",
        "            out = '\\u043c\\u0438\\u043d\\u0443\\u0441 ' + out\n",
        "        return out\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def build_normalized_memorizer(train_path: str, chunksize: int = 500_000):\n",
        "    counts = defaultdict(Counter)\n",
        "    rows = 0; chunk_idx = 0; t0 = time.time()\n",
        "    for chunk in pd.read_csv(train_path, compression='zip', usecols=['before','after'], chunksize=chunksize):\n",
        "        chunk_idx += 1\n",
        "        bnorm = chunk['before'].map(normalize_before).values\n",
        "        after = chunk['after'].values\n",
        "        for b, a in zip(bnorm, after):\n",
        "            counts[b][a] += 1\n",
        "        rows += len(chunk)\n",
        "        if chunk_idx % 5 == 0:\n",
        "            print(f'  memorizer processed {rows:,} rows (chunks {chunk_idx}), unique keys: {len(counts):,}')\n",
        "    mem = {b: ctr.most_common(1)[0][0] for b, ctr in counts.items()}\n",
        "    print(f'  memorizer size: {len(mem):,}. build {time.time()-t0:.1f}s')\n",
        "    return mem\n",
        "\n",
        "train_path = 'ru_train.csv.zip'\n",
        "test_path = 'ru_test_2.csv.zip'\n",
        "\n",
        "print('Building normalized memorizer (for first backoff) ...')\n",
        "memorized_norm = build_normalized_memorizer(train_path)\n",
        "\n",
        "print('Loading test ...')\n",
        "test = pd.read_csv(test_path, compression='zip')\n",
        "test['id'] = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)\n",
        "bn = test['before'].map(normalize_before)\n",
        "\n",
        "pred = []\n",
        "t0 = time.time()\n",
        "for i, (b_raw, b) in enumerate(zip(test['before'].values, bn.values)):\n",
        "    out = None\n",
        "    if isinstance(b, str):\n",
        "        # 0) Memorizer first\n",
        "        out = memorized_norm.get(b, None)\n",
        "        if out == '<self>':\n",
        "            out = None\n",
        "        # 1) ELECTRONIC early gate\n",
        "        if out is None and url_email_re.search(b):\n",
        "            tmp = memorized_norm.get(b, None)\n",
        "            out = tmp if (tmp is not None and tmp != '<self>') else b_raw\n",
        "        # 2) TELEPHONE\n",
        "        if out is None and (tel_re_generic.match(b) or tel_re_ru_1.match(b) or tel_re_ru_2.match(b)):\n",
        "            out = speak_digits_ru(b)\n",
        "        # 3) DATE\n",
        "        if out is None:\n",
        "            out = to_date_ru(b)\n",
        "        # 4) ORDINAL\n",
        "        if out is None:\n",
        "            out = to_ordinal_ru(b)\n",
        "        # 5) TIME\n",
        "        if out is None:\n",
        "            out = to_time_ru(b)\n",
        "        # 6) MONEY\n",
        "        if out is None:\n",
        "            out = to_money_ru(b)\n",
        "        # 7) FRACTION\n",
        "        if out is None:\n",
        "            out = to_fraction_ru(b)\n",
        "        # 8) PERCENT\n",
        "        if out is None and percent_re.match(b):\n",
        "            out = to_percent_ru(b)\n",
        "        # 9) DECIMAL with guards\n",
        "        if out is None and dec_re.match(b) and not ip_or_version_re.match(b) and not has_letter_re.search(b) and not url_email_re.search(b) and not date_ddmmyyyy_re.match(b):\n",
        "            out = to_decimal_ru(b)\n",
        "        # 10) MEASURE\n",
        "        if out is None:\n",
        "            out = to_measure_ru(b)\n",
        "        # 11) CARDINAL\n",
        "        if out is None and int_re.match(b) and not url_email_re.search(b):\n",
        "            out = to_int_cardinal_ru(b)\n",
        "        # 12) Fallback memorizer -> identity\n",
        "        if out is None:\n",
        "            tmp = memorized_norm.get(b, None)\n",
        "            if tmp is not None and tmp != '<self>':\n",
        "                out = tmp\n",
        "    if out is None:\n",
        "        out = b_raw\n",
        "    pred.append(out)\n",
        "    if (i+1) % 200000 == 0:\n",
        "        print(f'  processed {i+1:,}/{len(test)} tokens...')\n",
        "\n",
        "submission = pd.DataFrame({'id': test['id'], 'after': pred})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with', len(submission), 'rows. Elapsed:', f'{time.time()-t0:.1f}s')\n",
        "print(submission.head())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building normalized memorizer (for first backoff) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer processed 5,000,000 rows (chunks 10), unique keys: 503,997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer processed 7,500,000 rows (chunks 15), unique keys: 648,998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer processed 9,515,325 rows (chunks 20), unique keys: 751,509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  memorizer size: 751,509. build 22.3s\nLoading test ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 200,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 400,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 600,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 800,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  processed 1,000,000/1059191 tokens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv with 1059191 rows. Elapsed: 2.6s\n    id        after\n0  0_0       \u0422\u0435\u043f\u0435\u0440\u044c\n1  0_1          \u0432\u0441\u0435\n2  0_2  \u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\n3  0_3        \u0437\u043e\u0432\u0443\u0442\n4  0_4      \u042f\u043c\u0430\u043c\u043e\u0442\u043e\n"
          ]
        }
      ]
    },
    {
      "id": "1552513f-b529-41ac-890a-372f249dac91",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# v5: Enhanced DATE rules per expert advice (bare year, dd.mm, ranges, centuries, Roman months)\n",
        "import re\n",
        "from num2words import num2words as n2w\n",
        "\n",
        "# Reuse months/day maps from earlier cells if present; redefine to be safe\n",
        "months_gen = ['\u044f\u043d\u0432\u0430\u0440\u044f','\u0444\u0435\u0432\u0440\u0430\u043b\u044f','\u043c\u0430\u0440\u0442\u0430','\u0430\u043f\u0440\u0435\u043b\u044f','\u043c\u0430\u044f','\u0438\u044e\u043d\u044f','\u0438\u044e\u043b\u044f','\u0430\u0432\u0433\u0443\u0441\u0442\u0430','\u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044f','\u043e\u043a\u0442\u044f\u0431\u0440\u044f','\u043d\u043e\u044f\u0431\u0440\u044f','\u0434\u0435\u043a\u0430\u0431\u0440\u044f']\n",
        "day_ord_gen = {\n",
        " 1:'\u043f\u0435\u0440\u0432\u043e\u0433\u043e',2:'\u0432\u0442\u043e\u0440\u043e\u0433\u043e',3:'\u0442\u0440\u0435\u0442\u044c\u0435\u0433\u043e',4:'\u0447\u0435\u0442\u0432\u0435\u0440\u0442\u043e\u0433\u043e',5:'\u043f\u044f\u0442\u043e\u0433\u043e',6:'\u0448\u0435\u0441\u0442\u043e\u0433\u043e',7:'\u0441\u0435\u0434\u044c\u043c\u043e\u0433\u043e',8:'\u0432\u043e\u0441\u044c\u043c\u043e\u0433\u043e',9:'\u0434\u0435\u0432\u044f\u0442\u043e\u0433\u043e',10:'\u0434\u0435\u0441\u044f\u0442\u043e\u0433\u043e',\n",
        " 11:'\u043e\u0434\u0438\u043d\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',12:'\u0434\u0432\u0435\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',13:'\u0442\u0440\u0438\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',14:'\u0447\u0435\u0442\u044b\u0440\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',15:'\u043f\u044f\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',16:'\u0448\u0435\u0441\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',17:'\u0441\u0435\u043c\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',18:'\u0432\u043e\u0441\u0435\u043c\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',19:'\u0434\u0435\u0432\u044f\u0442\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',\n",
        " 20:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e',21:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u043f\u0435\u0440\u0432\u043e\u0433\u043e',22:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0432\u0442\u043e\u0440\u043e\u0433\u043e',23:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0442\u0440\u0435\u0442\u044c\u0435\u0433\u043e',24:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u043e\u0433\u043e',25:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u043f\u044f\u0442\u043e\u0433\u043e',26:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0448\u0435\u0441\u0442\u043e\u0433\u043e',27:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0441\u0435\u0434\u044c\u043c\u043e\u0433\u043e',28:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0432\u043e\u0441\u044c\u043c\u043e\u0433\u043e',29:'\u0434\u0432\u0430\u0434\u0446\u0430\u0442\u044c \u0434\u0435\u0432\u044f\u0442\u043e\u0433\u043e',\n",
        " 30:'\u0442\u0440\u0438\u0434\u0446\u0430\u0442\u043e\u0433\u043e',31:'\u0442\u0440\u0438\u0434\u0446\u0430\u0442\u044c \u043f\u0435\u0440\u0432\u043e\u0433\u043e'\n",
        "}\n",
        "\n",
        "def masc_nom_to_gen(word: str) -> str:\n",
        "    if word.endswith('\u044b\u0439') or word.endswith('\u0438\u0439') or word.endswith('\u043e\u0439'):\n",
        "        return word[:-2] + '\u043e\u0433\u043e'\n",
        "    return word\n",
        "def ordinal_phrase_nom_to_gen(phrase: str) -> str:\n",
        "    parts = phrase.split()\n",
        "    if not parts:\n",
        "        return phrase\n",
        "    parts[-1] = masc_nom_to_gen(parts[-1])\n",
        "    return ' '.join(parts)\n",
        "\n",
        "def year_to_ord_gen(y: int) -> str | None:\n",
        "    try:\n",
        "        if 1000 <= y <= 1999:\n",
        "            rest = y - 1000\n",
        "            if rest == 0:\n",
        "                return None\n",
        "            rest_ord = n2w(rest, lang='ru', to='ordinal')\n",
        "            rest_ord_gen = ordinal_phrase_nom_to_gen(rest_ord)\n",
        "            return f'\u0442\u044b\u0441\u044f\u0447\u0430 {rest_ord_gen}'\n",
        "        if y == 2000:\n",
        "            return '\u0434\u0432\u0443\u0445\u0442\u044b\u0441\u044f\u0447\u043d\u043e\u0433\u043e'\n",
        "        if 2001 <= y <= 2099:\n",
        "            rest = y - 2000\n",
        "            rest_ord = n2w(rest, lang='ru', to='ordinal')\n",
        "            rest_ord_gen = ordinal_phrase_nom_to_gen(rest_ord)\n",
        "            return f'\u0434\u0432\u0435 \u0442\u044b\u0441\u044f\u0447\u0438 {rest_ord_gen}'\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Regexes\n",
        "date_ddmmyyyy_re = re.compile(r'^(0[1-9]|[12]\\d|3[01])\\.(0[1-9]|1[0-2])\\.(\\d{4})$')\n",
        "date_d_month_yyyy_re = re.compile(r'^([1-9]|[12]\\d|3[01])\\s+(\u044f\u043d\u0432\u0430\u0440\u044f|\u0444\u0435\u0432\u0440\u0430\u043b\u044f|\u043c\u0430\u0440\u0442\u0430|\u0430\u043f\u0440\u0435\u043b\u044f|\u043c\u0430\u044f|\u0438\u044e\u043d\u044f|\u0438\u044e\u043b\u044f|\u0430\u0432\u0433\u0443\u0441\u0442\u0430|\u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044f|\u043e\u043a\u0442\u044f\u0431\u0440\u044f|\u043d\u043e\u044f\u0431\u0440\u044f|\u0434\u0435\u043a\u0430\u0431\u0440\u044f)\\s+(\\d{4})$', re.I)\n",
        "date_ddmm_re = re.compile(r'^(0[1-9]|[12]\\d|3[01])\\.(0[1-9]|1[0-2])$')\n",
        "year_word_re = re.compile(r'^(\\d{4})\\s*(\u0433\\.|\u0433\u043e\u0434)$', re.I)\n",
        "bare_year_re = re.compile(r'^(19\\d{2}|20\\d{2})$')\n",
        "range_years_re = re.compile(r'^(\\d{4})\\s*[\u2013\u2014-]\\s*(\\d{4})\\s*(\u0433\u0433?\\.|\u0433\u043e\u0434\u0430)?$', re.I)\n",
        "roman_century_re = re.compile(r'^(X|XX|XXX|I|II|III|IV|V|VI|VII|VIII|IX)\\s*(\u0432\u0435\u043a|\u0432\\.)$', re.I)\n",
        "date_dd_Roman_yyyy_re = re.compile(r'^([1-9]|[12]\\d|3[01])\\.(I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII)\\.(\\d{4})$', re.I)\n",
        "\n",
        "roman_to_int = {\n",
        "    'I':1,'II':2,'III':3,'IV':4,'V':5,'VI':6,'VII':7,'VIII':8,'IX':9,'X':10,'XI':11,'XII':12\n",
        "}\n",
        "def to_century_ordinal_ru(n: int) -> str:\n",
        "    try:\n",
        "        return n2w(n, lang='ru', to='ordinal')\n",
        "    except Exception:\n",
        "        return ''\n",
        "\n",
        "def to_date_ru(s: str) -> str | None:\n",
        "    # dd.mm.yyyy\n",
        "    m = date_ddmmyyyy_re.match(s)\n",
        "    if m:\n",
        "        d = int(m.group(1)); mo = int(m.group(2)); y = int(m.group(3))\n",
        "        d_sp = day_ord_gen.get(d); y_sp = year_to_ord_gen(y)\n",
        "        if d_sp and y_sp:\n",
        "            return f\"{d_sp} {months_gen[mo-1]} {y_sp} \u0433\u043e\u0434\u0430\"\n",
        "        return None\n",
        "    # d month yyyy\n",
        "    m = date_d_month_yyyy_re.match(s)\n",
        "    if m:\n",
        "        d = int(m.group(1)); month_word = m.group(2).lower(); y = int(m.group(3))\n",
        "        d_sp = day_ord_gen.get(d); y_sp = year_to_ord_gen(y)\n",
        "        if d_sp and y_sp and month_word in months_gen:\n",
        "            return f\"{d_sp} {month_word} {y_sp} \u0433\u043e\u0434\u0430\"\n",
        "        return None\n",
        "    # dd.mm (no year)\n",
        "    m = date_ddmm_re.match(s)\n",
        "    if m:\n",
        "        d = int(m.group(1)); mo = int(m.group(2))\n",
        "        d_sp = day_ord_gen.get(d)\n",
        "        if d_sp:\n",
        "            return f\"{d_sp} {months_gen[mo-1]}\"\n",
        "        return None\n",
        "    # yyyy + word (\u0433\u043e\u0434/\u0433.) -> nominative '\u0433\u043e\u0434'\n",
        "    m = year_word_re.match(s)\n",
        "    if m:\n",
        "        y = int(m.group(1))\n",
        "        y_sp = year_to_ord_gen(y)\n",
        "        if y_sp:\n",
        "            return f\"{y_sp} \u0433\u043e\u0434\"\n",
        "        return None\n",
        "    # bare year -> genitive '\u0433\u043e\u0434\u0430'\n",
        "    m = bare_year_re.match(s)\n",
        "    if m:\n",
        "        y = int(m.group(1))\n",
        "        y_sp = year_to_ord_gen(y)\n",
        "        if y_sp:\n",
        "            return f\"{y_sp} \u0433\u043e\u0434\u0430\"\n",
        "        return None\n",
        "    # ranges yyyy\u2014yyyy\n",
        "    m = range_years_re.match(s)\n",
        "    if m:\n",
        "        y1 = int(m.group(1)); y2 = int(m.group(2))\n",
        "        y1_sp = year_to_ord_gen(y1); y2_sp = year_to_ord_gen(y2)\n",
        "        if y1_sp and y2_sp:\n",
        "            return f\"{y1_sp} \u2014 {y2_sp} \u0433\u043e\u0434\u044b\"\n",
        "        return None\n",
        "    # Roman century\n",
        "    m = roman_century_re.match(s)\n",
        "    if m:\n",
        "        rc = m.group(1).upper()\n",
        "        # Map Roman to Arabic century number\n",
        "        roman_cent_map = {'I':1,'II':2,'III':3,'IV':4,'V':5,'VI':6,'VII':7,'VIII':8,'IX':9,'X':10,'XX':20,'XXX':30}\n",
        "        n = roman_cent_map.get(rc, None)\n",
        "        if n is None:\n",
        "            # Compose if like XIX, XXI etc. Basic support: try to parse by replacement\n",
        "            try:\n",
        "                # crude: convert using known numerals\n",
        "                vals = {'I':1,'V':5,'X':10}\n",
        "                total = 0\n",
        "                prev = 0\n",
        "                for ch in rc[::-1]:\n",
        "                    v = vals[ch]\n",
        "                    if v < prev: total -= v\n",
        "                    else: total += v; prev = v\n",
        "                n = total\n",
        "            except Exception:\n",
        "                n = None\n",
        "        if n:\n",
        "            ord_cent = to_century_ordinal_ru(n)\n",
        "            if ord_cent:\n",
        "                return f\"{ord_cent} \u0432\u0435\u043a\"\n",
        "        return None\n",
        "    # dd.Roman.yyyy\n",
        "    m = date_dd_Roman_yyyy_re.match(s)\n",
        "    if m:\n",
        "        d = int(m.group(1)); rm = m.group(2).upper(); y = int(m.group(3))\n",
        "        mo = roman_to_int.get(rm, None)\n",
        "        if mo:\n",
        "            d_sp = day_ord_gen.get(d); y_sp = year_to_ord_gen(y)\n",
        "            if d_sp and y_sp:\n",
        "                return f\"{d_sp} {months_gen[mo-1]} {y_sp} \u0433\u043e\u0434\u0430\"\n",
        "    return None"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "id": "5dd99e05-b0b5-4e18-9880-6efef56bf8ed",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# v5 router OOF: add context-aware memorizer (tri/bi/unigram), enhanced MONEY/DECIMAL/MEASURE, routing per expert\n",
        "import re, time, pandas as pd, numpy as np\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from collections import defaultdict, Counter\n",
        "from num2words import num2words as n2w\n",
        "\n",
        "# Reuse normalize_before, to_date_ru, to_time_ru, to_ordinal_ru, to_fraction_ru, to_percent_ru, to_int_cardinal_ru if defined\n",
        "try:\n",
        "    normalize_before\n",
        "except NameError:\n",
        "    import unicodedata\n",
        "    def normalize_before(x: str) -> str:\n",
        "        if not isinstance(x, str):\n",
        "            return x\n",
        "        x = unicodedata.normalize('NFKC', x)\n",
        "        x = x.replace('\\u00A0', ' ').replace('\\u2009', ' ')\n",
        "        return x\n",
        "\n",
        "digit_words = ['\\u043d\\u043e\\u043b\\u044c','\\u043e\\u0434\\u0438\\u043d','\\u0434\\u0432\\u0430','\\u0442\\u0440\\u0438','\\u0447\\u0435\\u0442\\u044b\\u0440\\u0435','\\u043f\\u044f\\u0442\\u044c','\\u0448\\u0435\\u0441\\u0442\\u044c','\\u0441\\u0435\\u043c\\u044c','\\u0432\\u043e\\u0441\\u0435\\u043c\\u044c','\\u0434\\u0435\\u0432\\u044f\\u0442\\u044c']\n",
        "\n",
        "# Gates/regex\n",
        "has_letter_re = re.compile(r'[A-Za-z\\u0410-\\u042f\\u0430-\\u044f]')\n",
        "url_email_re = re.compile(r'^[a-z0-9._%+\\-]+@[a-z0-9.\\-]+\\.[a-z]{2,}$|^https?://|^www\\.', re.I)\n",
        "ip_or_version_re = re.compile(r'^\\d+(?:\\.\\d+){2,}$')\n",
        "int_re = re.compile(r'^[-+]?\\d{1,3}(?:[ \\u00A0]?\\d{3})*$|^[-+]?\\d+$')\n",
        "date_ddmmyyyy_any_re = re.compile(r'^(0?[1-9]|[12]\\d|3[01])\\.(0?[1-9]|1[0-2])\\.(\\d{2,4})$')\n",
        "time_like_re = re.compile(r'^\\d{1,2}[:.]\\d{2}$')\n",
        "sci_re = re.compile(r'^[\\d.,]*e[-+]?\\d+$', re.I)\n",
        "\n",
        "# DECIMAL deny/accept\n",
        "decimal_accept_1 = re.compile(r'^[-+]?(?:\\d+|\\d{1,3}(?:[ \\u00A0]?\\d{3})*)[.,]\\d+$')\n",
        "decimal_accept_2 = re.compile(r'^[-+]?\\.\\d+$')\n",
        "\n",
        "# TELEPHONE (reuse from v4)\n",
        "tel_re_generic = re.compile(r'^\\+?\\d[\\d\\-\\(\\) \\u00A0]{6,}\\d$')\n",
        "tel_re_ru_1 = re.compile(r'^\\+7\\d{10}$')\n",
        "tel_re_ru_2 = re.compile(r'^8\\d{10}$')\n",
        "def speak_digits_ru(s: str) -> str:\n",
        "    out = []; lead_plus = s.strip().startswith('+')\n",
        "    for ch in s:\n",
        "        if ch.isdigit(): out.append(digit_words[int(ch)])\n",
        "    return ('\\u043f\\u043b\\u044e\\u0441 ' if lead_plus else '') + ' '.join(out)\n",
        "\n",
        "# MONEY regex gates (ordered, case-insensitive)\n",
        "money_res = [\n",
        "    re.compile(r'^(-?\\d+(?:[ \\u00A0]?\\d{3})*(?:[.,]\\d{1,2})?)\\s*(\\u0442\\u044b\\u0441\\.?|\\u043c\\u043b\\u043d\\.?|\\u043c\\u043b\\u0440\\u0434\\.?)\\s*(\\u20bd|\\u20BD|\\u0440\\u0443\\u0431(?:\\.|\\u043b\\u0435\\u0439|\\u043b\\u044f|\\u043b\\u044c)?|\\u0440\\.?|RUB)$', re.I),\n",
        "    re.compile(r'^(-?\\d+(?:[ \\u00A0]?\\d{3})*(?:[.,]\\d{1,2})?)\\s*(\\u20bd|\\u20BD|\\u0440\\u0443\\u0431(?:\\.|\\u043b\\u0435\\u0439|\\u043b\\u044f|\\u043b\\u044c)?|\\u0440\\.?|RUB|\\$|USD|US\\$|\\u20ac|EUR)$', re.I),\n",
        "    re.compile(r'^(\\u20bd|\\u20BD|\\u0440\\u0443\\u0431\\.?|\\u0440\\.?|RUB|\\$|USD|US\\$|\\u20ac|EUR)\\s*(-?\\d+(?:[ \\u00A0]?\\d{3})*(?:[.,]\\d{1,2})?)$', re.I),\n",
        "    re.compile(r'^-?\\d+(?:[ \\u00A0]?\\d{3})*\\s*(\\u20bd|\\u20BD|\\u0440\\u0443\\u0431\\.?|\\u0440\\.?|RUB)$', re.I),\n",
        "    re.compile(r'^-?\\d+(?:[ \\u00A0]?\\d{3})*(?:[.,]\\d{1,2})?\\s*(\\u0442\\u044b\\u0441\\.?|\\u043c\\u043b\\u043d\\.?|\\u043c\\u043b\\u0440\\u0434\\.?)\\s*(\\u20bd|\\u20BD|\\u0440\\u0443\\u0431\\.?|\\u0440\\.?|RUB)$', re.I),\n",
        "    re.compile(r'^(?:\\$|USD|US\\$|\\u20ac|EUR)\\s*-?\\d+(?:[ \\u00A0]?\\d{3})*(?:[.,]\\d{1,2})?$', re.I),\n",
        "]\n",
        "\n",
        "def plural_form(n, one, two_four, many):\n",
        "    try: n = abs(int(n))\n",
        "    except Exception: return many; \n",
        "    if 11 <= n % 100 <= 14: return many\n",
        "    if n % 10 == 1: return one\n",
        "    if n % 10 in (2,3,4): return two_four\n",
        "    return many\n",
        "\n",
        "def parse_number_parts(t: str):\n",
        "    t = t.replace('\\u00A0',' ').replace(' ','').lstrip('+')\n",
        "    neg = t.startswith('-');\n",
        "    if neg: t = t[1:]\n",
        "    if ',' in t: ip, fp = t.split(',', 1)\n",
        "    elif '.' in t: ip, fp = t.split('.', 1)\n",
        "    else: ip, fp = t, ''\n",
        "    return neg, ip, fp\n",
        "\n",
        "def to_money_ru(token: str) -> str | None:\n",
        "    s = token.strip()\n",
        "    for rx in money_res:\n",
        "        m = rx.match(s)\n",
        "        if not m: continue\n",
        "        try:\n",
        "            groups = m.groups()\n",
        "            # Determine where number and currency/scale are\n",
        "            if rx is money_res[0]:\n",
        "                num_s, scale, cur = groups[0], groups[1].lower().rstrip('.'), groups[2].upper()\n",
        "            elif rx is money_res[1]:\n",
        "                num_s, scale, cur = groups[0], None, groups[1].upper()\n",
        "            elif rx is money_res[2]:\n",
        "                cur, num_s = groups[0].upper(), groups[1]\n",
        "                scale = None\n",
        "            elif rx is money_res[3]:\n",
        "                num_s, cur, scale = s[:m.start(1)].strip(), groups[0].upper(), None\n",
        "            elif rx is money_res[4]:\n",
        "                num_s, scale, cur = s[:m.start(2)].strip(), groups[0].lower().rstrip('.'), groups[1].upper()\n",
        "            else: # last\n",
        "                cur = s[:m.start(0)+0].split()[0].upper() if s else ''\n",
        "                num_s = s[s.find(cur)+len(cur):].strip()\n",
        "                scale = None\n",
        "\n",
        "            neg, ip, fp = parse_number_parts(num_s)\n",
        "            ip_val = int(ip) if ip else 0\n",
        "            # Minor part (2 digits max)\n",
        "            minor = 0\n",
        "            if fp:\n",
        "                minor = int((fp + '0')[:2])\n",
        "\n",
        "            def rub_forms(n): return ('\\u0440\\u0443\\u0431\\u043b\\u044c','\\u0440\\u0443\\u0431\\u043b\\u044f','\\u0440\\u0443\\u0431\\u043b\\u0435\\u0439')[0 if (n%10==1 and n%100!=11) else 1 if (n%10 in (2,3,4) and not 12<=n%100<=14) else 2]\n",
        "            def kop_forms(n): return ('\\u043a\\u043e\\u043f\\u0435\\u0439\\u043a\\u0430','\\u043a\\u043e\\u043f\\u0435\\u0439\\u043a\\u0438','\\u043a\\u043e\\u043f\\u0435\\u0435\\u043a')[0 if (n%10==1 and n%100!=11) else 1 if (n%10 in (2,3,4) and not 12<=n%100<=14) else 2]\n",
        "            def usd_forms(n): return ('\\u0434\\u043e\\u043b\\u043b\\u0430\\u0440','\\u0434\\u043e\\u043b\\u043b\\u0430\\u0440\\u0430','\\u0434\\u043e\\u043b\\u043b\\u0430\\u0440\\u043e\\u0432')[0 if (n%10==1 and n%100!=11) else 1 if (n%10 in (2,3,4) and not 12<=n%100<=14) else 2]\n",
        "            def cent_forms(n): return ('\\u0446\\u0435\\u043d\\u0442','\\u0446\\u0435\\u043d\\u0442\\u0430','\\u0446\\u0435\\u043d\\u0442\\u043e\\u0432')[0 if (n%10==1 and n%100!=11) else 1 if (n%10 in (2,3,4) and not 12<=n%100<=14) else 2]\n",
        "\n",
        "            cur_is_rub = cur in {'\\u20bd','\\u20BD','\\u0420\\u0423\\u0411.','\\u0420\\u0423\\u0411','\\u0420.','RUB','\\u0420\\u0423\\u0411\\u041b\\u042c','\\u0420\\u0423\\u0411\\u041b\\u0415\\u0419','\\u0420\\u0423\\u0411\\u041b\\u042f'}\n",
        "            cur_is_usd = cur in {'$','USD','US$'}\n",
        "            cur_is_eur = cur in {'\\u20ac','EUR'}\n",
        "\n",
        "            # Scales\n",
        "            scale_word = None\n",
        "            if scale:\n",
        "                if scale.startswith('\\u0442\\u044b\\u0441'): scale_word = '\\u0442\\u044b\\u0441\\u044f\\u0447'\n",
        "                elif scale.startswith('\\u043c\\u043b\\u043d'): scale_word = '\\u043c\\u0438\\u043b\\u043b\\u0438\\u043e\\u043d\\u043e\\u0432'\n",
        "                elif scale.startswith('\\u043c\\u043b\\u0440\\u0434'): scale_word = '\\u043c\\u0438\\u043b\\u043b\\u0438\\u0430\\u0440\\u0434\\u043e\\u0432'\n",
        "\n",
        "            # Handle scales\n",
        "            if scale_word:\n",
        "                # Only safe fractional scale: 1.5\n",
        "                if fp and not (ip_val == 1 and (fp[:1] == '5')):\n",
        "                    return None\n",
        "                if ip_val == 1 and fp and fp[:1] == '5':\n",
        "                    # 1.5 scale\n",
        "                    if scale.startswith('\\u0442\\u044b\\u0441'): head = '\\u043f\\u043e\\u043b\\u0442\\u043e\\u0440\\u044b \\u0442\\u044b\\u0441\\u044f\\u0447\\u0438'\n",
        "                    else: head = '\\u043f\\u043e\\u043b\\u0442\\u043e\\u0440\\u0430 ' + ('\\u043c\\u0438\\u043b\\u043b\\u0438\\u043e\\u043d\\u0430' if scale.startswith('\\u043c\\u043b\\u043d') else '\\u043c\\u0438\\u043b\\u043b\\u0438\\u0430\\u0440\\u0434\\u0430')\n",
        "                else:\n",
        "                    head = f\"{n2w(ip_val, lang='ru')} {scale_word}\"\n",
        "                # Append currency plural by ip\n",
        "                if cur_is_rub:\n",
        "                    unit = plural_form(ip_val, '\\u0440\\u0443\\u0431\\u043b\\u044c','\\u0440\\u0443\\u0431\\u043b\\u044f','\\u0440\\u0443\\u0431\\u043b\\u0435\\u0439')\n",
        "                    out = f\"{head} {unit}\"\n",
        "                elif cur_is_usd:\n",
        "                    unit = usd_forms(ip_val)\n",
        "                    out = f\"{head} {unit}\"\n",
        "                elif cur_is_eur:\n",
        "                    out = f\"{head} \\u0435\\u0432\\u0440\\u043e\"\n",
        "                else:\n",
        "                    return None\n",
        "                if neg: out = '\\u043c\\u0438\\u043d\\u0443\\u0441 ' + out\n",
        "                return out\n",
        "\n",
        "            # No scale\n",
        "            major_sp = n2w(ip_val, lang='ru') if ip_val != 0 else '\\u043d\\u043e\\u043b\\u044c'\n",
        "            if cur_is_rub:\n",
        "                unit = plural_form(ip_val, '\\u0440\\u0443\\u0431\\u043b\\u044c','\\u0440\\u0443\\u0431\\u043b\\u044f','\\u0440\\u0443\\u0431\\u043b\\u0435\\u0439')\n",
        "                out = f\"{major_sp} {unit}\"\n",
        "                if minor > 0:\n",
        "                    k_unit = plural_form(minor, '\\u043a\\u043e\\u043f\\u0435\\u0439\\u043a\\u0430','\\u043a\\u043e\\u043f\\u0435\\u0439\\u043a\\u0438','\\u043a\\u043e\\u043f\\u0435\\u0435\\u043a')\n",
        "                    out += f\" {n2w(minor, lang='ru')} {k_unit}\"\n",
        "            elif cur_is_usd:\n",
        "                unit = usd_forms(ip_val)\n",
        "                out = f\"{major_sp} {unit}\"\n",
        "                if minor > 0:\n",
        "                    out += f\" {n2w(minor, lang='ru')} {cent_forms(minor)}\"\n",
        "            elif cur_is_eur:\n",
        "                out = f\"{major_sp} \\u0435\\u0432\\u0440\\u043e\"\n",
        "                if minor > 0:\n",
        "                    out += f\" {n2w(minor, lang='ru')} {cent_forms(minor)}\"\n",
        "            else:\n",
        "                return None\n",
        "            if neg: out = '\\u043c\\u0438\\u043d\\u0443\\u0441 ' + out\n",
        "            return out\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "# DECIMAL verbalizer with guards\n",
        "def to_decimal_ru(token: str) -> str | None:\n",
        "    s = token.strip()\n",
        "    # deny list\n",
        "    if date_ddmmyyyy_any_re.match(s): return None\n",
        "    if ip_or_version_re.match(s): return None\n",
        "    if has_letter_re.search(s): return None\n",
        "    if url_email_re.search(s): return None\n",
        "    if sci_re.match(s): return None\n",
        "    if time_like_re.match(s): return None\n",
        "    # accept list\n",
        "    if not (decimal_accept_1.match(s) or decimal_accept_2.match(s)):\n",
        "        return None\n",
        "    try:\n",
        "        neg = s.startswith('-') or s.startswith('+') and s[0] == '-'\n",
        "        t = s.lstrip('+-')\n",
        "        if t.startswith('.'): ip, fp = '0', t[1:]\n",
        "        else:\n",
        "            if ',' in t: ip, fp = t.split(',', 1)\n",
        "            else: ip, fp = t.split('.', 1)\n",
        "        ip_sp = n2w(int(ip) if ip else 0, lang='ru')\n",
        "        frac_digits = ' '.join(digit_words[int(ch)] for ch in fp if ch.isdigit())\n",
        "        if not frac_digits: return None\n",
        "        out = f\"{ip_sp} \\u0437\\u0430\\u043f\\u044f\\u0442\\u0430\\u044f {frac_digits}\"\n",
        "        if s.strip().startswith('-'): out = '\\u043c\\u0438\\u043d\\u0443\\u0441 ' + out\n",
        "        return out\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# MEASURE regex and unit forms\n",
        "measure_re = re.compile(r'^-?\\d{1,3}(?:[ \\u00A0]?\\d{3})*(?:[.,]\\d+)?\\s*(\\u043c\\u043c|\\u0441\\u043c|\\u0434\\u043c|\\u043c|\\u043a\\u043c|\\u043c\\u00b2|\\u043c2|\\u043a\\u043c\\u00b2|\\u043c\\u00b3|\\u043c3|\\u0433|\\u043a\\u0433|\\u0442|\\u0446|\\u043c\\u043b|\\u043b|\\u043a\\u043c/\\u0447|\\u043c/\\u0441|\\u00b0C|\\u00b0\\u0421|\\u00b0F|\\u0412|\\u0410|\\u0412\\u0442|\\u043a\\u0412\\u0442|\\u041c\\u0412\\u0442|\\u043a\\u0412\\u0442\\u00b7\\u0447|\\u0411|\\u041a\\u0411|\\u043a\\u0411|\\u041c\\u0411|\\u0413\\u0411|\\u0422\\u0411|\\u041f\\u0430|\\u043a\\u041f\\u0430|\\u0431\\u0430\\u0440|\\u0430\\u0442\\u043c|\\u043c\\u043c\\s\\u0440\\u0442\\.?\\s\\u0441\\u0442\\.)$', re.I)\n",
        "unit_forms = {\n",
        "    '\\u043c\\u043c': ('\\u043c\\u0438\\u043b\\u043b\\u0438\\u043c\\u0435\\u0442\\u0440','\\u043c\\u0438\\u043b\\u043b\\u0438\\u043c\\u0435\\u0442\\u0440\\u0430','\\u043c\\u0438\\u043b\\u043b\\u0438\\u043c\\u0435\\u0442\\u0440\\u043e\\u0432'),\n",
        "    '\\u0441\\u043c': ('\\u0441\\u0430\\u043d\\u0442\\u0438\\u043c\\u0435\\u0442\\u0440','\\u0441\\u0430\\u043d\\u0442\\u0438\\u043c\\u0435\\u0442\\u0440\\u0430','\\u0441\\u0430\\u043d\\u0442\\u0438\\u043c\\u0435\\u0442\\u0440\\u043e\\u0432'),\n",
        "    '\\u0434\\u043c': ('\\u0434\\u0435\\u0446\\u0438\\u043c\\u0435\\u0442\\u0440','\\u0434\\u0435\\u0446\\u0438\\u043c\\u0435\\u0442\\u0440\\u0430','\\u0434\\u0435\\u0446\\u0438\\u043c\\u0435\\u0442\\u0440\\u043e\\u0432'),\n",
        "    '\\u043c': ('\\u043c\\u0435\\u0442\\u0440','\\u043c\\u0435\\u0442\\u0440\\u0430','\\u043c\\u0435\\u0442\\u0440\\u043e\\u0432'),\n",
        "    '\\u043a\\u043c': ('\\u043a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440','\\u043a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440\\u0430','\\u043a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440\\u043e\\u0432'),\n",
        "    '\\u043c\\u00b2': ('\\u043a\\u0432\\u0430\\u0434\\u0440\\u0430\\u0442\\u043d\\u044b\\u0439 \\u043c\\u0435\\u0442\\u0440','\\u043a\\u0432\\u0430\\u0434\\u0440\\u0430\\u043d\\u0442\\u043d\\u044b\\u0445 \\u043c\\u0435\\u0442\\u0440\\u0430','\\u043a\\u0432\\u0430\\u0434\\u0440\\u0430\\u043d\\u0442\\u043d\\u044b\\u0445 \\u043c\\u0435\\u0442\\u0440\\u043e\\u0432'),\n",
        "    '\\u043c2': ('\\u043a\\u0432\\u0430\\u0434\\u0440\\u0430\\u043d\\u0442\\u043d\\u044b\\u0439 \\u043c\\u0435\\u0442\\u0440','\\u043a\\u0432\\u0430\\u0434\\u0440\\u0430\\u043d\\u0442\\u043d\\u044b\\u0445 \\u043c\\u0435\\u0442\\u0440\\u0430','\\u043a\\u0432\\u0430\\u0434\\u0440\\u0430\\u043d\\u0442\\u043d\\u044b\\u0445 \\u043c\\u0435\\u0442\\u0440\\u043e\\u0432'),\n",
        "    '\\u043a\\u043c\\u00b2': ('\\u043a\\u0432\\u0430\\u0434\\u0440\\u0430\\u0442\\u043d\\u044b\\u0439 \\u043a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440','\\u043a\\u0432\\u0430\\u0434\\u0440\\u0430\\u043d\\u0442\\u043d\\u044b\\u0445 \\u043a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440\\u0430','\\u043a\\u0432\\u0430\\u0434\\u0440\\u0430\\u043d\\u0442\\u043d\\u044b\\u0445 \\u043a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440\\u043e\\u0432'),\n",
        "    '\\u043c\\u00b3': ('\\u043a\\u0443\\u0431\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u043c\\u0435\\u0442\\u0440','\\u043a\\u0443\\u0431\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0445 \\u043c\\u0435\\u0442\\u0440\\u0430','\\u043a\\u0443\\u0431\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0445 \\u043c\\u0435\\u0442\\u0440\\u043e\\u0432'),\n",
        "    '\\u043c3': ('\\u043a\\u0443\\u0431\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u043c\\u0435\\u0442\\u0440','\\u043a\\u0443\\u0431\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0445 \\u043c\\u0435\\u0442\\u0440\\u0430','\\u043a\\u0443\\u0431\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0445 \\u043c\\u0435\\u0442\\u0440\\u043e\\u0432'),\n",
        "    '\\u0433': ('\\u0433\\u0440\\u0430\\u043c\\u043c','\\u0433\\u0440\\u0430\\u043c\\u043c\\u0430','\\u0433\\u0440\\u0430\\u043c\\u043c\\u043e\\u0432'),\n",
        "    '\\u043a\\u0433': ('\\u043a\\u0438\\u043b\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c','\\u043a\\u0438\\u043b\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0430','\\u043a\\u0438\\u043b\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u043e\\u0432'),\n",
        "    '\\u0442': ('\\u0442\\u043e\\u043d\\u043d\\u0430','\\u0442\\u043e\\u043d\\u043d\\u044b','\\u0442\\u043e\\u043d\\u043d'),\n",
        "    '\\u0446': ('\\u0446\\u0435\\u043d\\u0442\\u043d\\u0435\\u0440','\\u0446\\u0435\\u043d\\u0442\\u043d\\u0435\\u0440\\u0430','\\u0446\\u0435\\u043d\\u0442\\u043d\\u0435\\u0440\\u043e\\u0432'),\n",
        "    '\\u043c\\u043b': ('\\u043c\\u0438\\u043b\\u043b\\u0438\\u043b\\u0438\\u0442\\u0440','\\u043c\\u0438\\u043b\\u043b\\u0438\\u043b\\u0438\\u0442\\u0440\\u0430','\\u043c\\u0438\\u043b\\u043b\\u0438\\u043b\\u0438\\u0442\\u0440\\u043e\\u0432'),\n",
        "    '\\u043b': ('\\u043b\\u0438\\u0442\\u0440','\\u043b\\u0438\\u0442\\u0440\\u0430','\\u043b\\u0438\\u0442\\u0440\\u043e\\u0432'),\n",
        "    '\\u043a\\u043c/\\u0447': ('\\u043a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440 \\u0432 \\u0447\\u0430\\u0441','\\u043a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440\\u0430 \\u0432 \\u0447\\u0430\\u0441','\\u043a\\u0438\\u043b\\u043e\\u043c\\u0435\\u0442\\u0440\\u043e\\u0432 \\u0432 \\u0447\\u0430\\u0441'),\n",
        "    '\\u043c/\\u0441': ('\\u043c\\u0435\\u0442\\u0440 \\u0432 \\u0441\\u0435\\u043a\\u0443\\u043d\\u0434\\u0443','\\u043c\\u0435\\u0442\\u0440\\u0430 \\u0432 \\u0441\\u0435\\u043a\\u0443\\u043d\\u0434\\u0443','\\u043c\\u0435\\u0442\\u0440\\u043e\\u0432 \\u0432 \\u0441\\u0435\\u043a\\u0443\\u043d\\u0434\\u0443'),\n",
        "    '\\u00b0C': ('\\u0433\\u0440\\u0430\\u0434\\u0443\\u0441 \\u0426\\u0435\\u043b\\u044c\\u0441\\u0438\\u044f','\\u0433\\u0440\\u0430\\u0434\\u0443\\u0441\\u0430 \\u0426\\u0435\\u043b\\u044c\\u0441\\u0438\\u044f','\\u0433\\u0440\\u0430\\u0434\\u0443\\u0441\\u043e\\u0432 \\u0426\\u0435\\u043b\\u044c\\u0441\\u0438\\u044f'),\n",
        "    '\\u00b0\\u0421': ('\\u0433\\u0440\\u0430\\u0434\\u0443\\u0441 \\u0426\\u0435\\u043b\\u044c\\u0441\\u0438\\u044f','\\u0433\\u0440\\u0430\\u0434\\u0443\\u0441\\u0430 \\u0426\\u0435\\u043b\\u044c\\u0441\\u0438\\u044f','\\u0433\\u0440\\u0430\\u0434\\u0443\\u0441\\u043e\\u0432 \\u0426\\u0435\\u043b\\u044c\\u0441\\u0438\\u044f'),\n",
        "    '\\u00b0F': ('\\u0433\\u0440\\u0430\\u0434\\u0443\\u0441 \\u0424\\u0430\\u0440\\u0435\\u043d\\u0433\\u0435\\u0439\\u0442\\u0430','\\u0433\\u0440\\u0430\\u0434\\u0443\\u0441\\u0430 \\u0424\\u0430\\u0440\\u0435\\u043d\\u0433\\u0435\\u0439\\u0442\\u0430','\\u0433\\u0440\\u0430\\u0434\\u0443\\u0441\\u043e\\u0432 \\u0424\\u0430\\u0440\\u0435\\u043d\\u0433\\u0435\\u0439\\u0442\\u0430'),\n",
        "    '\\u0412': ('\\u0432\\u043e\\u043b\\u044c\\u0442','\\u0432\\u043e\\u043b\\u044c\\u0442\\u0430','\\u0432\\u043e\\u043b\\u044c\\u0442'),\n",
        "    '\\u0410': ('\\u0430\\u043c\\u043f\\u0435\\u0440','\\u0430\\u043c\\u043f\\u0435\\u0440\\u0430','\\u0430\\u043c\\u043f\\u0435\\u0440'),\n",
        "    '\\u0412\\u0442': ('\\u0432\\u0430\\u0442\\u0442','\\u0432\\u0430\\u0442\\u0442\\u0430','\\u0432\\u0430\\u0442\\u0442'),\n",
        "    '\\u043a\\u0412\\u0442': ('\\u043a\\u0438\\u043b\\u043e\\u0432\\u0430\\u0442\\u0442','\\u043a\\u0438\\u043b\\u043e\\u0432\\u0430\\u0442\\u0442\\u0430','\\u043a\\u0438\\u043b\\u043e\\u0432\\u0430\\u0442\\u0442'),\n",
        "    '\\u041c\\u0412\\u0442': ('\\u043c\\u0435\\u0433\\u0430\\u0432\\u0430\\u0442\\u0442','\\u043c\\u0435\\u0433\\u0430\\u0432\\u0430\\u0442\\u0442\\u0430','\\u043c\\u0435\\u0433\\u0430\\u0432\\u0430\\u0442\\u0442'),\n",
        "    '\\u043a\\u0412\\u0442\\u00b7\\u0447': ('\\u043a\\u0438\\u043b\\u043e\\u0432\\u0430\\u0442\\u0442-\\u0447\\u0430\\u0441','\\u043a\\u0438\\u043b\\u043e\\u0432\\u0430\\u0442\\u0442-\\u0447\\u0430\\u0441\\u0430','\\u043a\\u0438\\u043b\\u043e\\u0432\\u0430\\u0442\\u0442-\\u0447\\u0430\\u0441\\u043e\\u0432'),\n",
        "    '\\u0411': ('\\u0431\\u0430\\u0439\\u0442','\\u0431\\u0430\\u0439\\u0442\\u0430','\\u0431\\u0430\\u0439\\u0442'),\n",
        "    '\\u041a\\u0411': ('\\u043a\\u0438\\u043b\\u043e\\u0431\\u0430\\u0439\\u0442','\\u043a\\u0438\\u043b\\u043e\\u0431\\u0430\\u0439\\u0442\\u0430','\\u043a\\u0438\\u043b\\u043e\\u0431\\u0430\\u0439\\u0442'),\n",
        "    '\\u043a\\u0411': ('\\u043a\\u0438\\u043b\\u043e\\u0431\\u0430\\u0439\\u0442','\\u043a\\u0438\\u043b\\u043e\\u0431\\u0430\\u0439\\u0442\\u0430','\\u043a\\u0438\\u043b\\u043e\\u0431\\u0430\\u0439\\u0442'),\n",
        "    '\\u041c\\u0411': ('\\u043c\\u0435\\u0433\\u0430\\u0431\\u0430\\u0439\\u0442','\\u043c\\u0435\\u0433\\u0430\\u0431\\u0430\\u0439\\u0442\\u0430','\\u043c\\u0435\\u0433\\u0430\\u0431\\u0430\\u0439\\u0442'),\n",
        "    '\\u0413\\u0411': ('\\u0433\\u0438\\u0433\\u0430\\u0431\\u0430\\u0439\\u0442','\\u0433\\u0438\\u0433\\u0430\\u0431\\u0430\\u0439\\u0442\\u0430','\\u0433\\u0438\\u0433\\u0430\\u0431\\u0430\\u0439\\u0442'),\n",
        "    '\\u0422\\u0411': ('\\u0442\\u0435\\u0440\\u0430\\u0431\\u0430\\u0439\\u0442','\\u0442\\u0435\\u0440\\u0430\\u0431\\u0430\\u0439\\u0442\\u0430','\\u0442\\u0435\\u0440\\u0430\\u0431\\u0430\\u0439\\u0442'),\n",
        "    '\\u041f\\u0430': ('\\u043f\\u0430\\u0441\\u043a\\u0430\\u043b\\u044c','\\u043f\\u0430\\u0441\\u043a\\u0430\\u043b\\u044f','\\u043f\\u0430\\u0441\\u043a\\u0430\\u043b\\u0435\\u0439'),\n",
        "    '\\u043a\\u041f\\u0430': ('\\u043a\\u0438\\u043b\\u043e\\u043f\\u0430\\u0441\\u043a\\u0430\\u043b\\u044c','\\u043a\\u0438\\u043b\\u043e\\u043f\\u0430\\u0441\\u043a\\u0430\\u043b\\u044f','\\u043a\\u0438\\u043b\\u043e\\u043f\\u0430\\u0441\\u043a\\u0430\\u043b\\u0435\\u0439'),\n",
        "    '\\u0431\\u0430\\u0440': ('\\u0431\\u0430\\u0440','\\u0431\\u0430\\u0440\\u0430','\\u0431\\u0430\\u0440'),\n",
        "    '\\u0430\\u0442\\u043c': ('\\u0430\\u0442\\u043c\\u043e\\u0441\\u0444\\u0435\\u0440\\u0430','\\u0430\\u0442\\u043c\\u043e\\u0441\\u0444\\u0435\\u0440\\u044b','\\u0430\\u0442\\u043c\\u043e\\u0441\\u0444\\u0435\\u0440'),\n",
        "    '\\u043c\\u043c \\u0440\\u0442. \\u0441\\u0442.': ('\\u043c\\u0438\\u043b\\u043b\\u0438\\u043c\\u0435\\u0442\\u0440 \\u0440\\u0442\\u0443\\u0442\\u043d\\u043e\\u0433\\u043e \\u0441\\u0442\\u043e\\u043b\\u0431\\u0430','\\u043c\\u0438\\u043b\\u043b\\u0438\\u043c\\u0435\\u0442\\u0440\\u0430 \\u0440\\u0442\\u0443\\u0442\\u043d\\u043e\\u0433\\u043e \\u0441\\u0442\\u043e\\u043b\\u0431\\u0430','\\u043c\\u0438\\u043b\\u043b\\u0438\\u043c\\u0435\\u0442\\u0440\\u043e\\u0432 \\u0440\\u0442\\u0443\\u0442\\u043d\\u043e\\u0433\\u043e \\u0441\\u0442\\u043e\\u043b\\u0431\\u0430'),\n",
        "}\n",
        "\n",
        "def to_measure_ru(token: str) -> str | None:\n",
        "    m = measure_re.match(token.strip())\n",
        "    if not m: return None\n",
        "    try:\n",
        "        unit_raw = m.group(1)\n",
        "        # unify unit key\n",
        "        unit_key = unit_raw\n",
        "        # special normalize for '\\u043c\\u043c \\u0440\\u0442. \\u0441\\u0442.'\n",
        "        if unit_key.lower().startswith('\\u043c\\u043c'): unit_key = '\\u043c\\u043c \\u0440\\u0442. \\u0441\\u0442.' if '\\u0440\\u0442' in token else unit_key\n",
        "        # numeric part\n",
        "        num_part = token[:m.start(1)].strip()\n",
        "        neg, ip, fp = parse_number_parts(num_part)\n",
        "        ip_val = int(ip) if ip else 0\n",
        "        num_sp = n2w(ip_val, lang='ru')\n",
        "        forms = unit_forms.get(unit_key, None)\n",
        "        if forms is None: return None\n",
        "        unit_word = plural_form(ip_val, forms[0], forms[1], forms[2])\n",
        "        out = f\"{num_sp} {unit_word}\"\n",
        "        if neg: out = '\\u043c\\u0438\\u043d\\u0443\\u0441 ' + out\n",
        "        return out\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Context-aware memorizer builders\n",
        "def build_context_maps(df: pd.DataFrame, min_tri=3, min_bi=5):\n",
        "    # df must have before_norm, after, prev_before_norm, next_before_norm\n",
        "    # trigram\n",
        "    tri_counts = df.value_counts(['prev_before_norm','before_norm','next_before_norm','after']).reset_index(name='cnt')\n",
        "    tri_counts = tri_counts.sort_values(['prev_before_norm','before_norm','next_before_norm','cnt'], ascending=[True,True,True,False])\n",
        "    tri_counts = tri_counts[tri_counts['cnt']>=min_tri]\n",
        "    tri_map = tri_counts.drop_duplicates(['prev_before_norm','before_norm','next_before_norm']).set_index(['prev_before_norm','before_norm','next_before_norm'])['after'].to_dict()\n",
        "    # left bigram\n",
        "    lb_counts = df.value_counts(['prev_before_norm','before_norm','after']).reset_index(name='cnt')\n",
        "    lb_counts = lb_counts.sort_values(['prev_before_norm','before_norm','cnt'], ascending=[True,True,False])\n",
        "    lb_counts = lb_counts[lb_counts['cnt']>=min_bi]\n",
        "    lb_map = lb_counts.drop_duplicates(['prev_before_norm','before_norm']).set_index(['prev_before_norm','before_norm'])['after'].to_dict()\n",
        "    # right bigram\n",
        "    rb_counts = df.value_counts(['before_norm','next_before_norm','after']).reset_index(name='cnt')\n",
        "    rb_counts = rb_counts.sort_values(['before_norm','next_before_norm','cnt'], ascending=[True,True,False])\n",
        "    rb_counts = rb_counts[rb_counts['cnt']>=min_bi]\n",
        "    rb_map = rb_counts.drop_duplicates(['before_norm','next_before_norm']).set_index(['before_norm','next_before_norm'])['after'].to_dict()\n",
        "    # unigram\n",
        "    uni_counts = df.value_counts(['before_norm','after']).reset_index(name='cnt')\n",
        "    uni_counts = uni_counts.sort_values(['before_norm','cnt'], ascending=[True,False])\n",
        "    uni_map = uni_counts.drop_duplicates(['before_norm']).set_index(['before_norm'])['after'].to_dict()\n",
        "    return tri_map, lb_map, rb_map, uni_map\n",
        "\n",
        "print('Loading full train...')\n",
        "train = pd.read_csv('ru_train.csv.zip', compression='zip', usecols=['sentence_id','token_id','class','before','after'])\n",
        "train['before_norm'] = train['before'].map(normalize_before)\n",
        "print('Train shape:', train.shape)\n",
        "# attach prev/next normalized tokens\n",
        "train = train.sort_values(['sentence_id','token_id'])\n",
        "train['prev_before_norm'] = train.groupby('sentence_id')['before_norm'].shift(1).fillna('')\n",
        "train['next_before_norm'] = train.groupby('sentence_id')['before_norm'].shift(-1).fillna('')\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "groups = train['sentence_id'].values\n",
        "oof_pred = pd.Series(index=train.index, dtype=object)\n",
        "t0 = time.time()\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(train, groups=groups), 1):\n",
        "    tf = time.time()\n",
        "    print(f'Fold {fold}: train {len(tr_idx):,}, valid {len(va_idx):,}')\n",
        "    tr = train.loc[tr_idx, ['before_norm','after','prev_before_norm','next_before_norm']]\n",
        "    tri_map, lb_map, rb_map, uni_map = build_context_maps(tr, min_tri=3, min_bi=5)\n",
        "    # validation slice with context keys\n",
        "    va = train.loc[va_idx, ['before','before_norm','after','prev_before_norm','next_before_norm']]\n",
        "\n",
        "    preds = []\n",
        "    for (_, row) in va.iterrows():\n",
        "        b_raw = row['before']; b = row['before_norm']\n",
        "        prevb = row['prev_before_norm']; nextb = row['next_before_norm']\n",
        "        out = None\n",
        "        if isinstance(b, str):\n",
        "            # Context-aware memorizer cascade: tri -> left bi -> right bi -> uni\n",
        "            tmp = tri_map.get((prevb,b,nextb), None)\n",
        "            if tmp and tmp != '<self>': out = tmp\n",
        "            if out is None:\n",
        "                tmp = lb_map.get((prevb,b), None)\n",
        "                if tmp and tmp != '<self>': out = tmp\n",
        "            if out is None:\n",
        "                tmp = rb_map.get((b,nextb), None)\n",
        "                if tmp and tmp != '<self>': out = tmp\n",
        "            if out is None:\n",
        "                tmp = uni_map.get(b, None)\n",
        "                if tmp and tmp != '<self>': out = tmp\n",
        "            # ELECTRONIC guard\n",
        "            if out is None and url_email_re.search(b):\n",
        "                tmp = uni_map.get(b, None)\n",
        "                out = tmp if (tmp is not None and tmp != '<self>') else b_raw\n",
        "            # TELEPHONE\n",
        "            if out is None and (tel_re_generic.match(b) or tel_re_ru_1.match(b) or tel_re_ru_2.match(b)):\n",
        "                out = speak_digits_ru(b)\n",
        "            # DATE\n",
        "            if out is None:\n",
        "                try:\n",
        "                    out = to_date_ru(b)\n",
        "                except NameError:\n",
        "                    out = None\n",
        "            # ORDINAL\n",
        "            if out is None:\n",
        "                try:\n",
        "                    out = to_ordinal_ru(b)\n",
        "                except NameError:\n",
        "                    out = None\n",
        "            # TIME\n",
        "            if out is None:\n",
        "                try:\n",
        "                    out = to_time_ru(b)\n",
        "                except NameError:\n",
        "                    out = None\n",
        "            # MONEY\n",
        "            if out is None:\n",
        "                out = to_money_ru(b)\n",
        "            # FRACTION\n",
        "            if out is None:\n",
        "                try:\n",
        "                    out = to_fraction_ru(b)\n",
        "                except NameError:\n",
        "                    out = None\n",
        "            # PERCENT\n",
        "            if out is None and '%' in b:\n",
        "                try:\n",
        "                    out = to_percent_ru(b)\n",
        "                except NameError:\n",
        "                    out = None\n",
        "            # DECIMAL guarded\n",
        "            if out is None:\n",
        "                out = to_decimal_ru(b)\n",
        "            # MEASURE\n",
        "            if out is None:\n",
        "                out = to_measure_ru(b)\n",
        "            # CARDINAL\n",
        "            if out is None and int_re.match(b) and not url_email_re.search(b):\n",
        "                try:\n",
        "                    out = to_int_cardinal_ru(b)\n",
        "                except NameError:\n",
        "                    out = None\n",
        "        if out is None: out = b_raw\n",
        "        preds.append(out)\n",
        "    preds = pd.Series(preds, index=va_idx)\n",
        "    oof_pred.loc[va_idx] = preds\n",
        "    acc = (preds.values == train.loc[va_idx, 'after'].values).mean()\n",
        "    print(f'  fold {fold} acc: {acc:.6f} | elapsed {time.time()-tf:.1f}s')\n",
        "\n",
        "overall = (oof_pred.values == train['after'].values).mean()\n",
        "print(f'v5 router OOF overall accuracy: {overall:.6f}')\n",
        "print('Per-class OOF (top 20):')\n",
        "cls_acc = train.assign(pred=oof_pred).groupby('class').apply(lambda df: (df['pred'].values == df['after'].values).mean()).sort_values(ascending=False)\n",
        "print(cls_acc.head(20))\n",
        "print('Done. Total elapsed:', f'{time.time()-t0:.1f}s')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading full train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (9515325, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 1 acc: 0.977857 | elapsed 171.3s\nFold 2: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 2 acc: 0.977781 | elapsed 169.6s\nFold 3: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 3 acc: 0.977824 | elapsed 169.4s\nFold 4: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 4 acc: 0.977869 | elapsed 169.1s\nFold 5: train 7,612,260, valid 1,903,065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  fold 5 acc: 0.977819 | elapsed 167.9s\nv5 router OOF overall accuracy: 0.977830\nPer-class OOF (top 20):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class\nPUNCT         0.999345\nVERBATIM      0.989484\nPLAIN         0.986002\nLETTERS       0.941345\nDIGIT         0.900336\nTIME          0.872244\nCARDINAL      0.869176\nORDINAL       0.849830\nDATE          0.798244\nFRACTION      0.637986\nMEASURE       0.569446\nELECTRONIC    0.418627\nDECIMAL       0.361136\nMONEY         0.223794\nTELEPHONE     0.204724\ndtype: float64\nDone. Total elapsed: 859.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_61/3002954038.py:388: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  cls_acc = train.assign(pred=oof_pred).groupby('class').apply(lambda df: (df['pred'].values == df['after'].values).mean()).sort_values(ascending=False)\n"
          ]
        }
      ]
    },
    {
      "id": "4a13191d-29de-4e0a-b5f3-71b10ceff54b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inference v5: context-aware memorizer (tri/bi/uni) + advanced routing, mirrors Cell 15\n",
        "import re, time, pandas as pd, numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "from num2words import num2words as n2w\n",
        "\n",
        "# Reuse helpers from Cell 15 (assumes already defined in kernel):\n",
        "# normalize_before, to_date_ru, to_time_ru, to_ordinal_ru, to_fraction_ru, to_percent_ru, to_int_cardinal_ru,\n",
        "# speak_digits_ru, to_money_ru, to_decimal_ru, to_measure_ru, plural_form,\n",
        "# regex: url_email_re, tel_re_generic, tel_re_ru_1, tel_re_ru_2, int_re, dec accept/deny inside to_decimal_ru\n",
        "\n",
        "def build_context_maps_full(train_df: pd.DataFrame, min_tri=3, min_bi=5):\n",
        "    # train_df must have before_norm, after, prev_before_norm, next_before_norm\n",
        "    t0 = time.time()\n",
        "    print('Building trigram map...')\n",
        "    tri_counts = train_df.value_counts(['prev_before_norm','before_norm','next_before_norm','after']).reset_index(name='cnt')\n",
        "    tri_counts = tri_counts.sort_values(['prev_before_norm','before_norm','next_before_norm','cnt'], ascending=[True,True,True,False])\n",
        "    tri_counts = tri_counts[tri_counts['cnt']>=min_tri]\n",
        "    tri_map = tri_counts.drop_duplicates(['prev_before_norm','before_norm','next_before_norm']).set_index(['prev_before_norm','before_norm','next_before_norm'])['after'].to_dict()\n",
        "    print(f'  trigram entries: {len(tri_map):,} | elapsed {time.time()-t0:.1f}s'); t1=time.time()\n",
        "    print('Building left-bigram map...')\n",
        "    lb_counts = train_df.value_counts(['prev_before_norm','before_norm','after']).reset_index(name='cnt')\n",
        "    lb_counts = lb_counts.sort_values(['prev_before_norm','before_norm','cnt'], ascending=[True,True,False])\n",
        "    lb_counts = lb_counts[lb_counts['cnt']>=min_bi]\n",
        "    lb_map = lb_counts.drop_duplicates(['prev_before_norm','before_norm']).set_index(['prev_before_norm','before_norm'])['after'].to_dict()\n",
        "    print(f'  left-bigram entries: {len(lb_map):,} | elapsed {time.time()-t1:.1f}s'); t2=time.time()\n",
        "    print('Building right-bigram map...')\n",
        "    rb_counts = train_df.value_counts(['before_norm','next_before_norm','after']).reset_index(name='cnt')\n",
        "    rb_counts = rb_counts.sort_values(['before_norm','next_before_norm','cnt'], ascending=[True,True,False])\n",
        "    rb_counts = rb_counts[rb_counts['cnt']>=min_bi]\n",
        "    rb_map = rb_counts.drop_duplicates(['before_norm','next_before_norm']).set_index(['before_norm','next_before_norm'])['after'].to_dict()\n",
        "    print(f'  right-bigram entries: {len(rb_map):,} | elapsed {time.time()-t2:.1f}s'); t3=time.time()\n",
        "    print('Building unigram map...')\n",
        "    uni_counts = train_df.value_counts(['before_norm','after']).reset_index(name='cnt')\n",
        "    uni_counts = uni_counts.sort_values(['before_norm','cnt'], ascending=[True,False])\n",
        "    uni_map = uni_counts.drop_duplicates(['before_norm']).set_index(['before_norm'])['after'].to_dict()\n",
        "    print(f'  unigram entries: {len(uni_map):,} | elapsed {time.time()-t3:.1f}s')\n",
        "    print('Context maps built. Total elapsed:', f'{time.time()-t0:.1f}s')\n",
        "    return tri_map, lb_map, rb_map, uni_map\n",
        "\n",
        "def route_token_v5(b_raw, b_norm, prevb, nextb, tri_map, lb_map, rb_map, uni_map):\n",
        "    out = None\n",
        "    if isinstance(b_norm, str):\n",
        "        # Context-aware memorizer cascade: tri -> left bi -> right bi -> uni (skip literal '<self>')\n",
        "        tmp = tri_map.get((prevb,b_norm,nextb), None)\n",
        "        if tmp and tmp != '<self>': out = tmp\n",
        "        if out is None:\n",
        "            tmp = lb_map.get((prevb,b_norm), None)\n",
        "            if tmp and tmp != '<self>': out = tmp\n",
        "        if out is None:\n",
        "            tmp = rb_map.get((b_norm,nextb), None)\n",
        "            if tmp and tmp != '<self>': out = tmp\n",
        "        if out is None:\n",
        "            tmp = uni_map.get(b_norm, None)\n",
        "            if tmp and tmp != '<self>': out = tmp\n",
        "        # ELECTRONIC guard (early)\n",
        "        if out is None and url_email_re.search(b_norm):\n",
        "            tmp = uni_map.get(b_norm, None)\n",
        "            return tmp if (tmp is not None and tmp != '<self>') else b_raw\n",
        "        # TELEPHONE\n",
        "        if out is None and (tel_re_generic.match(b_norm) or tel_re_ru_1.match(b_norm) or tel_re_ru_2.match(b_norm)):\n",
        "            out = speak_digits_ru(b_norm)\n",
        "        # DATE\n",
        "        if out is None:\n",
        "            out = to_date_ru(b_norm)\n",
        "        # ORDINAL\n",
        "        if out is None:\n",
        "            out = to_ordinal_ru(b_norm)\n",
        "        # TIME\n",
        "        if out is None:\n",
        "            out = to_time_ru(b_norm)\n",
        "        # MONEY\n",
        "        if out is None:\n",
        "            out = to_money_ru(b_norm)\n",
        "        # FRACTION\n",
        "        if out is None:\n",
        "            out = to_fraction_ru(b_norm)\n",
        "        # PERCENT\n",
        "        if out is None and '%' in b_norm:\n",
        "            out = to_percent_ru(b_norm)\n",
        "        # DECIMAL (guarded within function)\n",
        "        if out is None:\n",
        "            out = to_decimal_ru(b_norm)\n",
        "        # MEASURE\n",
        "        if out is None:\n",
        "            out = to_measure_ru(b_norm)\n",
        "        # CARDINAL\n",
        "        if out is None and int_re.match(b_norm) and not url_email_re.search(b_norm):\n",
        "            out = to_int_cardinal_ru(b_norm)\n",
        "    return out if out is not None else b_raw\n",
        "\n",
        "train_path = 'ru_train.csv.zip'\n",
        "test_path = 'ru_test_2.csv.zip'\n",
        "\n",
        "t0 = time.time()\n",
        "print('Loading train for context maps...')\n",
        "train = pd.read_csv(train_path, compression='zip', usecols=['sentence_id','token_id','before','after'])\n",
        "train['before_norm'] = train['before'].map(normalize_before)\n",
        "train = train.sort_values(['sentence_id','token_id'])\n",
        "train['prev_before_norm'] = train.groupby('sentence_id')['before_norm'].shift(1).fillna('')\n",
        "train['next_before_norm'] = train.groupby('sentence_id')['before_norm'].shift(-1).fillna('')\n",
        "print('Train ready:', train.shape)\n",
        "\n",
        "tri_map, lb_map, rb_map, uni_map = build_context_maps_full(train[['prev_before_norm','before_norm','next_before_norm','after']], min_tri=3, min_bi=5)\n",
        "del train\n",
        "\n",
        "print('Loading test...')\n",
        "test = pd.read_csv(test_path, compression='zip')\n",
        "test = test.sort_values(['sentence_id','token_id'])\n",
        "test['id'] = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)\n",
        "test['before_norm'] = test['before'].map(normalize_before)\n",
        "test['prev_before_norm'] = test.groupby('sentence_id')['before_norm'].shift(1).fillna('')\n",
        "test['next_before_norm'] = test.groupby('sentence_id')['before_norm'].shift(-1).fillna('')\n",
        "print('Test ready:', test.shape)\n",
        "\n",
        "pred = []\n",
        "t1 = time.time()\n",
        "for i, row in enumerate(test.itertuples(index=False), 1):\n",
        "    b_raw = row.before\n",
        "    b = row.before_norm\n",
        "    prevb = row.prev_before_norm\n",
        "    nextb = row.next_before_norm\n",
        "    out = route_token_v5(b_raw, b, prevb, nextb, tri_map, lb_map, rb_map, uni_map)\n",
        "    pred.append(out)\n",
        "    if i % 200000 == 0:\n",
        "        print(f'  processed {i:,}/{len(test)} | elapsed {time.time()-t1:.1f}s', flush=True)\n",
        "\n",
        "submission = pd.DataFrame({'id': test['id'].values, 'after': pred})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with', len(submission), 'rows. Total elapsed:', f'{time.time()-t0:.1f}s')\n",
        "print(submission.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train for context maps...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}