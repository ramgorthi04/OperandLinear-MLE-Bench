{
  "cells": [
    {
      "id": "06bb6859-2da2-4586-9124-d807d9d0952e",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RANZCR CLiP - Medaled Solution Notebook (Revised Plan)\n",
        "\n",
        "## Objective\n",
        "- Metric: AUC-ROC\n",
        "- Output: submission.csv\n",
        "- Targets: Bronze \u2265 0.97090, Silver \u2265 0.97152, Gold \u2265 0.97357\n",
        "\n",
        "## 0) Reproducibility & Ops\n",
        "- Global seeds: numpy/torch/random; torch.backends.cudnn.deterministic=True; benchmark=False.\n",
        "- Capture env: python/pkg versions, CUDA, timm/albumentations versions.\n",
        "- Experiment logging (structured): run_id, timestamp, data versions (file hashes, counts), code git/hash, config (model, size, aug, loss, opt, lr schedule, epochs, batch, folds, CV splitter, group key), CV scores (macro AUC, per-label AUC), OOF paths, checkpoints, inference/TTA config, submission path.\n",
        "- Notebook backups before major changes.\n",
        "\n",
        "## 1) Data Inventory & Strategy\n",
        "- Programmatically list files/dirs; read CSV schemas:\n",
        "  - train.csv (verify id/key cols; likely StudyInstanceUID/ImageID/PatientID + 11 targets).\n",
        "  - sample_submission.csv (defines exact target column order).\n",
        "  - train_annotations.csv (if present): bounding boxes per device/region.\n",
        "  - train/ and test/ images (jpg/png).\n",
        "- Verify grouping key: prefer StudyInstanceUID; fallback PatientID. Use programmatic checks (n unique per key).\n",
        "- If annotations exist:\n",
        "  - Generate auxiliary supervision: per-class heatmaps from boxes.\n",
        "  - ROI crops: create tight crops around devices (ETT/NGT/CVC/SGT) to feed a small head or as multi-scale inputs.\n",
        "  - Attention priors: blur outside union of boxes; or add extra channel with gaussian masks.\n",
        "- Medical preprocessing:\n",
        "  - Use x-ray friendly normalization (single-channel or 3x replicated with chest radiograph means) and histogram equalization/CLAHE (light).\n",
        "  - Optional lung/mediastinum segmentation (pretrained U-Net on CXR) to mask/weight regions; plan as an experiment if time allows.\n",
        "  - Prefer backbones pretrained on chest x-ray datasets (CheXpert/MIMIC/ChestX-ray14) when available in timm or via public weights.\n",
        "\n",
        "## 2) Labels & Multi-Label Methodology\n",
        "- Targets are multi-label with structured groups (e.g., ETT_Normal/Borderline/Abnormal mutually exclusive; same for NGT, CVC, SGT).\n",
        "- Modeling strategy:\n",
        "  - Multi-head, multi-class for each device group (softmax heads) plus binary heads for presence-type labels if any; map softmax outputs back to per-label probabilities aligned with sample_submission columns.\n",
        "  - Parallel baseline: pure multi-label BCE/ASL head for comparison; ensemble both.\n",
        "- Losses:\n",
        "  - Start with BCEWithLogitsLoss with per-label pos_weight.\n",
        "  - Try Asymmetric Focal Loss (ASL) and Focal BCE.\n",
        "  - Label smoothing 0.01\u20130.05 for softmax heads.\n",
        "- Thresholding:\n",
        "  - Optimize per-label thresholds on OOF via AUC-friendly approach (Youden/J-statistic or maximize macro F2 proxy; for AUC reporting thresholds not needed, but for stacking/meta features and pseudo-labels they are).\n",
        "\n",
        "## 3) Cross-Validation & Splits\n",
        "- Grouped CV to prevent leakage: Group = StudyInstanceUID (or PatientID if confirmed).\n",
        "- Stratification:\n",
        "  - Use Iterative Stratification for multi-label balance at the group level. If libraries lack direct grouped-iterative split, perform iterative stratification on group-aggregated labels, then map groups to folds.\n",
        "- Plan: 5 folds (fallback 3 if compute-bound). Track macro AUC and per-label AUC. Save OOF.\n",
        "\n",
        "## 4) Baseline \u2192 Strong Models\n",
        "- Image sizes: start 384\u2013512; escalate to 640\u2013768 where feasible.\n",
        "- Augmentations (x-ray safe): RandomResizedCrop/Resize+CenterCrop, HorizontalFlip, small ShiftScaleRotate, RandomBrightnessContrast (\u00b10.1\u20130.2), CLAHE (p\u22640.2), Cutout/CoarseDropout small p, avoid heavy color jitter.\n",
        "- Optimizer & schedule: AdamW (lr\u22482e-4, wd\u22481e-4), cosine with warmup; EMA; AMP; gradient accumulation if needed.\n",
        "- Backbones (architectural diversity):\n",
        "  1) ConvNeXt-Tiny/Base (ImageNet then try CXR-pretrained if available).\n",
        "  2) EfficientNet-B3/B4 (and/or tf_efficientnet variants).\n",
        "  3) SE-ResNeXt50_32x4d.\n",
        "  4) ViT/Swin-Tiny/Small (transformers for diversity).\n",
        "- Heads:\n",
        "  - Multi-head softmax per device group + optional shared features; auxiliary BCE head for global presence if applicable.\n",
        "  - Calibrate logits with temperature scaling on OOF if beneficial.\n",
        "- Training: 5\u20138 epochs for baseline; extend to 12\u201320 at higher res. Early stopping on CV.\n",
        "\n",
        "## 5) Annotations-Driven Experiments (if boxes exist)\n",
        "- Two-stage pipeline:\n",
        "  - Stage A: Lightweight detector/heatmap regressor to localize devices (supervised by boxes).\n",
        "  - Stage B: Classifier consuming global image + ROI crops/heatmaps (concat channels or late fusion).\n",
        "- Region prior augmentation: mask-downweight non-ROI; or multi-crop inference on detected ROIs.\n",
        "\n",
        "## 6) Inference & Ensembling\n",
        "- Fold-averaging; TTA (hflip, multi-scale 0.9x/1.0x/1.1x center crop).\n",
        "- Model ensembling:\n",
        "  - Diverse backbones (CNN + ViT) and loss variants (BCE/ASL) and multi-head vs BCE-only.\n",
        "  - Weighted average by CV AUC; then stacking: XGBoost/LightGBM meta-learner on OOF probabilities (features = per-model per-label probs, plus simple interactions).\n",
        "- Calibration: temperature scaling or Platt per label if it helps OOF AUC stability.\n",
        "\n",
        "## 7) Advanced Boosts (time-permitting)\n",
        "- Pseudo-labeling: 1\u20132 rounds with high-confidence test predictions (label-wise thresholds), retrain last 3\u20135 epochs.\n",
        "- SWA after convergence; EMA maintained throughout.\n",
        "- Multi-resolution training/inference.\n",
        "\n",
        "## 8) Checkpoints & Audits\n",
        "- After this revised plan (require approval).\n",
        "- After data loading + inventory validation + EDA.\n",
        "- After baseline CV run and first submission.\n",
        "- After multi-head models and ensembling.\n",
        "- Before final submission.\n",
        "\n",
        "## 9) Experiment Log Template\n",
        "- run_id: \n",
        "- timestamp: \n",
        "- data_inventory: {train.csv hash/rows, sample_submission hash/rows, annotations present?: bool + hash/rows, n_train_imgs, n_test_imgs}\n",
        "- grouping_key: \n",
        "- config: {model, size, aug, loss, opt, lr, schedule, epochs, batch, folds, TTA, heads, pos_weight, seed}\n",
        "- cv_scores: {macro_auc, per_label_auc: dict}\n",
        "- oof_path: \n",
        "- checkpoints: [paths]\n",
        "- inference_config: {tta, scales}\n",
        "- submission_path: \n",
        "- notes: \n",
        "\n",
        "## 10) Minimal First Milestone\n",
        "- Build grouped multi-label CV with iterative strat on groups; convnext_tiny 384\u2013512, BCE baseline; generate OOF + submission; audit and submit.\n",
        "- Then add multi-head softmax per device and ensemble with baseline; add ViT small; stack.\n",
        "\n",
        "## Next\n",
        "- Implement data inventory code to verify files, schemas, grouping key, and whether annotations exist. Submit for audit after inventory/EDA."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "42609b65-e833-4cfd-9e09-70a1b9bfefc4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, json, hashlib, glob, platform, importlib\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def file_md5(path, chunk_size=1<<20):\n",
        "    m = hashlib.md5()\n",
        "    with open(path, 'rb') as f:\n",
        "        while True:\n",
        "            data = f.read(chunk_size)\n",
        "            if not data:\n",
        "                break\n",
        "            m.update(data)\n",
        "    return m.hexdigest()\n",
        "\n",
        "root = '.'\n",
        "paths = {\n",
        "    'train_dir': os.path.join(root, 'train'),\n",
        "    'test_dir': os.path.join(root, 'test'),\n",
        "    'train_csv': os.path.join(root, 'train.csv'),\n",
        "    'sample_sub': os.path.join(root, 'sample_submission.csv'),\n",
        "    'train_ann': os.path.join(root, 'train_annotations.csv'),\n",
        "}\n",
        "\n",
        "print('=== Environment ===')\n",
        "print({'python': sys.version.split()[0], 'platform': platform.platform()})\n",
        "for pkg in ['pandas','numpy','timm','albumentations','torch','iterstrat']:\n",
        "    try:\n",
        "        mod = importlib.import_module(pkg)\n",
        "        ver = getattr(mod, '__version__', 'n/a')\n",
        "        print(f'{pkg}: {ver}')\n",
        "    except Exception as e:\n",
        "        print(f'{pkg}: not available')\n",
        "\n",
        "print('\\n=== File inventory ===')\n",
        "for k,v in paths.items():\n",
        "    print(k, os.path.exists(v), v)\n",
        "\n",
        "train_imgs = sorted(glob.glob(os.path.join(paths['train_dir'], '*.jpg')))\n",
        "test_imgs = sorted(glob.glob(os.path.join(paths['test_dir'], '*.jpg')))\n",
        "print('n_train_imgs:', len(train_imgs))\n",
        "print('n_test_imgs:', len(test_imgs))\n",
        "\n",
        "inv = {}\n",
        "def safe_read_csv(p):\n",
        "    try:\n",
        "        df = pd.read_csv(p)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print('Failed to read', p, e)\n",
        "        return None\n",
        "\n",
        "train_df = safe_read_csv(paths['train_csv'])\n",
        "sub_df = safe_read_csv(paths['sample_sub'])\n",
        "ann_df = safe_read_csv(paths['train_ann']) if os.path.exists(paths['train_ann']) else None\n",
        "\n",
        "if train_df is not None:\n",
        "    inv['train_csv'] = {'rows': len(train_df), 'cols': train_df.columns.tolist(), 'hash': file_md5(paths['train_csv'])}\n",
        "    print('\\n=== train.csv ===')\n",
        "    print('shape:', train_df.shape)\n",
        "    print('columns:', train_df.columns.tolist())\n",
        "    print('head:\\n', train_df.head(3))\n",
        "\n",
        "if sub_df is not None:\n",
        "    inv['sample_submission'] = {'rows': len(sub_df), 'cols': sub_df.columns.tolist(), 'hash': file_md5(paths['sample_sub'])}\n",
        "    print('\\n=== sample_submission.csv ===')\n",
        "    print('shape:', sub_df.shape)\n",
        "    print('columns:', sub_df.columns.tolist())\n",
        "    print('head:\\n', sub_df.head(3))\n",
        "\n",
        "if ann_df is not None:\n",
        "    inv['train_annotations'] = {'rows': len(ann_df), 'cols': ann_df.columns.tolist(), 'hash': file_md5(paths['train_ann'])}\n",
        "    print('\\n=== train_annotations.csv ===')\n",
        "    print('shape:', ann_df.shape)\n",
        "    print('columns:', ann_df.columns.tolist())\n",
        "    print('head:\\n', ann_df.head(3))\n",
        "else:\n",
        "    inv['train_annotations'] = {'present': False}\n",
        "\n",
        "# Determine ID/grouping key candidates\n",
        "id_cols_candidates = []\n",
        "if train_df is not None:\n",
        "    for c in train_df.columns:\n",
        "        if train_df[c].dtype == object:\n",
        "            uniq = train_df[c].nunique()\n",
        "            if uniq == len(train_df) or 'Study' in c or 'UID' in c or 'Patient' in c or 'Id' in c.lower():\n",
        "                id_cols_candidates.append((c, uniq))\n",
        "    print('\\nID/grouping key candidates (column, nunique):', id_cols_candidates)\n",
        "\n",
        "group_key = None\n",
        "if train_df is not None:\n",
        "    if 'StudyInstanceUID' in train_df.columns:\n",
        "        group_key = 'StudyInstanceUID'\n",
        "    elif 'PatientID' in train_df.columns:\n",
        "        group_key = 'PatientID';\n",
        "    else:\n",
        "        # fallback: choose the object column with highest nunique that looks like an ID\n",
        "        if id_cols_candidates:\n",
        "            group_key = sorted(id_cols_candidates, key=lambda x: (-x[1], x[0]))[0][0]\n",
        "    print('Selected grouping key:', group_key)\n",
        "    # Validate one-to-many PatientID->Study if both exist\n",
        "    if 'PatientID' in train_df.columns and 'StudyInstanceUID' in train_df.columns:\n",
        "        grp_counts = train_df.groupby('PatientID')['StudyInstanceUID'].nunique().describe()\n",
        "        print('PatientID -> unique StudyInstanceUID stats:', grp_counts.to_dict())\n",
        "\n",
        "# Derive targets from sample_submission column order\n",
        "targets = []\n",
        "id_col_in_sub = None\n",
        "if sub_df is not None:\n",
        "    sub_cols = sub_df.columns.tolist()\n",
        "    # assume first column is ID\n",
        "    id_col_in_sub = sub_cols[0]\n",
        "    targets = sub_cols[1:]\n",
        "    print('\\nID column in sample_submission:', id_col_in_sub)\n",
        "    print('Number of target columns:', len(targets))\n",
        "    print('Targets (ordered):', targets)\n",
        "\n",
        "# Verify that train has all target columns\n",
        "if train_df is not None and targets:\n",
        "    missing = [c for c in targets if c not in train_df.columns]\n",
        "    extra = [c for c in train_df.columns if c not in ([group_key] if group_key else []) + targets]\n",
        "    print('\\nTarget columns missing in train:', missing)\n",
        "    print('Non-target extras in train (first 20):', extra[:20])\n",
        "\n",
        "# Label groups discovery (prefix before underscore)\n",
        "label_groups = {}\n",
        "if targets:\n",
        "    for c in targets:\n",
        "        if '_' in c:\n",
        "            pref = c.split('_', 1)[0]\n",
        "        else:\n",
        "            pref = c\n",
        "        label_groups.setdefault(pref, []).append(c)\n",
        "    print('\\nDiscovered label groups:')\n",
        "    for k,v in label_groups.items():\n",
        "        print(f'  {k}: {v}')\n",
        "\n",
        "# Prevalence statistics\n",
        "if train_df is not None and targets:\n",
        "    prev = train_df[targets].mean().sort_values(ascending=False)\n",
        "    print('\\nLabel prevalence (mean):')\n",
        "    print(prev)\n",
        "\n",
        "# Image-ID alignment quick check\n",
        "def stem(p):\n",
        "    return os.path.splitext(os.path.basename(p))[0]\n",
        "train_img_stems = set(map(stem, train_imgs))\n",
        "test_img_stems = set(map(stem, test_imgs))\n",
        "if train_df is not None:\n",
        "    candidate_id_col = None\n",
        "    # pick between group_key and id_col_in_sub\n",
        "    for candidate in [c for c,_ in id_cols_candidates] + ([group_key] if group_key else []):\n",
        "        if candidate and candidate in train_df.columns:\n",
        "            # Heuristic: contains dot-separated UID style\n",
        "            sample_vals = train_df[candidate].astype(str).head(3).tolist()\n",
        "            looks_like_uid = any('.' in s for s in sample_vals)\n",
        "            if looks_like_uid:\n",
        "                candidate_id_col = candidate\n",
        "                break\n",
        "    if candidate_id_col:\n",
        "        train_uid_in_images = train_df[candidate_id_col].astype(str).isin(train_img_stems).mean()\n",
        "        print(f\"\\nProportion of train {candidate_id_col} matching image stems:\", float(train_uid_in_images))\n",
        "\n",
        "# Hashes summary\n",
        "if os.path.exists(paths['train_csv']):\n",
        "    print('\\nMD5 train.csv:', file_md5(paths['train_csv']))\n",
        "if os.path.exists(paths['sample_sub']):\n",
        "    print('MD5 sample_submission.csv:', file_md5(paths['sample_sub']))\n",
        "if os.path.exists(paths['train_ann']):\n",
        "    print('MD5 train_annotations.csv:', file_md5(paths['train_ann']))\n",
        "\n",
        "# Structured log snapshot\n",
        "snapshot = {\n",
        "    'timestamp': datetime.utcnow().isoformat() + 'Z',\n",
        "    'data_inventory': {\n",
        "        'train_csv': inv.get('train_csv', {}),\n",
        "        'sample_submission': inv.get('sample_submission', {}),\n",
        "        'train_annotations': inv.get('train_annotations', {}),\n",
        "        'n_train_imgs': len(train_imgs),\n",
        "        'n_test_imgs': len(test_imgs)\n",
        "    },\n",
        "    'grouping_key': group_key,\n",
        "    'id_col_in_submission': id_col_in_sub,\n",
        "    'targets': targets,\n",
        "    'label_groups': label_groups,\n",
        "}\n",
        "print('\\n=== INVENTORY SNAPSHOT ===')\n",
        "print(json.dumps(snapshot, indent=2)[:2000])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Environment ===\n{'python': '3.11.0rc1', 'platform': 'Linux-6.1.0-37-cloud-amd64-x86_64-with-glibc2.35'}\npandas: 2.2.2\nnumpy: 1.26.4\ntimm: not available\nalbumentations: not available\ntorch: not available\niterstrat: not available\n\n=== File inventory ===\ntrain_dir True ./train\ntest_dir True ./test\ntrain_csv True ./train.csv\nsample_sub True ./sample_submission.csv\ntrain_ann True ./train_annotations.csv\nn_train_imgs: 27074\nn_test_imgs: 3009\n\n=== train.csv ===\nshape: (27074, 13)\ncolumns: ['StudyInstanceUID', 'ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal', 'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal', 'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal', 'Swan Ganz Catheter Present', 'PatientID']\nhead:\n                                     StudyInstanceUID  ETT - Abnormal  \\\n0  1.2.826.0.1.3680043.8.498.93452244702936724316...               0   \n1  1.2.826.0.1.3680043.8.498.93702111677661381919...               0   \n2  1.2.826.0.1.3680043.8.498.16433919399532177466...               0   \n\n   ETT - Borderline  ETT - Normal  NGT - Abnormal  NGT - Borderline  \\\n0                 0             0               0                 0   \n1                 0             0               0                 0   \n2                 0             0               0                 0   \n\n   NGT - Incompletely Imaged  NGT - Normal  CVC - Abnormal  CVC - Borderline  \\\n0                          0             0               1                 0   \n1                          0             0               0                 1   \n2                          0             0               0                 0   \n\n   CVC - Normal  Swan Ganz Catheter Present  PatientID  \n0             0                           0  258689f05  \n1             0                           0  6df02c500  \n2             1                           0  e83db5695  \n\n=== sample_submission.csv ===\nshape: (3009, 10)\ncolumns: ['StudyInstanceUID', 'ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal', 'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal', 'CVC - Abnormal', 'CVC - Borderline']\nhead:\n                                     StudyInstanceUID  ETT - Abnormal  \\\n0  1.2.826.0.1.3680043.8.498.25512976433640891933...               0   \n1  1.2.826.0.1.3680043.8.498.24449897997512078380...               0   \n2  1.2.826.0.1.3680043.8.498.38485493636649999035...               0   \n\n   ETT - Borderline  ETT - Normal  NGT - Abnormal  NGT - Borderline  \\\n0                 0             0               0                 0   \n1                 0             0               0                 0   \n2                 0             0               0                 0   \n\n   NGT - Incompletely Imaged  NGT - Normal  CVC - Abnormal  CVC - Borderline  \n0                          0             0               0                 0  \n1                          0             0               0                 0  \n2                          0             0               0                 0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=== train_annotations.csv ===\nshape: (16261, 3)\ncolumns: ['StudyInstanceUID', 'label', 'data']\nhead:\n                                     StudyInstanceUID             label  \\\n0  1.2.826.0.1.3680043.8.498.12616281126973421762...      CVC - Normal   \n1  1.2.826.0.1.3680043.8.498.12616281126973421762...      CVC - Normal   \n2  1.2.826.0.1.3680043.8.498.72921907356394389969...  CVC - Borderline   \n\n                                                data  \n0  [[1487, 1279], [1477, 1168], [1472, 1052], [14...  \n1  [[1328, 7], [1347, 101], [1383, 193], [1400, 2...  \n2  [[801, 1207], [812, 1112], [823, 1023], [842, ...  \n\nID/grouping key candidates (column, nunique): [('StudyInstanceUID', 27074), ('PatientID', 3202)]\nSelected grouping key: StudyInstanceUID\nPatientID -> unique StudyInstanceUID stats: {'count': 3202.0, 'mean': 8.455340412242348, 'std': 11.428193072425781, 'min': 1.0, '25%': 2.0, '50%': 5.0, '75%': 9.75, 'max': 154.0}\n\nID column in sample_submission: StudyInstanceUID\nNumber of target columns: 9\nTargets (ordered): ['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal', 'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal', 'CVC - Abnormal', 'CVC - Borderline']\n\nTarget columns missing in train: []\nNon-target extras in train (first 20): ['CVC - Normal', 'Swan Ganz Catheter Present', 'PatientID']\n\nDiscovered label groups:\n  ETT - Abnormal: ['ETT - Abnormal']\n  ETT - Borderline: ['ETT - Borderline']\n  ETT - Normal: ['ETT - Normal']\n  NGT - Abnormal: ['NGT - Abnormal']\n  NGT - Borderline: ['NGT - Borderline']\n  NGT - Incompletely Imaged: ['NGT - Incompletely Imaged']\n  NGT - Normal: ['NGT - Normal']\n  CVC - Abnormal: ['CVC - Abnormal']\n  CVC - Borderline: ['CVC - Borderline']\n\nLabel prevalence (mean):\nCVC - Borderline             0.281894\nETT - Normal                 0.240194\nNGT - Normal                 0.159193\nCVC - Abnormal               0.106929\nNGT - Incompletely Imaged    0.090604\nETT - Borderline             0.037933\nNGT - Borderline             0.017988\nNGT - Abnormal               0.009382\nETT - Abnormal               0.002807\ndtype: float64\n\nProportion of train StudyInstanceUID matching image stems: 1.0\n\nMD5 train.csv: 620fa6759729f96539ed66ae860a84f9\nMD5 sample_submission.csv: fade90160bce8a12138711b20fa89584\nMD5 train_annotations.csv: 1929aab17c1206719cc7edf6d7e9e57b\n\n=== INVENTORY SNAPSHOT ===\n{\n  \"timestamp\": \"2025-08-24T00:16:37.596603Z\",\n  \"data_inventory\": {\n    \"train_csv\": {\n      \"rows\": 27074,\n      \"cols\": [\n        \"StudyInstanceUID\",\n        \"ETT - Abnormal\",\n        \"ETT - Borderline\",\n        \"ETT - Normal\",\n        \"NGT - Abnormal\",\n        \"NGT - Borderline\",\n        \"NGT - Incompletely Imaged\",\n        \"NGT - Normal\",\n        \"CVC - Abnormal\",\n        \"CVC - Borderline\",\n        \"CVC - Normal\",\n        \"Swan Ganz Catheter Present\",\n        \"PatientID\"\n      ],\n      \"hash\": \"620fa6759729f96539ed66ae860a84f9\"\n    },\n    \"sample_submission\": {\n      \"rows\": 3009,\n      \"cols\": [\n        \"StudyInstanceUID\",\n        \"ETT - Abnormal\",\n        \"ETT - Borderline\",\n        \"ETT - Normal\",\n        \"NGT - Abnormal\",\n        \"NGT - Borderline\",\n        \"NGT - Incompletely Imaged\",\n        \"NGT - Normal\",\n        \"CVC - Abnormal\",\n        \"CVC - Borderline\"\n      ],\n      \"hash\": \"fade90160bce8a12138711b20fa89584\"\n    },\n    \"train_annotations\": {\n      \"rows\": 16261,\n      \"cols\": [\n        \"StudyInstanceUID\",\n        \"label\",\n        \"data\"\n      ],\n      \"hash\": \"1929aab17c1206719cc7edf6d7e9e57b\"\n    },\n    \"n_train_imgs\": 27074,\n    \"n_test_imgs\": 3009\n  },\n  \"grouping_key\": \"StudyInstanceUID\",\n  \"id_col_in_submission\": \"StudyInstanceUID\",\n  \"targets\": [\n    \"ETT - Abnormal\",\n    \"ETT - Borderline\",\n    \"ETT - Normal\",\n    \"NGT - Abnormal\",\n    \"NGT - Borderline\",\n    \"NGT - Incompletely Imaged\",\n    \"NGT - Normal\",\n    \"CVC - Abnormal\",\n    \"CVC - Borderline\"\n  ],\n  \"label_groups\": {\n    \"ETT - Abnormal\": [\n      \"ETT - Abnormal\"\n    ],\n    \"ETT - Borderline\": [\n      \"ETT - Borderline\"\n    ],\n    \"ETT - Normal\": [\n      \"ETT - Normal\"\n    ],\n    \"NGT - Abnormal\": [\n      \"NGT - Abnormal\"\n    ],\n    \"NGT - Borderline\": [\n      \"NGT - Borderline\"\n    ],\n    \"NGT - Incompletely Imaged\": [\n      \"NGT - Incompletely Imaged\"\n    ],\n    \"NGT - Normal\": [\n      \"NGT - Normal\"\n    ],\n    \"CVC - Abnormal\": [\n      \"CVC - Abnormal\"\n    ],\n    \"CVC - Border\n"
          ]
        }
      ]
    },
    {
      "id": "0a2fd0ec-0c86-4e5e-b0be-d0aea798c44a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fixes per audit: correct grouping key to PatientID, correct label grouping parsing on ' - ', and validate mutual exclusivity.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "\n",
        "# Reuse existing train_df, sub_df, targets from previous cell; reload if missing\n",
        "if 'train_df' not in globals() or train_df is None:\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "if 'sub_df' not in globals() or sub_df is None:\n",
        "    sub_df = pd.read_csv('sample_submission.csv')\n",
        "if 'targets' not in globals() or not targets:\n",
        "    sub_cols = sub_df.columns.tolist()\n",
        "    id_col_in_sub = sub_cols[0]\n",
        "    targets = sub_cols[1:]\n",
        "\n",
        "print('--- Correct Grouping Key Selection ---')\n",
        "n_rows = len(train_df)\n",
        "n_studies = train_df['StudyInstanceUID'].nunique() if 'StudyInstanceUID' in train_df.columns else None\n",
        "n_patients = train_df['PatientID'].nunique() if 'PatientID' in train_df.columns else None\n",
        "group_key = 'PatientID' if 'PatientID' in train_df.columns else 'StudyInstanceUID'\n",
        "print({'n_rows': n_rows, 'n_unique_StudyInstanceUID': n_studies, 'n_unique_PatientID': n_patients, 'selected_group_key': group_key})\n",
        "if 'PatientID' in train_df.columns and 'StudyInstanceUID' in train_df.columns:\n",
        "    stats = train_df.groupby('PatientID')['StudyInstanceUID'].nunique().describe().to_dict()\n",
        "    print('PatientID -> unique StudyInstanceUID stats:', stats)\n",
        "    print('Justification: Many-to-one PatientID->multiple studies observed; thus group by PatientID to avoid leakage.')\n",
        "\n",
        "print('\\n--- Correct Label Group Parsing on \" - \" ---')\n",
        "def parse_group(label):\n",
        "    # Expect pattern like 'ETT - Normal'\n",
        "    if ' - ' in label:\n",
        "        return label.split(' - ', 1)[0].strip()\n",
        "    return label.strip()\n",
        "\n",
        "label_groups = {}\n",
        "for c in targets:\n",
        "    g = parse_group(c)\n",
        "    label_groups.setdefault(g, []).append(c)\n",
        "print('Label groups (from submission targets):')\n",
        "pprint(label_groups)\n",
        "\n",
        "print('\\n--- Mutual Exclusivity Validation (per device group) ---')\n",
        "exclusivity_report = {}\n",
        "if set(targets).issubset(set(train_df.columns)):\n",
        "    for g, cols in label_groups.items():\n",
        "        if len(cols) >= 2:\n",
        "            s = train_df[cols].sum(axis=1)\n",
        "            ok = (s <= 1).mean()\n",
        "            violations = int((s > 1).sum())\n",
        "            exclusivity_report[g] = {'n_cols': len(cols), 'ok_prop': float(ok), 'violations': violations}\n",
        "        else:\n",
        "            exclusivity_report[g] = {'n_cols': len(cols), 'note': 'single-column group; exclusivity not applicable'}\n",
        "pprint(exclusivity_report)\n",
        "\n",
        "print('\\n--- Extra labels in train (not in submission) ---')\n",
        "extra_cols = [c for c in train_df.columns if c not in ['StudyInstanceUID','PatientID'] + targets]\n",
        "print('Extra train columns:', extra_cols)\n",
        "extras_stats = {c: float(train_df[c].mean()) for c in extra_cols if train_df[c].dtype != object}\n",
        "print('Extra label prevalence:', extras_stats)\n",
        "\n",
        "print('\\n--- Label Correlation (targets) ---')\n",
        "corr = train_df[targets].corr()\n",
        "print(corr.round(3))\n",
        "\n",
        "# Snapshot update for logging\n",
        "snapshot_update = {\n",
        "    'final_grouping_key': group_key,\n",
        "    'label_groups': label_groups,\n",
        "    'exclusivity_report': exclusivity_report,\n",
        "    'notes': {\n",
        "        'iterative_stratification_planned_version': 'iterative-stratification==0.1.7 (to be installed)',\n",
        "        'reason_group_by_patient': 'Prevent leakage across multiple studies per patient',\n",
        "        'extras_handling': 'CVC - Normal and Swan Ganz Catheter Present reserved for auxiliary/heads later'\n",
        "    }\n",
        "}\n",
        "print('\\n=== UPDATED SNAPSHOT ===')\n",
        "pprint(snapshot_update)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Correct Grouping Key Selection ---\n{'n_rows': 27074, 'n_unique_StudyInstanceUID': 27074, 'n_unique_PatientID': 3202, 'selected_group_key': 'PatientID'}\nPatientID -> unique StudyInstanceUID stats: {'count': 3202.0, 'mean': 8.455340412242348, 'std': 11.428193072425781, 'min': 1.0, '25%': 2.0, '50%': 5.0, '75%': 9.75, 'max': 154.0}\nJustification: Many-to-one PatientID->multiple studies observed; thus group by PatientID to avoid leakage.\n\n--- Correct Label Group Parsing on \" - \" ---\nLabel groups (from submission targets):\n{'CVC': ['CVC - Abnormal', 'CVC - Borderline'],\n 'ETT': ['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal'],\n 'NGT': ['NGT - Abnormal',\n         'NGT - Borderline',\n         'NGT - Incompletely Imaged',\n         'NGT - Normal']}\n\n--- Mutual Exclusivity Validation (per device group) ---\n{'CVC': {'n_cols': 2, 'ok_prop': 0.9872940828839477, 'violations': 344},\n 'ETT': {'n_cols': 3, 'ok_prop': 1.0, 'violations': 0},\n 'NGT': {'n_cols': 4, 'ok_prop': 0.9984856319716333, 'violations': 41}}\n\n--- Extra labels in train (not in submission) ---\nExtra train columns: ['CVC - Normal', 'Swan Ganz Catheter Present']\nExtra label prevalence: {'CVC - Normal': 0.7081332643864963, 'Swan Ganz Catheter Present': 0.027295560316170496}\n\n--- Label Correlation (targets) ---\n                           ETT - Abnormal  ETT - Borderline  ETT - Normal  \\\nETT - Abnormal                      1.000            -0.011        -0.030   \nETT - Borderline                   -0.011             1.000        -0.112   \nETT - Normal                       -0.030            -0.112         1.000   \nNGT - Abnormal                      0.009             0.039         0.073   \nNGT - Borderline                    0.003             0.031         0.116   \nNGT - Incompletely Imaged           0.000             0.126         0.379   \nNGT - Normal                        0.049             0.201         0.430   \nCVC - Abnormal                      0.006             0.015        -0.007   \nCVC - Borderline                    0.006             0.034         0.056   \n\n                           NGT - Abnormal  NGT - Borderline  \\\nETT - Abnormal                      0.009             0.003   \nETT - Borderline                    0.039             0.031   \nETT - Normal                        0.073             0.116   \nNGT - Abnormal                      1.000            -0.010   \nNGT - Borderline                   -0.010             1.000   \nNGT - Incompletely Imaged          -0.019            -0.038   \nNGT - Normal                       -0.035            -0.049   \nCVC - Abnormal                     -0.008            -0.001   \nCVC - Borderline                    0.007             0.000   \n\n                           NGT - Incompletely Imaged  NGT - Normal  \\\nETT - Abnormal                                 0.000         0.049   \nETT - Borderline                               0.126         0.201   \nETT - Normal                                   0.379         0.430   \nNGT - Abnormal                                -0.019        -0.035   \nNGT - Borderline                              -0.038        -0.049   \nNGT - Incompletely Imaged                      1.000        -0.135   \nNGT - Normal                                  -0.135         1.000   \nCVC - Abnormal                                -0.004         0.011   \nCVC - Borderline                               0.033         0.050   \n\n                           CVC - Abnormal  CVC - Borderline  \nETT - Abnormal                      0.006             0.006  \nETT - Borderline                    0.015             0.034  \nETT - Normal                       -0.007             0.056  \nNGT - Abnormal                     -0.008             0.007  \nNGT - Borderline                   -0.001             0.000  \nNGT - Incompletely Imaged          -0.004             0.033  \nNGT - Normal                        0.011             0.050  \nCVC - Abnormal                      1.000            -0.125  \nCVC - Borderline                   -0.125             1.000  \n\n=== UPDATED SNAPSHOT ===\n{'exclusivity_report': {'CVC': {'n_cols': 2,\n                                'ok_prop': 0.9872940828839477,\n                                'violations': 344},\n                        'ETT': {'n_cols': 3, 'ok_prop': 1.0, 'violations': 0},\n                        'NGT': {'n_cols': 4,\n                                'ok_prop': 0.9984856319716333,\n                                'violations': 41}},\n 'final_grouping_key': 'PatientID',\n 'label_groups': {'CVC': ['CVC - Abnormal', 'CVC - Borderline'],\n                  'ETT': ['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal'],\n                  'NGT': ['NGT - Abnormal',\n                          'NGT - Borderline',\n                          'NGT - Incompletely Imaged',\n                          'NGT - Normal']},\n 'notes': {'extras_handling': 'CVC - Normal and Swan Ganz Catheter Present '\n                              'reserved for auxiliary/heads later',\n           'iterative_stratification_planned_version': 'iterative-stratification==0.1.7 '\n                                                       '(to be installed)',\n           'reason_group_by_patient': 'Prevent leakage across multiple studies '\n                                      'per patient'}}\n"
          ]
        }
      ]
    },
    {
      "id": "3801886f-200e-4127-b126-788a631bafa5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Strategic fix: define label_groups from full train.csv (includes 'CVC - Normal'),\n",
        "# validate exclusivity using full device groups, and plot correlation heatmap if seaborn is available.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import importlib\n",
        "\n",
        "if 'train_df' not in globals() or train_df is None:\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "if 'sub_df' not in globals() or sub_df is None:\n",
        "    sub_df = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "all_label_cols = [c for c in train_df.columns if c not in ['StudyInstanceUID','PatientID']]\n",
        "\n",
        "def parse_group(label):\n",
        "    return label.split(' - ', 1)[0].strip() if ' - ' in label else label.strip()\n",
        "\n",
        "# Build groups from full train columns, but keep only device-related labels (exclude Swan Ganz for now from multi-class groups)\n",
        "full_groups = {}\n",
        "for c in all_label_cols:\n",
        "    if c == 'Swan Ganz Catheter Present':\n",
        "        continue\n",
        "    g = parse_group(c)\n",
        "    if g in ['ETT','NGT','CVC']:\n",
        "        full_groups.setdefault(g, []).append(c)\n",
        "\n",
        "# Ensure deterministic ordering within each group\n",
        "for g in full_groups:\n",
        "    full_groups[g] = sorted(full_groups[g])\n",
        "print('Full device label_groups (from train.csv):')\n",
        "for k,v in full_groups.items():\n",
        "    print(f'  {k}: {v}')\n",
        "\n",
        "# Validate mutual exclusivity on full groups\n",
        "exclusivity_full = {}\n",
        "for g, cols in full_groups.items():\n",
        "    s = train_df[cols].sum(axis=1)\n",
        "    exclusivity_full[g] = {\n",
        "        'n_cols': len(cols),\n",
        "        'ok_prop': float((s <= 1).mean()),\n",
        "        'violations': int((s > 1).sum())\n",
        "    }\n",
        "print('\\nExclusivity report (full groups):', exclusivity_full)\n",
        "\n",
        "# Document handling plan for violations with a priority scheme\n",
        "# Priority per group (highest first): Abnormal > Borderline > Incompletely Imaged > Normal\n",
        "priority_order = {\n",
        "    'ETT': ['ETT - Abnormal','ETT - Borderline','ETT - Normal'],\n",
        "    'NGT': ['NGT - Abnormal','NGT - Borderline','NGT - Incompletely Imaged','NGT - Normal'],\n",
        "    'CVC': ['CVC - Abnormal','CVC - Borderline','CVC - Normal']\n",
        "}\n",
        "print('\\nViolation resolution priority (per group):')\n",
        "for g, order in priority_order.items():\n",
        "    print(f'  {g}: {order}')\n",
        "\n",
        "# Optional preview: how many rows would be reassigned to each class under this rule (without mutating train_df)\n",
        "def resolve_group_row(row, group_cols, order):\n",
        "    # Return chosen column name or None\n",
        "    positives = [c for c in group_cols if row[c] == 1]\n",
        "    if len(positives) <= 1:\n",
        "        return positives[0] if positives else None\n",
        "    for c in order:\n",
        "        if c in positives:\n",
        "            return c\n",
        "    return positives[0] if positives else None\n",
        "\n",
        "resolution_summary = {}\n",
        "for g, cols in full_groups.items():\n",
        "    order = priority_order[g]\n",
        "    chosen = train_df[cols].apply(lambda r: resolve_group_row(r, cols, order), axis=1)\n",
        "    counts = chosen.value_counts(dropna=True).to_dict()\n",
        "    resolution_summary[g] = counts\n",
        "print('\\nResolution summary (preview counts by chosen class):')\n",
        "for g, d in resolution_summary.items():\n",
        "    print(f'  {g}: {d}')\n",
        "\n",
        "# Correlation heatmap (EDA polish) including CVC - Normal for full view\n",
        "corr_cols = [c for g in full_groups.values() for c in g]\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(train_df[corr_cols].corr(), cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "    plt.title('Label Correlation Heatmap (Full Groups incl. CVC - Normal)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print('Heatmap rendered.')\n",
        "except Exception as e:\n",
        "    print('Heatmap skipped (seaborn/matplotlib unavailable):', e)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full device label_groups (from train.csv):\n  ETT: ['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal']\n  NGT: ['NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal']\n  CVC: ['CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal']\n\nExclusivity report (full groups): {'ETT': {'n_cols': 3, 'ok_prop': 1.0, 'violations': 0}, 'NGT': {'n_cols': 4, 'ok_prop': 0.9984856319716333, 'violations': 41}, 'CVC': {'n_cols': 3, 'ok_prop': 0.8806604122035901, 'violations': 3231}}\n\nViolation resolution priority (per group):\n  ETT: ['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal']\n  NGT: ['NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal']\n  CVC: ['CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nResolution summary (preview counts by chosen class):\n  ETT: {'ETT - Normal': 6503, 'ETT - Borderline': 1027, 'ETT - Abnormal': 76}\n  NGT: {'NGT - Normal': 4284, 'NGT - Incompletely Imaged': 2439, 'NGT - Borderline': 486, 'NGT - Abnormal': 254}\n  CVC: {'CVC - Normal': 16224, 'CVC - Borderline': 7288, 'CVC - Abnormal': 2895}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heatmap rendered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": "[Captured Matplotlib Figure]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "3e501601-b38b-4fda-ad23-74dedbed259e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ultra-fast baseline submission: use train label prevalences as constant probabilities for all test rows.\n",
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "sub_df = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "id_col = sub_df.columns[0]\n",
        "targets = sub_df.columns[1:].tolist()\n",
        "\n",
        "# Compute prevalence for each submission target from train\n",
        "prev = train_df[targets].mean()\n",
        "print('Prevalences used for submission:')\n",
        "print(prev.to_dict())\n",
        "\n",
        "submission = sub_df[[id_col]].copy()\n",
        "for col in targets:\n",
        "    submission[col] = float(prev[col])\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with shape:', submission.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prevalences used for submission:\n{'ETT - Abnormal': 0.0028071212233138805, 'ETT - Borderline': 0.03793307232030731, 'ETT - Normal': 0.2401935436211864, 'NGT - Abnormal': 0.009381694614759548, 'NGT - Borderline': 0.017987737312550788, 'NGT - Incompletely Imaged': 0.09060353106301249, 'NGT - Normal': 0.15919332200635297, 'CVC - Abnormal': 0.1069291571249169, 'CVC - Borderline': 0.2818940681096255}\nSaved submission.csv with shape: (3009, 10)\n"
          ]
        }
      ]
    },
    {
      "id": "92176103-fe47-47d3-8f2c-0dce720cbac0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fast subset baseline: train on ~5000 sampled train images to unblock a learned submission; predict on all test images.\n",
        "import os, random, numpy as np, pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "train_df = pd.read_csv('train.csv')\n",
        "sub_df = pd.read_csv('sample_submission.csv')\n",
        "id_col = sub_df.columns[0]\n",
        "targets = sub_df.columns[1:].tolist()\n",
        "\n",
        "# Sample a manageable subset ensuring positives are represented\n",
        "max_n = 5000\n",
        "idx_all = set()\n",
        "for col in targets:\n",
        "    pos_idx = np.where(train_df[col].values == 1)[0]\n",
        "    if len(pos_idx) > 0:\n",
        "        take = min(600, len(pos_idx))\n",
        "        idx_all.update(rng.choice(pos_idx, size=take, replace=False).tolist())\n",
        "remaining = max(0, max_n - len(idx_all))\n",
        "if remaining > 0:\n",
        "    add_idx = rng.choice(np.setdiff1d(np.arange(len(train_df)), np.array(sorted(idx_all))), size=remaining, replace=False).tolist()\n",
        "    idx_all.update(add_idx)\n",
        "subset_idx = np.array(sorted(idx_all))\n",
        "print('Subset size:', len(subset_idx))\n",
        "\n",
        "IMG_SIZE = 48\n",
        "HIST_BINS = 16\n",
        "\n",
        "def img_feat(path):\n",
        "    im = Image.open(path).convert('L').resize((IMG_SIZE, IMG_SIZE))\n",
        "    arr = np.asarray(im, dtype=np.uint8)\n",
        "    flat = (arr.astype(np.float32) / 255.0).ravel()\n",
        "    hist, _ = np.histogram(arr, bins=HIST_BINS, range=(0,255), density=True)\n",
        "    return np.concatenate([flat, hist.astype(np.float32)])\n",
        "\n",
        "def build_X(uids, img_dir):\n",
        "    X = np.zeros((len(uids), IMG_SIZE*IMG_SIZE + HIST_BINS), dtype=np.float32)\n",
        "    for i, uid in enumerate(uids):\n",
        "        p = os.path.join(img_dir, f'{uid}.jpg')\n",
        "        if os.path.exists(p):\n",
        "            X[i] = img_feat(p)\n",
        "    return X\n",
        "\n",
        "train_uids_sub = train_df.iloc[subset_idx][id_col].tolist()\n",
        "X_tr = build_X(train_uids_sub, 'train')\n",
        "y_tr = train_df.iloc[subset_idx][targets].astype(np.float32).values\n",
        "\n",
        "scaler = StandardScaler().fit(X_tr)\n",
        "X_trs = scaler.transform(X_tr)\n",
        "\n",
        "print('Extracting test features (all test images)...')\n",
        "test_uids = sub_df[id_col].tolist()\n",
        "X_te = build_X(test_uids, 'test')\n",
        "X_tes = scaler.transform(X_te)\n",
        "\n",
        "preds = {}\n",
        "for j, col in enumerate(targets):\n",
        "    clf = LogisticRegression(max_iter=400, solver='liblinear', class_weight='balanced')\n",
        "    clf.fit(X_trs, y_tr[:, j])\n",
        "    preds[col] = clf.predict_proba(X_tes)[:, 1]\n",
        "\n",
        "sub = pd.DataFrame({id_col: test_uids})\n",
        "for col in targets:\n",
        "    sub[col] = preds[col]\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', sub.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset size: 5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting test features (all test images)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (3009, 10)\n"
          ]
        }
      ]
    },
    {
      "id": "b52f89df-dc3d-48e4-8a4e-e53e4b5e6a68",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal viable DL pipeline (CPU): MobileNetV3-Small pretrained, BCEWithLogits on 9 targets,\n",
        "# GroupKFold by PatientID (use 1 fold), subsample for speed, train 2 epochs, log losses, and write submission.csv.\n",
        "import os, random, numpy as np, pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "sub_df = pd.read_csv('sample_submission.csv')\n",
        "ID_COL = sub_df.columns[0]\n",
        "TARGETS = sub_df.columns[1:].tolist()\n",
        "\n",
        "class ChestDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, id_col, targets=None, transform=None):\n",
        "        self.df = df.reset_index(drop=True); self.img_dir = img_dir; self.id_col = id_col\n",
        "        self.targets = targets; self.transform = transform\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]; uid = row[self.id_col]\n",
        "        path = os.path.join(self.img_dir, f\"{uid}.jpg\")\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        if self.transform is not None: img = self.transform(img)\n",
        "        if self.targets is None: return img, uid\n",
        "        y = torch.tensor(row[self.targets].values.astype(np.float32))\n",
        "        return img, y\n",
        "\n",
        "# Transforms (smaller size to keep CPU runtime reasonable)\n",
        "IMG_SIZE = 160\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "valid_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# 1-fold GroupKFold split by PatientID\n",
        "groups = train_df['PatientID'].astype(str).values\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "tr_idx, va_idx = next(gkf.split(train_df, train_df[TARGETS].values, groups))\n",
        "tr_df = train_df.iloc[tr_idx].copy()\n",
        "va_df = train_df.iloc[va_idx].copy()\n",
        "\n",
        "# Subsample for speed on CPU\n",
        "MAX_TRAIN = 3000\n",
        "if len(tr_df) > MAX_TRAIN: tr_df = tr_df.sample(MAX_TRAIN, random_state=SEED)\n",
        "MAX_VALID = 800\n",
        "if len(va_df) > MAX_VALID: va_df = va_df.sample(MAX_VALID, random_state=SEED)\n",
        "\n",
        "train_ds = ChestDataset(tr_df, 'train', ID_COL, targets=TARGETS, transform=train_tfms)\n",
        "valid_ds = ChestDataset(va_df, 'train', ID_COL, targets=TARGETS, transform=valid_tfms)\n",
        "test_ds  = ChestDataset(sub_df, 'test', ID_COL, targets=None, transform=valid_tfms)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "# Model: MobileNetV3-Small (faster on CPU) pretrained\n",
        "model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
        "in_features = model.classifier[3].in_features\n",
        "model.classifier[3] = nn.Linear(in_features, len(TARGETS))\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "\n",
        "def run_epoch(dl, training=True):\n",
        "    if training: model.train()\n",
        "    else: model.eval()\n",
        "    total = 0.0; n = 0\n",
        "    with torch.set_grad_enabled(training):\n",
        "        for imgs, ys in dl:\n",
        "            imgs = imgs.to(device); ys = ys.to(device)\n",
        "            if training: optimizer.zero_grad()\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, ys)\n",
        "            if training:\n",
        "                loss.backward(); optimizer.step()\n",
        "            bs = imgs.size(0); total += loss.item() * bs; n += bs\n",
        "    return total / max(1, n)\n",
        "\n",
        "EPOCHS = 2\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss = run_epoch(train_loader, training=True)\n",
        "    va_loss = run_epoch(valid_loader, training=False)\n",
        "    print(f'Epoch {epoch}/{EPOCHS} - train_loss: {tr_loss:.4f} - valid_loss: {va_loss:.4f}')\n",
        "\n",
        "# Inference on test -> submission.csv\n",
        "model.eval(); sigmoid = nn.Sigmoid()\n",
        "all_uids = []; all_probs = {t: [] for t in TARGETS}\n",
        "with torch.no_grad():\n",
        "    for imgs, uids in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        probs = sigmoid(model(imgs)).cpu().numpy()\n",
        "        for i, uid in enumerate(uids):\n",
        "            all_uids.append(uid)\n",
        "            for j, t in enumerate(TARGETS):\n",
        "                all_probs[t].append(float(probs[i, j]))\n",
        "\n",
        "sub = pd.DataFrame({ID_COL: all_uids})\n",
        "for t in TARGETS: sub[t] = all_probs[t]\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', sub.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /app/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0.00/9.83M [00:00<?, ?B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 8.82M/9.83M [00:00<00:00, 92.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.83M/9.83M [00:00<00:00, 96.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2 - train_loss: 0.3014 - valid_loss: 0.3360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/2 - train_loss: 0.2145 - valid_loss: 0.2539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (3009, 10)\n"
          ]
        }
      ]
    },
    {
      "id": "f665e4c0-4b36-4b8c-b88e-eb4522ce7775",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pretrained feature extraction (CPU) + LogisticRegression: resnet18 frozen features at 96px on ~3k subset for speed.\n",
        "import os, numpy as np, pandas as pd, torch, torch.nn as nn\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "SEED = 42\n",
        "rng = np.random.default_rng(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "sub_df = pd.read_csv('sample_submission.csv')\n",
        "ID_COL = sub_df.columns[0]\n",
        "TARGETS = sub_df.columns[1:].tolist()\n",
        "\n",
        "IMG_SIZE = 96\n",
        "BATCH = 128\n",
        "\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "class ImgDS(Dataset):\n",
        "    def __init__(self, df, img_dir, id_col, transform):\n",
        "        self.df = df.reset_index(drop=True); self.img_dir = img_dir; self.id_col = id_col; self.t = transform\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        uid = self.df.iloc[i][self.id_col]\n",
        "        img = Image.open(os.path.join(self.img_dir, f'{uid}.jpg')).convert('RGB')\n",
        "        return self.t(img), uid\n",
        "\n",
        "# Build resnet18 feature extractor (global pooled 512-d)\n",
        "base = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1).eval()\n",
        "for p in base.parameters(): p.requires_grad = False\n",
        "feat_extractor = nn.Sequential(*(list(base.children())[:-1]))  # -> (B,512,1,1)\n",
        "device = torch.device('cpu')\n",
        "feat_extractor.to(device)\n",
        "\n",
        "def extract_features(df, img_dir):\n",
        "    ds = ImgDS(df, img_dir, ID_COL, tfm)\n",
        "    dl = DataLoader(ds, batch_size=BATCH, shuffle=False, num_workers=8, pin_memory=True)\n",
        "    feats, ids = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, u in dl:\n",
        "            x = x.to(device)\n",
        "            f = feat_extractor(x).squeeze(-1).squeeze(-1).cpu().numpy()  # (B,512)\n",
        "            feats.append(f); ids.extend(list(u))\n",
        "    return np.vstack(feats), ids\n",
        "\n",
        "# Subsample ~3000 rows ensuring positives represented\n",
        "max_n = 3000\n",
        "idx = set()\n",
        "for col in TARGETS:\n",
        "    pos = np.where(train_df[col].values == 1)[0]\n",
        "    if len(pos) > 0:\n",
        "        take = min(400, len(pos))\n",
        "        idx.update(rng.choice(pos, size=take, replace=False).tolist())\n",
        "remain = max(0, max_n - len(idx))\n",
        "if remain > 0:\n",
        "    pool = np.setdiff1d(np.arange(len(train_df)), np.array(sorted(idx)))\n",
        "    if len(pool) > 0:\n",
        "        idx.update(rng.choice(pool, size=min(remain, len(pool)), replace=False).tolist())\n",
        "idx = np.array(sorted(idx))\n",
        "tr_sub = train_df.iloc[idx].copy()\n",
        "print('Feature extraction on subset size:', len(tr_sub))\n",
        "\n",
        "X_tr, _ = extract_features(tr_sub, 'train')\n",
        "y_tr = tr_sub[TARGETS].astype(np.float32).values\n",
        "\n",
        "print('Extracting test features...')\n",
        "X_te, te_ids = extract_features(sub_df, 'test')\n",
        "\n",
        "scaler = StandardScaler().fit(X_tr)\n",
        "X_trs = scaler.transform(X_tr)\n",
        "X_tes = scaler.transform(X_te)\n",
        "\n",
        "preds = {}\n",
        "for j, col in enumerate(TARGETS):\n",
        "    clf = LogisticRegression(max_iter=300, solver='liblinear', class_weight='balanced')\n",
        "    clf.fit(X_trs, y_tr[:, j])\n",
        "    preds[col] = clf.predict_proba(X_tes)[:,1]\n",
        "\n",
        "sub = pd.DataFrame({ID_COL: te_ids})\n",
        "for col in TARGETS: sub[col] = preds[col]\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', sub.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature extraction on subset size: 3000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting test features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (3009, 10)\n"
          ]
        }
      ]
    },
    {
      "id": "5f7ba3ee-37cd-43c8-8cfe-6007ac94e991",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Strategically correct baseline scaffold: multi-head softmax (ETT:3, NGT:4, CVC:3),\n",
        "# priority-based label resolution, and 5-fold GroupKFold by PatientID. No training yet.\n",
        "import os, json, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms, models\n",
        "\n",
        "SEED = 42\n",
        "rng = np.random.default_rng(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "sub_df = pd.read_csv('sample_submission.csv')\n",
        "ID_COL = 'StudyInstanceUID'\n",
        "\n",
        "# Define device label groups from train.csv (includes CVC - Normal) with fixed priority\n",
        "ETT_COLS = ['ETT - Abnormal','ETT - Borderline','ETT - Normal']\n",
        "NGT_COLS = ['NGT - Abnormal','NGT - Borderline','NGT - Incompletely Imaged','NGT - Normal']\n",
        "CVC_COLS = ['CVC - Abnormal','CVC - Borderline','CVC - Normal']\n",
        "\n",
        "PRIORITY = {\n",
        "    'ETT': ETT_COLS,\n",
        "    'NGT': ['NGT - Abnormal','NGT - Borderline','NGT - Incompletely Imaged','NGT - Normal'],\n",
        "    'CVC': ['CVC - Abnormal','CVC - Borderline','CVC - Normal']\n",
        "}\n",
        "\n",
        "# Map columns to class indices per head in the specified order\n",
        "ETT_TO_IDX = {c:i for i,c in enumerate(ETT_COLS)}\n",
        "NGT_TO_IDX = {c:i for i,c in enumerate(NGT_COLS)}\n",
        "CVC_TO_IDX = {c:i for i,c in enumerate(CVC_COLS)}\n",
        "\n",
        "def resolve_group(row, cols, priority_order):\n",
        "    pos = [c for c in cols if row.get(c, 0) == 1]\n",
        "    if len(pos) == 0:\n",
        "        # If completely unlabeled for the group, fallback to last (Normal) when available\n",
        "        # This mirrors the dataset where negatives imply Normal often.\n",
        "        for c in reversed(priority_order):\n",
        "            if c in cols:\n",
        "                return c\n",
        "        return cols[-1]\n",
        "    if len(pos) == 1:\n",
        "        return pos[0]\n",
        "    # Conflicts: choose highest-priority\n",
        "    for c in priority_order:\n",
        "        if c in pos:\n",
        "            return c\n",
        "    return pos[0]\n",
        "\n",
        "def row_to_heads(row):\n",
        "    ett_c = resolve_group(row, ETT_COLS, PRIORITY['ETT'])\n",
        "    ngt_c = resolve_group(row, NGT_COLS, PRIORITY['NGT'])\n",
        "    cvc_c = resolve_group(row, CVC_COLS, PRIORITY['CVC'])\n",
        "    return ETT_TO_IDX[ett_c], NGT_TO_IDX[ngt_c], CVC_TO_IDX[cvc_c]\n",
        "\n",
        "# Build integer targets per head\n",
        "ett_targets = np.zeros(len(train_df), dtype=np.int64)\n",
        "ngt_targets = np.zeros(len(train_df), dtype=np.int64)\n",
        "cvc_targets = np.zeros(len(train_df), dtype=np.int64)\n",
        "for i, row in train_df.iterrows():\n",
        "    e, n, c = row_to_heads(row)\n",
        "    ett_targets[i] = e; ngt_targets[i] = n; cvc_targets[i] = c\n",
        "\n",
        "# 5-fold GroupKFold by PatientID; store indices for reproducible CV\n",
        "groups = train_df['PatientID'].astype(str).values\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "folds = []\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(train_df, ett_targets, groups)):\n",
        "    folds.append({'fold': int(fold), 'train_idx': tr_idx.tolist(), 'valid_idx': va_idx.tolist()})\n",
        "with open('cv_folds_patientid_5fold.json', 'w') as f:\n",
        "    json.dump(folds, f)\n",
        "print('Prepared 5-fold GroupKFold by PatientID and saved to cv_folds_patientid_5fold.json')\n",
        "\n",
        "# Dataset returning image and three integer targets\n",
        "IMG_SIZE = 224\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "valid_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "class MultiHeadCXRDataset(Dataset):\n",
        "    def __init__(self, df, ett, ngt, cvc, img_dir, id_col, transform):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.ett = ett; self.ngt = ngt; self.cvc = cvc\n",
        "        self.img_dir = img_dir; self.id_col = id_col; self.t = transform\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        uid = self.df.iloc[i][self.id_col]\n",
        "        img = Image.open(os.path.join(self.img_dir, f'{uid}.jpg')).convert('RGB')\n",
        "        x = self.t(img)\n",
        "        return x, (\n",
        "            torch.tensor(int(self.ett[i]), dtype=torch.long),\n",
        "            torch.tensor(int(self.ngt[i]), dtype=torch.long),\n",
        "            torch.tensor(int(self.cvc[i]), dtype=torch.long)\n",
        "        ), uid\n",
        "\n",
        "# Simple multi-head model wrapper on MobileNetV3-Small backbone\n",
        "class MultiHeadMobileNetV3(nn.Module):\n",
        "    def __init__(self, num_ett=3, num_ngt=4, num_cvc=3):\n",
        "        super().__init__()\n",
        "        m = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
        "        self.features = m.features\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(m.classifier[0].in_features, 576, kernel_size=1),\n",
        "            nn.Hardswish(),\n",
        "        ) if hasattr(m.classifier[0], 'in_features') else nn.Identity()\n",
        "        # Determine feature dim after pooling\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1,3,IMG_SIZE,IMG_SIZE)\n",
        "            f = self.features(dummy)\n",
        "            f = self.pool(f)\n",
        "            feat_dim = f.shape[1]\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.ett_head = nn.Linear(feat_dim, num_ett)\n",
        "        self.ngt_head = nn.Linear(feat_dim, num_ngt)\n",
        "        self.cvc_head = nn.Linear(feat_dim, num_cvc)\n",
        "    def forward(self, x):\n",
        "        f = self.features(x)\n",
        "        f = self.pool(f).flatten(1)\n",
        "        f = self.dropout(f)\n",
        "        ett = self.ett_head(f)\n",
        "        ngt = self.ngt_head(f)\n",
        "        cvc = self.cvc_head(f)\n",
        "        return ett, ngt, cvc\n",
        "\n",
        "print('Multi-head scaffold ready: label resolution, CV folds, dataset, and model class defined.')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 5-fold GroupKFold by PatientID and saved to cv_folds_patientid_5fold.json\nMulti-head scaffold ready: label resolution, CV folds, dataset, and model class defined.\n"
          ]
        }
      ]
    },
    {
      "id": "a3e9edf0-6f27-4173-9d3d-a0510b501a08",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Multi-head CV training with CrossEntropy per head, per-epoch validation, OOF AUC (9 targets), and fold-averaged submission.\n",
        "import os, json, numpy as np, pandas as pd, torch, torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "torch.manual_seed(42); np.random.seed(42)\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# Reuse items from scaffold (Cell 12): train_df, sub_df, ETT_COLS/NGT_COLS/CVC_COLS,\n",
        "# ETT_TO_IDX/NGT_TO_IDX/CVC_TO_IDX, MultiHeadCXRDataset, transforms, ID_COL,\n",
        "# and precomputed ett_targets/ngt_targets/cvc_targets\n",
        "\n",
        "SUB_COLS = sub_df.columns.tolist()[1:]  # 9 targets in submission\n",
        "# Map head indices to submission columns\n",
        "ett_sub = ['ETT - Abnormal','ETT - Borderline','ETT - Normal']\n",
        "ngt_sub = ['NGT - Abnormal','NGT - Borderline','NGT - Incompletely Imaged','NGT - Normal']\n",
        "cvc_sub = ['CVC - Abnormal','CVC - Borderline']  # 'CVC - Normal' not in submission\n",
        "\n",
        "with open('cv_folds_patientid_5fold.json', 'r') as f:\n",
        "    folds = json.load(f)\n",
        "\n",
        "device = torch.device('cpu')\n",
        "BATCH = 64\n",
        "EPOCHS = 3  # reduced for faster CPU run\n",
        "LR = 3e-4\n",
        "MAX_TR = 3000  # subsample training fold\n",
        "MAX_VA = 800   # subsample validation fold\n",
        "MAX_FOLDS = 1  # run a single fold to complete quickly\n",
        "\n",
        "# Clean, local multi-head model (no unused modules)\n",
        "from torchvision import models\n",
        "class MultiHeadMobileNetClean(nn.Module):\n",
        "    def __init__(self, num_ett=3, num_ngt=4, num_cvc=3, img_size=224):\n",
        "        super().__init__()\n",
        "        m = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
        "        self.features = m.features\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1,3,img_size,img_size)\n",
        "            f = self.pool(self.features(dummy))\n",
        "            feat_dim = f.shape[1]\n",
        "        self.drop = nn.Dropout(0.2)\n",
        "        self.ett_head = nn.Linear(feat_dim, num_ett)\n",
        "        self.ngt_head = nn.Linear(feat_dim, num_ngt)\n",
        "        self.cvc_head = nn.Linear(feat_dim, num_cvc)\n",
        "    def forward(self, x):\n",
        "        f = self.pool(self.features(x)).flatten(1)\n",
        "        f = self.drop(f)\n",
        "        return self.ett_head(f), self.ngt_head(f), self.cvc_head(f)\n",
        "\n",
        "oof = np.zeros((len(train_df), len(SUB_COLS)), dtype=np.float32)\n",
        "oof_mask = np.zeros((len(train_df), len(SUB_COLS)), dtype=bool)\n",
        "test_preds_accum = np.zeros((len(sub_df), len(SUB_COLS)), dtype=np.float32)\n",
        "\n",
        "test_ds = MultiHeadCXRDataset(sub_df, np.zeros(len(sub_df)), np.zeros(len(sub_df)), np.zeros(len(sub_df)), 'test', ID_COL, valid_tfms)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "def heads_to_probs(logits_ett, logits_ngt, logits_cvc):\n",
        "    sm = nn.Softmax(dim=1)\n",
        "    p_ett = sm(logits_ett).cpu().numpy()\n",
        "    p_ngt = sm(logits_ngt).cpu().numpy()\n",
        "    p_cvc = sm(logits_cvc).cpu().numpy()\n",
        "    return p_ett, p_ngt, p_cvc\n",
        "\n",
        "def map_heads_to_submission(p_ett, p_ngt, p_cvc, sub_cols):\n",
        "    va_probs = np.zeros((p_ett.shape[0], len(sub_cols)), dtype=np.float32)\n",
        "    col_to_pos = {c:i for i,c in enumerate(sub_cols)}\n",
        "    for j,c in enumerate(ett_sub): va_probs[:, col_to_pos[c]] = p_ett[:, j]\n",
        "    for j,c in enumerate(ngt_sub): va_probs[:, col_to_pos[c]] = p_ngt[:, j]\n",
        "    for j,c in enumerate(cvc_sub): va_probs[:, col_to_pos[c]] = p_cvc[:, j]\n",
        "    return va_probs\n",
        "\n",
        "for fold_i, fold_entry in enumerate(folds[:MAX_FOLDS]):\n",
        "    tr_idx = np.array(fold_entry['train_idx']); va_idx = np.array(fold_entry['valid_idx'])\n",
        "    # Subsample within the fold for speed\n",
        "    if (MAX_TR is not None) and (len(tr_idx) > MAX_TR):\n",
        "        tr_idx = rng.choice(tr_idx, size=MAX_TR, replace=False)\n",
        "    if (MAX_VA is not None) and (len(va_idx) > MAX_VA):\n",
        "        va_idx = rng.choice(va_idx, size=MAX_VA, replace=False)\n",
        "\n",
        "    tr_df = train_df.iloc[tr_idx].reset_index(drop=True)\n",
        "    va_df = train_df.iloc[va_idx].reset_index(drop=True)\n",
        "    ett_tr = ett_targets[tr_idx]; ngt_tr = ngt_targets[tr_idx]; cvc_tr = cvc_targets[tr_idx]\n",
        "    ett_va = ett_targets[va_idx]; ngt_va = ngt_targets[va_idx]; cvc_va = cvc_targets[va_idx]\n",
        "\n",
        "    train_ds = MultiHeadCXRDataset(tr_df, ett_tr, ngt_tr, cvc_tr, 'train', ID_COL, train_tfms)\n",
        "    valid_ds = MultiHeadCXRDataset(va_df, ett_va, ngt_va, cvc_va, 'train', ID_COL, valid_tfms)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    valid_loader = DataLoader(valid_ds, batch_size=BATCH, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    model = MultiHeadMobileNetClean(img_size=224).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "\n",
        "    # Per-head class weights (inverse frequency on current fold)\n",
        "    def make_weights(y_int, n_classes):\n",
        "        counts = np.bincount(y_int, minlength=n_classes).astype(np.float32)\n",
        "        counts[counts == 0] = 1.0\n",
        "        w = counts.sum() / counts\n",
        "        w = w / w.mean()\n",
        "        return torch.tensor(w, dtype=torch.float32, device=device)\n",
        "    w_ett = make_weights(ett_tr, 3)\n",
        "    w_ngt = make_weights(ngt_tr, 4)\n",
        "    w_cvc = make_weights(cvc_tr, 3)\n",
        "    ce_ett = nn.CrossEntropyLoss(weight=w_ett)\n",
        "    ce_ngt = nn.CrossEntropyLoss(weight=w_ngt)\n",
        "    ce_cvc = nn.CrossEntropyLoss(weight=w_cvc)\n",
        "\n",
        "    best_auc = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        # Train epoch\n",
        "        model.train()\n",
        "        tr_loss_sum = 0.0; tr_count = 0\n",
        "        for xb, (ye, yn, yc), _ in train_loader:\n",
        "            xb = xb.to(device); ye = ye.to(device); yn = yn.to(device); yc = yc.to(device)\n",
        "            opt.zero_grad()\n",
        "            loe, lon, loc = model(xb)\n",
        "            loss = ce_ett(loe, ye) + ce_ngt(lon, yn) + ce_cvc(loc, yc)\n",
        "            loss.backward(); opt.step()\n",
        "            bs = xb.size(0); tr_loss_sum += loss.item() * bs; tr_count += bs\n",
        "        tr_loss = tr_loss_sum / max(1, tr_count)\n",
        "\n",
        "        # Validation epoch\n",
        "        model.eval()\n",
        "        va_loss_sum = 0.0; va_count = 0\n",
        "        all_ett = []; all_ngt = []; all_cvc = []\n",
        "        with torch.no_grad():\n",
        "            for xb, (ye, yn, yc), _ in valid_loader:\n",
        "                xb = xb.to(device); ye = ye.to(device); yn = yn.to(device); yc = yc.to(device)\n",
        "                loe, lon, loc = model(xb)\n",
        "                vloss = ce_ett(loe, ye) + ce_ngt(lon, yn) + ce_cvc(loc, yc)\n",
        "                bs = xb.size(0); va_loss_sum += vloss.item() * bs; va_count += bs\n",
        "                pett, pngt, pcvc = heads_to_probs(loe, lon, loc)\n",
        "                all_ett.append(pett); all_ngt.append(pngt); all_cvc.append(pcvc)\n",
        "        va_loss = va_loss_sum / max(1, va_count)\n",
        "        all_ett = np.vstack(all_ett); all_ngt = np.vstack(all_ngt); all_cvc = np.vstack(all_cvc)\n",
        "        va_probs = map_heads_to_submission(all_ett, all_ngt, all_cvc, SUB_COLS)\n",
        "\n",
        "        # Compute per-epoch macro AUC on validation\n",
        "        y_true_va = va_df[SUB_COLS].values.astype(np.float32)\n",
        "        per_label_auc_ep = []\n",
        "        for j in range(len(SUB_COLS)):\n",
        "            try:\n",
        "                per_label_auc_ep.append(roc_auc_score(y_true_va[:, j], va_probs[:, j]))\n",
        "            except Exception:\n",
        "                per_label_auc_ep.append(np.nan)\n",
        "        macro_auc_ep = float(np.nanmean([v for v in per_label_auc_ep if not np.isnan(v)])) if any(~np.isnan(per_label_auc_ep)) else float('nan')\n",
        "        print(f'Epoch {epoch}/{EPOCHS} - train_loss: {tr_loss:.4f} - valid_loss: {va_loss:.4f} - valid_macro_AUC: {macro_auc_ep if macro_auc_ep==macro_auc_ep else None}')\n",
        "\n",
        "        # Track best by macro AUC\n",
        "        if macro_auc_ep==macro_auc_ep and macro_auc_ep > best_auc:\n",
        "            best_auc = macro_auc_ep\n",
        "            best_state = {k: v.cpu().clone() for k,v in model.state_dict().items()}\n",
        "\n",
        "    # Load best state before OOF/test inference\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # OOF preds\n",
        "    model.eval()\n",
        "    all_ett = []; all_ngt = []; all_cvc = []\n",
        "    with torch.no_grad():\n",
        "        for xb, (_, _, _), _ in valid_loader:\n",
        "            xb = xb.to(device)\n",
        "            loe, lon, loc = model(xb)\n",
        "            pett, pngt, pcvc = heads_to_probs(loe, lon, loc)\n",
        "            all_ett.append(pett); all_ngt.append(pngt); all_cvc.append(pcvc)\n",
        "    all_ett = np.vstack(all_ett); all_ngt = np.vstack(all_ngt); all_cvc = np.vstack(all_cvc)\n",
        "    va_probs = map_heads_to_submission(all_ett, all_ngt, all_cvc, SUB_COLS)\n",
        "    oof[va_idx] = va_probs\n",
        "    oof_mask[va_idx, :] = True\n",
        "\n",
        "    # Test preds for this fold\n",
        "    fold_test = np.zeros((len(sub_df), len(SUB_COLS)), dtype=np.float32)\n",
        "    test_ptr = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, (_, _, _), _uids in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            loe, lon, loc = model(xb)\n",
        "            pett, pngt, pcvc = heads_to_probs(loe, lon, loc)\n",
        "            bs = xb.size(0)\n",
        "            idxs = np.arange(test_ptr, test_ptr + bs)\n",
        "            test_ptr += bs\n",
        "            col_to_pos = {c:i for i,c in enumerate(SUB_COLS)}\n",
        "            for j,c in enumerate(ett_sub): fold_test[idxs, col_to_pos[c]] = pett[:bs, j]\n",
        "            for j,c in enumerate(ngt_sub): fold_test[idxs, col_to_pos[c]] = pngt[:bs, j]\n",
        "            for j,c in enumerate(cvc_sub): fold_test[idxs, col_to_pos[c]] = pcvc[:bs, j]\n",
        "    test_preds_accum += fold_test\n",
        "    del model\n",
        "\n",
        "# Compute OOF AUCs with explicit mask\n",
        "y_true = train_df[SUB_COLS].values.astype(np.float32)\n",
        "per_label_auc = {}\n",
        "for j,c in enumerate(SUB_COLS):\n",
        "    mask = oof_mask[:, j]\n",
        "    try:\n",
        "        if mask.any():\n",
        "            per_label_auc[c] = float(roc_auc_score(y_true[mask, j], oof[mask, j]))\n",
        "        else:\n",
        "            per_label_auc[c] = float('nan')\n",
        "    except Exception:\n",
        "        per_label_auc[c] = float('nan')\n",
        "macro_auc = float(np.nanmean([v for v in per_label_auc.values() if not np.isnan(v)]))\n",
        "print('Per-label OOF AUC:', {k: round(v, 5) if v==v else None for k,v in per_label_auc.items()})\n",
        "print('Macro OOF AUC:', round(macro_auc, 5) if macro_auc==macro_auc else None)\n",
        "\n",
        "# Save OOF\n",
        "oof_df = pd.DataFrame({'StudyInstanceUID': train_df[ID_COL]})\n",
        "for j,c in enumerate(SUB_COLS): oof_df[c] = oof[:, j]\n",
        "oof_df.to_csv('oof_probs_multihead.csv', index=False)\n",
        "print('Saved OOF to oof_probs_multihead.csv', oof_df.shape)\n",
        "\n",
        "# Build submission by averaging folds run\n",
        "n_folds_run = max(1, min(MAX_FOLDS, len(folds)))\n",
        "test_preds = test_preds_accum / n_folds_run\n",
        "submission = pd.DataFrame({'StudyInstanceUID': sub_df[ID_COL]})\n",
        "for j,c in enumerate(SUB_COLS): submission[c] = test_preds[:, j]\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', submission.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 - train_loss: 3.3541 - valid_loss: 3.6217 - valid_macro_AUC: 0.5385028488468083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3 - train_loss: 2.8917 - valid_loss: 3.7150 - valid_macro_AUC: 0.5470577106548951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3 - train_loss: 2.5867 - valid_loss: 3.3323 - valid_macro_AUC: 0.5906214258139202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-label OOF AUC: {'ETT - Abnormal': 0.78473, 'ETT - Borderline': 0.84054, 'ETT - Normal': 0.17975, 'NGT - Abnormal': 0.62583, 'NGT - Borderline': 0.72284, 'NGT - Incompletely Imaged': 0.81604, 'NGT - Normal': 0.22903, 'CVC - Abnormal': 0.5874, 'CVC - Borderline': 0.52942}\nMacro OOF AUC: 0.59062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved OOF to oof_probs_multihead.csv (27074, 10)\nSaved submission.csv (3009, 10)\n"
          ]
        }
      ]
    },
    {
      "id": "e0e262bd-8d3b-418b-947e-f4e696c2b346",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CXR-pretrained feature extraction at 512px letterbox + CLAHE, resized to 224 for xrv DenseNet; cache features, UIDs, and manifest.\n",
        "import os, sys, json, time, gc, hashlib, subprocess\n",
        "from datetime import datetime\n",
        "import numpy as np, pandas as pd, cv2, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Mandatory perf tweak: avoid OpenCV thread oversubscription\n",
        "try:\n",
        "    cv2.setNumThreads(0)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Ensure deps\n",
        "def ensure(pkg, pip_name=None, extra_args=None):\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "    except Exception:\n",
        "        args = [sys.executable, '-m', 'pip', 'install', '-q', (pip_name or pkg)]\n",
        "        if extra_args: args += extra_args\n",
        "        subprocess.check_call(args)\n",
        "        __import__(pkg)\n",
        "ensure('torchxrayvision', 'torchxrayvision')\n",
        "import torchxrayvision as xrv\n",
        "\n",
        "torch.set_num_threads(min(16, os.cpu_count() or 16))\n",
        "SEED = 42\n",
        "rng = np.random.default_rng(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "# Config\n",
        "CUDA = torch.cuda.is_available()\n",
        "CFG = {\n",
        "  'letterbox_size': 512,\n",
        "  'resize_to': 224,\n",
        "  'clahe_clip': 2.0,\n",
        "  'clahe_tile': 8,\n",
        "  'batch_size': 64 if CUDA else 8,\n",
        "  'num_workers': 8 if CUDA else 4,\n",
        "  'backbone': 'xrv_densenet121_res224_all',\n",
        "  'cache_prefix': 'feats_densenet121_cxr_224pxfrom512_clahe'\n",
        "}\n",
        "\n",
        "# IO\n",
        "train_df = globals().get('train_df', pd.read_csv('train.csv'))\n",
        "sub_df = globals().get('sub_df', pd.read_csv('sample_submission.csv'))\n",
        "ID_COL = 'StudyInstanceUID'\n",
        "\n",
        "# Preprocessing: letterbox to 512 square (keep aspect) + CLAHE (on grayscale), then resize to 224\n",
        "def letterbox_gray(img, size):\n",
        "    h, w = img.shape[:2]\n",
        "    if h == 0 or w == 0:\n",
        "        return np.zeros((size, size), dtype=np.uint8)\n",
        "    scale = min(size / h, size / w)\n",
        "    nh, nw = int(round(h * scale)), int(round(w * scale))\n",
        "    resized = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_AREA)\n",
        "    out = np.zeros((size, size), dtype=np.uint8)\n",
        "    y0 = (size - nh) // 2; x0 = (size - nw) // 2\n",
        "    out[y0:y0+nh, x0:x0+nw] = resized\n",
        "    return out\n",
        "\n",
        "class CXRFeatDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, id_col, letterbox_size=512, out_size=224, clahe_clip=2.0, clahe_tile=8):\n",
        "        self.df = df.reset_index(drop=True); self.img_dir = img_dir; self.id_col = id_col\n",
        "        self.letterbox_size = letterbox_size; self.out_size = out_size\n",
        "        # Mandatory perf tweak: pre-create CLAHE once and reuse\n",
        "        self.clahe = cv2.createCLAHE(clipLimit=float(clahe_clip), tileGridSize=(int(clahe_tile), int(clahe_tile)))\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        uid = self.df.iloc[i][self.id_col]\n",
        "        path = os.path.join(self.img_dir, f\"{uid}.jpg\")\n",
        "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            img = np.zeros((self.letterbox_size, self.letterbox_size), dtype=np.uint8)\n",
        "        img = letterbox_gray(img, self.letterbox_size)\n",
        "        img = self.clahe.apply(img)\n",
        "        if (img.shape[0] != CFG['resize_to']) or (img.shape[1] != CFG['resize_to']):\n",
        "            img = cv2.resize(img, (CFG['resize_to'], CFG['resize_to']), interpolation=cv2.INTER_AREA)\n",
        "        x = img.astype(np.float32)[None, ...]  # (1,H,W) in [0,255]\n",
        "        x = xrv.utils.normalize(x, 255)        # model-specific normalization\n",
        "        return torch.from_numpy(x), uid\n",
        "\n",
        "# Build CXR-pretrained DenseNet121 backbone -> 1024-d pooled features\n",
        "def build_backbone():\n",
        "    model = xrv.models.DenseNet(weights='densenet121-res224-all')\n",
        "    model.eval()\n",
        "    for p in model.parameters(): p.requires_grad = False\n",
        "    # Mandatory tweak: remove extra ReLU to match pretrained distribution\n",
        "    backbone = nn.Sequential(\n",
        "        model.features,\n",
        "        nn.AdaptiveAvgPool2d((1,1))\n",
        "    )\n",
        "    return backbone\n",
        "\n",
        "device = torch.device('cuda' if CUDA else 'cpu')\n",
        "print('CUDA available:', CUDA)\n",
        "backbone = build_backbone().to(device)\n",
        "\n",
        "def extract_features(df, img_dir, batch, num_workers):\n",
        "    ds = CXRFeatDataset(df, img_dir, ID_COL,\n",
        "                        letterbox_size=CFG['letterbox_size'],\n",
        "                        out_size=CFG['resize_to'],\n",
        "                        clahe_clip=CFG['clahe_clip'],\n",
        "                        clahe_tile=CFG['clahe_tile'])\n",
        "    dl = DataLoader(\n",
        "        ds, batch_size=batch, shuffle=False, num_workers=num_workers,\n",
        "        pin_memory=torch.cuda.is_available(), persistent_workers=(num_workers > 0)\n",
        "    )\n",
        "    feats, uids = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, ids in dl:\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            f = backbone(xb).squeeze(-1).squeeze(-1)  # (B,1024)\n",
        "            f_np = f.detach().cpu().numpy()\n",
        "            assert np.isfinite(f_np).all(), 'Non-finite feature detected'\n",
        "            feats.append(f_np); uids.extend(list(ids))\n",
        "    F = np.vstack(feats)\n",
        "    assert F.shape[0] == len(uids) and F.shape[1] == 1024, f'Unexpected feature shape: {F.shape}'\n",
        "    return F, np.array(uids, dtype=object)\n",
        "\n",
        "# Cache paths + manifest\n",
        "train_cache = f\"{CFG['cache_prefix']}_train.npy\"\n",
        "test_cache  = f\"{CFG['cache_prefix']}_test.npy\"\n",
        "train_uids_path = f\"{CFG['cache_prefix']}_train_uids.npy\"\n",
        "test_uids_path  = f\"{CFG['cache_prefix']}_test_uids.npy\"\n",
        "manifest_path = f\"{CFG['cache_prefix']}_manifest.json\"\n",
        "\n",
        "def md5(p):\n",
        "    m = hashlib.md5()\n",
        "    with open(p, 'rb') as f:\n",
        "        for chunk in iter(lambda: f.read(1<<20), b''): m.update(chunk)\n",
        "    return m.hexdigest()\n",
        "\n",
        "def save_manifest():\n",
        "    man = {\n",
        "        'timestamp': datetime.utcnow().isoformat()+'Z',\n",
        "        'cfg': CFG,\n",
        "        'backbone': CFG['backbone'],\n",
        "        'device': str(device),\n",
        "        'train_csv_hash': md5('train.csv') if os.path.exists('train.csv') else None,\n",
        "        'sample_sub_hash': md5('sample_submission.csv') if os.path.exists('sample_submission.csv') else None,\n",
        "        'train_cache': train_cache if os.path.exists(train_cache) else None,\n",
        "        'test_cache': test_cache if os.path.exists(test_cache) else None,\n",
        "        'train_uids': train_uids_path if os.path.exists(train_uids_path) else None,\n",
        "        'test_uids': test_uids_path if os.path.exists(test_uids_path) else None,\n",
        "        'train_cache_md5': md5(train_cache) if os.path.exists(train_cache) else None,\n",
        "        'test_cache_md5': md5(test_cache) if os.path.exists(test_cache) else None,\n",
        "        'train_uids_md5': md5(train_uids_path) if os.path.exists(train_uids_path) else None,\n",
        "        'test_uids_md5': md5(test_uids_path) if os.path.exists(test_uids_path) else None\n",
        "    }\n",
        "    with open(manifest_path, 'w') as f: json.dump(man, f, indent=2)\n",
        "\n",
        "# Run extraction if missing\n",
        "need_train = not os.path.exists(train_cache) or not os.path.exists(train_uids_path)\n",
        "need_test = not os.path.exists(test_cache) or not os.path.exists(test_uids_path)\n",
        "print({'need_train': need_train, 'need_test': need_test, 'device': str(device), 'batch_size': CFG['batch_size']})\n",
        "if need_train:\n",
        "    t0 = time.time(); X_tr, U_tr = extract_features(train_df, 'train', CFG['batch_size'], CFG['num_workers'])\n",
        "    np.save(train_cache, X_tr.astype(np.float16)); np.save(train_uids_path, U_tr)\n",
        "    print('Saved', train_cache, X_tr.shape, 'UIDs:', U_tr.shape, 'time(sec)=', round(time.time()-t0,1))\n",
        "    del X_tr, U_tr; gc.collect()\n",
        "else:\n",
        "    print('Train cache exists:', train_cache, np.load(train_cache, mmap_mode='r+').shape, 'UIDs:', np.load(train_uids_path, allow_pickle=True).shape)\n",
        "if need_test:\n",
        "    t0 = time.time(); X_te, U_te = extract_features(sub_df, 'test', CFG['batch_size'], CFG['num_workers'])\n",
        "    np.save(test_cache, X_te.astype(np.float16)); np.save(test_uids_path, U_te)\n",
        "    print('Saved', test_cache, X_te.shape, 'UIDs:', U_te.shape, 'time(sec)=', round(time.time()-t0,1))\n",
        "    del X_te, U_te; gc.collect()\n",
        "else:\n",
        "    print('Test cache exists:', test_cache, np.load(test_cache, mmap_mode='r+').shape, 'UIDs:', np.load(test_uids_path, allow_pickle=True).shape)\n",
        "save_manifest()\n",
        "print('Manifest saved to', manifest_path)\n",
        "\n",
        "# Next: Train 9 LightGBM models (5-fold GroupKFold by PatientID) on these 1024-d CXR features."
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "id": "056d3ed6-58af-4dc4-a78e-155228c8d164",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CPU contingency: validate CXR extractor on a small grouped subset and report sec/image.\n",
        "import time, numpy as np, pandas as pd, torch\n",
        "\n",
        "# Ensure CXR components from Cell 16 are available\n",
        "assert 'CXRFeatDataset' in globals() and 'backbone' in globals() and 'ID_COL' in globals(), 'Run Cell 16 first to define CXR pipeline.'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Safe local config (avoid NameError if CFG not defined in this kernel)\n",
        "CFG_local = globals().get('CFG', {}) if 'CFG' in globals() else {}\n",
        "bs = int(CFG_local.get('batch_size', 8))\n",
        "nw = int(CFG_local.get('num_workers', 0))\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "def extract_features_cxr(df, img_dir, batch, num_workers):\n",
        "    ds = CXRFeatDataset(df, img_dir, ID_COL,\n",
        "                        letterbox_size=CFG_local.get('letterbox_size', 512),\n",
        "                        out_size=CFG_local.get('resize_to', 224),\n",
        "                        clahe_clip=CFG_local.get('clahe_clip', 2.0),\n",
        "                        clahe_tile=CFG_local.get('clahe_tile', 8))\n",
        "    dl = DataLoader(ds, batch_size=batch, shuffle=False, num_workers=num_workers,\n",
        "                    pin_memory=torch.cuda.is_available(), persistent_workers=(num_workers > 0))\n",
        "    feats, uids = [], []\n",
        "    backbone.eval()\n",
        "    with torch.no_grad():\n",
        "        for xb, ids in dl:\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            f = backbone(xb).squeeze(-1).squeeze(-1)  # (B,1024)\n",
        "            f_np = f.detach().cpu().numpy()\n",
        "            assert np.isfinite(f_np).all(), 'Non-finite feature detected'\n",
        "            feats.append(f_np); uids.extend(list(ids))\n",
        "    F = np.vstack(feats) if len(feats) else np.zeros((0,1024), dtype=np.float32)\n",
        "    if F.size > 0:\n",
        "        assert F.shape[1] == 1024, f'Unexpected feature dim: {F.shape}'\n",
        "    return F, np.array(uids, dtype=object)\n",
        "\n",
        "# Build a PatientID-grouped subset ~1500 studies\n",
        "rng = np.random.default_rng(42)\n",
        "patients = train_df['PatientID'].astype(str).unique()\n",
        "sample_patients = rng.choice(patients, size=min(300, len(patients)), replace=False)\n",
        "sub_train = train_df[train_df['PatientID'].astype(str).isin(sample_patients)].head(1500).reset_index(drop=True)\n",
        "print('Subset train size:', len(sub_train), '| bs:', bs, '| workers:', nw)\n",
        "t0 = time.time()\n",
        "X_sub_tr, U_sub_tr = extract_features_cxr(sub_train, 'train', bs, nw)\n",
        "t1 = time.time()\n",
        "np.save('feats_cxr_subset_train.npy', X_sub_tr.astype(np.float16)); np.save('uids_cxr_subset_train.npy', U_sub_tr)\n",
        "print({'train_subset_shape': X_sub_tr.shape, 'time_sec': round(t1-t0, 2), 'sec_per_image': round((t1-t0)/max(1,len(sub_train)), 4)})\n",
        "\n",
        "# Test subset ~500 images to validate both dirs\n",
        "sub_test = sub_df.head(500).reset_index(drop=True)\n",
        "t2 = time.time()\n",
        "X_sub_te, U_sub_te = extract_features_cxr(sub_test, 'test', bs, nw)\n",
        "t3 = time.time()\n",
        "np.save('feats_cxr_subset_test.npy', X_sub_te.astype(np.float16)); np.save('uids_cxr_subset_test.npy', U_sub_te)\n",
        "print({'test_subset_shape': X_sub_te.shape, 'time_sec': round(t3-t2, 2), 'sec_per_image': round((t3-t2)/max(1,len(sub_test)), 4)})\n",
        "\n",
        "print('Subset features saved: feats_cxr_subset_train.npy, feats_cxr_subset_test.npy')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset train size: 1500 | bs: 8 | workers: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train_subset_shape': (1500, 1024), 'time_sec': 123.64, 'sec_per_image': 0.0824}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'test_subset_shape': (500, 1024), 'time_sec': 40.85, 'sec_per_image': 0.0817}\nSubset features saved: feats_cxr_subset_train.npy, feats_cxr_subset_test.npy\n"
          ]
        }
      ]
    },
    {
      "id": "57d2eb75-f222-41dd-91ee-6461b920cc94",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lightweight CXR pipeline bootstrap (no extraction): defines CFG, ID_COL, CXRFeatDataset, backbone for use in subset cell.\n",
        "import os, sys, subprocess, numpy as np, cv2, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "try:\n",
        "    cv2.setNumThreads(0)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def ensure(pkg, pip_name=None):\n",
        "    try: __import__(pkg)\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', (pip_name or pkg)])\n",
        "        __import__(pkg)\n",
        "\n",
        "ensure('torchxrayvision', 'torchxrayvision')\n",
        "import torchxrayvision as xrv\n",
        "\n",
        "CUDA = torch.cuda.is_available()\n",
        "CFG = {\n",
        "  'letterbox_size': 512,\n",
        "  'resize_to': 224,\n",
        "  'clahe_clip': 2.0,\n",
        "  'clahe_tile': 8,\n",
        "  'batch_size': 64 if CUDA else 8,\n",
        "  'num_workers': 8 if CUDA else 0\n",
        "}\n",
        "ID_COL = 'StudyInstanceUID'\n",
        "\n",
        "def letterbox_gray(img, size):\n",
        "    h, w = img.shape[:2]\n",
        "    if h == 0 or w == 0: return np.zeros((size, size), dtype=np.uint8)\n",
        "    scale = min(size / h, size / w)\n",
        "    nh, nw = int(round(h * scale)), int(round(w * scale))\n",
        "    resized = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_AREA)\n",
        "    out = np.zeros((size, size), dtype=np.uint8)\n",
        "    y0 = (size - nh) // 2; x0 = (size - nw) // 2\n",
        "    out[y0:y0+nh, x0:x0+nw] = resized\n",
        "    return out\n",
        "\n",
        "class CXRFeatDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, id_col, letterbox_size=512, out_size=224, clahe_clip=2.0, clahe_tile=8):\n",
        "        self.df = df.reset_index(drop=True); self.img_dir = img_dir; self.id_col = id_col\n",
        "        self.letterbox_size = letterbox_size; self.out_size = out_size\n",
        "        self.clahe = cv2.createCLAHE(clipLimit=float(clahe_clip), tileGridSize=(int(clahe_tile), int(clahe_tile)))\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        uid = self.df.iloc[i][self.id_col]\n",
        "        path = os.path.join(self.img_dir, f\"{uid}.jpg\")\n",
        "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None: img = np.zeros((self.letterbox_size, self.letterbox_size), dtype=np.uint8)\n",
        "        img = letterbox_gray(img, self.letterbox_size)\n",
        "        img = self.clahe.apply(img)\n",
        "        if (img.shape[0] != CFG['resize_to']) or (img.shape[1] != CFG['resize_to']):\n",
        "            img = cv2.resize(img, (CFG['resize_to'], CFG['resize_to']), interpolation=cv2.INTER_AREA)\n",
        "        x = img.astype(np.float32)[None, ...]\n",
        "        x = xrv.utils.normalize(x, 255)\n",
        "        return torch.from_numpy(x), uid\n",
        "\n",
        "def build_backbone():\n",
        "    model = xrv.models.DenseNet(weights='densenet121-res224-all')\n",
        "    model.eval()\n",
        "    for p in model.parameters(): p.requires_grad = False\n",
        "    bb = nn.Sequential(model.features, nn.AdaptiveAvgPool2d((1,1)))\n",
        "    return bb\n",
        "\n",
        "backbone = build_backbone().to(torch.device('cuda' if CUDA else 'cpu'))\n",
        "print('CXR components ready:', {'CUDA': CUDA, 'batch_size': CFG['batch_size'], 'num_workers': CFG['num_workers']})"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchaudio 2.2.2+cpu requires torch==2.2.2, but you have torch 2.8.0 which is incompatible.\nWARNING: Target directory /app/.pip-target/torchvision-0.23.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchvision.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchvision already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch-2.8.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.7.3.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pandas-2.3.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pandas already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.10.2.21.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.3.3.83.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.5.8.93.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/python_dateutil-2.9.0.post0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/dateutil already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/requests-2.32.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/requests already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy-1.16.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton-3.4.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/certifi-2025.8.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/certifi already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/charset_normalizer-3.4.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/charset_normalizer already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec-2025.7.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/idna-3.10.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/idna already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/MarkupSafe-3.0.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.8.4.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.8.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.8.93.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.8.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufile_cu12-1.13.1.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.9.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.27.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.8.93.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.8.90.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging-25.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/setuptools-80.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/setuptools already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pkg_resources already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/_distutils_hack already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/distutils-precedence.pth already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/six-1.17.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/six.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tqdm-4.67.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tqdm already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.14.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tzdata-2025.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tzdata already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/urllib3-2.5.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/urllib3 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparselt_cu12-0.7.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pytz-2025.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pytz already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading weights...\nIf this fails you can run `wget https://github.com/mlmed/torchxrayvision/releases/download/v1/nih-pc-chex-mimic_ch-google-openi-kaggle-densenet121-d121-tw-lr001-rot45-tr15-sc15-seed0-best.pt -O /app/.torchxrayvision/models_data/nih-pc-chex-mimic_ch-google-openi-kaggle-densenet121-d121-tw-lr001-rot45-tr15-sc15-seed0-best.pt`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588.................................................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588...............................................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588.............................................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588...........................................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588.........................................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588.......................................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588......................................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588....................................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588..................................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588................................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588..............................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588............................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588..........................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588.........................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588.......................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588.....................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588...................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588.................]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588...............]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588..............]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588............]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588..........]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588........]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588......]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588....]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588..]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588.]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CXR components ready: {'CUDA': False, 'batch_size': 8, 'num_workers': 0}\n"
          ]
        }
      ]
    },
    {
      "id": "3e721d58-ad87-43d5-b01d-ede4648e2c4e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Resumable, chunked CXR feature extraction (CPU contingency). Uses existing CXRFeatDataset/backbone/CFG.\n",
        "import os, json, math, time, gc, hashlib\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "assert 'CXRFeatDataset' in globals() and 'backbone' in globals() and 'CFG' in globals() and 'ID_COL' in globals(), 'Bootstrap CXR pipeline first (Cells 16/18)'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def md5(p):\n",
        "    m = hashlib.md5()\n",
        "    with open(p, 'rb') as f:\n",
        "        for chunk in iter(lambda: f.read(1<<20), b''): m.update(chunk)\n",
        "    return m.hexdigest()\n",
        "\n",
        "def extract_features_cxr_slices(df, img_dir, batch, num_workers):\n",
        "    ds = CXRFeatDataset(df, img_dir, ID_COL,\n",
        "                        letterbox_size=CFG.get('letterbox_size', 512),\n",
        "                        out_size=CFG.get('resize_to', 224),\n",
        "                        clahe_clip=CFG.get('clahe_clip', 2.0),\n",
        "                        clahe_tile=CFG.get('clahe_tile', 8))\n",
        "    dl = DataLoader(ds, batch_size=batch, shuffle=False, num_workers=num_workers,\n",
        "                    pin_memory=torch.cuda.is_available(), persistent_workers=(num_workers > 0))\n",
        "    feats, uids = [], []\n",
        "    backbone.eval()\n",
        "    with torch.no_grad():\n",
        "        for xb, ids in dl:\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            f = backbone(xb).squeeze(-1).squeeze(-1)\n",
        "            f_np = f.detach().cpu().numpy()\n",
        "            assert np.isfinite(f_np).all(), 'Non-finite feature detected'\n",
        "            feats.append(f_np); uids.extend(list(ids))\n",
        "    if len(feats) == 0:\n",
        "        return np.zeros((0,1024), dtype=np.float32), np.array([], dtype=object)\n",
        "    F = np.vstack(feats)\n",
        "    assert F.shape[1] == 1024, f'Unexpected feature dim: {F.shape}'\n",
        "    return F, np.array(uids, dtype=object)\n",
        "\n",
        "def chunk_manifest_path(prefix):\n",
        "    return f\"{prefix}_chunk_manifest.json\"\n",
        "\n",
        "def load_manifest(prefix):\n",
        "    mp = chunk_manifest_path(prefix)\n",
        "    return json.load(open(mp)) if os.path.exists(mp) else {'prefix': prefix, 'chunks': {}, 'created': datetime.utcnow().isoformat()+'Z'}\n",
        "\n",
        "def save_manifest(prefix, man):\n",
        "    with open(chunk_manifest_path(prefix), 'w') as f: json.dump(man, f, indent=2)\n",
        "\n",
        "def stitch_if_complete(prefix, ds_name, n_chunks, feature_dim=1024):\n",
        "    # If all chunks present, stitch into single npy + uids and record md5\n",
        "    feat_parts, uid_parts = [], []\n",
        "    for i in range(n_chunks):\n",
        "        fpth = f\"{prefix}_{ds_name}_chunk{i}.npy\"\n",
        "        upth = f\"{prefix}_{ds_name}_chunk{i}_uids.npy\"\n",
        "        if not (os.path.exists(fpth) and os.path.exists(upth)):\n",
        "            return False\n",
        "        feat_parts.append(np.load(fpth, mmap_mode='r+'))\n",
        "        uid_parts.append(np.load(upth, allow_pickle=True))\n",
        "    F = np.vstack(feat_parts).astype(np.float16)\n",
        "    U = np.concatenate(uid_parts)\n",
        "    full_f = f\"{prefix}_{ds_name}.npy\"\n",
        "    full_u = f\"{prefix}_{ds_name}_uids.npy\"\n",
        "    np.save(full_f, F); np.save(full_u, U)\n",
        "    print(f'Stitched {ds_name}:', F.shape, '->', full_f, full_u)\n",
        "    return True\n",
        "\n",
        "def run_chunked_extraction(df, img_dir, ds_name, cache_prefix=None, chunk_size=2000,\n",
        "                            start_chunk=0, max_chunks=None, resume=True, batch=None, workers=None):\n",
        "    prefix = cache_prefix or CFG.get('cache_prefix', 'feats_densenet121_cxr_224pxfrom512_clahe')\n",
        "    N = len(df); n_chunks = math.ceil(N / chunk_size) if chunk_size > 0 else 1\n",
        "    if max_chunks is not None:\n",
        "        n_chunks = min(n_chunks, start_chunk + max_chunks)\n",
        "    man = load_manifest(prefix)\n",
        "    man['cfg'] = CFG; man['device'] = str(device); man['ds_name'] = ds_name; man['chunk_size'] = chunk_size; man['total'] = N\n",
        "    bs = int(CFG.get('batch_size', 8) if batch is None else batch)\n",
        "    nw = int(CFG.get('num_workers', 0) if workers is None else workers)\n",
        "    t_all0 = time.time()\n",
        "    for ci in range(start_chunk, n_chunks):\n",
        "        s = ci * chunk_size; e = min(N, (ci+1) * chunk_size)\n",
        "        if s >= e: break\n",
        "        fpth = f\"{prefix}_{ds_name}_chunk{ci}.npy\"\n",
        "        upth = f\"{prefix}_{ds_name}_chunk{ci}_uids.npy\"\n",
        "        if resume and os.path.exists(fpth) and os.path.exists(upth):\n",
        "            print(f'[chunk {ci}] exists -> skip')\n",
        "            man['chunks'][f'{ds_name}:{ci}'] = {'range': [int(s), int(e)], 'path': fpth, 'uids': upth, 'ts': datetime.utcnow().isoformat()+'Z'}\n",
        "            continue\n",
        "        print(f'[chunk {ci}] extracting rows [{s}:{e}) bs={bs} nw={nw}')\n",
        "        t0 = time.time()\n",
        "        X, U = extract_features_cxr_slices(df.iloc[s:e], img_dir, bs, nw)\n",
        "        np.save(fpth, X.astype(np.float16)); np.save(upth, U)\n",
        "        man['chunks'][f'{ds_name}:{ci}'] = {'range': [int(s), int(e)], 'path': fpth, 'uids': upth, 'n': int(len(U)), 'ts': datetime.utcnow().isoformat()+'Z'}\n",
        "        save_manifest(prefix, man)\n",
        "        print(f'[chunk {ci}] saved:', X.shape, 'time(sec)=', round(time.time()-t0, 1))\n",
        "        del X, U; gc.collect()\n",
        "    save_manifest(prefix, man)\n",
        "    ok = stitch_if_complete(prefix, ds_name, math.ceil(N / chunk_size))\n",
        "    print(f'Done {ds_name}. elapsed=', round(time.time()-t_all0,1), 'stitched=', ok)\n",
        "    return man\n",
        "\n",
        "print('Chunked extraction utilities ready. Use run_chunked_extraction(train_df, \"train\", ds_name=\"train\", chunk_size=2000) and similarly for test_df. Avoid full CPU runs; prefer GPU.')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunked extraction utilities ready. Use run_chunked_extraction(train_df, \"train\", ds_name=\"train\", chunk_size=2000) and similarly for test_df. Avoid full CPU runs; prefer GPU.\n"
          ]
        }
      ]
    },
    {
      "id": "65a42d43-85c0-44b4-93ca-cb704ecc9a0f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Full-scale CXR feature extraction (CPU contingency, resumable) - TRAIN ONLY\n",
        "import time\n",
        "assert 'run_chunked_extraction' in globals(), 'Load chunked utilities (Cell 19) first.'\n",
        "assert 'train_df' in globals(), 'train_df missing.'\n",
        "t0 = time.time()\n",
        "print('Starting chunked TRAIN extraction...')\n",
        "man_train = run_chunked_extraction(train_df, 'train', ds_name='train',\n",
        "                                   chunk_size=3000, resume=True, batch=8, workers=0)\n",
        "print('TRAIN extraction done in', round(time.time()-t0,1), 'sec')\n",
        "print('Manifest keys:', list(man_train.keys()))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting chunked TRAIN extraction...\n[chunk 0] extracting rows [0:3000) bs=8 nw=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[chunk 0] saved: (3000, 1024) time(sec)= 246.4\n[chunk 1] extracting rows [3000:6000) bs=8 nw=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[chunk 1] saved: (3000, 1024) time(sec)= 247.4\n[chunk 2] extracting rows [6000:9000) bs=8 nw=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[chunk 2] saved: (3000, 1024) time(sec)= 246.5\n[chunk 3] extracting rows [9000:12000) bs=8 nw=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[chunk 3] saved: (3000, 1024) time(sec)= 246.8\n[chunk 4] extracting rows [12000:15000) bs=8 nw=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[chunk 4] saved: (3000, 1024) time(sec)= 245.2\n[chunk 5] extracting rows [15000:18000) bs=8 nw=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[chunk 5] saved: (3000, 1024) time(sec)= 246.2\n[chunk 6] extracting rows [18000:21000) bs=8 nw=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[chunk 6] saved: (3000, 1024) time(sec)= 245.4\n[chunk 7] extracting rows [21000:24000) bs=8 nw=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[chunk 7] saved: (3000, 1024) time(sec)= 247.2\n[chunk 8] extracting rows [24000:27000) bs=8 nw=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[chunk 8] saved: (3000, 1024) time(sec)= 247.4\n[chunk 9] extracting rows [27000:27074) bs=8 nw=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[chunk 9] saved: (74, 1024) time(sec)= 6.2\nStitched train: (27074, 1024) -> feats_densenet121_cxr_224pxfrom512_clahe_train.npy feats_densenet121_cxr_224pxfrom512_clahe_train_uids.npy\nDone train. elapsed= 2227.5 stitched= True\nTRAIN extraction done in 2227.5 sec\nManifest keys: ['prefix', 'chunks', 'created', 'cfg', 'device', 'ds_name', 'chunk_size', 'total']\n"
          ]
        }
      ]
    },
    {
      "id": "defb7ab9-0035-486c-9db0-90a9eb2193ae",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Full-scale CXR feature extraction (CPU contingency, resumable) - TEST ONLY\n",
        "import time\n",
        "assert 'run_chunked_extraction' in globals(), 'Load chunked utilities (Cell 19) first.'\n",
        "assert 'sub_df' in globals(), 'sub_df missing.'\n",
        "t0 = time.time()\n",
        "print('Starting chunked TEST extraction...')\n",
        "man_test = run_chunked_extraction(sub_df, 'test', ds_name='test',\n",
        "                                  chunk_size=3000, resume=True, batch=8, workers=0)\n",
        "print('TEST extraction done in', round(time.time()-t0,1), 'sec')\n",
        "print('Manifest keys:', list(man_test.keys()))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting chunked TEST extraction...\n[chunk 0] extracting rows [0:3000) bs=8 nw=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[chunk 0] saved: (3000, 1024) time(sec)= 246.9\n[chunk 1] extracting rows [3000:3009) bs=8 nw=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[chunk 1] saved: (9, 1024) time(sec)= 0.8\nStitched test: (3009, 1024) -> feats_densenet121_cxr_224pxfrom512_clahe_test.npy feats_densenet121_cxr_224pxfrom512_clahe_test_uids.npy\nDone test. elapsed= 248.2 stitched= True\nTEST extraction done in 248.2 sec\nManifest keys: ['prefix', 'chunks', 'created', 'cfg', 'device', 'ds_name', 'chunk_size', 'total']\n"
          ]
        }
      ]
    },
    {
      "id": "11641bb6-46f6-45ee-b66f-f7a6ea7b2bec",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare UPDATED GPU fine-tuning handoff with mandatory operational fixes and ENSEMBLE runner (hardened: fold-level resumable + lean sanity + data preflight).\n",
        "import os, json, textwrap\n",
        "\n",
        "readme = '''\n",
        "# GPU Fine-Tuning Handoff (Updated: Early Stopping + Multi-Model Ensemble + Fold-Level Resumable Runner)\n",
        "\n",
        "Key features:\n",
        "- Multi-head softmax heads (ETT:3, NGT:4, CVC:3) with priority-based label resolution.\n",
        "- Annotation mask as 4th channel (always 4-ch when --use_annotations 1).\n",
        "- Weighted CrossEntropy per head; Warmup+Cosine LR; AMP; EMA with correct save/restore; logits-avg TTA.\n",
        "- Early stopping: --early_stopping_patience N.\n",
        "- ENSEMBLE runner with FOLD-LEVEL resumability and minimal sanity (1 fold, 1 epoch).\n",
        "- Multi-model portfolio + final averaging of model submissions to form the ensemble.\n",
        "\n",
        "Model portfolio:\n",
        "1) convnext_tiny @ 512\n",
        "2) tf_efficientnet_b4_ns @ 448\n",
        "3) swin_base_patch4_window7_224_in22k @ 384\n",
        "\n",
        "Data provisioning (REQUIRED on GPU host):\n",
        "- Ensure ./train and ./test directories exist and contain the .jpg images referenced by train.csv and sample_submission.csv.\n",
        "- Example sync commands:\n",
        "  - rsync -avP /path/to/train/ ./train/\n",
        "  - rsync -avP /path/to/test/  ./test/\n",
        "  - or scp -r train test <gpu_host>:~/ranzcr/\n",
        "\n",
        "Quick start:\n",
        "1) Verify CUDA availability (nvidia-smi or torch.cuda.is_available())\n",
        "2) Place gpu_handoff_bundle.tar.gz and the train/ test/ dirs in the same folder (or sync train/ test/ after extract).\n",
        "3) tar -xzf gpu_handoff_bundle.tar.gz\n",
        "4) bash run_gpu.sh\n",
        "'''\n",
        "open('README_GPU_PLAN.md', 'w').write(readme)\n",
        "\n",
        "script = '''\n",
        "import os, json, argparse, math, time, random, numpy as np, pandas as pd, cv2, ast\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from timm import create_model\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "def set_seed(s=42):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "\n",
        "def parse_args():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument('--train_csv', type=str, default='train.csv')\n",
        "    ap.add_argument('--test_csv', type=str, default='sample_submission.csv')\n",
        "    ap.add_argument('--img_dir', type=str, default='train')\n",
        "    ap.add_argument('--test_img_dir', type=str, default='test')\n",
        "    ap.add_argument('--folds_json', type=str, default='cv_folds_patientid_5fold.json')\n",
        "    ap.add_argument('--size', type=int, default=512)\n",
        "    ap.add_argument('--epochs', type=int, default=12)\n",
        "    ap.add_argument('--early_stopping_patience', type=int, default=2)\n",
        "    ap.add_argument('--batch', type=int, default=32)\n",
        "    ap.add_argument('--grad_accum', type=int, default=1)\n",
        "    ap.add_argument('--num_workers', type=int, default=8)\n",
        "    ap.add_argument('--lr', type=float, default=2e-4)\n",
        "    ap.add_argument('--wd', type=float, default=1e-4)\n",
        "    ap.add_argument('--backbone', type=str, default='convnext_tiny')\n",
        "    ap.add_argument('--out_dir', type=str, default='outputs')\n",
        "    ap.add_argument('--ema', type=int, default=1)\n",
        "    ap.add_argument('--tta', type=int, default=1)\n",
        "    ap.add_argument('--inference_only', type=int, default=0)\n",
        "    ap.add_argument('--use_annotations', type=int, default=1)\n",
        "    ap.add_argument('--ann_csv', type=str, default='train_annotations.csv')\n",
        "    ap.add_argument('--fold', type=int, default=-1, help='-1=all folds; otherwise run only this fold index for train/infer')\n",
        "    return ap.parse_args()\n",
        "\n",
        "ETT_COLS = ['ETT - Abnormal','ETT - Borderline','ETT - Normal']\n",
        "NGT_COLS = ['NGT - Abnormal','NGT - Borderline','NGT - Incompletely Imaged','NGT - Normal']\n",
        "CVC_COLS = ['CVC - Abnormal','CVC - Borderline','CVC - Normal']\n",
        "PRIORITY = {\n",
        "    'ETT': ETT_COLS,\n",
        "    'NGT': ['NGT - Abnormal','NGT - Borderline','NGT - Incompletely Imaged','NGT - Normal'],\n",
        "    'CVC': ['CVC - Abnormal','CVC - Borderline','CVC - Normal']\n",
        "}\n",
        "\n",
        "def resolve_group(row, cols, order):\n",
        "    pos = [c for c in cols if row.get(c, 0) == 1]\n",
        "    if len(pos) == 0: return order[-1]\n",
        "    if len(pos) == 1: return pos[0]\n",
        "    for c in order:\n",
        "        if c in pos: return c\n",
        "    return pos[0]\n",
        "\n",
        "def build_head_targets(df):\n",
        "    ETT_TO_IDX = {c:i for i,c in enumerate(ETT_COLS)}\n",
        "    NGT_TO_IDX = {c:i for i,c in enumerate(NGT_COLS)}\n",
        "    CVC_TO_IDX = {c:i for i,c in enumerate(CVC_COLS)}\n",
        "    ett = np.zeros(len(df), np.int64); ngt = np.zeros(len(df), np.int64); cvc = np.zeros(len(df), np.int64)\n",
        "    for i, r in df.iterrows():\n",
        "        e = resolve_group(r, ETT_COLS, PRIORITY['ETT'])\n",
        "        n = resolve_group(r, NGT_COLS, PRIORITY['NGT'])\n",
        "        c = resolve_group(r, CVC_COLS, PRIORITY['CVC'])\n",
        "        ett[i] = ETT_TO_IDX[e]; ngt[i] = NGT_TO_IDX[n]; cvc[i] = CVC_TO_IDX[c]\n",
        "    return ett, ngt, cvc\n",
        "\n",
        "def parse_submission_cols(sample_sub_path):\n",
        "    sub_df = pd.read_csv(sample_sub_path)\n",
        "    cols = sub_df.columns.tolist()\n",
        "    id_col = cols[0]\n",
        "    targets = cols[1:]\n",
        "    return id_col, targets\n",
        "\n",
        "def make_tfms(size):\n",
        "    train_tfms = A.Compose([\n",
        "        A.LongestMaxSize(max_size=size),\n",
        "        A.PadIfNeeded(size, size, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
        "        A.CLAHE(clip_limit=2.0, tile_grid_size=(8,8), p=0.3),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.1, rotate_limit=7, border_mode=cv2.BORDER_CONSTANT, value=0, p=0.5),\n",
        "        A.RandomBrightnessContrast(0.1, 0.1, p=0.5),\n",
        "        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "    valid_tfms = A.Compose([\n",
        "        A.LongestMaxSize(max_size=size),\n",
        "        A.PadIfNeeded(size, size, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
        "        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "    return train_tfms, valid_tfms\n",
        "\n",
        "def load_annotations(ann_csv):\n",
        "    if not os.path.exists(ann_csv):\n",
        "        return {}\n",
        "    ann = pd.read_csv(ann_csv)\n",
        "    by_uid = {}\n",
        "    for _, row in ann.iterrows():\n",
        "        uid = row['StudyInstanceUID']\n",
        "        try:\n",
        "            pts = ast.literal_eval(row['data'])\n",
        "        except Exception:\n",
        "            continue\n",
        "        if not pts:\n",
        "            continue\n",
        "        xs = [p[0] for p in pts if isinstance(p, (list, tuple)) and len(p)==2]\n",
        "        ys = [p[1] for p in pts if isinstance(p, (list, tuple)) and len(p)==2]\n",
        "        if len(xs)==0 or len(ys)==0:\n",
        "            continue\n",
        "        x0, y0, x1, y1 = min(xs), min(ys), max(xs), max(ys)\n",
        "        if uid not in by_uid:\n",
        "            by_uid[uid] = []\n",
        "        by_uid[uid].append([x0, y0, x1, y1])\n",
        "    return by_uid\n",
        "\n",
        "def rasterize_mask(h, w, boxes):\n",
        "    m = np.zeros((h, w), np.uint8)\n",
        "    for x0,y0,x1,y1 in boxes:\n",
        "        x0 = max(0, min(int(x0), w-1)); x1 = max(0, min(int(x1), w-1))\n",
        "        y0 = max(0, min(int(y0), h-1)); y1 = max(0, min(int(y1), h-1))\n",
        "        if x1> x0 and y1> y0:\n",
        "            m[y0:y1, x0:x1] = 255\n",
        "    return m\n",
        "\n",
        "class DS(Dataset):\n",
        "    def __init__(self, df, img_dir, id_col, tfm, ett=None, ngt=None, cvc=None, ann_boxes=None, use_ann=False, out_size=512):\n",
        "        self.df = df.reset_index(drop=True); self.img_dir = img_dir; self.id_col = id_col; self.tfm = tfm\n",
        "        self.ett = ett; self.ngt = ngt; self.cvc = cvc; self.ann_boxes = ann_boxes or {}; self.use_ann = bool(use_ann); self.out_size = out_size\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        uid = self.df.iloc[i][self.id_col]\n",
        "        img = cv2.imread(os.path.join(self.img_dir, f'{uid}.jpg'), cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None: img = np.zeros((self.out_size, self.out_size), np.uint8)\n",
        "        h, w = img.shape[:2]\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "        if self.use_ann:\n",
        "            if uid in self.ann_boxes:\n",
        "                ann_mask = rasterize_mask(h, w, self.ann_boxes[uid])\n",
        "            else:\n",
        "                ann_mask = np.zeros((h, w), dtype=np.uint8)\n",
        "            aug = self.tfm(image=img_rgb, mask=ann_mask)\n",
        "            x = aug['image']\n",
        "            m = aug['mask'].unsqueeze(0).float() / 255.0\n",
        "            mean_gray, std_gray = 0.485, 0.229\n",
        "            m = (m - mean_gray) / std_gray\n",
        "            x = torch.cat([x, m], dim=0)\n",
        "        else:\n",
        "            aug = self.tfm(image=img_rgb)\n",
        "            x = aug['image']\n",
        "        if self.ett is None:\n",
        "            return x, uid\n",
        "        return x, (torch.tensor(int(self.ett[i])), torch.tensor(int(self.ngt[i])), torch.tensor(int(self.cvc[i]))), uid\n",
        "\n",
        "class MultiHeadNet(nn.Module):\n",
        "    def __init__(self, backbone_name='convnext_tiny', in_chans=3, num_ett=3, num_ngt=4, num_cvc=3):\n",
        "        super().__init__()\n",
        "        self.backbone = create_model(backbone_name, pretrained=True, num_classes=0, global_pool='avg', in_chans=in_chans)\n",
        "        feat_dim = getattr(self.backbone, 'num_features', None) or getattr(self.backbone, 'num_features', None)\n",
        "        if feat_dim is None:\n",
        "            try:\n",
        "                feat_dim = self.backbone.num_features\n",
        "            except Exception:\n",
        "                raise RuntimeError('Unable to infer feature dim from backbone')\n",
        "        self.drop = nn.Dropout(0.2)\n",
        "        self.ett = nn.Linear(feat_dim, num_ett)\n",
        "        self.ngt = nn.Linear(feat_dim, num_ngt)\n",
        "        self.cvc = nn.Linear(feat_dim, num_cvc)\n",
        "    def forward(self, x):\n",
        "        f = self.backbone(x)\n",
        "        f = self.drop(f)\n",
        "        return self.ett(f), self.ngt(f), self.cvc(f)\n",
        "\n",
        "def softmax_np(x):\n",
        "    ex = np.exp(x - x.max(axis=1, keepdims=True)); return ex / ex.sum(axis=1, keepdims=True)\n",
        "\n",
        "def heads_to_submission(p_ett, p_ngt, p_cvc, sub_cols):\n",
        "    out = np.zeros((p_ett.shape[0], len(sub_cols)), np.float32)\n",
        "    col_pos = {c:i for i,c in enumerate(sub_cols)}\n",
        "    for j,c in enumerate(['ETT - Abnormal','ETT - Borderline','ETT - Normal']): out[:, col_pos[c]] = p_ett[:, j]\n",
        "    for j,c in enumerate(['NGT - Abnormal','NGT - Borderline','NGT - Incompletely Imaged','NGT - Normal']): out[:, col_pos[c]] = p_ngt[:, j]\n",
        "    if 'CVC - Abnormal' in col_pos:\n",
        "        out[:, col_pos['CVC - Abnormal']] = p_cvc[:, 0]\n",
        "    if 'CVC - Borderline' in col_pos:\n",
        "        out[:, col_pos['CVC - Borderline']] = p_cvc[:, 1]\n",
        "    return out\n",
        "\n",
        "class ModelEMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.ema = nn.Module()\n",
        "        self.ema_state = {k: v.detach().clone().to(v.device) for k,v in model.state_dict().items()}\n",
        "        self.decay = decay\n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        for k, v in model.state_dict().items():\n",
        "            if k in self.ema_state:\n",
        "                self.ema_state[k].mul_((self.decay)).add_(v.detach(), alpha=1.0 - self.decay)\n",
        "    def copy_to(self, model):\n",
        "        model.load_state_dict(self.ema_state, strict=True)\n",
        "\n",
        "class WarmupCosine:\n",
        "    def __init__(self, optimizer, warmup_iters, max_iters, min_lr=1e-6, base_lr=None):\n",
        "        self.opt = optimizer; self.warm = warmup_iters; self.max = max_iters; self.it = 0; self.min_lr = min_lr\n",
        "        self.base = base_lr or [g['lr'] for g in optimizer.param_groups]\n",
        "    def step(self):\n",
        "        self.it += 1\n",
        "        for i, g in enumerate(self.opt.param_groups):\n",
        "            base_lr = self.base[i] if isinstance(self.base, list) else self.base\n",
        "            if self.it <= self.warm:\n",
        "                lr = base_lr * self.it / max(1, self.warm)\n",
        "            else:\n",
        "                t = (self.it - self.warm) / max(1, (self.max - self.warm))\n",
        "                lr = self.min_lr + 0.5*(base_lr - self.min_lr)*(1 + math.cos(math.pi * t))\n",
        "            g['lr'] = lr\n",
        "\n",
        "def make_class_weights(y_int, n_classes, device):\n",
        "    cnt = np.bincount(y_int, minlength=n_classes).astype(np.float32)\n",
        "    cnt[cnt==0] = 1.0\n",
        "    w = cnt.sum() / cnt\n",
        "    w = w / w.mean()\n",
        "    return torch.tensor(w, dtype=torch.float32, device=device)\n",
        "\n",
        "def main():\n",
        "    args = parse_args(); os.makedirs(args.out_dir, exist_ok=True); set_seed(42)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    id_col, SUB_COLS = parse_submission_cols(args.test_csv)\n",
        "    train_df = pd.read_csv(args.train_csv)\n",
        "    sub_df = pd.read_csv(args.test_csv)\n",
        "    with open(args.folds_json,'r') as f: folds = json.load(f)\n",
        "\n",
        "    # Initialize OOF containers for across-fold evaluation\n",
        "    oof = np.zeros((len(train_df), len(SUB_COLS)), dtype=np.float32)\n",
        "    oof_mask = np.zeros((len(train_df), len(SUB_COLS)), dtype=bool)\n",
        "\n",
        "    ann_boxes = load_annotations(args.ann_csv) if args.use_annotations else {}\n",
        "    ett, ngt, cvc = build_head_targets(train_df)\n",
        "    ttfm, vtfm = make_tfms(args.size)\n",
        "\n",
        "    in_chans = 4 if args.use_annotations else 3\n",
        "\n",
        "    # Select folds to run\n",
        "    if args.fold is not None and args.fold >= 0:\n",
        "        folds_to_run = [(args.fold, folds[args.fold])]\n",
        "    else:\n",
        "        folds_to_run = list(enumerate(folds))\n",
        "\n",
        "    if args.inference_only:\n",
        "        model = MultiHeadNet(args.backbone, in_chans=in_chans).to(device).to(memory_format=torch.channels_last)\n",
        "        model.eval()\n",
        "        test_ds = DS(sub_df, args.test_img_dir, id_col, vtfm, ann_boxes=ann_boxes, use_ann=bool(args.use_annotations), out_size=args.size)\n",
        "        dl_te = DataLoader(test_ds, batch_size=args.batch, shuffle=False, num_workers=args.num_workers, pin_memory=True, persistent_workers=args.num_workers>0)\n",
        "        all_fold = []\n",
        "        for k, fold in folds_to_run:\n",
        "            ckpt = os.path.join(args.out_dir, f'best_fold{k}.pt')\n",
        "            state = torch.load(ckpt, map_location=device)\n",
        "            model.load_state_dict(state, strict=True)\n",
        "            preds = []\n",
        "            with torch.no_grad():\n",
        "                for xb, u in dl_te:\n",
        "                    xb = xb.to(device, memory_format=torch.channels_last)\n",
        "                    le, ln, lc = model(xb)\n",
        "                    if args.tta and args.tta >= 2:\n",
        "                        le2, ln2, lc2 = model(torch.flip(xb, dims=[3]))\n",
        "                        le = (le + le2) / 2; ln = (ln + ln2) / 2; lc = (lc + lc2) / 2\n",
        "                    pe = softmax_np(le.detach().cpu().numpy()); pn = softmax_np(ln.detach().cpu().numpy()); pc = softmax_np(lc.detach().cpu().numpy())\n",
        "                    preds.append(heads_to_submission(pe, pn, pc, SUB_COLS))\n",
        "            all_fold.append(np.vstack(preds))\n",
        "        P = np.mean(all_fold, axis=0) if len(all_fold) > 1 else all_fold[0]\n",
        "        sub = pd.DataFrame({id_col: sub_df[id_col]})\n",
        "        for j,c in enumerate(SUB_COLS): sub[c] = P[:, j]\n",
        "        out_path = os.path.join(args.out_dir, 'submission.csv')\n",
        "        sub.to_csv(out_path, index=False)\n",
        "        print('Saved submission to', out_path)\n",
        "        return\n",
        "\n",
        "    # Train\n",
        "    for k, fold in folds_to_run:\n",
        "        tr_idx = np.array(fold['train_idx']); va_idx = np.array(fold['valid_idx'])\n",
        "        tr_df = train_df.iloc[tr_idx].reset_index(drop=True)\n",
        "        va_df = train_df.iloc[va_idx].reset_index(drop=True)\n",
        "        ds_tr = DS(tr_df, args.img_dir, id_col, ttfm, ett[tr_idx], ngt[tr_idx], cvc[tr_idx], ann_boxes=ann_boxes, use_ann=bool(args.use_annotations), out_size=args.size)\n",
        "        ds_va = DS(va_df, args.img_dir, id_col, vtfm, ett[va_idx], ngt[va_idx], cvc[va_idx], ann_boxes=ann_boxes, use_ann=bool(args.use_annotations), out_size=args.size)\n",
        "        dl_tr = DataLoader(ds_tr, batch_size=args.batch, shuffle=True, num_workers=args.num_workers, pin_memory=True, persistent_workers=args.num_workers>0)\n",
        "        dl_va = DataLoader(ds_va, batch_size=args.batch, shuffle=False, num_workers=args.num_workers, pin_memory=True, persistent_workers=args.num_workers>0)\n",
        "\n",
        "        model = MultiHeadNet(args.backbone, in_chans=in_chans).to(device).to(memory_format=torch.channels_last)\n",
        "        opt = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "        iters_per_epoch = max(1, len(dl_tr) // max(1, args.grad_accum))\n",
        "        sched = WarmupCosine(opt, warmup_iters=iters_per_epoch, max_iters=args.epochs * iters_per_epoch, min_lr=1e-6, base_lr=args.lr)\n",
        "        scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "        ema = ModelEMA(model, decay=0.999) if args.ema else None\n",
        "\n",
        "        # class weights per fold\n",
        "        w_e = make_class_weights(ett[tr_idx], 3, device)\n",
        "        w_n = make_class_weights(ngt[tr_idx], 4, device)\n",
        "        w_c = make_class_weights(cvc[tr_idx], 3, device)\n",
        "        ce_e = nn.CrossEntropyLoss(weight=w_e); ce_n = nn.CrossEntropyLoss(weight=w_n); ce_c = nn.CrossEntropyLoss(weight=w_c)\n",
        "\n",
        "        best_auc = -1.0; best_state = None; no_improve = 0\n",
        "        for epoch in range(1, args.epochs+1):\n",
        "            model.train(); tr_loss = 0.0; n_seen = 0; opt.zero_grad(set_to_none=True)\n",
        "            for it, (xb, (ye, yn, yc), _) in enumerate(dl_tr, start=1):\n",
        "                xb = xb.to(device, memory_format=torch.channels_last); ye = ye.to(device); yn = yn.to(device); yc = yc.to(device)\n",
        "                with torch.cuda.amp.autocast(enabled=True):\n",
        "                    le, ln, lc = model(xb)\n",
        "                    loss = ce_e(le, ye) + ce_n(ln, yn) + ce_c(lc, yc)\n",
        "                    loss = loss / max(1, args.grad_accum)\n",
        "                scaler.scale(loss).backward()\n",
        "                if it % args.grad_accum == 0:\n",
        "                    scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True); sched.step()\n",
        "                    if ema: ema.update(model)\n",
        "                bs = xb.size(0); tr_loss += loss.item() * bs; n_seen += bs\n",
        "\n",
        "            # validation with EMA hotfix\n",
        "            eval_model = model\n",
        "            orig_state = None\n",
        "            if ema:\n",
        "                orig_state = {kk: vv.detach().clone() for kk, vv in model.state_dict().items()}\n",
        "                ema.copy_to(model)\n",
        "                eval_model = model\n",
        "            eval_model.eval(); all_e=[]; all_n=[]; all_c=[]\n",
        "            with torch.no_grad():\n",
        "                for xb, (ye, yn, yc), _ in dl_va:\n",
        "                    xb = xb.to(device, memory_format=torch.channels_last)\n",
        "                    le, ln, lc = eval_model(xb)\n",
        "                    pe = le.cpu().numpy(); pn = ln.cpu().numpy(); pc = lc.cpu().numpy()\n",
        "                    if args.tta and args.tta >= 2:\n",
        "                        le2, ln2, lc2 = eval_model(torch.flip(xb, dims=[3]))\n",
        "                        pe = (pe + le2.cpu().numpy())/2; pn = (pn + ln2.cpu().numpy())/2; pc = (pc + lc2.cpu().numpy())/2\n",
        "                    all_e.append(pe); all_n.append(pn); all_c.append(pc)\n",
        "            pe = softmax_np(np.vstack(all_e)); pn = softmax_np(np.vstack(all_n)); pc = softmax_np(np.vstack(all_c))\n",
        "            va_probs = heads_to_submission(pe, pn, pc, SUB_COLS)\n",
        "            y_true = va_df[SUB_COLS].values.astype(np.float32)\n",
        "            aucs=[]\n",
        "            for j in range(len(SUB_COLS)):\n",
        "                try: aucs.append(roc_auc_score(y_true[:,j], va_probs[:,j]))\n",
        "                except: aucs.append(np.nan)\n",
        "            macro_auc = float(np.nanmean([v for v in aucs if not np.isnan(v)]))\n",
        "            print(f'Fold {k} Epoch {epoch}: macro AUC {macro_auc:.5f}')\n",
        "\n",
        "            if ema and orig_state is not None:\n",
        "                model.load_state_dict(orig_state, strict=True)\n",
        "\n",
        "            if macro_auc > best_auc:\n",
        "                best_auc = macro_auc; no_improve = 0\n",
        "                if ema:\n",
        "                    best_state = {kk: vv.detach().cpu() for kk, vv in ema.ema_state.items()}\n",
        "                else:\n",
        "                    best_state = {kk: vv.detach().cpu() for kk, vv in model.state_dict().items()}\n",
        "            else:\n",
        "                no_improve += 1\n",
        "                if args.early_stopping_patience > 0 and no_improve >= args.early_stopping_patience:\n",
        "                    print(f'Early stopping at epoch {epoch} (no improvement for {no_improve} epochs). Best AUC: {best_auc:.5f}')\n",
        "                    break\n",
        "\n",
        "        if best_state is not None:\n",
        "            torch.save(best_state, os.path.join(args.out_dir, f'best_fold{k}.pt'))\n",
        "            print('Saved best checkpoint for fold', k, 'AUC=', best_auc)\n",
        "\n",
        "        # OOF fill for this fold using the exact same TTA policy as validation\n",
        "        # Load best (EMA if enabled) weights for OOF inference to ensure consistency\n",
        "        if best_state is not None:\n",
        "            model.load_state_dict(best_state, strict=True)\n",
        "        model.eval(); all_e=[]; all_n=[]; all_c=[]\n",
        "        with torch.no_grad():\n",
        "            for xb, (ye, yn, yc), _ in dl_va:\n",
        "                xb = xb.to(device, memory_format=torch.channels_last)\n",
        "                le, ln, lc = model(xb)\n",
        "                pe = le.cpu().numpy(); pn = ln.cpu().numpy(); pc = lc.cpu().numpy()\n",
        "                if args.tta and args.tta >= 2:\n",
        "                    le2, ln2, lc2 = model(torch.flip(xb, dims=[3]))\n",
        "                    pe = (pe + le2.cpu().numpy())/2; pn = (pn + ln2.cpu().numpy())/2; pc = (pc + lc2.cpu().numpy())/2\n",
        "                all_e.append(pe); all_n.append(pn); all_c.append(pc)\n",
        "        pe = softmax_np(np.vstack(all_e)); pn = softmax_np(np.vstack(all_n)); pc = softmax_np(np.vstack(all_c))\n",
        "        va_probs_final = heads_to_submission(pe, pn, pc, SUB_COLS)\n",
        "        oof[va_idx] = va_probs_final\n",
        "        oof_mask[va_idx, :] = True\n",
        "\n",
        "    # After all folds (or subset), compute OOF AUC if we ran across any folds\n",
        "    if oof_mask.any():\n",
        "        y_true_all = train_df[SUB_COLS].values.astype(np.float32)\n",
        "        per_label_auc = []\n",
        "        for j in range(len(SUB_COLS)):\n",
        "            m = oof_mask[:, j]\n",
        "            try:\n",
        "                if m.any():\n",
        "                    per_label_auc.append(roc_auc_score(y_true_all[m, j], oof[m, j]))\n",
        "                else:\n",
        "                    per_label_auc.append(np.nan)\n",
        "            except Exception:\n",
        "                per_label_auc.append(np.nan)\n",
        "        macro_oof = float(np.nanmean([v for v in per_label_auc if not np.isnan(v)])) if any([v==v for v in per_label_auc]) else float('nan')\n",
        "        print('OOF Macro AUC:', macro_oof)\n",
        "        # Save OOF to out_dir\n",
        "        oof_df = pd.DataFrame({id_col: train_df[id_col].values})\n",
        "        for j,c in enumerate(SUB_COLS): oof_df[c] = oof[:, j]\n",
        "        oof_path = os.path.join(args.out_dir, 'oof_probs.csv')\n",
        "        oof_df.to_csv(oof_path, index=False)\n",
        "        print('Saved OOF to', oof_path)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "'''\n",
        "open('gpu_train_multihead.py','w').write(script)\n",
        "\n",
        "# Pinned userland dependencies (torch is expected to be present in GPU runtime).\n",
        "requirements = '''\n",
        "timm==1.0.19\n",
        "albumentations==1.3.1\n",
        "opencv-python-headless==4.11.0.86\n",
        "scikit-learn==1.5.2\n",
        "pandas==2.2.2\n",
        "numpy==1.26.4\n",
        "# torch and torchvision are expected to be preinstalled in the GPU runtime; if needed, install matching CUDA wheels manually.\n",
        "'''\n",
        "open('requirements.txt','w').write(requirements.strip() + '\\n')\n",
        "\n",
        "# ENSEMBLE runner: FOLD-LEVEL resumable + minimal sanity (single fold, single model) + DATA PREFLIGHT, then portfolio training and averaging.\n",
        "runner = '''#!/usr/bin/env bash\n",
        "set -euo pipefail\n",
        "\n",
        "echo \"== Env check ==\"\n",
        "python - <<'PY'\n",
        "import torch, sys\n",
        "print({'cuda': torch.cuda.is_available(), 'device_count': torch.cuda.device_count(), 'cuda_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None})\n",
        "PY\n",
        "\n",
        "echo \"== Data preflight ==\"\n",
        "if [ ! -d \"train\" ] || [ ! -d \"test\" ]; then\n",
        "  echo \"ERROR: Missing train/ or test/ directories. Please sync image data into ./train and ./test before running.\"\n",
        "  echo \"Examples: rsync -avP /path/to/train/ ./train/  |  rsync -avP /path/to/test/ ./test/\"\n",
        "  exit 2\n",
        "fi\n",
        "n_train=$(ls -1 train/*.jpg 2>/dev/null | wc -l || true)\n",
        "n_test=$(ls -1 test/*.jpg 2>/dev/null | wc -l || true)\n",
        "echo \"Found images -> train: ${n_train}, test: ${n_test}\"\n",
        "if [ \"${n_train}\" -lt 1000 ] || [ \"${n_test}\" -lt 100 ]; then\n",
        "  echo \"ERROR: Insufficient images detected. Ensure all competition JPEGs are present in ./train and ./test.\"\n",
        "  exit 3\n",
        "fi\n",
        "\n",
        "echo \"== Install pinned deps ==\"\n",
        "pip -q install -r requirements.txt\n",
        "\n",
        "echo \"== Minimal sanity check: 1 epoch, 1 fold, fastest model (convnext_tiny@512) ==\"\n",
        "python gpu_train_multihead.py \\\n",
        "  --train_csv train.csv --img_dir train \\\n",
        "  --test_csv sample_submission.csv --test_img_dir test \\\n",
        "  --folds_json cv_folds_patientid_5fold.json \\\n",
        "  --size 512 --epochs 1 --early_stopping_patience 1 --batch 16 --grad_accum 1 --lr 2e-4 \\\n",
        "  --backbone convnext_tiny --out_dir sanity_convnext_tiny_512 \\\n",
        "  --ema 1 --tta 1 --use_annotations 1 --num_workers 8 --fold 0\n",
        "\n",
        "train_one() {\n",
        "  local BACKBONE=\"$1\"\n",
        "  local IMG_SIZE=\"$2\"\n",
        "  local LR=\"$3\"\n",
        "  local OUT_DIR=\"$4\"\n",
        "  local BATCH_SIZE=\"$5\"\n",
        "\n",
        "  # If all fold checkpoints already exist, skip training\n",
        "  all_done=true\n",
        "  for FOLD_NUM in 0 1 2 3 4; do\n",
        "    if [ ! -f \"${OUT_DIR}/best_fold${FOLD_NUM}.pt\" ]; then\n",
        "      all_done=false\n",
        "      break\n",
        "    fi\n",
        "  done\n",
        "  if [ \"$all_done\" = true ]; then\n",
        "    echo \"[SKIP] ${BACKBONE} @${IMG_SIZE}: all fold checkpoints present.\"\n",
        "  else\n",
        "    mkdir -p \"${OUT_DIR}\"\n",
        "    echo \"== Train (8 epochs, patience=2) ${BACKBONE} @${IMG_SIZE} (fold-level resumable) ==\"\n",
        "    for FOLD_NUM in 0 1 2 3 4; do\n",
        "      if [ -f \"${OUT_DIR}/best_fold${FOLD_NUM}.pt\" ]; then\n",
        "        echo \"[SKIP] Fold ${FOLD_NUM} for ${BACKBONE}: checkpoint exists.\"\n",
        "        continue\n",
        "      fi\n",
        "      echo \"== Training Fold ${FOLD_NUM} for ${BACKBONE} ==\"\n",
        "      python gpu_train_multihead.py \\\n",
        "        --train_csv train.csv --img_dir train \\\n",
        "        --test_csv sample_submission.csv --test_img_dir test \\\n",
        "        --folds_json cv_folds_patientid_5fold.json \\\n",
        "        --size ${IMG_SIZE} --epochs 8 --early_stopping_patience 2 --batch ${BATCH_SIZE} --grad_accum 2 --lr ${LR} --wd 1e-4 \\\n",
        "        --backbone ${BACKBONE} --out_dir ${OUT_DIR} \\\n",
        "        --ema 1 --tta 2 --use_annotations 1 --num_workers 8 \\\n",
        "        --fold ${FOLD_NUM}\n",
        "    done\n",
        "  fi\n",
        "\n",
        "  echo \"== Inference ${BACKBONE} @${IMG_SIZE} (fold-average with TTA) ==\"\n",
        "  python gpu_train_multihead.py \\\n",
        "    --inference_only 1 --test_csv sample_submission.csv --test_img_dir test \\\n",
        "    --folds_json cv_folds_patientid_5fold.json --size ${IMG_SIZE} \\\n",
        "    --backbone ${BACKBONE} --out_dir ${OUT_DIR} --tta 2 --use_annotations 1 --num_workers 8 --fold -1\n",
        "}\n",
        "\n",
        "# Portfolio definitions\n",
        "CNX_BACKBONE=\"convnext_tiny\"; CNX_SIZE=512; CNX_LR=2e-4; CNX_OUT=\"outputs_convnext_tiny_512\"; CNX_BS=32\n",
        "EFB_BACKBONE=\"tf_efficientnet_b4_ns\"; EFB_SIZE=448; EFB_LR=2e-4; EFB_OUT=\"outputs_tf_efficientnet_b4_ns_448\"; EFB_BS=24\n",
        "SWB_BACKBONE=\"swin_base_patch4_window7_224_in22k\"; SWB_SIZE=384; SWB_LR=2e-4; SWB_OUT=\"outputs_swin_base_patch4_window7_224_in22k_384\"; SWB_BS=24\n",
        "\n",
        "# Train each model (fold-level resumable)\n",
        "train_one \"$CNX_BACKBONE\" \"$CNX_SIZE\" \"$CNX_LR\" \"$CNX_OUT\" \"$CNX_BS\"\n",
        "train_one \"$EFB_BACKBONE\" \"$EFB_SIZE\" \"$EFB_LR\" \"$EFB_OUT\" \"$EFB_BS\"\n",
        "train_one \"$SWB_BACKBONE\" \"$SWB_SIZE\" \"$SWB_LR\" \"$SWB_OUT\" \"$SWB_BS\"\n",
        "\n",
        "echo \"== Ensembling submissions (simple average) ==\"\n",
        "python - <<'PY'\n",
        "import os, pandas as pd\n",
        "paths = [\n",
        "  'outputs_convnext_tiny_512/submission.csv',\n",
        "  'outputs_tf_efficientnet_b4_ns_448/submission.csv',\n",
        "  'outputs_swin_base_patch4_window7_224_in22k_384/submission.csv',\n",
        "]\n",
        "for p in paths:\n",
        "    if not os.path.exists(p):\n",
        "        raise FileNotFoundError(f\"Missing model submission: {p}\")\n",
        "dfs = [pd.read_csv(p) for p in paths]\n",
        "sub = dfs[0].copy()\n",
        "for c in sub.columns[1:]:\n",
        "    sub[c] = sum(df[c] for df in dfs) / len(dfs)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Ensembled submission.csv saved from:', paths)\n",
        "PY\n",
        "\n",
        "echo \"All done. submission.csv ready.\"\n",
        "'''\n",
        "open('run_gpu.sh','w').write(runner)\n",
        "import os, stat\n",
        "os.chmod('run_gpu.sh', os.stat('run_gpu.sh').st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)\n",
        "print('Updated README_GPU_PLAN.md, gpu_train_multihead.py, requirements.txt, and run_gpu.sh with data preflight + fold-level resumability + mask normalization + OOF TTA consistency. Ready for GPU execution.')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated README_GPU_PLAN.md, gpu_train_multihead.py, requirements.txt, and run_gpu.sh with data preflight + fold-level resumability + mask normalization + OOF TTA consistency. Ready for GPU execution.\n"
          ]
        }
      ]
    },
    {
      "id": "da293581-06f3-47fa-9242-4a8ba0a4116e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a GPU handoff bundle with scripts and manifests (no images), ensuring requirements.txt and correct run_gpu.sh are included.\n",
        "import os, json, hashlib, tarfile, glob\n",
        "from datetime import datetime\n",
        "\n",
        "def md5(path, chunk=1<<20):\n",
        "    m = hashlib.md5()\n",
        "    with open(path, 'rb') as f:\n",
        "        for b in iter(lambda: f.read(chunk), b''):\n",
        "            m.update(b)\n",
        "    return m.hexdigest()\n",
        "\n",
        "# Ensure expected files are present\n",
        "files = [\n",
        "    'gpu_train_multihead.py',\n",
        "    'run_gpu.sh',\n",
        "    'requirements.txt',\n",
        "    'cv_folds_patientid_5fold.json',\n",
        "    'README_GPU_PLAN.md',\n",
        "    'train.csv',\n",
        "    'sample_submission.csv',\n",
        "    'train_annotations.csv'\n",
        "]\n",
        "\n",
        "# Filter existing files only\n",
        "files = [f for f in files if os.path.exists(f)]\n",
        "\n",
        "# Inventory + quick sanity snippet from run_gpu.sh\n",
        "snippet = ''\n",
        "if os.path.exists('run_gpu.sh'):\n",
        "    try:\n",
        "        with open('run_gpu.sh', 'r') as fh:\n",
        "            snippet = fh.read(240)\n",
        "    except Exception:\n",
        "        snippet = ''\n",
        "\n",
        "inv = {\n",
        "  'timestamp': datetime.utcnow().isoformat()+'Z',\n",
        "  'cwd': os.getcwd(),\n",
        "  'n_train_imgs': len(glob.glob('train/*.jpg')),\n",
        "  'n_test_imgs': len(glob.glob('test/*.jpg')),\n",
        "  'files': [{ 'path': f, 'size': os.path.getsize(f), 'md5': md5(f) } for f in files],\n",
        "  'run_gpu_head': snippet,\n",
        "  'notes': 'Move bundle to a GPU runtime. Run: bash run_gpu.sh'\n",
        "}\n",
        "with open('gpu_handoff_manifest.json','w') as f:\n",
        "    json.dump(inv, f, indent=2)\n",
        "files.append('gpu_handoff_manifest.json')\n",
        "\n",
        "bundle = 'gpu_handoff_bundle.tar.gz'\n",
        "with tarfile.open(bundle, 'w:gz') as tar:\n",
        "    for f in files:\n",
        "        tar.add(f)\n",
        "print('Created bundle:', bundle)\n",
        "print('Contents:')\n",
        "for e in inv['files']:\n",
        "    print(' -', e['path'], e['size'], e['md5'])\n",
        "print('\\nrun_gpu.sh head (sanity):')\n",
        "print(inv['run_gpu_head'])\n",
        "print('Manifest saved to gpu_handoff_manifest.json')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created bundle: gpu_handoff_bundle.tar.gz\nContents:\n - gpu_train_multihead.py 20073 ee9ec8d16a0c1984ca08806cf3e3de9d\n - run_gpu.sh 4403 cbcd475216698dbceb541fbfe2ae9224\n - requirements.txt 243 3876ff6a869db49540c1b1fd9b0d76ea\n - cv_folds_patientid_5fold.json 892255 e2abaa2990b8fe0191d7c82703d34452\n - README_GPU_PLAN.md 1295 26bbb2f9a2ddcac95d155e102d89ae87\n - train.csv 2626393 620fa6759729f96539ed66ae860a84f9\n - sample_submission.csv 249912 fade90160bce8a12138711b20fa89584\n - train_annotations.csv 4476635 1929aab17c1206719cc7edf6d7e9e57b\n\nrun_gpu.sh head (sanity):\n#!/usr/bin/env bash\nset -euo pipefail\n\necho \"== Env check ==\"\npython - <<'PY'\nimport torch, sys\nprint({'cuda': torch.cuda.is_available(), 'device_count': torch.cuda.device_count(), 'cuda_name': torch.cuda.get_device_name(0) if torch.cuda.is\nManifest saved to gpu_handoff_manifest.json\n"
          ]
        }
      ]
    },
    {
      "id": "f86f56f0-7762-4913-88cf-d64837c90c3d",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Handoff: Quick-Start Instructions\n",
        "\n",
        "- Files packaged: gpu_handoff_bundle.tar.gz with:\n",
        "  - gpu_train_multihead.py\n",
        "  - run_gpu.sh (MD5: cbcd475216698dbceb541fbfe2ae9224) \u2014 ensemble runner with fold-level resumability, data preflight, and final averaging\n",
        "  - requirements.txt (MD5: 3876ff6a869db49540c1b1fd9b0d76ea)\n",
        "  - cv_folds_patientid_5fold.json, README_GPU_PLAN.md, train.csv, sample_submission.csv, train_annotations.csv\n",
        "\n",
        "Before running (REQUIRED on GPU host):\n",
        "- Ensure image data is present: ./train and ./test must contain all competition JPEGs.\n",
        "- Example provisioning:\n",
        "  - rsync -avP /path/to/train/ ./train/\n",
        "  - rsync -avP /path/to/test/  ./test/\n",
        "  - or scp -r train test <gpu_host>:~/ranzcr/\n",
        "\n",
        "Run on a CUDA GPU runtime (Kaggle/Colab/RunPod/etc.):\n",
        "\n",
        "1) Verify CUDA\n",
        "```bash\n",
        "python - <<'PY'\n",
        "import torch; print({'cuda': torch.cuda.is_available(), 'n': torch.cuda.device_count(), 'name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None})\n",
        "PY\n",
        "```\n",
        "\n",
        "2) Extract bundle\n",
        "```bash\n",
        "tar -xzf gpu_handoff_bundle.tar.gz\n",
        "```\n",
        "\n",
        "3) Execute multi-model training + inference + ensemble\n",
        "```bash\n",
        "bash run_gpu.sh\n",
        "```\n",
        "\n",
        "Outputs:\n",
        "- sanity_convnext_tiny_512/: quick 1-epoch, single-fold sanity\n",
        "- outputs_convnext_tiny_512/: fold checkpoints best_fold{0..4}.pt + submission.csv\n",
        "- outputs_tf_efficientnet_b4_ns_448/: fold checkpoints + submission.csv\n",
        "- outputs_swin_base_patch4_window7_224_in22k_384/: fold checkpoints + submission.csv\n",
        "- Final ensemble saved to ./submission.csv (simple average across the three submissions)\n",
        "\n",
        "Notes:\n",
        "- Data preflight will halt if train/ or test/ images are missing or counts are too small.\n",
        "- Fold-level resumability: re-running will skip completed folds via best_fold{N}.pt.\n",
        "- Early stopping via --early_stopping_patience prevents timeouts.\n",
        "- EMA and TTA enabled; annotations used as 4th channel consistently for both training and inference (--use_annotations 1).\n",
        "- GroupKFold by PatientID; Warmup+Cosine LR; AMP.\n",
        "- Torch with matching CUDA should be preinstalled; requirements.txt installs userland deps."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "a02164a2-999b-4ee2-b90e-0445544c5945",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Backup and GPU run helper: timestamped bundle copy + MD5s + ready-to-copy commands\n",
        "import os, hashlib, shutil, datetime, json\n",
        "\n",
        "def md5(p, chunk=1<<20):\n",
        "    m = hashlib.md5()\n",
        "    with open(p, 'rb') as f:\n",
        "        for b in iter(lambda: f.read(chunk), b''):\n",
        "            m.update(b)\n",
        "    return m.hexdigest()\n",
        "\n",
        "ts = datetime.datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
        "bundle = 'gpu_handoff_bundle.tar.gz'\n",
        "assert os.path.exists(bundle), 'gpu_handoff_bundle.tar.gz not found; run Cell 21 to build it.'\n",
        "backup = f'gpu_handoff_bundle_{ts}.tar.gz'\n",
        "shutil.copy2(bundle, backup)\n",
        "\n",
        "files = ['run_gpu.sh','gpu_train_multihead.py','requirements.txt','cv_folds_patientid_5fold.json','train.csv','sample_submission.csv','train_annotations.csv']\n",
        "info = {\n",
        "  'timestamp_utc': ts,\n",
        "  'bundle': {'path': bundle, 'md5': md5(bundle), 'size': os.path.getsize(bundle)},\n",
        "  'bundle_backup': {'path': backup, 'md5': md5(backup), 'size': os.path.getsize(backup)},\n",
        "  'files': []\n",
        "}\n",
        "for f in files:\n",
        "    if os.path.exists(f):\n",
        "        info['files'].append({'path': f, 'md5': md5(f), 'size': os.path.getsize(f)})\n",
        "\n",
        "print('MD5 summary:')\n",
        "print(json.dumps(info, indent=2))\n",
        "\n",
        "print('\\nGPU run commands (copy-paste):')\n",
        "print('# 1) Copy bundle to GPU host (edit <gpu_host> and path as needed)')\n",
        "print(f'scp {bundle} <gpu_host>:~/ranzcr/')\n",
        "print('# 2) On GPU host: extract and run')\n",
        "print('ssh <gpu_host> \"cd ~/ranzcr && tar -xzf gpu_handoff_bundle.tar.gz && bash run_gpu.sh\"')\n",
        "print('# 3) Retrieve final submission back here')\n",
        "print('scp <gpu_host>:~/ranzcr/submission.csv ./submission.csv')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MD5 summary:\n{\n  \"timestamp_utc\": \"20250824_163606\",\n  \"bundle\": {\n    \"path\": \"gpu_handoff_bundle.tar.gz\",\n    \"md5\": \"ef7bc533f07bfeacb18f451384bc08e6\",\n    \"size\": 2507662\n  },\n  \"bundle_backup\": {\n    \"path\": \"gpu_handoff_bundle_20250824_163606.tar.gz\",\n    \"md5\": \"ef7bc533f07bfeacb18f451384bc08e6\",\n    \"size\": 2507662\n  },\n  \"files\": [\n    {\n      \"path\": \"run_gpu.sh\",\n      \"md5\": \"cbcd475216698dbceb541fbfe2ae9224\",\n      \"size\": 4403\n    },\n    {\n      \"path\": \"gpu_train_multihead.py\",\n      \"md5\": \"ee9ec8d16a0c1984ca08806cf3e3de9d\",\n      \"size\": 20073\n    },\n    {\n      \"path\": \"requirements.txt\",\n      \"md5\": \"3876ff6a869db49540c1b1fd9b0d76ea\",\n      \"size\": 243\n    },\n    {\n      \"path\": \"cv_folds_patientid_5fold.json\",\n      \"md5\": \"e2abaa2990b8fe0191d7c82703d34452\",\n      \"size\": 892255\n    },\n    {\n      \"path\": \"train.csv\",\n      \"md5\": \"620fa6759729f96539ed66ae860a84f9\",\n      \"size\": 2626393\n    },\n    {\n      \"path\": \"sample_submission.csv\",\n      \"md5\": \"fade90160bce8a12138711b20fa89584\",\n      \"size\": 249912\n    },\n    {\n      \"path\": \"train_annotations.csv\",\n      \"md5\": \"1929aab17c1206719cc7edf6d7e9e57b\",\n      \"size\": 4476635\n    }\n  ]\n}\n\nGPU run commands (copy-paste):\n# 1) Copy bundle to GPU host (edit <gpu_host> and path as needed)\nscp gpu_handoff_bundle.tar.gz <gpu_host>:~/ranzcr/\n# 2) On GPU host: extract and run\nssh <gpu_host> \"cd ~/ranzcr && tar -xzf gpu_handoff_bundle.tar.gz && bash run_gpu.sh\"\n# 3) Retrieve final submission back here\nscp <gpu_host>:~/ranzcr/submission.csv ./submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "e2de337d-94f2-43dd-8017-804dbbcce6f3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Auto GPU-run helper: if a CUDA GPU is available in this runtime, execute the hardened ensemble runner.\n",
        "import os, sys, subprocess, json, shutil, hashlib\n",
        "import importlib\n",
        "\n",
        "def md5(path, chunk=1<<20):\n",
        "    m = hashlib.md5()\n",
        "    with open(path, 'rb') as f:\n",
        "        for b in iter(lambda: f.read(chunk), b''):\n",
        "            m.update(b)\n",
        "    return m.hexdigest()\n",
        "\n",
        "def run(cmd, check=True):\n",
        "    print('>>>', cmd)\n",
        "    rc = subprocess.call(cmd, shell=True)\n",
        "    if check and rc != 0:\n",
        "        raise SystemExit(rc)\n",
        "    return rc\n",
        "\n",
        "cuda = False\n",
        "try:\n",
        "    import torch\n",
        "    cuda = torch.cuda.is_available()\n",
        "except Exception:\n",
        "    cuda = False\n",
        "\n",
        "print('CUDA available:', cuda)\n",
        "if not cuda:\n",
        "    print('No CUDA GPU detected in this environment. Please run on a CUDA host:')\n",
        "    print('  tar -xzf gpu_handoff_bundle.tar.gz && bash run_gpu.sh')\n",
        "    print('After completion, copy back ./submission.csv here.')\n",
        "else:\n",
        "    # Light env check and dependency install for userland only\n",
        "    print('Installing pinned userland requirements (torch expected preinstalled with CUDA)...')\n",
        "    run('pip -q install -r requirements.txt')\n",
        "    # Data preflight mirrors run_gpu.sh checks\n",
        "    if not os.path.isdir('train') or not os.path.isdir('test'):\n",
        "        raise SystemExit('ERROR: Missing train/ or test/ directories.')\n",
        "    n_train = len([p for p in os.scandir('train') if p.name.endswith('.jpg')])\n",
        "    n_test = len([p for p in os.scandir('test') if p.name.endswith('.jpg')])\n",
        "    print({'n_train': n_train, 'n_test': n_test})\n",
        "    if n_train < 1000 or n_test < 100:\n",
        "        raise SystemExit('ERROR: Insufficient images detected. Sync full datasets into ./train and ./test.')\n",
        "    # Execute the hardened runner (fold-level resumable, EMA+TTA, annotations)\n",
        "    run('bash run_gpu.sh')\n",
        "    if os.path.exists('submission.csv'):\n",
        "        print('SUCCESS: submission.csv generated with GPU ensemble.')\n",
        "        print('MD5:', md5('submission.csv'), 'Size:', os.path.getsize('submission.csv'))\n",
        "    else:\n",
        "        raise SystemExit('Runner finished but submission.csv not found.')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: False\nNo CUDA GPU detected in this environment. Please run on a CUDA host:\n  tar -xzf gpu_handoff_bundle.tar.gz && bash run_gpu.sh\nAfter completion, copy back ./submission.csv here.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}