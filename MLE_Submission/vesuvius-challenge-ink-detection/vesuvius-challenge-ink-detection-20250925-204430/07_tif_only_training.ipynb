{
  "cells": [
    {
      "id": "40fc1b84-fc77-4ceb-b149-b79d56b0a4cd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- 0. Pre-flight Check: Verify Data Integrity ---\n",
        "import os\n",
        "import re\n",
        "\n",
        "print(\"--- Verifying available Z-slices for each fragment ---\")\n",
        "\n",
        "def get_available_slices(fragment_id):\n",
        "    try:\n",
        "        folder_path = f'train/{fragment_id}/surface_volume'\n",
        "        files = os.listdir(folder_path)\n",
        "        slice_numbers = [int(re.search(r'(\\d+)', f).group(1)) for f in files if f.endswith('.tif')]\n",
        "        if not slice_numbers:\n",
        "            return None, None\n",
        "        return min(slice_numbers), max(slice_numbers)\n",
        "    except FileNotFoundError:\n",
        "        return None, None\n",
        "\n",
        "fragments_to_check = ['1', '2', '3']\n",
        "all_slices_min = []\n",
        "all_slices_max = []\n",
        "\n",
        "for frag_id in fragments_to_check:\n",
        "    min_slice, max_slice = get_available_slices(frag_id)\n",
        "    if min_slice is not None:\n",
        "        print(f\"Fragment {frag_id}: Slices from {min_slice} to {max_slice}\")\n",
        "        all_slices_min.append(min_slice)\n",
        "        all_slices_max.append(max_slice)\n",
        "    else:\n",
        "        print(f\"Fragment {frag_id}: Could not find slice data.\")\n",
        "\n",
        "# Determine the common range\n",
        "if all_slices_min and all_slices_max:\n",
        "    common_min = max(all_slices_min)\n",
        "    common_max = min(all_slices_max)\n",
        "    print(f\"\\nCommon available Z-slice range: {common_min} to {common_max}\")\n",
        "    print(f\"Recommended CFG.Z_START = {common_min}\")\n",
        "    # The end of range in Python is exclusive, so we need to add 1 to the max slice number\n",
        "    print(f\"Recommended CFG.Z_END = {common_max + 1}\")\n",
        "else:\n",
        "    print(\"\\nCould not determine a common slice range.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "46a8b10c-a721-473f-bfd5-d527afa2732e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import gc\n",
        "import random\n",
        "import time\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# --- Configuration ---\n",
        "class CFG:\n",
        "    # General\n",
        "    DEBUG = False\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    USE_AMP = True\n",
        "    \n",
        "    # Data\n",
        "    DATA_PATH = 'train/'\n",
        "    TRAIN_FRAGMENTS = [1]\n",
        "    VAL_FRAGMENTS = [2]\n",
        "    \n",
        "    # Slices (as per expert advice)\n",
        "    Z_START = 20\n",
        "    Z_END = 45\n",
        "    IN_CHANNELS = Z_END - Z_START\n",
        "    \n",
        "    # Tiling (as per expert advice)\n",
        "    TILE_SIZE = 256\n",
        "    STRIDE = TILE_SIZE // 2\n",
        "\n",
        "    # Model\n",
        "    MODEL_NAME = 'Unet'\n",
        "    ENCODER_NAME = 'efficientnet-b4'\n",
        "    ENCODER_WEIGHTS = 'imagenet'\n",
        "    \n",
        "    # Training (as per expert advice)\n",
        "    BATCH_SIZE = 8\n",
        "    EPOCHS = 12\n",
        "    LEARNING_RATE = 1e-4\n",
        "    WEIGHT_DECAY = 1e-6\n",
        "    EARLY_STOPPING_PATIENCE = 3\n",
        "    \n",
        "    # Pre-caching (as per expert advice)\n",
        "    CACHE_DIR = './cache_tif_only_v2/'\n",
        "    REBUILD_CACHE = True\n",
        "\n",
        "if CFG.DEBUG:\n",
        "    CFG.TRAIN_FRAGMENTS = [1]\n",
        "    CFG.VAL_FRAGMENTS = [1]\n",
        "    CFG.EPOCHS = 2\n",
        "    CFG.REBUILD_CACHE = True\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "print(f\"Device: {CFG.DEVICE}\")\n",
        "print(f\"Input Channels: {CFG.IN_CHANNELS}\")\n",
        "print(f\"Rebuilding Cache: {CFG.REBUILD_CACHE}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "3c3cb1c8-1ee7-4bb9-a0ff-b88ceff89339",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- 1. Configuration ---\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import segmentation_models_pytorch as smp\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "class CFG:\n",
        "    # General\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    SEED = 42\n",
        "    \n",
        "    # Data Paths & Cache\n",
        "    DATA_PATH = './'\n",
        "    CACHE_DIR = 'cache_tif_only_v5_b0' # Changed version to avoid conflicts\n",
        "    \n",
        "    # Fragments\n",
        "    TRAIN_FRAGMENTS = [1]\n",
        "    VALID_FRAGMENTS = [2]\n",
        "    \n",
        "    # Slices (as per expert advice)\n",
        "    Z_START = 20\n",
        "    Z_END = 45\n",
        "    IN_CHANNELS = Z_END - Z_START\n",
        "\n",
        "    # Tiling & Sampling\n",
        "    TILE_SIZE = 256\n",
        "    STRIDE = TILE_SIZE // 2\n",
        "    NEGATIVE_INK_RATIO = 1.0\n",
        "\n",
        "    # Model\n",
        "    ENCODER_NAME = 'efficientnet-b0'\n",
        "    ENCODER_WEIGHTS = 'imagenet'\n",
        "    MODEL_SAVE_PATH = 'best_tif_only_model_v5_b0.pth' # Changed version\n",
        "\n",
        "    # Training\n",
        "    EPOCHS = 15\n",
        "    BATCH_SIZE = 16\n",
        "    LEARNING_RATE = 1e-4\n",
        "    WEIGHT_DECAY = 1e-6\n",
        "    USE_AMP = True\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(CFG.SEED)\n",
        "print(f\"Using device: {CFG.DEVICE}\")\n",
        "print(f\"Input channels: {CFG.IN_CHANNELS}\")\n",
        "\n",
        "# --- 2. Data Preparation (with Caching) ---\n",
        "\n",
        "def get_train_valid_tiles(fragment_id):\n",
        "    images = []\n",
        "    for i in range(CFG.Z_START, CFG.Z_END):\n",
        "        image = cv2.imread(f\"{CFG.DATA_PATH}train/{fragment_id}/surface_volume/{i:02}.tif\", 0)\n",
        "        images.append(image)\n",
        "    images = np.stack(images, axis=-1)\n",
        "\n",
        "    mask = cv2.imread(f\"{CFG.DATA_PATH}train/{fragment_id}/inklabels.png\", 0)\n",
        "    mask = (mask > 0).astype(np.uint8)\n",
        "    \n",
        "    roi = cv2.imread(f\"{CFG.DATA_PATH}train/{fragment_id}/mask.png\", 0)\n",
        "    roi = (roi > 0).astype(np.uint8)\n",
        "\n",
        "    coords = []\n",
        "    positive_coords = []\n",
        "    negative_coords = []\n",
        "\n",
        "    for y in range(0, images.shape[0] - CFG.TILE_SIZE + 1, CFG.STRIDE):\n",
        "        for x in range(0, images.shape[1] - CFG.TILE_SIZE + 1, CFG.STRIDE):\n",
        "            if roi[y:y+CFG.TILE_SIZE, x:x+CFG.TILE_SIZE].mean() > 0.99:\n",
        "                tile_mask = mask[y:y+CFG.TILE_SIZE, x:x+CFG.TILE_SIZE]\n",
        "                if tile_mask.sum() > 0:\n",
        "                    positive_coords.append((y, x))\n",
        "                else:\n",
        "                    negative_coords.append((y, x))\n",
        "\n",
        "    num_positive = len(positive_coords)\n",
        "    num_negative_to_sample = int(num_positive * CFG.NEGATIVE_INK_RATIO)\n",
        "    \n",
        "    print(f\"Found {num_positive} positive tiles and sampling {min(num_negative_to_sample, len(negative_coords))} negative tiles.\")\n",
        "\n",
        "    random.shuffle(negative_coords)\n",
        "    coords.extend(positive_coords)\n",
        "    coords.extend(negative_coords[:min(num_negative_to_sample, len(negative_coords))])\n",
        "    \n",
        "    os.makedirs(CFG.CACHE_DIR, exist_ok=True)\n",
        "    for i, (y, x) in enumerate(tqdm(coords, desc=f'Saving Tiles for {fragment_id}')):\n",
        "        tile_img = images[y:y+CFG.TILE_SIZE, x:x+CFG.TILE_SIZE]\n",
        "        tile_mask = mask[y:y+CFG.TILE_SIZE, x:x+CFG.TILE_SIZE]\n",
        "        np.save(f'{CFG.CACHE_DIR}/{fragment_id}_{i}_img.npy', tile_img)\n",
        "        np.save(f'{CFG.CACHE_DIR}/{fragment_id}_{i}_mask.npy', tile_mask)\n",
        "        \n",
        "    print(f\"Saved {len(coords)} total tiles for fragment {fragment_id}.\")\n",
        "    return [f'{fragment_id}_{i}' for i in range(len(coords))]\n",
        "\n",
        "class VesuviusDataset(Dataset):\n",
        "    def __init__(self, tile_ids, transform=None):\n",
        "        self.tile_ids = tile_ids\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tile_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tile_id = self.tile_ids[idx]\n",
        "        image = np.load(f'{CFG.CACHE_DIR}/{tile_id}_img.npy')\n",
        "        mask = np.load(f'{CFG.CACHE_DIR}/{tile_id}_mask.npy')\n",
        "\n",
        "        p_low, p_high = np.percentile(image, [1, 99])\n",
        "        image = np.clip((image.astype(np.float32) - p_low) / (p_high - p_low + 1e-6), 0, 1)\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "        \n",
        "        return image, mask.unsqueeze(0)\n",
        "\n",
        "def get_transforms():\n",
        "    train_transform = A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.RandomRotate90(p=0.5),\n",
        "        ToTensorV2(transpose_mask=True),\n",
        "    ])\n",
        "    valid_transform = A.Compose([\n",
        "        ToTensorV2(transpose_mask=True),\n",
        "    ])\n",
        "    return train_transform, valid_transform\n",
        "\n",
        "# --- 3. Loss, Model, and Training Loop ---\n",
        "\n",
        "class BCETverskyLoss(nn.Module):\n",
        "    def __init__(self, bce_weight=0.5, tversky_alpha=0.5, tversky_beta=0.5):\n",
        "        super(BCETverskyLoss, self).__init__()\n",
        "        self.bce_weight = bce_weight\n",
        "        self.tversky_alpha = tversky_alpha\n",
        "        self.tversky_beta = tversky_beta\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        bce_loss = self.bce(inputs, targets)\n",
        "        probs = torch.sigmoid(inputs)\n",
        "        tp = (probs * targets).sum()\n",
        "        fp = (probs * (1 - targets)).sum()\n",
        "        fn = ((1 - probs) * targets).sum()\n",
        "        tversky_index = tp / (tp + self.tversky_alpha * fp + self.tversky_beta * fn + 1e-6)\n",
        "        tversky_loss = 1 - tversky_index\n",
        "        return self.bce_weight * bce_loss + (1 - self.bce_weight) * tversky_loss\n",
        "\n",
        "def fbeta_score(y_pred, y_true, beta=0.5, thr=0.5):\n",
        "    y_pred = (y_pred > thr).float()\n",
        "    tp = (y_true * y_pred).sum()\n",
        "    fp = ((1 - y_true) * y_pred).sum()\n",
        "    fn = (y_true * (1 - y_pred)).sum()\n",
        "    precision = tp / (tp + fp + 1e-6)\n",
        "    recall = tp / (tp + fn + 1e-6)\n",
        "    fbeta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall + 1e-6)\n",
        "    return fbeta\n",
        "\n",
        "def get_dataloaders():\n",
        "    train_tiles, valid_tiles = [], []\n",
        "    print(\"--- Preparing Training Data ---\")\n",
        "    for fragment_id in CFG.TRAIN_FRAGMENTS:\n",
        "        print(f\"Processing fragment {fragment_id}...\")\n",
        "        train_tiles.extend(get_train_valid_tiles(fragment_id))\n",
        "    print(\"--- Preparing Validation Data ---\")\n",
        "    for fragment_id in CFG.VALID_FRAGMENTS:\n",
        "        print(f\"Processing fragment {fragment_id}...\")\n",
        "        valid_tiles.extend(get_train_valid_tiles(fragment_id))\n",
        "\n",
        "    train_transform, valid_transform = get_transforms()\n",
        "    train_dataset = VesuviusDataset(train_tiles, transform=train_transform)\n",
        "    valid_dataset = VesuviusDataset(valid_tiles, transform=valid_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CFG.BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    return train_loader, valid_loader\n",
        "\n",
        "def train_one_epoch(model, optimizer, criterion, loader, scaler):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, masks in tqdm(loader, desc=\"Training\"):\n",
        "        images, masks = images.to(CFG.DEVICE), masks.to(CFG.DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast(enabled=CFG.USE_AMP):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def validate(model, criterion, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(loader, desc=\"Validating\"):\n",
        "            images, masks = images.to(CFG.DEVICE), masks.to(CFG.DEVICE)\n",
        "            with autocast(enabled=CFG.USE_AMP):\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "            total_loss += loss.item()\n",
        "            all_preds.append(torch.sigmoid(outputs).cpu())\n",
        "            all_targets.append(masks.cpu())\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_targets = torch.cat(all_targets)\n",
        "    best_thr, best_score = 0, 0\n",
        "    for thr in np.arange(0.1, 0.9, 0.05):\n",
        "        score = fbeta_score(all_preds, all_targets, beta=0.5, thr=thr)\n",
        "        if score > best_score:\n",
        "            best_score, best_thr = score, thr\n",
        "    return avg_loss, best_score, best_thr\n",
        "\n",
        "def run_training():\n",
        "    # 1. Instantiate model FIRST to isolate memory issues\n",
        "    print(\"Instantiating model and moving to GPU...\")\n",
        "    try:\n",
        "        model = smp.Unet(CFG.ENCODER_NAME, encoder_weights=CFG.ENCODER_WEIGHTS, in_channels=CFG.IN_CHANNELS, classes=1, activation=None).to(CFG.DEVICE)\n",
        "        print(\"Model loaded on GPU successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model onto GPU: {e}\")\n",
        "        os.system('nvidia-smi')\n",
        "        return\n",
        "\n",
        "    # 2. Prepare data and cache\n",
        "    if os.path.exists(CFG.CACHE_DIR):\n",
        "        print(f\"Removing old cache directory: {CFG.CACHE_DIR}\")\n",
        "        shutil.rmtree(CFG.CACHE_DIR)\n",
        "    \n",
        "    train_loader, valid_loader = get_dataloaders()\n",
        "    print(\"Data loaded and cached.\")\n",
        "    \n",
        "    # 3. Setup for training\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=CFG.LEARNING_RATE, weight_decay=CFG.WEIGHT_DECAY)\n",
        "    criterion = BCETverskyLoss(bce_weight=0.5, tversky_alpha=0.7, tversky_beta=0.3)\n",
        "    scaler = GradScaler(enabled=CFG.USE_AMP)\n",
        "    best_val_score = 0\n",
        "\n",
        "    # 4. Run training loop\n",
        "    for epoch in range(CFG.EPOCHS):\n",
        "        start_time = time.time()\n",
        "        train_loss = train_one_epoch(model, optimizer, criterion, train_loader, scaler)\n",
        "        val_loss, val_score, val_thr = validate(model, criterion, valid_loader)\n",
        "        end_time = time.time()\n",
        "        print(f\"Epoch {epoch+1}/{CFG.EPOCHS} | Time: {end_time-start_time:.2f}s | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val F0.5: {val_score:.4f} @ Thr: {val_thr:.2f}\")\n",
        "        if val_score > best_val_score:\n",
        "            best_val_score = val_score\n",
        "            torch.save(model.state_dict(), CFG.MODEL_SAVE_PATH)\n",
        "            print(f\"   -> New best model saved with F0.5 score: {best_val_score:.4f}\")\n",
        "    \n",
        "    print(f\"\\nTraining complete. Best validation F0.5 score: {best_val_score:.4f}\")\n",
        "    del model, train_loader, valid_loader\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# --- 4. Execute Training ---\n",
        "run_training()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\nInput channels: 25\nInstantiating model and moving to GPU...\nFailed to load model onto GPU: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\nFri Sep 26 04:19:14 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |   21842MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "id": "226949cc-cc49-4df7-84fd-69f6cdf8b051",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- 3. Run Training ---\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if CFG.REBUILD_CACHE and os.path.exists(CFG.CACHE_DIR):\n",
        "        print(f\"Removing existing cache directory: {CFG.CACHE_DIR}\")\n",
        "        import shutil\n",
        "        shutil.rmtree(CFG.CACHE_DIR)\n",
        "    \n",
        "    run_training()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}