{
  "cells": [
    {
      "id": "39c7d2b5-60d2-48fc-a1b3-231dc4981123",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: Google QUEST Q&A Labeling - Target: answer_helpful\n",
        "\n",
        "Objectives:\n",
        "- Build strong, fast baseline for answer_helpful using robust CV.\n",
        "- Iterate with text features + GPU models; aim \u2265 bronze CV threshold.\n",
        "\n",
        "Initial Strategy:\n",
        "1) Environment & GPU check (exit if no GPU).\n",
        "2) Data load + schema/size checks; preview text fields.\n",
        "3) Validation design:\n",
        "   - Stratify by binned answer_helpful (regression target) if possible or plain KFold with shuffling.\n",
        "   - 5 folds, fixed seed; save folds to disk.\n",
        "4) Baseline models:\n",
        "   A) TF-IDF (char + word n-grams on question_title, question_body, answer) \u2192 Ridge/Lasso/ElasticNet/SVR/XGBoost (GPU).\n",
        "   B) Fast transformer baseline (e.g., distilbert) pooled embeddings + linear head (optional if time).\n",
        "5) Features v1:\n",
        "   - Text: separate and concatenated fields; length features; overlap metrics (title/body, body/answer).\n",
        "   - Simple punctuation/word count ratios.\n",
        "6) Train, log per-fold Spearman (OOF), cache OOF/test preds.\n",
        "7) Error analysis: inspect top/bottom deciles of OOF; iterate.\n",
        "8) Ensembling: linear blend of diverse models if deltas are validated.\n",
        "\n",
        "Metric:\n",
        "- column-wise Spearman (single target here); compute Spearman on OOF.\n",
        "\n",
        "Milestones (request expert review after each):\n",
        "- Plan (this cell), Data/EDA, Baseline CV, FE v1, Model tuning/ensembling, Pre-submission checks.\n",
        "\n",
        "Next:\n",
        "- Add env check cell and run.\n",
        "- Load data, inspect targets and text fields."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8aec49c6-8ebc-4f54-87e1-0ce667de1640",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment & GPU check\n",
        "import os, sys, subprocess, platform, time\n",
        "\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"Time:\", time.strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "print(\"\\n== NVIDIA SMI ==\")\n",
        "try:\n",
        "    res = subprocess.run(['bash','-lc','nvidia-smi || true'], capture_output=True, text=True, check=False)\n",
        "    print(res.stdout)\n",
        "except Exception as e:\n",
        "    print(\"nvidia-smi error:\", e)\n",
        "\n",
        "print(\"\\n== CPU/RAM quick check ==\")\n",
        "try:\n",
        "    import psutil\n",
        "    vm = psutil.virtual_memory()\n",
        "    print(f\"CPU cores: {psutil.cpu_count(logical=True)} | RAM: {vm.total/1e9:.1f} GB, avail {vm.available/1e9:.1f} GB\")\n",
        "except Exception:\n",
        "    print(\"psutil not available; skipping RAM check\")\n",
        "\n",
        "print(\"\\nIf GPU is unavailable, exit the competition per best-practices.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\nPlatform: Linux-6.8.0-1031-azure-x86_64-with-glibc2.35\nTime: 2025-09-28 23:58:14\n\n== NVIDIA SMI ==\nSun Sep 28 23:58:14 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     128MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\n\n== CPU/RAM quick check ==\nCPU cores: 36 | RAM: 464.9 GB, avail 460.0 GB\n\nIf GPU is unavailable, exit the competition per best-practices.\n"
          ]
        }
      ]
    },
    {
      "id": "5a44547b-3010-448e-bf8e-aa51dce52641",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Data load, schema check, and CV folds creation\n",
        "import pandas as pd, numpy as np, os, hashlib, re\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "pd.set_option('display.max_columns', 200)\n",
        "\n",
        "train_path, test_path = 'train.csv', 'test.csv'\n",
        "print('Loading data...')\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "print('Train shape:', train.shape, 'Test shape:', test.shape)\n",
        "print('Train columns:', list(train.columns))\n",
        "print('Test columns:', list(test.columns))\n",
        "\n",
        "# Identify ID and target columns from sample_submission to avoid schema mistakes\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "print('Sample submission columns:', list(sample_sub.columns))\n",
        "\n",
        "# Determine ID col and target cols\n",
        "id_col = sample_sub.columns[0]\n",
        "target_cols = [c for c in sample_sub.columns if c != id_col]\n",
        "print('ID column:', id_col, '| #Targets:', len(target_cols))\n",
        "\n",
        "# Primary target for this task\n",
        "primary_target = 'answer_helpful'\n",
        "assert primary_target in train.columns, f'Primary target {primary_target} not found in train columns'\n",
        "assert primary_target in target_cols, f'Primary target {primary_target} not in sample_submission columns'\n",
        "assert id_col in train.columns and id_col in test.columns, 'ID column missing in train/test'\n",
        "\n",
        "# Preview a few rows to verify text fields exist\n",
        "text_fields = [c for c in ['question_title','question_body','answer'] if c in train.columns]\n",
        "print('Detected text fields:', text_fields)\n",
        "print(train[text_fields + [primary_target]].head(2) if text_fields else train.head(2))\n",
        "\n",
        "# Target stats (primary)\n",
        "y = train[primary_target].astype(float)\n",
        "print('Primary target describe:')\n",
        "print(y.describe())\n",
        "print('Primary target quantiles (0, .1, .2, ..., 1):')\n",
        "qs = np.linspace(0,1,11)\n",
        "print(pd.Series(np.quantile(y, qs), index=qs))\n",
        "\n",
        "# Simple duplicate/near-duplicate check on question identity to inform CV groups\n",
        "def stable_hash(s: str) -> str:\n",
        "    return hashlib.md5(s.encode('utf-8')).hexdigest()\n",
        "\n",
        "if set(['question_title','question_body']).issubset(train.columns):\n",
        "    qsig = (train['question_title'].fillna('') + '\\n' + train['question_body'].fillna('')).astype(str)\n",
        "    train['_qhash'] = qsig.apply(stable_hash)\n",
        "    dup_rate = 1.0 - train['_qhash'].nunique() / len(train)\n",
        "    print(f'Question signature duplicate rate: {dup_rate:.3f}')\n",
        "else:\n",
        "    train['_qhash'] = train[id_col].astype(str)\n",
        "    print('No question fields found for duplicate check; using ID as group placeholder.')\n",
        "\n",
        "# Create 5-fold GroupKFold splits grouped by question hash to prevent leakage\n",
        "n_splits = 5\n",
        "gkf = GroupKFold(n_splits=n_splits)\n",
        "groups = train['_qhash'].values\n",
        "train['fold'] = -1\n",
        "for fold, (tr_idx, va_idx) in enumerate(gkf.split(train, y.values, groups=groups)):\n",
        "    train.loc[va_idx, 'fold'] = fold\n",
        "fold_counts = train['fold'].value_counts().sort_index()\n",
        "print('Fold counts:', fold_counts.to_dict())\n",
        "print('Fold primary target means:', train.groupby('fold')[primary_target].mean().round(4).to_dict())\n",
        "\n",
        "# Persist folds for reuse across notebooks/models\n",
        "folds_out = 'folds.npy'\n",
        "np.save(folds_out, train['fold'].values.astype(int))\n",
        "train[[id_col, 'fold', primary_target]].to_csv('train_folds.csv', index=False)\n",
        "print(f'Saved folds to {folds_out} and train_folds.csv')\n",
        "\n",
        "# Save a minimal schema snapshot to guard against later drift\n",
        "with open('schema_info.txt','w') as f:\n",
        "    f.write('ID:'+id_col+'\\n')\n",
        "    f.write('PRIMARY_TARGET:'+primary_target+'\\n')\n",
        "    f.write('ALL_TARGETS:'+','.join(target_cols)+'\\n')\n",
        "    f.write('TEXT:'+','.join(text_fields)+'\\n')\n",
        "print('Wrote schema_info.txt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\nTrain shape: (5471, 41) Test shape: (608, 11)\nTrain columns: ['qa_id', 'question_title', 'question_body', 'question_user_name', 'question_user_page', 'answer', 'answer_user_name', 'answer_user_page', 'url', 'category', 'host', 'question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\nTest columns: ['qa_id', 'question_title', 'question_body', 'question_user_name', 'question_user_page', 'answer', 'answer_user_name', 'answer_user_page', 'url', 'category', 'host']\nSample submission columns: ['qa_id', 'question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\nID column: qa_id | #Targets: 30\nDetected text fields: ['question_title', 'question_body', 'answer']\n                                      question_title  \\\n0  Which parts of fresh Fenugreek am I supposed t...   \n1  Is decoherence even possible in anti de Sitter...   \n\n                                       question_body  \\\n0  The fresh Fenugreek which I bought contains:\\n...   \n1  Is decoherence even possible in anti de Sitter...   \n\n                                              answer  answer_helpful  \n0  I would just pull off all the little stems wit...             1.0  \n1  Your question is not about AdS at all, it is a...             1.0  \nPrimary target describe:\ncount    5471.000000\nmean        0.925110\nstd         0.114943\nmin         0.333333\n25%         0.888889\n50%         1.000000\n75%         1.000000\nmax         1.000000\nName: answer_helpful, dtype: float64\nPrimary target quantiles (0, .1, .2, ..., 1):\n0.0    0.333333\n0.1    0.777778\n0.2    0.888889\n0.3    0.888889\n0.4    1.000000\n0.5    1.000000\n0.6    1.000000\n0.7    1.000000\n0.8    1.000000\n0.9    1.000000\n1.0    1.000000\ndtype: float64\nQuestion signature duplicate rate: 0.380\nFold counts: {0: 1095, 1: 1094, 2: 1094, 3: 1094, 4: 1094}\nFold primary target means: {0: 0.9307, 1: 0.9205, 2: 0.9281, 3: 0.922, 4: 0.9242}\nSaved folds to folds.npy and train_folds.csv\nWrote schema_info.txt\n"
          ]
        }
      ]
    },
    {
      "id": "d476302b-0b5d-497f-af07-c720e813bb4e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Patched Baseline per expert: GroupKFold + TF-IDF (word+char, tuned) + stats + RidgeCV on rank target; no per-fold rank OOF\n",
        "import time, gc, math, random\n",
        "import numpy as np, pandas as pd, scipy.sparse as sp\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from scipy.stats import spearmanr, rankdata\n",
        "\n",
        "np.random.seed(42); random.seed(42)\n",
        "\n",
        "# Reload essentials and folds\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "id_col = sample_sub.columns[0]\n",
        "target_cols = [c for c in sample_sub.columns if c != id_col]\n",
        "primary_target = 'answer_helpful'\n",
        "folds = np.load('folds.npy')\n",
        "base_fields = [c for c in ['question_title','question_body','answer'] if c in train.columns]\n",
        "assert all(f in train.columns for f in base_fields), 'Missing required text fields'\n",
        "\n",
        "# Cleaners: prefer BeautifulSoup, fallback to regex HTML strip\n",
        "import re\n",
        "try:\n",
        "    from bs4 import BeautifulSoup\n",
        "    _USE_BS = True\n",
        "except Exception:\n",
        "    _USE_BS = False\n",
        "URL_RE = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "EMAIL_RE = re.compile(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.[A-Za-z]{2,}\\b')\n",
        "CODE_BLOCK_RE = re.compile(r'`{1,3}.*?`{1,3}', re.S)\n",
        "HTML_TAG_RE = re.compile(r'<[^>]+>')\n",
        "DIGIT_RE = re.compile(r'\\d')\n",
        "PUNCT_CHARS = set(list('.,!?:;\\-\\\"\\'\u2019\u201d'))\n",
        "STOPWORDS = set('the a an and or to of in for is it on with this that as at by be are was were from has have had you your we they he she them his her its our their i'.split())\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        s = '' if pd.isna(s) else str(s)\n",
        "    if _USE_BS:\n",
        "        try:\n",
        "            s = BeautifulSoup(s, 'html.parser').get_text(separator=' ')\n",
        "        except Exception:\n",
        "            s = HTML_TAG_RE.sub(' ', s)\n",
        "    else:\n",
        "        s = HTML_TAG_RE.sub(' ', s)\n",
        "    s = CODE_BLOCK_RE.sub(' [CODE] ', s)\n",
        "    s = URL_RE.sub(' [URL] ', s)\n",
        "    s = EMAIL_RE.sub(' [EMAIL] ', s)\n",
        "    return s.lower()\n",
        "\n",
        "def make_corpus(df: pd.DataFrame):\n",
        "    c = {f: df[f].fillna('').map(clean_text).astype(str).values for f in base_fields}\n",
        "    qa_all = (c['question_title'] + ' [T] ' + c['question_body'] + ' [A] ' + c['answer'])\n",
        "    c['qa_all'] = qa_all.values if hasattr(qa_all, 'values') else np.array(list(qa_all))\n",
        "    return c\n",
        "\n",
        "def compute_stats(corpus: dict) -> np.ndarray:\n",
        "    qt = corpus['question_title']; qb = corpus['question_body']; an = corpus['answer']\n",
        "    n = len(qt)\n",
        "    feats = np.zeros((n, 18), dtype=np.float32)\n",
        "    for i in range(n):\n",
        "        s_t, s_b, s_a = qt[i], qb[i], an[i]\n",
        "        lt, lb, la = len(s_t), len(s_b), len(s_a)\n",
        "        wt = (s_t.count(' ') + 1) if lt>0 else 0\n",
        "        wb = (s_b.count(' ') + 1) if lb>0 else 0\n",
        "        wa = (s_a.count(' ') + 1) if la>0 else 0\n",
        "        len_ratio = la / (lb + 1.0)\n",
        "        wc_ratio  = wa / (wb + 1.0)\n",
        "        qmark = s_a.count('?')\n",
        "        excl  = s_a.count('!')\n",
        "        digits = len(DIGIT_RE.findall(s_a))\n",
        "        url_cnt = s_a.count('[url]')\n",
        "        code_cnt = s_a.count('[code]')\n",
        "        has_url = 1.0 if url_cnt > 0 else 0.0\n",
        "        has_code = 1.0 if code_cnt > 0 else 0.0\n",
        "        # overlaps\n",
        "        set_t = set(s_t.split()) if lt else set()\n",
        "        set_b = set(s_b.split()) if lb else set()\n",
        "        set_a = set(s_a.split()) if la else set()\n",
        "        def jacc(a,b):\n",
        "            if not a or not b: return 0.0\n",
        "            inter = len(a & b); uni = len(a | b)\n",
        "            return (inter / uni) if uni>0 else 0.0\n",
        "        jac_ta = jacc(set_t, set_a)\n",
        "        jac_ba = jacc(set_b, set_a)\n",
        "        # new stats\n",
        "        starts_punct = 1.0 if la>0 and s_a[0] in PUNCT_CHARS else 0.0\n",
        "        ends_punct   = 1.0 if la>0 and s_a[-1] in PUNCT_CHARS else 0.0\n",
        "        letters = sum(ch.isalpha() for ch in s_a)\n",
        "        uppers  = sum(ch.isupper() for ch in s_a)\n",
        "        upper_frac = (uppers / (letters + 1.0)) if letters>0 else 0.0\n",
        "        toks_a = s_a.split()\n",
        "        sw_hits = sum(1 for w in toks_a if w in STOPWORDS)\n",
        "        sw_ratio = sw_hits / (len(toks_a) + 1.0)\n",
        "        feats[i] = [lt, lb, la, wt, wb, wa, len_ratio, wc_ratio, qmark, excl, digits, has_url, has_code, jac_ta + jac_ba, starts_punct, ends_punct, upper_frac, sw_ratio]\n",
        "    feats[:, [0,1,2,3,4,5,8,9,10]] = np.log1p(feats[:, [0,1,2,3,4,5,8,9,10]])\n",
        "    return feats\n",
        "\n",
        "print('Cleaning text...')\n",
        "t0 = time.time()\n",
        "train_corpus = make_corpus(train)\n",
        "test_corpus  = make_corpus(test)\n",
        "print(f'Cleaned in {time.time() - t0:.2f}s')\n",
        "\n",
        "print('Computing stats features...')\n",
        "t1 = time.time()\n",
        "train_stats = compute_stats(train_corpus)\n",
        "test_stats  = compute_stats(test_corpus)\n",
        "print(f'Stats ready in {time.time() - t1:.2f}s; dims train {train_stats.shape}, test {test_stats.shape}')\n",
        "\n",
        "# Vectorizer configs per field (drop qa_all char branch), tuned caps\n",
        "feature_fields = ['question_title','question_body','answer','qa_all']\n",
        "vec_cfgs = {\n",
        "    'question_title': dict(word_max=50000, char_max=60000),\n",
        "    'question_body':  dict(word_max=120000, char_max=200000),\n",
        "    'answer':         dict(word_max=150000, char_max=220000),\n",
        "    'qa_all':         dict(word_max=180000, char_max=0),  # char skipped\n",
        "}\n",
        "\n",
        "def build_features(X_text: dict, fit: bool, vecs_store: dict | None):\n",
        "    mats = []\n",
        "    for field in feature_fields:\n",
        "        # word\n",
        "        wkey = (field, 'word')\n",
        "        if fit:\n",
        "            v_w = TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=3, max_df=0.97,\n",
        "                                   sublinear_tf=True, strip_accents='unicode',\n",
        "                                   max_features=vec_cfgs[field]['word_max'])\n",
        "            mat_w = v_w.fit_transform(X_text[field])\n",
        "            vecs_store[wkey] = v_w\n",
        "        else:\n",
        "            v_w = vecs_store[wkey]\n",
        "            mat_w = v_w.transform(X_text[field])\n",
        "        mats.append(mat_w)\n",
        "        # char (skip for qa_all)\n",
        "        if field != 'qa_all':\n",
        "            ckey = (field, 'char')\n",
        "            if fit:\n",
        "                v_c = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,6), min_df=3,\n",
        "                                       sublinear_tf=True,\n",
        "                                       max_features=vec_cfgs[field]['char_max'])\n",
        "                mat_c = v_c.fit_transform(X_text[field])\n",
        "                vecs_store[ckey] = v_c\n",
        "            else:\n",
        "                v_c = vecs_store[ckey]\n",
        "                mat_c = v_c.transform(X_text[field])\n",
        "            mats.append(mat_c)\n",
        "    X = sp.hstack(mats).tocsr()\n",
        "    return X\n",
        "\n",
        "def to_rank01(v: np.ndarray) -> np.ndarray:\n",
        "    r = rankdata(v, method='average').astype(np.float64)\n",
        "    denom = max(1.0, len(r) - 1)\n",
        "    return ((r - 1.0) / denom).astype(np.float32)\n",
        "\n",
        "# CV training\n",
        "y = train[primary_target].astype(float).values\n",
        "unique_folds = np.unique(folds)\n",
        "oof = np.zeros(len(train), dtype=np.float32)\n",
        "test_preds_accum = np.zeros((len(unique_folds), len(test)), dtype=np.float32)\n",
        "\n",
        "print('Starting CV...')\n",
        "overall_t0 = time.time()\n",
        "for fi, fold in enumerate(unique_folds):\n",
        "    f_t0 = time.time()\n",
        "    tr_idx = np.where(folds != fold)[0]\n",
        "    va_idx = np.where(folds == fold)[0]\n",
        "    print(f'Fold {fold}: tr={len(tr_idx)} va={len(va_idx)}')\n",
        "    vecs = {}\n",
        "    X_tr = build_features({k: train_corpus[k][tr_idx] for k in feature_fields}, fit=True, vecs_store=vecs)\n",
        "    X_va = build_features({k: train_corpus[k][va_idx] for k in feature_fields}, fit=False, vecs_store=vecs)\n",
        "    X_te = build_features(test_corpus, fit=False, vecs_store=vecs)\n",
        "    # Add stats\n",
        "    X_tr = sp.hstack([X_tr, sp.csr_matrix(train_stats[tr_idx])]).tocsr()\n",
        "    X_va = sp.hstack([X_va, sp.csr_matrix(train_stats[va_idx])]).tocsr()\n",
        "    X_te = sp.hstack([X_te, sp.csr_matrix(test_stats)]).tocsr()\n",
        "    # Model with rank-transformed target + tiny jitter\n",
        "    jitter = (np.random.rand(len(tr_idx)).astype(np.float32) - 0.5) * 2e-6\n",
        "    y_tr_rank = rankdata(y[tr_idx] + jitter, method='average') / len(tr_idx)\n",
        "    model = RidgeCV(alphas=[4.0, 6.0, 8.0, 12.0, 20.0], fit_intercept=True)\n",
        "    m_t0 = time.time()\n",
        "    model.fit(X_tr, y_tr_rank.astype(np.float32))\n",
        "    print(f'  Fit time: {time.time()-m_t0:.2f}s, nnz/tr {X_tr.nnz/ max(1,X_tr.shape[0]):.1f}, alpha*={getattr(model, \"alpha_\", None)}')\n",
        "    # Predict (raw per fold, do NOT rank-normalize for OOF)\n",
        "    va_pred = model.predict(X_va).astype(np.float32)\n",
        "    oof[va_idx] = va_pred\n",
        "    te_pred = model.predict(X_te).astype(np.float32)\n",
        "    test_preds_accum[fi] = te_pred\n",
        "    # Metrics\n",
        "    rho = spearmanr(va_pred, y[va_idx]).correlation\n",
        "    print(f'  Fold {fold} Spearman: {rho:.5f}, elapsed {time.time()-f_t0:.1f}s', flush=True)\n",
        "    del X_tr, X_va, X_te, vecs, model, va_pred, te_pred\n",
        "    gc.collect()\n",
        "\n",
        "oo_rho = spearmanr(oof, y).correlation\n",
        "print(f'OOF Spearman (primary {primary_target}): {oo_rho:.5f}')\n",
        "\n",
        "# Aggregate test preds by averaging raw fold predictions; optional final rank to [0,1]\n",
        "test_raw = test_preds_accum.mean(axis=0).astype(np.float32)\n",
        "test_final = np.clip(to_rank01(test_raw), 0.0, 1.0).astype(np.float32)\n",
        "oof_clip = np.clip(oof, 0.0, 1.0).astype(np.float32)\n",
        "\n",
        "# Save OOF/test preds\n",
        "np.save(f'oof_{primary_target}.npy', oof_clip)\n",
        "np.save(f'test_{primary_target}.npy', test_final)\n",
        "\n",
        "# Build submission with full schema; TODO: replace placeholders by real per-target models later\n",
        "sub = sample_sub.copy()\n",
        "sub[id_col] = test[id_col].values\n",
        "fill_means = train[target_cols].mean().clip(0,1)\n",
        "for col in target_cols:\n",
        "    if col == primary_target:\n",
        "        sub[col] = test_final\n",
        "    else:\n",
        "        sub[col] = float(fill_means[col])\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Wrote submission.csv with primary model and other targets filled by train means (temporary).')\n",
        "\n",
        "print(f'Total CV time: {time.time()-overall_t0:.1f}s')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned in 0.41s\nComputing stats features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stats ready in 0.61s; dims train (5471, 18), test (608, 18)\nStarting CV...\nFold 0: tr=4376 va=1095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fit time: 16.12s, nnz/tr 2611.8, alpha*=20.0\n  Fold 0 Spearman: 0.21306, elapsed 30.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: tr=4377 va=1094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fit time: 16.31s, nnz/tr 2619.9, alpha*=20.0\n  Fold 1 Spearman: 0.22673, elapsed 30.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: tr=4377 va=1094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fit time: 16.15s, nnz/tr 2614.6, alpha*=20.0\n  Fold 2 Spearman: 0.18055, elapsed 30.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: tr=4377 va=1094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fit time: 16.59s, nnz/tr 2647.1, alpha*=20.0\n  Fold 3 Spearman: 0.18591, elapsed 30.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: tr=4377 va=1094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fit time: 16.36s, nnz/tr 2636.5, alpha*=20.0\n  Fold 4 Spearman: 0.15876, elapsed 30.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF Spearman (primary answer_helpful): 0.19227\nWrote submission.csv with primary model and other targets filled by train means (temporary).\nTotal CV time: 151.6s\n"
          ]
        }
      ]
    },
    {
      "id": "0107168b-958d-4241-aa0c-6884df26d40a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install GPU-enabled PyTorch (cu121) and transformers stack; then sanity-check CUDA\n",
        "import os, sys, subprocess, shutil, time\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print('>', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "# Uninstall any existing torch stack to avoid conflicts\n",
        "for pkg in ('torch','torchvision','torchaudio'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "\n",
        "# Clean stray site dirs that can shadow correct wheels (idempotent)\n",
        "for d in (\n",
        "    '/app/.pip-target/torch',\n",
        "    '/app/.pip-target/torchvision',\n",
        "    '/app/.pip-target/torchaudio',\n",
        "    '/app/.pip-target/torch-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torch-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.23.0.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.19.1.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchgen',\n",
        "    '/app/.pip-target/functorch',\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# 1) Install exact cu121 torch stack\n",
        "pip('install',\n",
        "    '--index-url', 'https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url', 'https://pypi.org/simple',\n",
        "    'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1')\n",
        "\n",
        "# 2) Freeze versions for later installs\n",
        "Path('constraints.txt').write_text(\n",
        "    'torch==2.4.1\\n'\n",
        "    'torchvision==0.19.1\\n'\n",
        "    'torchaudio==2.4.1\\n'\n",
        ")\n",
        "\n",
        "# 3) Install transformers stack honoring constraints\n",
        "pip('install', '-c', 'constraints.txt',\n",
        "    'transformers==4.44.2', 'accelerate==0.34.2',\n",
        "    'datasets==2.21.0', 'evaluate==0.4.2',\n",
        "    'sentencepiece', 'scikit-learn', '--upgrade-strategy', 'only-if-needed')\n",
        "\n",
        "# 4) Sanity gate\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'built CUDA:', getattr(torch.version, 'cuda', None))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "assert str(getattr(torch.version,'cuda','')).startswith('12.1'), f'Wrong CUDA build: {torch.version.cuda}'\n",
        "assert torch.cuda.is_available(), 'CUDA not available'\n",
        "print('GPU:', torch.cuda.get_device_name(0))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torch as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchvision as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> install --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchaudio as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 MB 510.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 427.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 152.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 77.6 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 172.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 515.3 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 220.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 141.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 416.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 207.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 457.0 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 268.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 189.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 300.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 196.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 424.6 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 238.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 235.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 257.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 410.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 287.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 275.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 119.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 421.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.3 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> install -c constraints.txt transformers==4.44.2 accelerate==0.34.2 datasets==2.21.0 evaluate==0.4.2 sentencepiece scikit-learn --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.5/9.5 MB 129.3 MB/s eta 0:00:00\nCollecting accelerate==0.34.2\n  Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 324.4/324.4 KB 484.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==2.21.0\n  Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 527.3/527.3 KB 527.2 MB/s eta 0:00:00\nCollecting evaluate==0.4.2\n  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 84.1/84.1 KB 436.2 MB/s eta 0:00:00\nCollecting sentencepiece\n  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 379.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 215.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting packaging>=20.0\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 409.4 MB/s eta 0:00:00\nCollecting tqdm>=4.27\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 433.3 MB/s eta 0:00:00\nCollecting huggingface-hub<1.0,>=0.23.2\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 563.3/563.3 KB 321.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors>=0.4.1\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 513.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.17\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 549.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex!=2019.12.17\n  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 KB 467.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers<0.20,>=0.19\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.6/3.6 MB 138.0 MB/s eta 0:00:00\nCollecting pyyaml>=5.1\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 115.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 447.2 MB/s eta 0:00:00\nCollecting torch>=1.10.0\n  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.1/797.1 MB 103.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psutil\n  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 291.2/291.2 KB 523.5 MB/s eta 0:00:00\nCollecting fsspec[http]<=2024.6.1,>=2023.1.0\n  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 177.6/177.6 KB 490.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aiohttp\n  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.7/1.7 MB 456.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas\n  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.4/12.4 MB 277.5 MB/s eta 0:00:00\nCollecting multiprocess\n  Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 144.5/144.5 KB 457.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow>=15.0.0\n  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 42.8/42.8 MB 75.1 MB/s eta 0:00:00\nCollecting dill<0.3.9,>=0.3.0\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 116.3/116.3 KB 480.7 MB/s eta 0:00:00\nCollecting xxhash\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 194.8/194.8 KB 532.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy>=1.8.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 230.2 MB/s eta 0:00:00\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 546.7 MB/s eta 0:00:00\nCollecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting frozenlist>=1.1.1\n  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 235.3/235.3 KB 514.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yarl<2.0,>=1.17.0\n  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 349.0/349.0 KB 420.6 MB/s eta 0:00:00\nCollecting aiohappyeyeballs>=2.5.0\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nCollecting attrs>=17.3.0\n  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 63.8/63.8 KB 458.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting multidict<7.0,>=4.5\n  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 246.7/246.7 KB 424.5 MB/s eta 0:00:00\nCollecting aiosignal>=1.4.0\n  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nCollecting propcache>=0.2.0\n  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 213.5/213.5 KB 504.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 345.0 MB/s eta 0:00:00\nCollecting typing-extensions>=3.7.4.3\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 414.4 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 406.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 413.6 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 423.5 MB/s eta 0:00:00\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 506.8 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 97.9 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 472.2 MB/s eta 0:00:00\nCollecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 455.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 95.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 286.1 MB/s eta 0:00:00\nCollecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 435.4 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 168.1 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 167.1 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 91.1 MB/s eta 0:00:00\nCollecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 94.0 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 243.7 MB/s eta 0:00:00\nCollecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 313.0 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 142.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 554.7 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 119.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 218.0 MB/s eta 0:00:00\nCollecting multiprocess\n  Downloading multiprocess-0.70.17-py311-none-any.whl (144 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 144.3/144.3 KB 520.4 MB/s eta 0:00:00\n  Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 143.5/143.5 KB 505.8 MB/s eta 0:00:00\nCollecting tzdata>=2022.7\n  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 347.8/347.8 KB 541.3 MB/s eta 0:00:00\nCollecting python-dateutil>=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.9/229.9 KB 469.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytz>=2020.1\n  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 509.2/509.2 KB 380.3 MB/s eta 0:00:00\nCollecting six>=1.5\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 532.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, six, sentencepiece, safetensors, regex, pyyaml, pyarrow, psutil, propcache, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, MarkupSafe, joblib, idna, hf-xet, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, triton, scipy, requests, python-dateutil, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jinja2, aiosignal, scikit-learn, pandas, nvidia-cusolver-cu12, huggingface-hub, aiohttp, torch, tokenizers, transformers, datasets, accelerate, evaluate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.3 accelerate-0.34.2 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 certifi-2025.8.3 charset_normalizer-3.4.3 datasets-2.21.0 dill-0.3.8 evaluate-0.4.2 filelock-3.19.1 frozenlist-1.7.0 fsspec-2024.6.1 hf-xet-1.1.10 huggingface-hub-0.35.1 idna-3.10 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 multidict-6.6.4 multiprocess-0.70.16 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 packaging-25.0 pandas-2.3.2 propcache-0.3.2 psutil-7.1.0 pyarrow-21.0.0 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.3 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sentencepiece-0.2.1 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.19.1 torch-2.4.1 tqdm-4.67.1 transformers-4.44.2 triton-3.0.0 typing-extensions-4.15.0 tzdata-2025.2 urllib3-2.5.0 xxhash-3.5.0 yarl-1.20.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.4.5.107.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.1.0.106.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/triton-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe-3.0.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.1.3.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.0.2.54.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.2.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.20.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.9.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.4.1+cu121 built CUDA: 12.1\nCUDA available: True\nGPU: NVIDIA A10-24Q\n"
          ]
        }
      ]
    },
    {
      "id": "d3a587fb-4ab8-46b5-8b9c-185584b58448",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DeBERTa-v3-base multitask (30 targets) with improved pooling and diagnostics (patched per expert)\n",
        "import os, time, math, gc, random, sys\n",
        "import numpy as np, pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, get_cosine_schedule_with_warmup\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "# Load data and folds\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "id_col = sample_sub.columns[0]\n",
        "target_cols = [c for c in sample_sub.columns if c != id_col]\n",
        "assert target_cols == list(sample_sub.columns[1:]), 'target_cols order drift'\n",
        "assert all(c in train.columns for c in target_cols), 'Missing targets in train'\n",
        "print('Target dtypes:', train[target_cols].dtypes.unique())\n",
        "folds = np.load('folds.npy')\n",
        "\n",
        "# Text fields\n",
        "title_col, body_col, ans_col = 'question_title', 'question_body', 'answer'\n",
        "assert all(c in train.columns for c in [title_col, body_col, ans_col])\n",
        "\n",
        "ANSWER_HELPFUL_IDX = target_cols.index('answer_helpful') if 'answer_helpful' in target_cols else None\n",
        "\n",
        "# Model/Tokenizer\n",
        "model_name = 'microsoft/deberta-v3-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "MAX_LEN = 512\n",
        "\n",
        "def pack_inputs(title, body, answer):\n",
        "    # Let tokenizer handle pair separation; disable token_type_ids\n",
        "    text = f'Title: {title} Body: {body}'\n",
        "    text_pair = f'Answer: {answer}'\n",
        "    return tokenizer(text=text, text_pair=text_pair, truncation=True, padding='max_length',\n",
        "                    max_length=MAX_LEN, return_tensors='pt', return_token_type_ids=False)\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, df, targets=None):\n",
        "        self.titles = df[title_col].fillna('').astype(str).values\n",
        "        self.bodies = df[body_col].fillna('').astype(str).values\n",
        "        self.answers = df[ans_col].fillna('').astype(str).values\n",
        "        self.targets = None if targets is None else np.asarray(targets, dtype=np.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.titles)\n",
        "    def __getitem__(self, idx):\n",
        "        enc = pack_inputs(self.titles[idx], self.bodies[idx], self.answers[idx])\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        if self.targets is not None:\n",
        "            item['labels'] = torch.tensor(self.targets[idx], dtype=torch.float32)  # shape [30]\n",
        "        return item\n",
        "\n",
        "def spearman_cols(y_pred: np.ndarray, y_true: np.ndarray):\n",
        "    rhos = []\n",
        "    for i in range(y_pred.shape[1]):\n",
        "        try:\n",
        "            r = spearmanr(y_pred[:, i], y_true[:, i]).correlation\n",
        "        except Exception:\n",
        "            r = np.nan\n",
        "        rhos.append(0.0 if np.isnan(r) else float(r))\n",
        "    return float(np.mean(rhos)), rhos\n",
        "\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.99):\n",
        "        self.decay = decay\n",
        "        self.shadow = {}\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[name] = p.detach().clone()\n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[name].mul_(self.decay).add_(p.detach(), alpha=1.0 - self.decay)\n",
        "    def apply_to(self, model):\n",
        "        self.backup = {}\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.backup[name] = p.detach().clone()\n",
        "                p.data.copy_(self.shadow[name].data)\n",
        "    def restore(self, model):\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad and name in self.backup:\n",
        "                p.data.copy_(self.backup[name])\n",
        "        self.backup = {}\n",
        "\n",
        "def masked_mean_pooling(last_hidden_state, attention_mask):\n",
        "    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "    masked = last_hidden_state * mask\n",
        "    summed = masked.sum(dim=1)\n",
        "    counts = mask.sum(dim=1).clamp(min=1e-6)\n",
        "    return summed / counts\n",
        "\n",
        "class WeightedLayerPooling(nn.Module):\n",
        "    def __init__(self, num_layers: int, layer_start: int = -4):\n",
        "        super().__init__()\n",
        "        self.layer_start = layer_start\n",
        "        n = -layer_start\n",
        "        self.weights = nn.Parameter(torch.ones(n) / n)\n",
        "    def forward(self, all_hidden_states):\n",
        "        selected = all_hidden_states[self.layer_start:]\n",
        "        stacked = torch.stack(selected, dim=0)  # [n, bs, seq, hidden]\n",
        "        w = torch.softmax(self.weights, dim=0).view(-1, 1, 1, 1)\n",
        "        return (w * stacked).sum(dim=0)  # [bs, seq, hidden]\n",
        "\n",
        "class DebertaMT(nn.Module):\n",
        "    def __init__(self, name, out_dim=30, dropout_p=0.2, msd_k=1):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(name)\n",
        "        if hasattr(self.backbone, 'gradient_checkpointing_enable'):\n",
        "            self.backbone.gradient_checkpointing_enable()\n",
        "        hidden = self.backbone.config.hidden_size\n",
        "        self.layer_pool = WeightedLayerPooling(num_layers=getattr(self.backbone.config, 'num_hidden_layers', 12), layer_start=-4)\n",
        "        self.msd_k = msd_k\n",
        "        self.dropouts = nn.ModuleList([nn.Dropout(dropout_p) for _ in range(msd_k)])\n",
        "        self.head = nn.Linear(hidden, out_dim)\n",
        "        self.loss_fn = nn.SmoothL1Loss(reduction='mean')\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        pooled_seq = self.layer_pool(out.hidden_states)\n",
        "        feat = masked_mean_pooling(pooled_seq, attention_mask)\n",
        "        logits_accum = 0\n",
        "        for dp in self.dropouts:\n",
        "            logits_accum = logits_accum + self.head(dp(feat))\n",
        "        logits = logits_accum / self.msd_k\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fn(logits, labels)\n",
        "        return logits, loss\n",
        "\n",
        "def run_fold(fold, train_idx, val_idx):\n",
        "    print(f'Fold {fold} start: tr={len(train_idx)} va={len(val_idx)}')\n",
        "    df_tr = train.iloc[train_idx].reset_index(drop=True)\n",
        "    df_va = train.iloc[val_idx].reset_index(drop=True)\n",
        "    y_tr = df_tr[target_cols].astype(np.float32).values\n",
        "    y_va = df_va[target_cols].astype(np.float32).values\n",
        "\n",
        "    ds_tr = QADataset(df_tr, y_tr)\n",
        "    ds_va = QADataset(df_va, y_va)\n",
        "    ds_te = QADataset(test, None)\n",
        "\n",
        "    train_loader = DataLoader(ds_tr, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader   = DataLoader(ds_va, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    test_loader  = DataLoader(ds_te, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = DebertaMT(model_name, out_dim=len(target_cols), dropout_p=0.2, msd_k=1).to(device)\n",
        "    # Initialize head bias toward target mean to stabilize early ranks\n",
        "    with torch.no_grad():\n",
        "        if hasattr(model.head, 'bias') and model.head.bias is not None:\n",
        "            model.head.bias.fill_(0.5)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01, betas=(0.9,0.999), eps=1e-6)\n",
        "    num_epochs = 4\n",
        "    grad_accum = 2  # effective batch 32\n",
        "    num_training_steps = math.ceil(len(train_loader) / grad_accum) * num_epochs\n",
        "    warmup_steps = max(10, int(0.1 * num_training_steps))\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
        "\n",
        "    scaler = torch.amp.GradScaler('cuda', enabled=True)\n",
        "    ema = EMA(model, decay=0.99)\n",
        "\n",
        "    best_score = -1.0\n",
        "    best_val_preds = None\n",
        "\n",
        "    t0 = time.time()\n",
        "    global_step = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        tr_loss = 0.0\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        for step, batch in enumerate(train_loader):\n",
        "            inputs = {k: v.to(device, non_blocking=True) for k, v in batch.items() if k not in ('labels','token_type_ids')}\n",
        "            labels = batch['labels'].to(device, non_blocking=True)\n",
        "            with torch.amp.autocast('cuda', enabled=True):\n",
        "                logits, loss = model(**inputs, labels=labels)\n",
        "                loss = loss / grad_accum\n",
        "            scaler.scale(loss).backward()\n",
        "            if (step + 1) % grad_accum == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                scheduler.step()\n",
        "                if global_step >= warmup_steps:\n",
        "                    ema.update(model)\n",
        "                global_step += 1\n",
        "            tr_loss += loss.item() * grad_accum\n",
        "            if (step+1) % 100 == 0:\n",
        "                print(f'  Epoch {epoch+1} step {step+1}/{len(train_loader)} loss={tr_loss/(step+1):.4f}', flush=True)\n",
        "\n",
        "        # Validation: compare plain vs EMA\n",
        "        def evaluate(use_ema: bool):\n",
        "            if use_ema:\n",
        "                ema.apply_to(model)\n",
        "            model.eval()\n",
        "            preds, tgts = [], []\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    inputs = {k: v.to(device, non_blocking=True) for k, v in batch.items() if k not in ('labels','token_type_ids')}\n",
        "                    labels = batch['labels']\n",
        "                    logits, _ = model(**inputs, labels=None)\n",
        "                    preds.append(logits.float().cpu().numpy())\n",
        "                    tgts.append(labels.float().cpu().numpy())\n",
        "            if use_ema:\n",
        "                ema.restore(model)\n",
        "            preds = np.concatenate(preds, axis=0)\n",
        "            tgts = np.concatenate(tgts, axis=0)\n",
        "            score, per_col = spearman_cols(preds, tgts)\n",
        "            return score, per_col, preds\n",
        "\n",
        "        s_plain, per_plain, vp_plain = evaluate(use_ema=False)\n",
        "        s_ema,   per_ema,   vp_ema   = evaluate(use_ema=True)\n",
        "        p_stats = vp_plain.ravel(); t_stats = df_va[target_cols].to_numpy(dtype=np.float32).ravel()\n",
        "        print(f'    val preds (plain) min/max/mean/std: {p_stats.min():.3f}/{p_stats.max():.3f}/{p_stats.mean():.3f}/{p_stats.std():.3f}')\n",
        "        print(f'    val tgts           min/max/mean/std: {t_stats.min():.3f}/{t_stats.max():.3f}/{t_stats.mean():.3f}/{t_stats.std():.3f}')\n",
        "        if ANSWER_HELPFUL_IDX is not None:\n",
        "            print(f\"    answer_helpful Spearman plain/EMA: {per_plain[ANSWER_HELPFUL_IDX]:.5f}/{per_ema[ANSWER_HELPFUL_IDX]:.5f}\")\n",
        "        print(f'  Epoch {epoch+1} mean-30 Spearman plain/EMA: {s_plain:.5f}/{s_ema:.5f} | time {(time.time()-t0):.1f}s')\n",
        "        score = s_plain if s_plain >= s_ema else s_ema\n",
        "        val_preds = vp_plain if s_plain >= s_ema else vp_ema\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_val_preds = val_preds.copy()\n",
        "\n",
        "    # Test prediction with EMA weights by default\n",
        "    ema.apply_to(model)\n",
        "    model.eval()\n",
        "    test_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs = {k: v.to(device, non_blocking=True) for k, v in batch.items() if k not in ('labels','token_type_ids')}\n",
        "            logits, _ = model(**inputs, labels=None)\n",
        "            test_preds.append(logits.float().cpu().numpy())\n",
        "    ema.restore(model)\n",
        "    test_preds = np.concatenate(test_preds, axis=0)\n",
        "\n",
        "    del model, optimizer, scheduler, scaler, train_loader, val_loader, test_loader, ds_tr, ds_va, ds_te\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return best_val_preds, test_preds, best_score\n",
        "\n",
        "# Run CV\n",
        "unique_folds = np.unique(folds)\n",
        "oof = np.zeros((len(train), len(target_cols)), dtype=np.float32)\n",
        "test_accum = np.zeros((len(unique_folds), len(test), len(target_cols)), dtype=np.float32)\n",
        "fold_scores = []\n",
        "\n",
        "overall_t0 = time.time()\n",
        "for i, fold in enumerate(unique_folds):\n",
        "    tr_idx = np.where(folds != fold)[0]\n",
        "    va_idx = np.where(folds == fold)[0]\n",
        "    va_pred, te_pred, score = run_fold(fold, tr_idx, va_idx)\n",
        "    oof[va_idx] = va_pred\n",
        "    test_accum[i] = te_pred\n",
        "    fold_scores.append(float(score))\n",
        "    print(f'Fold {fold} best mean-30 Spearman: {score:.5f}', flush=True)\n",
        "\n",
        "oof_mean_score, oof_percol = spearman_cols(oof, train[target_cols].astype(np.float32).values)\n",
        "print('Fold mean Spearmans:', [round(s,5) for s in fold_scores])\n",
        "print(f'OOF mean-30 Spearman: {oof_mean_score:.5f}')\n",
        "\n",
        "# Save OOF/test\n",
        "np.save('oof_all_targets_deberta_base.npy', np.clip(oof, 0, 1).astype(np.float32))\n",
        "test_pred = test_accum.mean(axis=0).astype(np.float32)\n",
        "test_pred = np.clip(test_pred, 0.0, 1.0).astype(np.float32)\n",
        "np.save('test_all_targets_deberta_base.npy', test_pred)\n",
        "\n",
        "# Build submission\n",
        "sub = sample_sub.copy()\n",
        "sub[id_col] = test[id_col].values\n",
        "for i, col in enumerate(target_cols):\n",
        "    sub[col] = test_pred[:, i]\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with multitask transformer predictions. Total time:', round(time.time()-overall_t0,1),'s')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\nTarget dtypes: [dtype('float64')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 start: tr=4376 va=1095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1 step 100/274 loss=0.0696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1 step 200/274 loss=0.0505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.160/1.080/0.478/0.341\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.415\n    answer_helpful Spearman plain/EMA: 0.02759/0.01583\n  Epoch 1 mean-30 Spearman plain/EMA: 0.25957/0.16082 | time 221.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2 step 100/274 loss=0.0271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2 step 200/274 loss=0.0264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.145/1.125/0.484/0.357\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.415\n    answer_helpful Spearman plain/EMA: 0.08403/0.03672\n  Epoch 2 mean-30 Spearman plain/EMA: 0.32423/0.28156 | time 444.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3 step 100/274 loss=0.0236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3 step 200/274 loss=0.0232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.220/1.184/0.483/0.360\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.415\n    answer_helpful Spearman plain/EMA: 0.10203/0.08477\n  Epoch 3 mean-30 Spearman plain/EMA: 0.34020/0.32822 | time 669.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4 step 100/274 loss=0.0217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4 step 200/274 loss=0.0218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.219/1.209/0.484/0.363\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.415\n    answer_helpful Spearman plain/EMA: 0.10431/0.10182\n  Epoch 4 mean-30 Spearman plain/EMA: 0.34500/0.34205 | time 893.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 best mean-30 Spearman: 0.34500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 start: tr=4377 va=1094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1 step 100/274 loss=0.0717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1 step 200/274 loss=0.0514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.176/1.113/0.474/0.348\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.413\n    answer_helpful Spearman plain/EMA: 0.16863/0.11469\n  Epoch 1 mean-30 Spearman plain/EMA: 0.24620/0.13213 | time 223.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2 step 100/274 loss=0.0270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2 step 200/274 loss=0.0264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.179/1.139/0.473/0.355\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.413\n    answer_helpful Spearman plain/EMA: 0.17789/0.17127\n  Epoch 2 mean-30 Spearman plain/EMA: 0.29495/0.25601 | time 448.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3 step 100/274 loss=0.0236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3 step 200/274 loss=0.0232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.248/1.135/0.471/0.364\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.413\n    answer_helpful Spearman plain/EMA: 0.16486/0.18110\n  Epoch 3 mean-30 Spearman plain/EMA: 0.31772/0.30432 | time 672.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4 step 100/274 loss=0.0215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4 step 200/274 loss=0.0217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.246/1.146/0.476/0.364\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.413\n    answer_helpful Spearman plain/EMA: 0.16748/0.17165\n  Epoch 4 mean-30 Spearman plain/EMA: 0.32069/0.31730 | time 896.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 best mean-30 Spearman: 0.32069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 start: tr=4377 va=1094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1 step 100/274 loss=0.0668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1 step 200/274 loss=0.0487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.187/1.126/0.463/0.349\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.414\n    answer_helpful Spearman plain/EMA: 0.09119/0.05086\n  Epoch 1 mean-30 Spearman plain/EMA: 0.24995/0.14592 | time 223.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2 step 100/274 loss=0.0262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2 step 200/274 loss=0.0256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.216/1.182/0.470/0.359\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.414\n    answer_helpful Spearman plain/EMA: 0.10489/0.09390\n  Epoch 2 mean-30 Spearman plain/EMA: 0.30284/0.26319 | time 448.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3 step 100/274 loss=0.0230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3 step 200/274 loss=0.0227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.195/1.189/0.473/0.362\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.414\n    answer_helpful Spearman plain/EMA: 0.11766/0.10383\n  Epoch 3 mean-30 Spearman plain/EMA: 0.32208/0.30705 | time 672.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4 step 100/274 loss=0.0211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4 step 200/274 loss=0.0213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.203/1.206/0.474/0.365\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.414\n    answer_helpful Spearman plain/EMA: 0.10982/0.10682\n  Epoch 4 mean-30 Spearman plain/EMA: 0.32306/0.31976 | time 896.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 best mean-30 Spearman: 0.32306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 start: tr=4377 va=1094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1 step 100/274 loss=0.0596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1 step 200/274 loss=0.0451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.183/1.120/0.472/0.353\n    val tgts           min/max/mean/std: 0.000/1.000/0.474/0.415\n    answer_helpful Spearman plain/EMA: 0.09215/0.05395\n  Epoch 1 mean-30 Spearman plain/EMA: 0.25105/0.15890 | time 223.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2 step 100/274 loss=0.0263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2 step 200/274 loss=0.0257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.178/1.157/0.476/0.362\n    val tgts           min/max/mean/std: 0.000/1.000/0.474/0.415\n    answer_helpful Spearman plain/EMA: 0.13999/0.11882\n  Epoch 2 mean-30 Spearman plain/EMA: 0.31313/0.27545 | time 448.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3 step 100/274 loss=0.0230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3 step 200/274 loss=0.0225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.176/1.208/0.479/0.367\n    val tgts           min/max/mean/std: 0.000/1.000/0.474/0.415\n    answer_helpful Spearman plain/EMA: 0.14840/0.14676\n  Epoch 3 mean-30 Spearman plain/EMA: 0.33498/0.32023 | time 672.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4 step 100/274 loss=0.0215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4 step 200/274 loss=0.0213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.169/1.182/0.477/0.368\n    val tgts           min/max/mean/std: 0.000/1.000/0.474/0.415\n    answer_helpful Spearman plain/EMA: 0.14625/0.14744\n  Epoch 4 mean-30 Spearman plain/EMA: 0.33771/0.33383 | time 896.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 best mean-30 Spearman: 0.33771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 start: tr=4377 va=1094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1 step 100/274 loss=0.0567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1 step 200/274 loss=0.0434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.165/1.132/0.477/0.353\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.413\n    answer_helpful Spearman plain/EMA: -0.00698/-0.06542\n  Epoch 1 mean-30 Spearman plain/EMA: 0.25801/0.15612 | time 223.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2 step 100/274 loss=0.0261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2 step 200/274 loss=0.0254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.209/1.120/0.480/0.360\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.413\n    answer_helpful Spearman plain/EMA: 0.09658/0.02579\n  Epoch 2 mean-30 Spearman plain/EMA: 0.31544/0.27332 | time 448.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3 step 100/274 loss=0.0221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3 step 200/274 loss=0.0222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.254/1.155/0.479/0.362\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.413\n    answer_helpful Spearman plain/EMA: 0.12599/0.09767\n  Epoch 3 mean-30 Spearman plain/EMA: 0.33381/0.31829 | time 672.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4 step 100/274 loss=0.0214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4 step 200/274 loss=0.0212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val preds (plain) min/max/mean/std: -0.271/1.170/0.479/0.364\n    val tgts           min/max/mean/std: 0.000/1.000/0.475/0.413\n    answer_helpful Spearman plain/EMA: 0.12760/0.11942\n  Epoch 4 mean-30 Spearman plain/EMA: 0.33621/0.33194 | time 896.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 best mean-30 Spearman: 0.33621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold mean Spearmans: [0.345, 0.32069, 0.32306, 0.33771, 0.33621]\nOOF mean-30 Spearman: 0.32910\nSaved submission.csv with multitask transformer predictions. Total time: 4542.9 s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}