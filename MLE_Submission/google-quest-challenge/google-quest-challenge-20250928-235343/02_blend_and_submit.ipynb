{
  "cells": [
    {
      "id": "add6f54d-d00c-41ba-9971-c2ba0fd77dd4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blending and submission builder: base + large\n",
        "import numpy as np, pandas as pd, time\n",
        "from scipy.stats import spearmanr, rankdata\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "t0=time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "id_col = sample_sub.columns[0]\n",
        "target_cols = [c for c in sample_sub.columns if c != id_col]\n",
        "folds = np.load('folds.npy')\n",
        "\n",
        "def spearman_cols(y_pred: np.ndarray, y_true: np.ndarray):\n",
        "    rhos=[]\n",
        "    for i in range(y_pred.shape[1]):\n",
        "        r = spearmanr(y_pred[:,i], y_true[:,i]).correlation\n",
        "        rhos.append(0.0 if (r is None or np.isnan(r)) else float(r))\n",
        "    return float(np.mean(rhos)), rhos\n",
        "\n",
        "def to_rank01(v: np.ndarray) -> np.ndarray:\n",
        "    r = rankdata(v, method='average').astype(np.float64)\n",
        "    denom = max(1.0, len(r)-1)\n",
        "    return ((r-1.0)/denom).astype(np.float32)\n",
        "\n",
        "# Load OOF/test from base and large\n",
        "oof_base = np.load('oof_all_targets_deberta_base.npy')\n",
        "te_base  = np.load('test_all_targets_deberta_base.npy')\n",
        "oof_large = np.load('oof_all_targets_deberta_large.npy')\n",
        "te_large  = np.load('test_all_targets_deberta_large.npy')\n",
        "y_true = train[target_cols].to_numpy(dtype=np.float32)\n",
        "\n",
        "# Sanity shapes\n",
        "assert oof_base.shape == oof_large.shape == (len(train), len(target_cols))\n",
        "assert te_base.shape[0] == len(test) and te_large.shape[0] == len(test)\n",
        "\n",
        "# Individual OOF scores\n",
        "base_oof_score, _ = spearman_cols(oof_base, y_true)\n",
        "large_oof_score, _ = spearman_cols(oof_large, y_true)\n",
        "print(f'Base OOF mean-30: {base_oof_score:.5f} | Large OOF mean-30: {large_oof_score:.5f}')\n",
        "\n",
        "# Raw weighted blend\n",
        "w_large = 0.65; w_base = 0.35\n",
        "oof_raw = w_large * oof_large + w_base * oof_base\n",
        "te_raw  = w_large * te_large  + w_base * te_base\n",
        "raw_oof_score, _ = spearman_cols(oof_raw, y_true)\n",
        "print(f'Raw 0.65L/0.35B OOF mean-30: {raw_oof_score:.5f}')\n",
        "\n",
        "# Rank-based per-target blend\n",
        "oof_base_rank = np.zeros_like(oof_base, dtype=np.float32)\n",
        "oof_large_rank = np.zeros_like(oof_large, dtype=np.float32)\n",
        "for i in range(oof_base.shape[1]):\n",
        "    oof_base_rank[:,i]  = to_rank01(oof_base[:,i])\n",
        "    oof_large_rank[:,i] = to_rank01(oof_large[:,i])\n",
        "oof_rankblend = w_large * oof_large_rank + w_base * oof_base_rank\n",
        "rank_oof_score,_ = spearman_cols(oof_rankblend, y_true)\n",
        "print(f'Rank 0.65L/0.35B OOF mean-30: {rank_oof_score:.5f}')\n",
        "\n",
        "# Apply same rank-normalization to test before averaging (map each model's column to ranks on its test outputs)\n",
        "te_base_rank = np.zeros_like(te_base, dtype=np.float32)\n",
        "te_large_rank = np.zeros_like(te_large, dtype=np.float32)\n",
        "for i in range(te_base.shape[1]):\n",
        "    te_base_rank[:,i]  = to_rank01(te_base[:,i])\n",
        "    te_large_rank[:,i] = to_rank01(te_large[:,i])\n",
        "te_rankblend = w_large * te_large_rank + w_base * te_base_rank\n",
        "\n",
        "# Ridge stacking with out-of-fold meta (fold-correct) and test averaged over folds\n",
        "alphas = [0.1, 0.3, 1.0, 3.0, 10.0]\n",
        "unique_folds = np.unique(folds)\n",
        "oof_ridge = np.zeros_like(oof_base, dtype=np.float32)\n",
        "te_ridge = np.zeros_like(te_base, dtype=np.float32)\n",
        "for i_col in range(len(target_cols)):\n",
        "    X_full = np.stack([oof_base[:,i_col], oof_large[:,i_col]], axis=1)\n",
        "    y = y_true[:, i_col]\n",
        "    teX = np.stack([te_base[:,i_col], te_large[:,i_col]], axis=1)\n",
        "    te_fold_preds = []\n",
        "    for f in unique_folds:\n",
        "        tr_idx = np.where(folds != f)[0]; va_idx = np.where(folds == f)[0]\n",
        "        model = RidgeCV(alphas=alphas, fit_intercept=True)\n",
        "        model.fit(X_full[tr_idx], y[tr_idx])\n",
        "        oof_ridge[va_idx, i_col] = model.predict(X_full[va_idx]).astype(np.float32)\n",
        "        te_fold_preds.append(model.predict(teX).astype(np.float32))\n",
        "    te_ridge[:, i_col] = np.mean(np.stack(te_fold_preds, axis=0), axis=0).astype(np.float32)\n",
        "ridge_oof_score,_ = spearman_cols(oof_ridge, y_true)\n",
        "print(f'Ridge stack OOF mean-30: {ridge_oof_score:.5f}')\n",
        "\n",
        "# Pick best OOF strategy\n",
        "candidates = [\n",
        "    ('raw', raw_oof_score, np.clip(te_raw, 0,1).astype(np.float32)),\n",
        "    ('rank', rank_oof_score, np.clip(te_rankblend, 0,1).astype(np.float32)),\n",
        "    ('ridge', ridge_oof_score, np.clip(te_ridge, 0,1).astype(np.float32)),\n",
        "]\n",
        "best_name, best_oof, best_test = sorted(candidates, key=lambda x: x[1], reverse=True)[0]\n",
        "print('Best strategy:', best_name, 'OOF:', round(best_oof,5))\n",
        "\n",
        "# Save diagnostics and submissions\n",
        "np.save('oof_blend_raw.npy', np.clip(oof_raw, 0,1).astype(np.float32))\n",
        "np.save('oof_blend_rank.npy', np.clip(oof_rankblend, 0,1).astype(np.float32))\n",
        "np.save('oof_blend_ridge.npy', np.clip(oof_ridge, 0,1).astype(np.float32))\n",
        "np.save('test_blend_raw.npy', np.clip(te_raw, 0,1).astype(np.float32))\n",
        "np.save('test_blend_rank.npy', np.clip(te_rankblend, 0,1).astype(np.float32))\n",
        "np.save('test_blend_ridge.npy', np.clip(te_ridge, 0,1).astype(np.float32))\n",
        "\n",
        "def write_sub(pred, path):\n",
        "    sub = sample_sub.copy()\n",
        "    sub[id_col] = test[id_col].values\n",
        "    for i, col in enumerate(target_cols):\n",
        "        sub[col] = pred[:, i]\n",
        "    sub.to_csv(path, index=False)\n",
        "\n",
        "write_sub(np.clip(te_raw,0,1), 'submission_blend_raw.csv')\n",
        "write_sub(np.clip(te_rankblend,0,1), 'submission_blend_rank.csv')\n",
        "write_sub(np.clip(te_ridge,0,1), 'submission_blend_ridge.csv')\n",
        "write_sub(best_test, 'submission.csv')\n",
        "print('Saved submissions. Final chosen:', best_name, '| elapsed', round(time.time()-t0,1),'s')\n",
        "\n",
        "print('Done.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base OOF mean-30: 0.32984 | Large OOF mean-30: 0.34386\nRaw 0.65L/0.35B OOF mean-30: 0.35617\nRank 0.65L/0.35B OOF mean-30: 0.35561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge stack OOF mean-30: 0.35300\nBest strategy: raw OOF: 0.35617\nSaved submissions. Final chosen: raw | elapsed 1.1 s\nDone.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}