{
  "cells": [
    {
      "id": "eb697532-5483-4d53-87c5-c51e38f6f342",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Multi-output DeBERTa-v3-base (30 targets) with 5-fold SGKF, Q+A packing, mean Spearman metric\n",
        "import os, gc, time, json, numpy as np, pandas as pd, torch\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from scipy.stats import spearmanr\n",
        "from transformers import (AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
        "                          TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorWithPadding, set_seed)\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "\n",
        "# Data\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "id_col = 'qa_id'\n",
        "targets = [\n",
        "    'question_asker_intent_understanding','question_body_critical','question_conversational','question_expect_short_answer',\n",
        "    'question_fact_seeking','question_has_commonly_accepted_answer','question_interestingness_others','question_interestingness_self',\n",
        "    'question_multi_intent','question_not_really_a_question','question_opinion_seeking','question_type_choice','question_type_compare',\n",
        "    'question_type_consequence','question_type_definition','question_type_entity','question_type_instructions','question_type_procedure',\n",
        "    'question_type_reason_explanation','question_type_spelling','question_well_written','answer_helpful','answer_level_of_information',\n",
        "    'answer_plausible','answer_relevance','answer_satisfaction','answer_type_instructions','answer_type_procedure',\n",
        "    'answer_type_reason_explanation','answer_well_written'\n",
        "]\n",
        "assert set(targets).issubset(train.columns), 'Missing QUEST targets in train.csv'\n",
        "Y = train[targets].astype(float).values  # (N,30)\n",
        "\n",
        "# Folds: StratifiedGroupKFold by group mean of main target (reuse established protocol)\n",
        "main_target = 'question_asker_intent_understanding'\n",
        "y_main = train[main_target].values.astype(float)\n",
        "if Path('train_group_keys.csv').exists():\n",
        "    groups = pd.read_csv('train_group_keys.csv')['group_key'].values\n",
        "else:\n",
        "    groups = pd.util.hash_pandas_object((train['question_title'].fillna('')+'||'+train['question_body'].fillna('')), index=False).astype('int64').values\n",
        "df_groups = pd.DataFrame({'group': groups, 'y': y_main})\n",
        "grp_mean = df_groups.groupby('group')['y'].mean()\n",
        "bins = pd.qcut(grp_mean, q=10, labels=False, duplicates='drop')\n",
        "grp_to_bin = dict(zip(grp_mean.index.values, bins.astype(int)))\n",
        "row_bins = np.array([grp_to_bin[g] for g in groups], dtype=int)\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "splits = list(sgkf.split(np.zeros_like(y_main), y=row_bins, groups=groups))\n",
        "\n",
        "# Tokenization: Q+A template with dynamic truncation\n",
        "model_name = 'microsoft/deberta-v3-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "MAX_LEN = 512; TITLE_MAX = 64\n",
        "\n",
        "def pack_qa(title: str, body: str, answer: str, tokenizer, max_len=MAX_LEN, title_max=TITLE_MAX):\n",
        "    ti = tokenizer(title if isinstance(title, str) else '', add_special_tokens=False, truncation=True, max_length=title_max)['input_ids']\n",
        "    bi_full = tokenizer(body if isinstance(body, str) else '', add_special_tokens=False, truncation=False)['input_ids']\n",
        "    ai_full = tokenizer(answer if isinstance(answer, str) else '', add_special_tokens=False, truncation=False)['input_ids']\n",
        "    # CLS + 3*SEP\n",
        "    rem = max_len - (1 + 1 + 1 + 1) - len(ti)\n",
        "    rem = max(rem, 0)\n",
        "    lb, la = len(bi_full), len(ai_full)\n",
        "    if lb + la == 0:\n",
        "        bi, ai = [], []\n",
        "    else:\n",
        "        if rem >= 96:\n",
        "            qb = max(48, int(rem * (lb/(lb+la))))\n",
        "            qa = rem - qb\n",
        "        else:\n",
        "            qb = rem // 2\n",
        "            qa = rem - qb\n",
        "        bi, ai = bi_full[:qb], ai_full[:qa]\n",
        "    ids = [tokenizer.cls_token_id] + ti + [tokenizer.sep_token_id] + bi + [tokenizer.sep_token_id] + ai + [tokenizer.sep_token_id]\n",
        "    ids = ids[:max_len]\n",
        "    attn = [1]*len(ids)\n",
        "    pad = max_len - len(ids)\n",
        "    if pad>0:\n",
        "        ids += [tokenizer.pad_token_id]*pad\n",
        "        attn += [0]*pad\n",
        "    return ids, attn\n",
        "\n",
        "def build_inputs(df: pd.DataFrame):\n",
        "    T = df['question_title'].fillna('').astype(str).tolist()\n",
        "    B = df['question_body'].fillna('').astype(str).tolist()\n",
        "    A = df['answer'].fillna('').astype(str).tolist() if 'answer' in df.columns else ['']*len(df)\n",
        "    input_ids, attention_masks = [], []\n",
        "    for t,b,a in zip(T,B,A):\n",
        "        ids, attn = pack_qa(t,b,a, tokenizer)\n",
        "        input_ids.append(ids); attention_masks.append(attn)\n",
        "    return {'input_ids': np.array(input_ids, dtype=np.int64), 'attention_mask': np.array(attention_masks, dtype=np.int64)}\n",
        "\n",
        "print('[TOK] Building inputs (Q+A) ...', flush=True)\n",
        "t0_tok = time.time()\n",
        "tr_inputs = build_inputs(train)\n",
        "te_inputs = build_inputs(test)\n",
        "print(f'[TOK] Done in {time.time()-t0_tok:.1f}s; shapes tr={tr_inputs[\"input_ids\"].shape} te={te_inputs[\"input_ids\"].shape}', flush=True)\n",
        "\n",
        "class QADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ids, masks, labels=None):\n",
        "        self.ids = ids; self.masks = masks; self.labels = labels\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {'input_ids': torch.tensor(self.ids[idx]), 'attention_mask': torch.tensor(self.masks[idx])}\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred  # preds: (N,30), labels: (N,30)\n",
        "    vals = []\n",
        "    for j in range(labels.shape[1]):\n",
        "        p = preds[:, j]; y = labels[:, j]\n",
        "        if np.std(p)==0 or np.std(y)==0:\n",
        "            vals.append(0.0)\n",
        "        else:\n",
        "            vals.append(float(spearmanr(y, p).correlation))\n",
        "    return {'mean_spearman': float(np.mean(vals))}\n",
        "\n",
        "# Training loop (1 seed, 5 folds)\n",
        "config = AutoConfig.from_pretrained(model_name, num_labels=30, problem_type='regression')\n",
        "oof = np.zeros((len(train), 30), dtype=np.float32)\n",
        "test_fold_preds = []\n",
        "folds_idx = np.full(len(train), -1, dtype=int)\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(splits):\n",
        "    t0 = time.time()\n",
        "    folds_idx[val_idx] = fold\n",
        "    print(f'\\n[MULTI FOLD {fold}] train={len(trn_idx)} val={len(val_idx)}', flush=True)\n",
        "    tr_ds = QADataset(tr_inputs['input_ids'][trn_idx], tr_inputs['attention_mask'][trn_idx], Y[trn_idx])\n",
        "    va_ds = QADataset(tr_inputs['input_ids'][val_idx], tr_inputs['attention_mask'][val_idx], Y[val_idx])\n",
        "    te_ds = QADataset(te_inputs['input_ids'], te_inputs['attention_mask'], None)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "    # Enable gradient checkpointing (also via args)\n",
        "    try: model.gradient_checkpointing_enable()\n",
        "    except Exception: pass\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'deberta_multi_fold{fold}',\n",
        "        num_train_epochs=2,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=32,\n",
        "        gradient_accumulation_steps=2,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type='linear',\n",
        "        fp16=True,\n",
        "        gradient_checkpointing=True,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='mean_spearman',\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=1,\n",
        "        logging_steps=50,\n",
        "        seed=SEED,\n",
        "        report_to=[]\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tr_ds,\n",
        "        eval_dataset=va_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
        "        data_collator=DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    # Validation preds\n",
        "    val_pred = trainer.predict(va_ds).predictions  # (N_val,30)\n",
        "    oof[val_idx] = val_pred.astype(np.float32)\n",
        "    # Fold metric (mean Spearman)\n",
        "    vals = []\n",
        "    for j in range(val_pred.shape[1]):\n",
        "        p = val_pred[:, j]; y = Y[val_idx, j]\n",
        "        vals.append(0.0 if np.std(p)==0 or np.std(y)==0 else float(spearmanr(y, p).correlation))\n",
        "    print(f'[MULTI FOLD {fold}] mean Spearman={np.mean(vals):.5f} time={time.time()-t0:.1f}s', flush=True)\n",
        "    # Test preds\n",
        "    te_pred = trainer.predict(te_ds).predictions.astype(np.float32)  # (608,30)\n",
        "    test_fold_preds.append(te_pred)\n",
        "    del trainer, model; gc.collect()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "# Aggregate and save artifacts\n",
        "oof_mean = []\n",
        "for j in range(oof.shape[1]):\n",
        "    yj = train[targets[j]].values.astype(float)\n",
        "    pj = oof[:, j]\n",
        "    sc = 0.0 if np.std(pj)==0 or np.std(yj)==0 else float(spearmanr(yj, pj).correlation)\n",
        "    oof_mean.append(sc)\n",
        "print('[MULTI] OOF per-target Spearman (first 5):', np.round(oof_mean[:5], 5))\n",
        "print('[MULTI] OOF mean Spearman:', float(np.mean(oof_mean)))\n",
        "np.save('oof_deberta_multi.npy', oof)\n",
        "pd.DataFrame({'qa_id': train[id_col], 'fold': folds_idx}).assign(**{f't{j}': oof[:, j] for j in range(oof.shape[1])}).to_csv('oof_deberta_multi.csv', index=False)\n",
        "\n",
        "test_mean = np.mean(np.stack(test_fold_preds, axis=0), axis=0).astype(np.float32)  # (608,30)\n",
        "np.save('test_deberta_multi.npy', test_mean)\n",
        "\n",
        "# Build 31-col submission: fill all 30 targets with model preds (clipped [0,1])\n",
        "samp = pd.read_csv('sample_submission.csv')\n",
        "assert 'qa_id' in samp.columns and len(samp.columns)==31, 'Unexpected sample_submission schema'\n",
        "samp['qa_id'] = pd.to_numeric(test['qa_id'], errors='raise').astype('int64')\n",
        "for i, col in enumerate(targets):\n",
        "    samp[col] = np.clip(test_mean[:, i], 0.0, 1.0).astype(float)\n",
        "samp.to_csv('submission_multi.csv', index=False, float_format='%.8f')\n",
        "print('[SUB] submission_multi.csv written:', samp.shape)\n",
        "\n",
        "# Optional: override main target with our single-target ensemble if available\n",
        "if Path('test_ensemble.npy').exists():\n",
        "    preds_main = np.clip(np.load('test_ensemble.npy').astype(float), 0.0, 1.0)\n",
        "    samp_ovr = samp.copy()\n",
        "    samp_ovr[main_target] = preds_main\n",
        "    samp_ovr.to_csv('submission_multi_override.csv', index=False, float_format='%.8f')\n",
        "    print('[SUB] submission_multi_override.csv written with main target overridden.')\n",
        "\n",
        "print('[DONE] Multi-output pipeline complete. Artifacts: oof_deberta_multi.npy, test_deberta_multi.npy, submission_multi.csv')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TOK] Building inputs (Q+A) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TOK] Done in 5.0s; shapes tr=(5471, 512) te=(608, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[MULTI FOLD 0] train=4395 val=1076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/550 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MULTI FOLD 0] mean Spearman=0.32684 time=426.9s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[MULTI FOLD 1] train=4318 val=1153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/540 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/37 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MULTI FOLD 1] mean Spearman=0.31641 time=425.9s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[MULTI FOLD 2] train=4389 val=1082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/548 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MULTI FOLD 2] mean Spearman=0.29213 time=430.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[MULTI FOLD 3] train=4399 val=1072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/550 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MULTI FOLD 3] mean Spearman=0.30830 time=431.6s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[MULTI FOLD 4] train=4383 val=1088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/548 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MULTI FOLD 4] mean Spearman=0.32581 time=430.2s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MULTI] OOF per-target Spearman (first 5): [0.2753  0.4433  0.39041 0.23185 0.31649]\n[MULTI] OOF mean Spearman: 0.3114201564172349\n[SUB] submission_multi.csv written: (608, 31)\n[SUB] submission_multi_override.csv written with main target overridden.\n[DONE] Multi-output pipeline complete. Artifacts: oof_deberta_multi.npy, test_deberta_multi.npy, submission_multi.csv\n"
          ]
        }
      ]
    },
    {
      "id": "080d1f3d-d56b-4717-bc5e-60cbbce9abd9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Promote multi-output submission to submission.csv\n",
        "import pandas as pd, os\n",
        "src = 'submission_multi_override.csv' if os.path.exists('submission_multi_override.csv') else 'submission_multi.csv'\n",
        "df = pd.read_csv(src)\n",
        "assert df.shape[1] == 31 and 'qa_id' in df.columns, f'Unexpected schema in {src}: {df.shape} columns={list(df.columns)[:5]} ...'\n",
        "df.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('[SUBMIT] Wrote submission.csv from', src, 'shape=', df.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUBMIT] Wrote submission.csv from submission_multi_override.csv shape= (608, 31)\n"
          ]
        }
      ]
    },
    {
      "id": "95c376f0-0db6-459b-9b6f-d66fbfabbe0c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MC Dropout TTA (8 passes) for multi-output DeBERTa; build new submission\n",
        "import json, gc, time, numpy as np, torch\n",
        "from pathlib import Path\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "def spearmanr_mean_30(oof_mat, Y_true):\n",
        "    vals = []\n",
        "    for j in range(oof_mat.shape[1]):\n",
        "        p = oof_mat[:, j]; y = Y_true[:, j]\n",
        "        if np.std(p)==0 or np.std(y)==0: vals.append(0.0)\n",
        "        else: vals.append(float(spearmanr(y, p).correlation))\n",
        "    return float(np.mean(vals))\n",
        "\n",
        "def best_ckpt_path(out_dir: str):\n",
        "    # Try trainer_state.json at root\n",
        "    state_path = Path(out_dir)/'trainer_state.json'\n",
        "    if state_path.exists():\n",
        "        try:\n",
        "            st = json.loads(state_path.read_text())\n",
        "            best = st.get('best_model_checkpoint', None)\n",
        "            if best and Path(best).exists():\n",
        "                return best\n",
        "        except Exception:\n",
        "            pass\n",
        "    # Fallback to most recent checkpoint-*\n",
        "    p = Path(out_dir)\n",
        "    if p.exists():\n",
        "        cands = sorted([q for q in p.glob('checkpoint-*') if q.is_dir()])\n",
        "        if cands:\n",
        "            return str(cands[-1])\n",
        "    return None\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "@torch.inference_mode()\n",
        "def mc_predict_multi(model, ids, masks, passes=8, batch_size=16):\n",
        "    model.train()  # enable dropout\n",
        "    N = len(ids); out = np.zeros((N, 30), dtype=np.float32)\n",
        "    for rep in range(passes):\n",
        "        t0 = time.time()\n",
        "        preds = []\n",
        "        for i in range(0, N, batch_size):\n",
        "            s = slice(i, min(i+batch_size, N))\n",
        "            input_ids = torch.tensor(ids[s], device=device)\n",
        "            attention_mask = torch.tensor(masks[s], device=device)\n",
        "            if use_cuda:\n",
        "                with torch.autocast('cuda', dtype=torch.float16):\n",
        "                    logits = model(input_ids=input_ids, attention_mask=attention_mask).logits  # (B,30)\n",
        "            else:\n",
        "                logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "            preds.append(logits.float().cpu().numpy())\n",
        "        pass_preds = np.vstack(preds)\n",
        "        out += pass_preds\n",
        "        print(f\"    [mc_pass {rep+1}/{passes}] N={N} elapsed={time.time()-t0:.1f}s\", flush=True)\n",
        "    return out / passes\n",
        "\n",
        "print('[MC-MULTI] Starting MC TTA over 5 folds ...', flush=True)\n",
        "oof_mc = np.zeros_like(Y, dtype=np.float32)\n",
        "test_accum = []\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(splits):\n",
        "    out_dir = f'deberta_multi_fold{fold}'\n",
        "    ckpt = best_ckpt_path(out_dir)\n",
        "    if ckpt is None or not Path(ckpt).exists():\n",
        "        print(f\"[MC-MULTI] WARNING: checkpoint not found for {out_dir}\")\n",
        "    assert ckpt is not None and Path(ckpt).exists(), f'Checkpoint for fold {fold} not found'\n",
        "    print(f\"[MC-MULTI] fold={fold} ckpt={ckpt}\")\n",
        "\n",
        "    # Cache paths\n",
        "    val_cache = Path(f'val_multi_mc_fold{fold}_p8.npy')\n",
        "    test_cache = Path(f'test_multi_mc_fold{fold}_p8.npy')\n",
        "\n",
        "    if val_cache.exists() and test_cache.exists():\n",
        "        print(f\"[MC-MULTI] Loading cached mc preds for fold {fold}\")\n",
        "        pv = np.load(val_cache); pt = np.load(test_cache)\n",
        "    else:\n",
        "        t_load = time.time()\n",
        "        config = AutoConfig.from_pretrained('microsoft/deberta-v3-base', num_labels=30, problem_type='regression')\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(ckpt, config=config).to(device)\n",
        "        print(f\"[MC-MULTI] Model loaded in {time.time()-t_load:.1f}s; running MC inference...\", flush=True)\n",
        "        pv = mc_predict_multi(model, tr_inputs['input_ids'][val_idx], tr_inputs['attention_mask'][val_idx], passes=8, batch_size=16)\n",
        "        pt = mc_predict_multi(model, te_inputs['input_ids'], te_inputs['attention_mask'], passes=8, batch_size=16)\n",
        "        np.save(val_cache, pv.astype(np.float32))\n",
        "        np.save(test_cache, pt.astype(np.float32))\n",
        "        del model; gc.collect()\n",
        "        if use_cuda: torch.cuda.empty_cache()\n",
        "\n",
        "    oof_mc[val_idx] = pv.astype(np.float32)\n",
        "    test_accum.append(pt.astype(np.float32))\n",
        "    fold_sc = spearmanr_mean_30(pv, Y[val_idx])\n",
        "    print(f\"[MC-MULTI] fold={fold} mean Spearman={fold_sc:.5f}\", flush=True)\n",
        "\n",
        "test_mc = np.mean(np.stack(test_accum, axis=0), axis=0).astype(np.float32)  # (608,30)\n",
        "mc_oof_mean = spearmanr_mean_30(oof_mc, Y)\n",
        "print(f\"[MC-MULTI] OOF mean Spearman (MC): {mc_oof_mean:.5f}\")\n",
        "np.save('oof_deberta_multi_mc8.npy', oof_mc)\n",
        "np.save('test_deberta_multi_mc8.npy', test_mc)\n",
        "\n",
        "# Build submissions (raw MC and main-target override)\n",
        "samp = pd.read_csv('sample_submission.csv')\n",
        "samp['qa_id'] = pd.to_numeric(test['qa_id'], errors='raise').astype('int64')\n",
        "for i, col in enumerate(targets):\n",
        "    samp[col] = np.clip(test_mc[:, i], 0.0, 1.0).astype(float)\n",
        "samp.to_csv('submission_multi_mc.csv', index=False, float_format='%.8f')\n",
        "print('[SUB] submission_multi_mc.csv written:', samp.shape)\n",
        "\n",
        "from pathlib import Path as _Path\n",
        "if _Path('test_ensemble.npy').exists():\n",
        "    samp_ovr = samp.copy()\n",
        "    samp_ovr[main_target] = np.clip(np.load('test_ensemble.npy').astype(float), 0.0, 1.0)\n",
        "    samp_ovr.to_csv('submission_multi_mc_override.csv', index=False, float_format='%.8f')\n",
        "    print('[SUB] submission_multi_mc_override.csv written (main target overridden)')\n",
        "\n",
        "print('[MC-MULTI] Done.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] Starting MC TTA over 5 folds ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] fold=0 ckpt=deberta_multi_fold0/checkpoint-550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] Model loaded in 0.3s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1076 elapsed=13.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1076 elapsed=13.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1076 elapsed=13.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1076 elapsed=13.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1076 elapsed=13.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1076 elapsed=13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1076 elapsed=13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1076 elapsed=13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] fold=0 mean Spearman=0.31161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] fold=1 ckpt=deberta_multi_fold1/checkpoint-540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1153 elapsed=14.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1153 elapsed=14.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1153 elapsed=14.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1153 elapsed=14.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1153 elapsed=14.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1153 elapsed=14.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1153 elapsed=14.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1153 elapsed=14.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] fold=1 mean Spearman=0.29563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] fold=2 ckpt=deberta_multi_fold2/checkpoint-548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1082 elapsed=13.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1082 elapsed=13.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1082 elapsed=13.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1082 elapsed=13.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1082 elapsed=13.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1082 elapsed=13.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1082 elapsed=13.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1082 elapsed=13.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] fold=2 mean Spearman=0.27305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] fold=3 ckpt=deberta_multi_fold3/checkpoint-550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1072 elapsed=13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1072 elapsed=13.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1072 elapsed=13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1072 elapsed=13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1072 elapsed=13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1072 elapsed=13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1072 elapsed=13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1072 elapsed=13.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] fold=3 mean Spearman=0.28468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] fold=4 ckpt=deberta_multi_fold4/checkpoint-548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1088 elapsed=13.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1088 elapsed=13.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1088 elapsed=13.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1088 elapsed=13.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1088 elapsed=13.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1088 elapsed=13.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1088 elapsed=13.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1088 elapsed=13.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] fold=4 mean Spearman=0.30130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC-MULTI] OOF mean Spearman (MC): 0.29173\n[SUB] submission_multi_mc.csv written: (608, 31)\n[SUB] submission_multi_mc_override.csv written (main target overridden)\n[MC-MULTI] Done.\n"
          ]
        }
      ]
    },
    {
      "id": "c28a0c49-6371-4ad1-94f2-ea1d71f32372",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blend multi-output DeBERTa (test_deberta_multi.npy) with TFIDF+SVD+Ridge predictions per target; override main target with strong single-target ensemble\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import Ridge\n",
        "from scipy import sparse\n",
        "\n",
        "targets = [\n",
        "    'question_asker_intent_understanding','question_body_critical','question_conversational','question_expect_short_answer',\n",
        "    'question_fact_seeking','question_has_commonly_accepted_answer','question_interestingness_others','question_interestingness_self',\n",
        "    'question_multi_intent','question_not_really_a_question','question_opinion_seeking','question_type_choice','question_type_compare',\n",
        "    'question_type_consequence','question_type_definition','question_type_entity','question_type_instructions','question_type_procedure',\n",
        "    'question_type_reason_explanation','question_type_spelling','question_well_written','answer_helpful','answer_level_of_information',\n",
        "    'answer_plausible','answer_relevance','answer_satisfaction','answer_type_instructions','answer_type_procedure',\n",
        "    'answer_type_reason_explanation','answer_well_written'\n",
        "]\n",
        "main_target = 'question_asker_intent_understanding'\n",
        "\n",
        "# Load transformer test predictions (N_test,30)\n",
        "tfm_test = np.load('test_deberta_multi.npy') if Path('test_deberta_multi.npy').exists() else None\n",
        "if tfm_test is None:\n",
        "    # fallback to MC if only that exists (even if slightly worse OOF, test may still help via blend)\n",
        "    tfm_test = np.load('test_deberta_multi_mc8.npy')\n",
        "\n",
        "# Build TFIDF+SVD features once and train multi-output Ridge on full train to get test preds\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "def combine(df):\n",
        "    t = df.get('question_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "    b = df.get('question_body', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "    a = df.get('answer', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "    return (t + ' [SEP] ' + b + ' [SEP] ' + a).values\n",
        "txt_tr = combine(train); txt_te = combine(test)\n",
        "cfg_word = dict(analyzer='word', ngram_range=(1,2), sublinear_tf=True, strip_accents='unicode', lowercase=True, min_df=2)\n",
        "cfg_char = dict(analyzer='char_wb', ngram_range=(3,6), sublinear_tf=True, min_df=2)\n",
        "vec_w = TfidfVectorizer(max_features=200_000, **cfg_word)\n",
        "vec_c = TfidfVectorizer(max_features=200_000, **cfg_char)\n",
        "Xw_tr = vec_w.fit_transform(txt_tr); Xw_te = vec_w.transform(txt_te)\n",
        "Xc_tr = vec_c.fit_transform(txt_tr); Xc_te = vec_c.transform(txt_te)\n",
        "X_tr = sparse.hstack([Xw_tr, Xc_tr], format='csr')\n",
        "X_te = sparse.hstack([Xw_te, Xc_te], format='csr')\n",
        "svd = TruncatedSVD(n_components=256, random_state=42)\n",
        "Z_tr = svd.fit_transform(X_tr); Z_te = svd.transform(X_te)\n",
        "Y = train[targets].astype(float).values\n",
        "ridge = Ridge(alpha=10.0, random_state=42)\n",
        "ridge.fit(Z_tr, Y)\n",
        "ridge_test = np.clip(ridge.predict(Z_te).astype(float), 0.0, 1.0)\n",
        "\n",
        "# Simple global blend per target: pred = (1-w)*tfm + w*ridge with small w for stability\n",
        "w = 0.20\n",
        "blend = np.clip((1.0 - w) * tfm_test + w * ridge_test, 0.0, 1.0).astype(float)\n",
        "\n",
        "# Override main target with our best single-target ensemble if available\n",
        "if Path('test_ensemble.npy').exists():\n",
        "    main_pred = np.clip(np.load('test_ensemble.npy').astype(float), 0.0, 1.0)\n",
        "    j = targets.index(main_target)\n",
        "    blend[:, j] = main_pred\n",
        "\n",
        "# Write final submission.csv (31 columns) from sample template\n",
        "samp = pd.read_csv('sample_submission.csv')\n",
        "samp['qa_id'] = pd.to_numeric(test['qa_id'], errors='raise').astype('int64')\n",
        "for i, col in enumerate(targets):\n",
        "    samp[col] = blend[:, i].astype(float)\n",
        "samp.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('[BLEND SUB] submission.csv written:', samp.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BLEND SUB] submission.csv written: (608, 31)\n"
          ]
        }
      ]
    },
    {
      "id": "43f050f8-048e-419a-8c96-acb7fdca50ff",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DeBERTa-v3-base with mean-pooled head (30-dim regressor) - Question-only view (title + body); 5-fold, 4 epochs\n",
        "import os, gc, time, json, numpy as np, pandas as pd, torch, torch.nn as nn\n",
        "from pathlib import Path\n",
        "from scipy.stats import spearmanr\n",
        "from transformers import (AutoTokenizer, AutoConfig, AutoModel,\n",
        "                          TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorWithPadding)\n",
        "\n",
        "assert 'targets' in globals() and 'train' in globals() and 'test' in globals() and 'splits' in globals(), 'Run Cell 0 first.'\n",
        "model_name_q = 'microsoft/deberta-v3-base'\n",
        "if 'tokenizer' not in globals():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_q)\n",
        "\n",
        "MAX_LEN_Q = 512\n",
        "TITLE_MAX_Q = 64\n",
        "\n",
        "def pack_question_only(title: str, body: str, tokenizer, max_len=MAX_LEN_Q, title_max=TITLE_MAX_Q):\n",
        "    ti = tokenizer(title if isinstance(title, str) else '', add_special_tokens=False, truncation=True, max_length=title_max)['input_ids']\n",
        "    bi_full = tokenizer(body if isinstance(body, str) else '', add_special_tokens=False, truncation=False)['input_ids']\n",
        "    # CLS + 2*SEP\n",
        "    rem = max_len - (1 + 1 + 1) - len(ti)\n",
        "    rem = max(rem, 0)\n",
        "    bi = bi_full[:rem]\n",
        "    ids = [tokenizer.cls_token_id] + ti + [tokenizer.sep_token_id] + bi + [tokenizer.sep_token_id]\n",
        "    ids = ids[:max_len]\n",
        "    attn = [1]*len(ids)\n",
        "    pad = max_len - len(ids)\n",
        "    if pad>0:\n",
        "        ids += [tokenizer.pad_token_id]*pad\n",
        "        attn += [0]*pad\n",
        "    return ids, attn\n",
        "\n",
        "def build_inputs_q_only(df: pd.DataFrame):\n",
        "    T = df['question_title'].fillna('').astype(str).tolist()\n",
        "    B = df['question_body'].fillna('').astype(str).tolist()\n",
        "    input_ids, attention_masks = [], []\n",
        "    for t,b in zip(T,B):\n",
        "        ids, attn = pack_question_only(t,b, tokenizer)\n",
        "        input_ids.append(ids); attention_masks.append(attn)\n",
        "    return {'input_ids': np.array(input_ids, dtype=np.int64), 'attention_mask': np.array(attention_masks, dtype=np.int64)}\n",
        "\n",
        "print('[TOK-Q] Building inputs (Question-only) ...', flush=True)\n",
        "t0_tok = time.time()\n",
        "tr_q_inputs = build_inputs_q_only(train)\n",
        "te_q_inputs = build_inputs_q_only(test)\n",
        "print(f'[TOK-Q] Done in {time.time()-t0_tok:.1f}s; shapes tr={tr_q_inputs[\"input_ids\"].shape} te={te_q_inputs[\"input_ids\"].shape}', flush=True)\n",
        "\n",
        "class QDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ids, masks, labels=None):\n",
        "        self.ids = ids; self.masks = masks; self.labels = labels\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {'input_ids': torch.tensor(self.ids[idx]), 'attention_mask': torch.tensor(self.masks[idx])}\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "def masked_mean_pool(last_hidden_state, attention_mask):\n",
        "    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)  # (B,L,1)\n",
        "    summ = (last_hidden_state * mask).sum(dim=1)\n",
        "    denom = mask.sum(dim=1).clamp(min=1e-6)\n",
        "    return summ / denom\n",
        "\n",
        "class QuestMultiRegressor(nn.Module):\n",
        "    def __init__(self, model_name: str, out_dim: int = 30, dropout: float = 0.2, msd: int = 0):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        hidden = self.backbone.config.hidden_size\n",
        "        self.head = nn.Linear(hidden, out_dim)\n",
        "        self.msd = msd\n",
        "        # enable gradient checkpointing if available for backbone\n",
        "        try: self.backbone.gradient_checkpointing_enable()\n",
        "        except Exception: pass\n",
        "    # HF Trainer expects these on the model; proxy to backbone\n",
        "    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n",
        "        try:\n",
        "            if gradient_checkpointing_kwargs is None:\n",
        "                self.backbone.gradient_checkpointing_enable()\n",
        "            else:\n",
        "                self.backbone.gradient_checkpointing_enable(**gradient_checkpointing_kwargs)\n",
        "        except Exception:\n",
        "            pass\n",
        "    def gradient_checkpointing_disable(self):\n",
        "        try: self.backbone.gradient_checkpointing_disable()\n",
        "        except Exception: pass\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hs = out.last_hidden_state  # (B,L,H)\n",
        "        pooled = masked_mean_pool(hs, attention_mask)  # (B,H)\n",
        "        if self.msd and self.training:\n",
        "            logits_acc = 0.0\n",
        "            for _ in range(self.msd):\n",
        "                logits_acc = logits_acc + self.head(self.dropout(pooled))\n",
        "            logits = logits_acc / float(self.msd)\n",
        "        else:\n",
        "            logits = self.head(self.dropout(pooled))\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = nn.functional.mse_loss(logits, labels)\n",
        "        return {'loss': loss, 'logits': logits}\n",
        "\n",
        "def compute_metrics_30(eval_pred):\n",
        "    # Supports both tuple and EvalPrediction\n",
        "    preds = getattr(eval_pred, 'predictions', None)\n",
        "    labels = getattr(eval_pred, 'label_ids', None)\n",
        "    if preds is None:\n",
        "        preds, labels = eval_pred\n",
        "    vals = []\n",
        "    for j in range(labels.shape[1]):\n",
        "        p = preds[:, j]; y = labels[:, j]\n",
        "        if np.std(p)==0 or np.std(y)==0:\n",
        "            vals.append(0.0)\n",
        "        else:\n",
        "            vals.append(float(spearmanr(y, p).correlation))\n",
        "    return {'mean_spearman': float(np.mean(vals))}\n",
        "\n",
        "@torch.inference_mode()\n",
        "def infer_logits(model, dataset, batch_size=32):\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    outs = []\n",
        "    use_cuda = device.type == 'cuda'\n",
        "    for batch in loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        if use_cuda:\n",
        "            with torch.autocast('cuda', dtype=torch.float16):\n",
        "                out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        else:\n",
        "            out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = out['logits'] if isinstance(out, dict) else out.logits\n",
        "        outs.append(logits.float().cpu().numpy())\n",
        "    return np.vstack(outs)\n",
        "\n",
        "# Training loop - 5 folds, 4 epochs, MSE loss; saves OOF/test npy\n",
        "Y30 = train[targets].astype(float).values\n",
        "oof_q = np.zeros((len(train), 30), dtype=np.float32)\n",
        "test_fold_preds_q = []\n",
        "folds_idx_q = np.full(len(train), -1, dtype=int)\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(splits):\n",
        "    t0 = time.time()\n",
        "    folds_idx_q[val_idx] = fold\n",
        "    print(f'\\n[Q-ONLY FOLD {fold}] train={len(trn_idx)} val={len(val_idx)}', flush=True)\n",
        "    tr_ds = QDataset(tr_q_inputs['input_ids'][trn_idx], tr_q_inputs['attention_mask'][trn_idx], Y30[trn_idx])\n",
        "    va_ds = QDataset(tr_q_inputs['input_ids'][val_idx], tr_q_inputs['attention_mask'][val_idx], Y30[val_idx])\n",
        "    te_ds = QDataset(te_q_inputs['input_ids'], te_q_inputs['attention_mask'], None)\n",
        "\n",
        "    model = QuestMultiRegressor(model_name_q, out_dim=30, dropout=0.2, msd=0)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'deberta_q_only_fold{fold}',\n",
        "        num_train_epochs=4,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=32,\n",
        "        gradient_accumulation_steps=2,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type='linear',\n",
        "        fp16=True,\n",
        "        gradient_checkpointing=True,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='mean_spearman',\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=1,\n",
        "        logging_steps=50,\n",
        "        seed=42,\n",
        "        report_to=[]\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tr_ds,\n",
        "        eval_dataset=va_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics_30,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
        "        data_collator=DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    val_pred = trainer.predict(va_ds).predictions.astype(np.float32)\n",
        "    oof_q[val_idx] = val_pred\n",
        "    # Fold metric\n",
        "    vals = []\n",
        "    for j in range(val_pred.shape[1]):\n",
        "        p = val_pred[:, j]; y = Y30[val_idx, j]\n",
        "        vals.append(0.0 if np.std(p)==0 or np.std(y)==0 else float(spearmanr(y, p).correlation))\n",
        "    print(f'[Q-ONLY FOLD {fold}] mean Spearman={np.mean(vals):.5f} time={time.time()-t0:.1f}s', flush=True)\n",
        "    # Test preds via manual inference to avoid Trainer.predict issues\n",
        "    te_pred = infer_logits(trainer.model, te_ds, batch_size=32).astype(np.float32)\n",
        "    test_fold_preds_q.append(te_pred)\n",
        "    del trainer, model; gc.collect()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "# Aggregate and save artifacts\n",
        "oof_mean_q = []\n",
        "for j in range(oof_q.shape[1]):\n",
        "    yj = train[targets[j]].values.astype(float)\n",
        "    pj = oof_q[:, j]\n",
        "    sc = 0.0 if np.std(pj)==0 or np.std(yj)==0 else float(spearmanr(yj, pj).correlation)\n",
        "    oof_mean_q.append(sc)\n",
        "print('[Q-ONLY] OOF per-target Spearman (first 5):', np.round(oof_mean_q[:5], 5))\n",
        "print('[Q-ONLY] OOF mean Spearman:', float(np.mean(oof_mean_q)))\n",
        "np.save('oof_deberta_q.npy', oof_q)\n",
        "pd.DataFrame({'qa_id': train['qa_id'], 'fold': folds_idx_q}).assign(**{f't{j}': oof_q[:, j] for j in range(oof_q.shape[1])}).to_csv('oof_deberta_q.csv', index=False)\n",
        "\n",
        "test_q = np.mean(np.stack(test_fold_preds_q, axis=0), axis=0).astype(np.float32)  # (608,30)\n",
        "np.save('test_deberta_q.npy', test_q)\n",
        "print('[Q-ONLY] Saved: oof_deberta_q.npy, test_deberta_q.npy')\n",
        "\n",
        "# Note: blending across views will be done in a later cell. Ensure folds align across runs.\n",
        "print('[Q-ONLY] Done.')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TOK-Q] Building inputs (Question-only) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TOK-Q] Done in 2.6s; shapes tr=(5471, 512) te=(608, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[Q-ONLY FOLD 0] train=4395 val=1076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1100 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Q-ONLY FOLD 0] mean Spearman=0.31187 time=834.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[Q-ONLY FOLD 1] train=4318 val=1153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1080 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/37 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Q-ONLY FOLD 1] mean Spearman=0.31011 time=825.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[Q-ONLY FOLD 2] train=4389 val=1082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='1096' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1096 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Q-ONLY FOLD 2] mean Spearman=0.30532 time=833.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[Q-ONLY FOLD 3] train=4399 val=1072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1100 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Q-ONLY FOLD 3] mean Spearman=0.31799 time=832.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[Q-ONLY FOLD 4] train=4383 val=1088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='1096' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1096 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Q-ONLY FOLD 4] mean Spearman=0.31992 time=835.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Q-ONLY] OOF per-target Spearman (first 5): [0.27453 0.53029 0.38864 0.25553 0.32325]\n[Q-ONLY] OOF mean Spearman: 0.3120294183857368\n[Q-ONLY] Saved: oof_deberta_q.npy, test_deberta_q.npy\n[Q-ONLY] Done.\n"
          ]
        }
      ]
    },
    {
      "id": "6c989580-9cad-420d-a01b-5c2cec92704c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DeBERTa-v3-base with mean-pooled head (30-dim regressor) - Answer-only view (answer text); 5-fold, 4 epochs\n",
        "import os, gc, time, numpy as np, pandas as pd, torch, torch.nn as nn\n",
        "from pathlib import Path\n",
        "from scipy.stats import spearmanr\n",
        "from transformers import (AutoTokenizer, AutoModel, TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorWithPadding)\n",
        "\n",
        "assert 'targets' in globals() and 'train' in globals() and 'test' in globals() and 'splits' in globals(), 'Run Cell 0 first.'\n",
        "model_name_a = 'microsoft/deberta-v3-base'\n",
        "if 'tokenizer' not in globals():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_a)\n",
        "\n",
        "MAX_LEN_A = 512\n",
        "\n",
        "def pack_answer_only(answer: str, tokenizer, max_len=MAX_LEN_A):\n",
        "    ai = tokenizer(answer if isinstance(answer, str) else '', add_special_tokens=False, truncation=True, max_length=max_len-2)['input_ids']\n",
        "    ids = [tokenizer.cls_token_id] + ai + [tokenizer.sep_token_id]\n",
        "    ids = ids[:max_len]\n",
        "    attn = [1]*len(ids)\n",
        "    pad = max_len - len(ids)\n",
        "    if pad>0:\n",
        "        ids += [tokenizer.pad_token_id]*pad\n",
        "        attn += [0]*pad\n",
        "    return ids, attn\n",
        "\n",
        "def build_inputs_a_only(df: pd.DataFrame):\n",
        "    A = df['answer'].fillna('').astype(str).tolist() if 'answer' in df.columns else ['']*len(df)\n",
        "    input_ids, attention_masks = [], []\n",
        "    for a in A:\n",
        "        ids, attn = pack_answer_only(a, tokenizer)\n",
        "        input_ids.append(ids); attention_masks.append(attn)\n",
        "    return {'input_ids': np.array(input_ids, dtype=np.int64), 'attention_mask': np.array(attention_masks, dtype=np.int64)}\n",
        "\n",
        "print('[TOK-A] Building inputs (Answer-only) ...', flush=True)\n",
        "t0_tok = time.time()\n",
        "tr_a_inputs = build_inputs_a_only(train)\n",
        "te_a_inputs = build_inputs_a_only(test)\n",
        "print(f'[TOK-A] Done in {time.time()-t0_tok:.1f}s; shapes tr={tr_a_inputs[\"input_ids\"].shape} te={te_a_inputs[\"input_ids\"].shape}', flush=True)\n",
        "\n",
        "class ADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ids, masks, labels=None):\n",
        "        self.ids = ids; self.masks = masks; self.labels = labels\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {'input_ids': torch.tensor(self.ids[idx]), 'attention_mask': torch.tensor(self.masks[idx])}\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "def masked_mean_pool(last_hidden_state, attention_mask):\n",
        "    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
        "    summ = (last_hidden_state * mask).sum(dim=1)\n",
        "    denom = mask.sum(dim=1).clamp(min=1e-6)\n",
        "    return summ / denom\n",
        "\n",
        "class QuestMultiRegressorA(nn.Module):\n",
        "    def __init__(self, model_name: str, out_dim: int = 30, dropout: float = 0.2, msd: int = 0):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        hidden = self.backbone.config.hidden_size\n",
        "        self.head = nn.Linear(hidden, out_dim)\n",
        "        self.msd = msd\n",
        "        try: self.backbone.gradient_checkpointing_enable()\n",
        "        except Exception: pass\n",
        "    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n",
        "        try:\n",
        "            if gradient_checkpointing_kwargs is None:\n",
        "                self.backbone.gradient_checkpointing_enable()\n",
        "            else:\n",
        "                self.backbone.gradient_checkpointing_enable(**gradient_checkpointing_kwargs)\n",
        "        except Exception:\n",
        "            pass\n",
        "    def gradient_checkpointing_disable(self):\n",
        "        try: self.backbone.gradient_checkpointing_disable()\n",
        "        except Exception: pass\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = masked_mean_pool(out.last_hidden_state, attention_mask)\n",
        "        if self.msd and self.training:\n",
        "            logits_acc = 0.0\n",
        "            for _ in range(self.msd):\n",
        "                logits_acc = logits_acc + self.head(self.dropout(pooled))\n",
        "            logits = logits_acc / float(self.msd)\n",
        "        else:\n",
        "            logits = self.head(self.dropout(pooled))\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = nn.functional.mse_loss(logits, labels)\n",
        "        return {'loss': loss, 'logits': logits}\n",
        "\n",
        "def compute_metrics_30(eval_pred):\n",
        "    preds = getattr(eval_pred, 'predictions', None)\n",
        "    labels = getattr(eval_pred, 'label_ids', None)\n",
        "    if preds is None:\n",
        "        preds, labels = eval_pred\n",
        "    vals = []\n",
        "    for j in range(labels.shape[1]):\n",
        "        p = preds[:, j]; y = labels[:, j]\n",
        "        if np.std(p)==0 or np.std(y)==0:\n",
        "            vals.append(0.0)\n",
        "        else:\n",
        "            vals.append(float(spearmanr(y, p).correlation))\n",
        "    return {'mean_spearman': float(np.mean(vals))}\n",
        "\n",
        "@torch.inference_mode()\n",
        "def infer_logits(model, dataset, batch_size=32):\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    outs = []\n",
        "    use_cuda = device.type == 'cuda'\n",
        "    for batch in loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        if use_cuda:\n",
        "            with torch.autocast('cuda', dtype=torch.float16):\n",
        "                out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        else:\n",
        "            out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = out['logits'] if isinstance(out, dict) else out.logits\n",
        "        outs.append(logits.float().cpu().numpy())\n",
        "    return np.vstack(outs)\n",
        "\n",
        "Y30 = train[targets].astype(float).values\n",
        "oof_a = np.zeros((len(train), 30), dtype=np.float32)\n",
        "test_fold_preds_a = []\n",
        "folds_idx_a = np.full(len(train), -1, dtype=int)\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(splits):\n",
        "    t0 = time.time()\n",
        "    folds_idx_a[val_idx] = fold\n",
        "    print(f'\\n[A-ONLY FOLD {fold}] train={len(trn_idx)} val={len(val_idx)}', flush=True)\n",
        "    tr_ds = ADataset(tr_a_inputs['input_ids'][trn_idx], tr_a_inputs['attention_mask'][trn_idx], Y30[trn_idx])\n",
        "    va_ds = ADataset(tr_a_inputs['input_ids'][val_idx], tr_a_inputs['attention_mask'][val_idx], Y30[val_idx])\n",
        "    te_ds = ADataset(te_a_inputs['input_ids'], te_a_inputs['attention_mask'], None)\n",
        "\n",
        "    model = QuestMultiRegressorA(model_name_a, out_dim=30, dropout=0.2, msd=0)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'deberta_a_only_fold{fold}',\n",
        "        num_train_epochs=4,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=32,\n",
        "        gradient_accumulation_steps=2,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type='linear',\n",
        "        fp16=True,\n",
        "        gradient_checkpointing=True,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='mean_spearman',\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=1,\n",
        "        logging_steps=50,\n",
        "        seed=42,\n",
        "        report_to=[]\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tr_ds,\n",
        "        eval_dataset=va_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics_30,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
        "        data_collator=DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    val_pred = trainer.predict(va_ds).predictions.astype(np.float32)\n",
        "    oof_a[val_idx] = val_pred\n",
        "    vals = []\n",
        "    for j in range(val_pred.shape[1]):\n",
        "        p = val_pred[:, j]; y = Y30[val_idx, j]\n",
        "        vals.append(0.0 if np.std(p)==0 or np.std(y)==0 else float(spearmanr(y, p).correlation))\n",
        "    print(f'[A-ONLY FOLD {fold}] mean Spearman={np.mean(vals):.5f} time={time.time()-t0:.1f}s', flush=True)\n",
        "    # Test preds via manual inference to avoid Trainer.predict issues\n",
        "    te_pred = infer_logits(trainer.model, te_ds, batch_size=32).astype(np.float32)\n",
        "    test_fold_preds_a.append(te_pred)\n",
        "    del trainer, model; gc.collect()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "oof_mean_a = []\n",
        "for j in range(oof_a.shape[1]):\n",
        "    yj = train[targets[j]].values.astype(float)\n",
        "    pj = oof_a[:, j]\n",
        "    sc = 0.0 if np.std(pj)==0 or np.std(yj)==0 else float(spearmanr(yj, pj).correlation)\n",
        "    oof_mean_a.append(sc)\n",
        "print('[A-ONLY] OOF per-target Spearman (first 5):', np.round(oof_mean_a[:5], 5))\n",
        "print('[A-ONLY] OOF mean Spearman:', float(np.mean(oof_mean_a)))\n",
        "np.save('oof_deberta_a.npy', oof_a)\n",
        "pd.DataFrame({'qa_id': train['qa_id'], 'fold': folds_idx_a}).assign(**{f't{j}': oof_a[:, j] for j in range(oof_a.shape[1])}).to_csv('oof_deberta_a.csv', index=False)\n",
        "\n",
        "test_a = np.mean(np.stack(test_fold_preds_a, axis=0), axis=0).astype(np.float32)  # (608,30)\n",
        "np.save('test_deberta_a.npy', test_a)\n",
        "print('[A-ONLY] Saved: oof_deberta_a.npy, test_deberta_a.npy')\n",
        "print('[A-ONLY] Done.')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TOK-A] Building inputs (Answer-only) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TOK-A] Done in 2.4s; shapes tr=(5471, 512) te=(608, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[A-ONLY FOLD 0] train=4395 val=1076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1100 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A-ONLY FOLD 0] mean Spearman=0.26294 time=836.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[A-ONLY FOLD 1] train=4318 val=1153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1080 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/37 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A-ONLY FOLD 1] mean Spearman=0.24829 time=822.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[A-ONLY FOLD 2] train=4389 val=1082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='1096' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1096 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A-ONLY FOLD 2] mean Spearman=0.24264 time=839.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[A-ONLY FOLD 3] train=4399 val=1072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1100 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A-ONLY FOLD 3] mean Spearman=0.26156 time=837.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[A-ONLY FOLD 4] train=4383 val=1088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='1096' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1096 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A-ONLY FOLD 4] mean Spearman=0.27642 time=833.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[A-ONLY] OOF per-target Spearman (first 5): [0.15729 0.30378 0.3401  0.13804 0.25532]\n[A-ONLY] OOF mean Spearman: 0.25770002422279903\n[A-ONLY] Saved: oof_deberta_a.npy, test_deberta_a.npy\n[A-ONLY] Done.\n"
          ]
        }
      ]
    },
    {
      "id": "450c04f8-b797-440c-ba16-124b82ea962a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Rank-standardized per-target blend: Q-only, A-only, Q+A, Ridge; override main target; write submission.csv\n",
        "import os, time, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import Ridge\n",
        "from scipy import sparse\n",
        "\n",
        "id_col = 'qa_id'\n",
        "targets = [\n",
        "    'question_asker_intent_understanding','question_body_critical','question_conversational','question_expect_short_answer',\n",
        "    'question_fact_seeking','question_has_commonly_accepted_answer','question_interestingness_others','question_interestingness_self',\n",
        "    'question_multi_intent','question_not_really_a_question','question_opinion_seeking','question_type_choice','question_type_compare',\n",
        "    'question_type_consequence','question_type_definition','question_type_entity','question_type_instructions','question_type_procedure',\n",
        "    'question_type_reason_explanation','question_type_spelling','question_well_written','answer_helpful','answer_level_of_information',\n",
        "    'answer_plausible','answer_relevance','answer_satisfaction','answer_type_instructions','answer_type_procedure',\n",
        "    'answer_type_reason_explanation','answer_well_written'\n",
        "]\n",
        "main_target = 'question_asker_intent_understanding'\n",
        "\n",
        "def frac_rank_col(col):\n",
        "    # fractional rank in [0,1]\n",
        "    n = len(col)\n",
        "    order = np.argsort(col, kind='mergesort')\n",
        "    ranks = np.empty(n, dtype=np.float64); ranks[order] = np.arange(n, dtype=np.float64)\n",
        "    return ranks / max(n-1, 1)\n",
        "\n",
        "def rank_standardize(mat):\n",
        "    # mat: (N,30) -> per-column fractional ranks in [0,1]\n",
        "    out = np.zeros_like(mat, dtype=np.float64)\n",
        "    for j in range(mat.shape[1]):\n",
        "        out[:, j] = frac_rank_col(mat[:, j])\n",
        "    return out\n",
        "\n",
        "print('[BLEND] Loading model preds ...', flush=True)\n",
        "need = []\n",
        "paths = {\n",
        "    'qa_oof': 'oof_deberta_multi.npy',\n",
        "    'qa_test': 'test_deberta_multi.npy',\n",
        "    'q_oof': 'oof_deberta_q.npy',\n",
        "    'q_test': 'test_deberta_q.npy',\n",
        "    'a_oof': 'oof_deberta_a.npy',\n",
        "    'a_test': 'test_deberta_a.npy',\n",
        "}\n",
        "loaded = {}\n",
        "for k,p in paths.items():\n",
        "    if Path(p).exists():\n",
        "        loaded[k] = np.load(p)\n",
        "    else:\n",
        "        need.append(k)\n",
        "print('[BLEND] Missing keys:', need)\n",
        "\n",
        "# Ensure base requirements: at least Q+A + one of Q-only/A-only\n",
        "assert ('qa_test' in loaded) or Path('test_deberta_multi_mc8.npy').exists(), 'No Q+A test predictions found'\n",
        "if 'qa_test' not in loaded:\n",
        "    loaded['qa_test'] = np.load('test_deberta_multi_mc8.npy')\n",
        "\n",
        "# Build Ridge test (and OOF optionally) if not cached\n",
        "ridge_test_path = Path('test_ridge_svd256.npy')\n",
        "ridge_oof_path = Path('oof_ridge_svd256.npy')\n",
        "if not ridge_test_path.exists() or not ridge_oof_path.exists():\n",
        "    print('[BLEND][RIDGE] Building TFIDF+SVD features and computing OOF/test ...', flush=True)\n",
        "    t0 = time.time()\n",
        "    train_df = pd.read_csv('train.csv'); test_df = pd.read_csv('test.csv')\n",
        "    def combine(df):\n",
        "        t = df.get('question_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "        b = df.get('question_body', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "        a = df.get('answer', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "        return (t + ' [SEP] ' + b + ' [SEP] ' + a).values\n",
        "    txt_tr = combine(train_df); txt_te = combine(test_df)\n",
        "    cfg_word = dict(analyzer='word', ngram_range=(1,2), sublinear_tf=True, strip_accents='unicode', lowercase=True, min_df=2)\n",
        "    cfg_char = dict(analyzer='char_wb', ngram_range=(3,6), sublinear_tf=True, min_df=2)\n",
        "    vec_w = TfidfVectorizer(max_features=200_000, **cfg_word)\n",
        "    vec_c = TfidfVectorizer(max_features=200_000, **cfg_char)\n",
        "    Xw_tr = vec_w.fit_transform(txt_tr); Xw_te = vec_w.transform(txt_te)\n",
        "    Xc_tr = vec_c.fit_transform(txt_tr); Xc_te = vec_c.transform(txt_te)\n",
        "    X_tr = sparse.hstack([Xw_tr, Xc_tr], format='csr'); X_te = sparse.hstack([Xw_te, Xc_te], format='csr')\n",
        "    svd = TruncatedSVD(n_components=256, random_state=42)\n",
        "    Z_tr = svd.fit_transform(X_tr); Z_te = svd.transform(X_te)\n",
        "    Y = pd.read_csv('train.csv')[targets].astype(float).values\n",
        "    # 5-fold OOF for Ridge to align with CV (use the same splits from globals if available)\n",
        "    if 'splits' in globals():\n",
        "        oof_r = np.zeros_like(Y, dtype=np.float64)\n",
        "        for f,(trn_idx, val_idx) in enumerate(splits):\n",
        "            rg = Ridge(alpha=10.0, random_state=42)\n",
        "            rg.fit(Z_tr[trn_idx], Y[trn_idx])\n",
        "            oof_r[val_idx] = rg.predict(Z_tr[val_idx])\n",
        "        ridge_oof = np.clip(oof_r, 0.0, 1.0).astype(np.float32)\n",
        "    else:\n",
        "        ridge_oof = None\n",
        "    rg_full = Ridge(alpha=10.0, random_state=42).fit(Z_tr, Y)\n",
        "    ridge_test = np.clip(rg_full.predict(Z_te), 0.0, 1.0).astype(np.float32)\n",
        "    np.save(ridge_test_path, ridge_test)\n",
        "    if ridge_oof is not None: np.save(ridge_oof_path, ridge_oof)\n",
        "    print(f'[BLEND][RIDGE] Done in {time.time()-t0:.1f}s; ridge_test shape={ridge_test.shape}', flush=True)\n",
        "else:\n",
        "    ridge_test = np.load(ridge_test_path)\n",
        "    ridge_oof = np.load(ridge_oof_path) if ridge_oof_path.exists() else None\n",
        "\n",
        "# Prepare test matrices available\n",
        "tests = []\n",
        "names = []\n",
        "if 'q_test' in loaded:\n",
        "    tests.append(loaded['q_test'].astype(np.float64)); names.append('Q')\n",
        "if 'a_test' in loaded:\n",
        "    tests.append(loaded['a_test'].astype(np.float64)); names.append('A')\n",
        "tests.append(loaded['qa_test'].astype(np.float64)); names.append('QA')\n",
        "tests.append(ridge_test.astype(np.float64)); names.append('Ridge')\n",
        "print('[BLEND] Models included:', names)\n",
        "\n",
        "# Rank-standardize each model's test predictions per column\n",
        "tests_ranked = [rank_standardize(x) for x in tests]  # each (N_test,30) in [0,1]\n",
        "N_test = tests_ranked[0].shape[0]\n",
        "blend_rank = np.zeros((N_test, 30), dtype=np.float64)\n",
        "\n",
        "q_cols = [i for i,c in enumerate(targets) if c.startswith('question_')]\n",
        "a_cols = [i for i,c in enumerate(targets) if c.startswith('answer_')]\n",
        "\n",
        "idx_name = {i:n for i,n in enumerate(names)}\n",
        "def get_model_by(name):\n",
        "    return tests_ranked[names.index(name)] if name in names else None\n",
        "\n",
        "# Default weights\n",
        "for j in range(30):\n",
        "    if j in q_cols:\n",
        "        w = {}\n",
        "        if 'Q' in names: w['Q'] = 0.5\n",
        "        w['QA'] = 0.4 if 'Q' in names else 0.7\n",
        "        w['Ridge'] = 0.1\n",
        "        s = sum(w.values());\n",
        "        for k in w: w[k] /= s\n",
        "        tmp = np.zeros(N_test, dtype=np.float64)\n",
        "        for k,wt in w.items():\n",
        "            tmp += wt * get_model_by(k)[:, j]\n",
        "        blend_rank[:, j] = tmp\n",
        "    else:\n",
        "        # answer_*\n",
        "        w = {}\n",
        "        if 'A' in names: w['A'] = 0.5\n",
        "        w['QA'] = 0.4 if 'A' in names else 0.7\n",
        "        w['Ridge'] = 0.1\n",
        "        s = sum(w.values());\n",
        "        for k in w: w[k] /= s\n",
        "        tmp = np.zeros(N_test, dtype=np.float64)\n",
        "        for k,wt in w.items():\n",
        "            tmp += wt * get_model_by(k)[:, j]\n",
        "        blend_rank[:, j] = tmp\n",
        "\n",
        "# Convert blended ranks to [0,1] via identity (already [0,1]) and clip\n",
        "blend = np.clip(blend_rank, 0.0, 1.0).astype(np.float32)\n",
        "\n",
        "# Override main target with best single-target ensemble if present (raw scores, clip at end)\n",
        "if Path('test_ensemble.npy').exists():\n",
        "    main_pred = np.clip(np.load('test_ensemble.npy').astype(float), 0.0, 1.0)\n",
        "    j = targets.index(main_target)\n",
        "    blend[:, j] = main_pred\n",
        "\n",
        "# Write submission\n",
        "samp = pd.read_csv('sample_submission.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "samp['qa_id'] = pd.to_numeric(test_df['qa_id'], errors='raise').astype('int64')\n",
        "for i, col in enumerate(targets):\n",
        "    samp[col] = blend[:, i].astype(float)\n",
        "samp.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('[BLEND] submission.csv written:', samp.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BLEND] Loading model preds ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BLEND] Missing keys: []\n[BLEND][RIDGE] Building TFIDF+SVD features and computing OOF/test ...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}