{
  "cells": [
    {
      "id": "9e74f676-4811-4310-b84f-bedf348a874b",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plan: Google QUEST Q&A Labeling \u2013 Single-Target (question_asker_intent_understanding)\n",
        "\n",
        "Objectives:\n",
        "- Build a fast, strong baseline and iterate to medal-level column-wise Spearman.\n",
        "- Lock robust CV, avoid leakage, cache artifacts, and ensemble diverse models.\n",
        "\n",
        "Data understanding:\n",
        "- Train/test CSVs with text fields (question_title, question_body, answer) plus metadata (e.g., category).\n",
        "- Target: continuous [0,1] label question_asker_intent_understanding.\n",
        "\n",
        "Validation:\n",
        "- Use StratifiedKFold on target binned into ~10 bins to stabilize fold distributions.\n",
        "- 5 folds, fixed seed; report OOF Spearman and CI via multiple seeds later if time allows.\n",
        "- Fit all text preprocessors inside each fold; precompute/cached TF-IDF matrices once and slice per fold.\n",
        "\n",
        "Metric:\n",
        "- column-wise Spearman; here single target, so Spearman correlation between OOF preds and target.\n",
        "\n",
        "Baseline v1 (fast):\n",
        "- Text only: concatenate question_title + question_body + answer.\n",
        "- TF-IDF (char n-grams 3\u20136 + word n-grams 1\u20132), max_features ~200k (tune), lowercase, strip accents.\n",
        "- Ridge regression (or SGDRegressor with elasticnet) on TF-IDF. OOF Spearman as baseline.\n",
        "- Log fold times and memory; cache X_tfidf.npz.\n",
        "\n",
        "Model v2:\n",
        "- Separate fields: build per-field TF-IDF and concatenate; optionally weights per field.\n",
        "- Add simple features: lengths (#chars, #words), punctuation counts, exclamation/question marks, capitalization ratio.\n",
        "- Refit Ridge/Lasso/ElasticNet; compare OOF.\n",
        "\n",
        "Model v3 (tree/boost):\n",
        "- Use SVR (linear/RBF) and/or CatBoost/XGBoost on TF-IDF svd-reduced features (TruncatedSVD 256\u2013512).\n",
        "- Alternatively, use boosted trees on engineered scalar features + dense SVD text embeddings.\n",
        "\n",
        "Model v4 (transformers \u2013 GPU):\n",
        "- Fine-tune a small encoder (deberta-v3-base or electra-base) with regression head on concatenated fields with special tokens.\n",
        "- Max length ~384\u2013512; batch size tuned for 24GB A10; 3\u20135 epochs with early stopping; layer-wise LR decay.\n",
        "- 5-fold CV; cache OOF and test predictions.\n",
        "\n",
        "Ensembling:\n",
        "- Weighted average of OOF/test predictions from v2 Ridge, v3 SVR/boosted trees, v4 transformer.\n",
        "- Weights tuned on OOF Spearman (e.g., grid over simplex).\n",
        "\n",
        "Diagnostics & efficiency:\n",
        "- Environment check: GPU via nvidia-smi; install torch cu121 stack once.\n",
        "- Log per-fold durations; save OOF, test preds, and submission.csv; verify format.\n",
        "- Adversarial validation quick check (optional) to ensure TF-IDF features not leaking IDs.\n",
        "\n",
        "Milestones & expert checkpoints:\n",
        "1) Plan review (this cell).\n",
        "2) Environment + data load/EDA.\n",
        "3) Baseline TF-IDF + Ridge OOF.\n",
        "4) Per-field TF-IDF + features.\n",
        "5) SVD + SVR/boost.\n",
        "6) Transformer fine-tune.\n",
        "7) Ensemble + finalize submission."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "5b10bae6-3386-429b-80c1-cbd0c827e845",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment check + data load/quick EDA\n",
        "import os, sys, subprocess, time, hashlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def run(cmd):\n",
        "    print(\">>\", \" \".join(cmd), flush=True)\n",
        "    return subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True).stdout\n",
        "\n",
        "t0 = time.time()\n",
        "print(\"[Env] Checking GPU via nvidia-smi...\")\n",
        "print(run(['bash','-lc','nvidia-smi || true']))\n",
        "\n",
        "# Load data\n",
        "train_path = Path('train.csv')\n",
        "test_path = Path('test.csv')\n",
        "assert train_path.exists() and test_path.exists(), \"train.csv/test.csv missing\"\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "print(f\"train shape: {train.shape}; test shape: {test.shape}\")\n",
        "print(\"train columns:\", list(train.columns))\n",
        "print(\"test columns:\", list(test.columns))\n",
        "\n",
        "# Identify id and target\n",
        "id_col = None\n",
        "for c in ['qa_id','id','question_id','id_q']:\n",
        "    if c in train.columns and c in test.columns:\n",
        "        id_col = c\n",
        "        break\n",
        "print(\"ID column:\", id_col)\n",
        "target = 'question_asker_intent_understanding'\n",
        "assert target in train.columns, f\"Target {target} not found\"\n",
        "print(train[target].describe())\n",
        "\n",
        "# Group key to avoid leakage across answers of same question\n",
        "def make_group(df: pd.DataFrame):\n",
        "    # Prefer url if present\n",
        "    if 'url' in df.columns:\n",
        "        key = df['url'].fillna('')\n",
        "    else:\n",
        "        # fallback: stable hash of title||body\n",
        "        t = df['question_title'].fillna('') if 'question_title' in df.columns else ''\n",
        "        b = df['question_body'].fillna('') if 'question_body' in df.columns else ''\n",
        "        key = (t.astype(str) + '||' + b.astype(str))\n",
        "    # Hash to int64 for GroupKFold compatibility\n",
        "    h = pd.util.hash_pandas_object(key, index=False).astype('int64')\n",
        "    return h\n",
        "\n",
        "train['group_key'] = make_group(train)\n",
        "test['group_key'] = make_group(test)\n",
        "print(\"Unique groups in train:\", train['group_key'].nunique(), \"/ rows:\", len(train))\n",
        "\n",
        "# Basic sanity: no NaNs in target, in-range\n",
        "nan_rate = train[target].isna().mean()\n",
        "print(f\"Target NaN rate: {nan_rate:.5f}\")\n",
        "print(\"Target min/max:\", train[target].min(), train[target].max())\n",
        "\n",
        "# Save quick artifacts\n",
        "train[['group_key']].to_csv('train_group_keys.csv', index=False)\n",
        "test[['group_key']].to_csv('test_group_keys.csv', index=False)\n",
        "print(f\"[Done] Env+EDA in {time.time()-t0:.2f}s\")\n",
        "\n",
        "# Next: implement GroupKFold splitter and baseline TF-IDF+Ridge."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Env] Checking GPU via nvidia-smi...\n>> bash -lc nvidia-smi || true\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 29 04:17:06 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     182MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\ntrain shape: (5471, 41); test shape: (608, 11)\ntrain columns: ['qa_id', 'question_title', 'question_body', 'question_user_name', 'question_user_page', 'answer', 'answer_user_name', 'answer_user_page', 'url', 'category', 'host', 'question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\ntest columns: ['qa_id', 'question_title', 'question_body', 'question_user_name', 'question_user_page', 'answer', 'answer_user_name', 'answer_user_page', 'url', 'category', 'host']\nID column: qa_id\ncount    5471.000000\nmean        0.892605\nstd         0.131874\nmin         0.333333\n25%         0.777778\n50%         0.888889\n75%         1.000000\nmax         1.000000\nName: question_asker_intent_understanding, dtype: float64\nUnique groups in train: 3392 / rows: 5471\nTarget NaN rate: 0.00000\nTarget min/max: 0.3333333333333333 1.0\n[Done] Env+EDA in 0.15s\n"
          ]
        }
      ]
    },
    {
      "id": "b6e094e8-d51e-4d01-85b4-4d8e4775778c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Baseline v1: Per-field TF-IDF (word+char) + Ridge with GroupKFold CV\n",
        "import time\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import make_scorer\n",
        "from scipy import sparse\n",
        "from scipy.stats import spearmanr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def spearmanr_safe(y_true, y_pred):\n",
        "    if np.std(y_pred) == 0:\n",
        "        return 0.0\n",
        "    return spearmanr(y_true, y_pred).correlation\n",
        "\n",
        "texts_train = {\n",
        "    'title': train['question_title'].fillna('').astype(str),\n",
        "    'body': train['question_body'].fillna('').astype(str),\n",
        "    'answer': train['answer'].fillna('').astype(str),\n",
        "}\n",
        "texts_test = {\n",
        "    'title': test['question_title'].fillna('').astype(str),\n",
        "    'body': test['question_body'].fillna('').astype(str),\n",
        "    'answer': test['answer'].fillna('').astype(str),\n",
        "}\n",
        "\n",
        "# Vectorizer configs (proven defaults)\n",
        "cfg_word = dict(analyzer='word', ngram_range=(1,2), sublinear_tf=True, strip_accents='unicode', lowercase=True, min_df=3, stop_words='english')\n",
        "cfg_char = dict(analyzer='char_wb', ngram_range=(3,6), sublinear_tf=True, min_df=3)\n",
        "max_feats = {\n",
        "    'word': {'title': 50000, 'body': 150000, 'answer': 150000},\n",
        "    'char': {'title': 30000, 'body': 120000, 'answer': 120000},\n",
        "}\n",
        "\n",
        "y = train[target].values.astype(float)\n",
        "groups = train['group_key'].values\n",
        "\n",
        "gkf = GroupKFold(n_splits=5)\n",
        "oof = np.zeros(len(train), dtype=float)\n",
        "test_preds_folds = []\n",
        "folds = np.full(len(train), -1, dtype=int)\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(gkf.split(X=np.zeros(len(train)), y=y, groups=groups)):\n",
        "    t0 = time.time()\n",
        "    folds[val_idx] = fold\n",
        "    print(f\"\\n[FOLD {fold}] train={len(trn_idx)} val={len(val_idx)}\", flush=True)\n",
        "    # Fit per-field vectorizers on train split only\n",
        "    X_tr_parts = []\n",
        "    X_va_parts = []\n",
        "    X_te_parts = []\n",
        "    for field in ['title','body','answer']:\n",
        "        # Word\n",
        "        vec_w = TfidfVectorizer(max_features=max_feats['word'][field], **cfg_word)\n",
        "        Xw_tr = vec_w.fit_transform(texts_train[field].iloc[trn_idx])\n",
        "        Xw_va = vec_w.transform(texts_train[field].iloc[val_idx])\n",
        "        Xw_te = vec_w.transform(texts_test[field])\n",
        "        # Char\n",
        "        vec_c = TfidfVectorizer(max_features=max_feats['char'][field], **cfg_char)\n",
        "        Xc_tr = vec_c.fit_transform(texts_train[field].iloc[trn_idx])\n",
        "        Xc_va = vec_c.transform(texts_train[field].iloc[val_idx])\n",
        "        Xc_te = vec_c.transform(texts_test[field])\n",
        "        # Stack per field\n",
        "        X_tr_parts.append(sparse.hstack([Xw_tr, Xc_tr], format='csr'))\n",
        "        X_va_parts.append(sparse.hstack([Xw_va, Xc_va], format='csr'))\n",
        "        X_te_parts.append(sparse.hstack([Xw_te, Xc_te], format='csr'))\n",
        "    # Concatenate fields horizontally\n",
        "    X_tr = sparse.hstack(X_tr_parts, format='csr')\n",
        "    X_va = sparse.hstack(X_va_parts, format='csr')\n",
        "    X_te = sparse.hstack(X_te_parts, format='csr')\n",
        "    print(f\"[FOLD {fold}] Shapes: X_tr={X_tr.shape} X_va={X_va.shape} X_te={X_te.shape}\")\n",
        "\n",
        "    # Ridge (alpha tuned lightly per fold)\n",
        "    best_alpha = None\n",
        "    best_score = -1e9\n",
        "    best_pred = None\n",
        "    alphas = [1.0, 2.0, 5.0, 10.0]\n",
        "    for a in alphas:\n",
        "        model = Ridge(alpha=a, random_state=42)\n",
        "        model.fit(X_tr, y[trn_idx])\n",
        "        p = model.predict(X_va)\n",
        "        sc = spearmanr_safe(y[val_idx], p)\n",
        "        print(f\"[FOLD {fold}] alpha={a} val_spearman={sc:.5f}\")\n",
        "        if sc > best_score:\n",
        "            best_score = sc\n",
        "            best_alpha = a\n",
        "            best_pred = p\n",
        "    oof[val_idx] = best_pred\n",
        "    # Refit on full fold train for test preds with best alpha\n",
        "    model = Ridge(alpha=best_alpha, random_state=42)\n",
        "    model.fit(X_tr, y[trn_idx])\n",
        "    test_pred = model.predict(X_te)\n",
        "    test_preds_folds.append(test_pred)\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"[FOLD {fold}] best_alpha={best_alpha} fold_spearman={best_score:.5f} time={elapsed:.1f}s\", flush=True)\n",
        "\n",
        "cv_score = spearmanr_safe(y, oof)\n",
        "print(f\"\\n[CV] OOF Spearman: {cv_score:.5f}\")\n",
        "\n",
        "# Save OOF and fold assignments\n",
        "np.save('oof_ridge.npy', oof)\n",
        "pd.DataFrame({'qa_id': train[id_col], 'fold': folds, 'oof': oof, 'target': y}).to_csv('oof_ridge.csv', index=False)\n",
        "\n",
        "# Aggregate test predictions (mean across folds), clip to [0,1]\n",
        "test_pred_mean = np.mean(np.vstack(test_preds_folds), axis=0)\n",
        "test_pred_mean = np.clip(test_pred_mean, 0.0, 1.0)\n",
        "np.save('test_ridge.npy', test_pred_mean)\n",
        "\n",
        "# Build submission\n",
        "sub = pd.DataFrame({id_col: test[id_col], target: test_pred_mean})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(\"Saved submission.csv\", sub.shape, \"head:\\n\", sub.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[FOLD 0] train=4376 val=1095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 0] Shapes: X_tr=(4376, 317157) X_va=(1095, 317157) X_te=(608, 317157)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 0] alpha=1.0 val_spearman=0.22364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 0] alpha=2.0 val_spearman=0.24501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 0] alpha=5.0 val_spearman=0.27993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 0] alpha=10.0 val_spearman=0.30730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 0] best_alpha=10.0 fold_spearman=0.30730 time=15.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[FOLD 1] train=4377 val=1094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 1] Shapes: X_tr=(4377, 316578) X_va=(1094, 316578) X_te=(608, 316578)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 1] alpha=1.0 val_spearman=0.14794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 1] alpha=2.0 val_spearman=0.16752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 1] alpha=5.0 val_spearman=0.19993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 1] alpha=10.0 val_spearman=0.22630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 1] best_alpha=10.0 fold_spearman=0.22630 time=14.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[FOLD 2] train=4377 val=1094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 2] Shapes: X_tr=(4377, 317059) X_va=(1094, 317059) X_te=(608, 317059)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 2] alpha=1.0 val_spearman=0.19841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 2] alpha=2.0 val_spearman=0.22097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 2] alpha=5.0 val_spearman=0.25595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 2] alpha=10.0 val_spearman=0.28347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 2] best_alpha=10.0 fold_spearman=0.28347 time=14.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[FOLD 3] train=4377 val=1094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 3] Shapes: X_tr=(4377, 317028) X_va=(1094, 317028) X_te=(608, 317028)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 3] alpha=1.0 val_spearman=0.25230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 3] alpha=2.0 val_spearman=0.27106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 3] alpha=5.0 val_spearman=0.29466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 3] alpha=10.0 val_spearman=0.30805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 3] best_alpha=10.0 fold_spearman=0.30805 time=14.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[FOLD 4] train=4377 val=1094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 4] Shapes: X_tr=(4377, 315933) X_va=(1094, 315933) X_te=(608, 315933)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 4] alpha=1.0 val_spearman=0.25176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 4] alpha=2.0 val_spearman=0.26409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 4] alpha=5.0 val_spearman=0.28661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 4] alpha=10.0 val_spearman=0.30573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 4] best_alpha=10.0 fold_spearman=0.30573 time=14.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[CV] OOF Spearman: 0.28549\nSaved submission.csv (608, 2) head:\n    qa_id  question_asker_intent_understanding\n0   6516                             0.924649\n1   6168                             0.876612\n2   8575                             0.952263\n3    618                             0.862909\n4   3471                             0.942982\n"
          ]
        }
      ]
    },
    {
      "id": "cd882409-3542-401f-a684-79b463d18d7b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Improved Baseline: Q-only TF-IDF (keep stopwords), title upweight, StratifiedGroupKFold, + cheap scalar features\n",
        "import time, re\n",
        "from sklearn.model_selection import GroupKFold\n",
        "try:\n",
        "    from sklearn.model_selection import StratifiedGroupKFold\n",
        "    HAS_SGK = True\n",
        "except Exception:\n",
        "    HAS_SGK = False\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "from scipy import sparse\n",
        "from scipy.stats import spearmanr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def spearmanr_safe(y_true, y_pred):\n",
        "    if np.std(y_pred) == 0:\n",
        "        return 0.0\n",
        "    return spearmanr(y_true, y_pred).correlation\n",
        "\n",
        "# Prepare texts (QUESTION ONLY) and simple scalar features\n",
        "q_title_tr = train['question_title'].fillna('').astype(str)\n",
        "q_body_tr  = train['question_body'].fillna('').astype(str)\n",
        "q_title_te = test['question_title'].fillna('').astype(str)\n",
        "q_body_te  = test['question_body'].fillna('').astype(str)\n",
        "\n",
        "def basic_feats(title: pd.Series, body: pd.Series):\n",
        "    # Cheap, intent-related features\n",
        "    def counts(s):\n",
        "        s2 = s.fillna('')\n",
        "        chars = s2.str.len().astype(float)\n",
        "        words = s2.str.split().apply(len).astype(float)\n",
        "        # Use regex-escaped patterns for special chars\n",
        "        qcnt = s2.str.count(r'\\?').astype(float)\n",
        "        ecnt = s2.str.count(r'!').astype(float)\n",
        "        q2   = s2.str.count(r'\\?\\?+').astype(float)\n",
        "        e2   = s2.str.count(r'!!+').astype(float)\n",
        "        ell  = s2.str.count(r'\\.\\.\\.+').astype(float)\n",
        "        upper = s2.apply(lambda t: sum(ch.isupper() for ch in t)).astype(float)\n",
        "        upper_ratio = (upper / (chars.replace(0, np.nan))).fillna(0.0).astype(float)\n",
        "        nl = s2.str.count(r'\\n').astype(float)\n",
        "        has_url = s2.str.contains(r'http[s]?://', regex=True).astype(float)\n",
        "        has_code = s2.str.contains(r'`').astype(float)\n",
        "        list_mark = s2.str.contains(r'(^|\\n)[\\-\\*] ', regex=True).astype(float)\n",
        "        quote = s2.str.contains(r'(^|\\n)\\>', regex=True).astype(float)\n",
        "        digits = s2.apply(lambda t: sum(ch.isdigit() for ch in t)).astype(float)\n",
        "        digit_ratio = (digits / (chars.replace(0, np.nan))).fillna(0.0).astype(float)\n",
        "        uniq_ratio = s2.apply(lambda t: (len(set(t.split())) / max(1, len(t.split())))).astype(float)\n",
        "        return [chars, words, qcnt, ecnt, q2, e2, ell, upper_ratio, nl, has_url, has_code, list_mark, quote, digit_ratio, uniq_ratio]\n",
        "    t_feats = counts(title)\n",
        "    b_feats = counts(body)\n",
        "    # Also add simple ratios title/body\n",
        "    t_chars, t_words = t_feats[0], t_feats[1]\n",
        "    b_chars, b_words = b_feats[0], b_feats[1]\n",
        "    len_ratio_c = (t_chars / (b_chars.replace(0, np.nan))).fillna(0.0).astype(float)\n",
        "    len_ratio_w = (t_words / (b_words.replace(0, np.nan))).fillna(0.0).astype(float)\n",
        "    feats = t_feats + b_feats + [len_ratio_c, len_ratio_w]\n",
        "    F = np.vstack([f.values for f in feats]).T.astype(np.float32)\n",
        "    return F, [\n",
        "        't_chars','t_words','t_q','t_e','t_q2','t_e2','t_ell','t_upper_ratio','t_nl','t_has_url','t_has_code','t_list','t_quote','t_digit_ratio','t_uniq_ratio',\n",
        "        'b_chars','b_words','b_q','b_e','b_q2','b_e2','b_ell','b_upper_ratio','b_nl','b_has_url','b_has_code','b_list','b_quote','b_digit_ratio','b_uniq_ratio',\n",
        "        'len_ratio_c','len_ratio_w'\n",
        "    ]\n",
        "\n",
        "F_tr, feat_names = basic_feats(q_title_tr, q_body_tr)\n",
        "F_te, _ = basic_feats(q_title_te, q_body_te)\n",
        "\n",
        "# Build stratified group folds by binned group-level target\n",
        "y = train[target].values.astype(float)\n",
        "groups = train['group_key'].values\n",
        "df_groups = pd.DataFrame({'group': groups, 'y': y})\n",
        "grp_mean = df_groups.groupby('group')['y'].mean()\n",
        "bins = pd.qcut(grp_mean, q=10, labels=False, duplicates='drop')\n",
        "grp_to_bin = dict(zip(grp_mean.index.values, bins.astype(int)))\n",
        "row_bins = np.array([grp_to_bin[g] for g in groups], dtype=int)\n",
        "\n",
        "if HAS_SGK:\n",
        "    splitter = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    split_iter = splitter.split(X=np.zeros_like(y), y=row_bins, groups=groups)\n",
        "else:\n",
        "    # Fallback: GroupKFold (no strat) but we keep the same API\n",
        "    splitter = GroupKFold(n_splits=5)\n",
        "    split_iter = splitter.split(X=np.zeros_like(y), y=y, groups=groups)\n",
        "\n",
        "# TF-IDF configs: keep stopwords (None), Q-only fields, upweight title\n",
        "cfg_word = dict(analyzer='word', ngram_range=(1,2), sublinear_tf=True, strip_accents='unicode', lowercase=True, min_df=3, stop_words=None)\n",
        "cfg_char = dict(analyzer='char_wb', ngram_range=(3,6), sublinear_tf=True, min_df=3)\n",
        "max_feats = {\n",
        "    'word': {'title': 50000, 'body': 150000},\n",
        "    'char': {'title': 30000, 'body': 120000},\n",
        "}\n",
        "title_weight = 1.8\n",
        "\n",
        "oof = np.zeros(len(train), dtype=float)\n",
        "test_preds_folds = []\n",
        "folds = np.full(len(train), -1, dtype=int)\n",
        "\n",
        "alphas = [5.0, 10.0, 20.0, 50.0, 100.0, 200.0, 500.0]\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(split_iter):\n",
        "    t0 = time.time()\n",
        "    folds[val_idx] = fold\n",
        "    print(f\"\\n[IMP FOLD {fold}] train={len(trn_idx)} val={len(val_idx)}\", flush=True)\n",
        "    # Vectorize per field on train split (Q-only)\n",
        "    # Title\n",
        "    vec_w_t = TfidfVectorizer(max_features=max_feats['word']['title'], **cfg_word)\n",
        "    Xw_t_tr = vec_w_t.fit_transform(q_title_tr.iloc[trn_idx])\n",
        "    Xw_t_va = vec_w_t.transform(q_title_tr.iloc[val_idx])\n",
        "    Xw_t_te = vec_w_t.transform(q_title_te)\n",
        "    vec_c_t = TfidfVectorizer(max_features=max_feats['char']['title'], **cfg_char)\n",
        "    Xc_t_tr = vec_c_t.fit_transform(q_title_tr.iloc[trn_idx])\n",
        "    Xc_t_va = vec_c_t.transform(q_title_tr.iloc[val_idx])\n",
        "    Xc_t_te = vec_c_t.transform(q_title_te)\n",
        "    X_t_tr = sparse.hstack([Xw_t_tr, Xc_t_tr], format='csr').multiply(title_weight)\n",
        "    X_t_va = sparse.hstack([Xw_t_va, Xc_t_va], format='csr').multiply(title_weight)\n",
        "    X_t_te = sparse.hstack([Xw_t_te, Xc_t_te], format='csr').multiply(title_weight)\n",
        "    # Body\n",
        "    vec_w_b = TfidfVectorizer(max_features=max_feats['word']['body'], **cfg_word)\n",
        "    Xw_b_tr = vec_w_b.fit_transform(q_body_tr.iloc[trn_idx])\n",
        "    Xw_b_va = vec_w_b.transform(q_body_tr.iloc[val_idx])\n",
        "    Xw_b_te = vec_w_b.transform(q_body_te)\n",
        "    vec_c_b = TfidfVectorizer(max_features=max_feats['char']['body'], **cfg_char)\n",
        "    Xc_b_tr = vec_c_b.fit_transform(q_body_tr.iloc[trn_idx])\n",
        "    Xc_b_va = vec_c_b.transform(q_body_tr.iloc[val_idx])\n",
        "    Xc_b_te = vec_c_b.transform(q_body_te)\n",
        "    X_b_tr = sparse.hstack([Xw_b_tr, Xc_b_tr], format='csr')\n",
        "    X_b_va = sparse.hstack([Xw_b_va, Xc_b_va], format='csr')\n",
        "    X_b_te = sparse.hstack([Xw_b_te, Xc_b_te], format='csr')\n",
        "\n",
        "    # Cheap scalar features: fit scaler (mean/std) on train and apply\n",
        "    Ft_tr = F_tr[trn_idx]\n",
        "    Ft_va = F_tr[val_idx]\n",
        "    Ft_te = F_te\n",
        "    mu = Ft_tr.mean(axis=0)\n",
        "    sd = Ft_tr.std(axis=0) + 1e-6\n",
        "    Ft_tr_z = (Ft_tr - mu) / sd\n",
        "    Ft_va_z = (Ft_va - mu) / sd\n",
        "    Ft_te_z = (Ft_te - mu) / sd\n",
        "    Xf_tr = sparse.csr_matrix(Ft_tr_z, dtype=np.float32)\n",
        "    Xf_va = sparse.csr_matrix(Ft_va_z, dtype=np.float32)\n",
        "    Xf_te = sparse.csr_matrix(Ft_te_z, dtype=np.float32)\n",
        "\n",
        "    # Final design matrices\n",
        "    X_tr = sparse.hstack([X_t_tr, X_b_tr, Xf_tr], format='csr')\n",
        "    X_va = sparse.hstack([X_t_va, X_b_va, Xf_va], format='csr')\n",
        "    X_te = sparse.hstack([X_t_te, X_b_te, Xf_te], format='csr')\n",
        "    print(f\"[IMP FOLD {fold}] Shapes: X_tr={X_tr.shape} X_va={X_va.shape} X_te={X_te.shape}\")\n",
        "\n",
        "    # Ridge with expanded alpha grid\n",
        "    best_alpha, best_score, best_pred = None, -1e9, None\n",
        "    for a in alphas:\n",
        "        model = Ridge(alpha=a, random_state=42)\n",
        "        model.fit(X_tr, y[trn_idx])\n",
        "        p = model.predict(X_va)\n",
        "        sc = spearmanr_safe(y[val_idx], p)\n",
        "        print(f\"[IMP FOLD {fold}] alpha={a:.1f} val_spearman={sc:.5f}\")\n",
        "        if sc > best_score:\n",
        "            best_alpha, best_score, best_pred = a, sc, p\n",
        "    oof[val_idx] = best_pred\n",
        "    model = Ridge(alpha=best_alpha, random_state=42)\n",
        "    model.fit(X_tr, y[trn_idx])\n",
        "    test_pred = model.predict(X_te)\n",
        "    test_preds_folds.append(test_pred)\n",
        "    print(f\"[IMP FOLD {fold}] best_alpha={best_alpha} fold_spearman={best_score:.5f} time={time.time()-t0:.1f}s\", flush=True)\n",
        "\n",
        "cv_score = spearmanr_safe(y, oof)\n",
        "print(f\"\\n[IMP CV] OOF Spearman: {cv_score:.5f}\")\n",
        "np.save('oof_ridge_improved.npy', oof)\n",
        "pd.DataFrame({'qa_id': train[id_col], 'fold': folds, 'oof': oof, 'target': y}).to_csv('oof_ridge_improved.csv', index=False)\n",
        "\n",
        "test_pred_mean = np.mean(np.vstack(test_preds_folds), axis=0)\n",
        "test_pred_mean = np.clip(test_pred_mean, 0.0, 1.0)\n",
        "np.save('test_ridge_improved.npy', test_pred_mean)\n",
        "sub_imp = pd.DataFrame({id_col: test[id_col], target: test_pred_mean})\n",
        "sub_imp.to_csv('submission.csv', index=False)\n",
        "print(\"[IMP] Saved submission.csv\", sub_imp.shape, \"head:\\n\", sub_imp.head())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_183/944291721.py:44: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  list_mark = s2.str.contains(r'(^|\\n)[\\-\\*] ', regex=True).astype(float)\n/tmp/ipykernel_183/944291721.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  quote = s2.str.contains(r'(^|\\n)\\>', regex=True).astype(float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_183/944291721.py:44: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  list_mark = s2.str.contains(r'(^|\\n)[\\-\\*] ', regex=True).astype(float)\n/tmp/ipykernel_183/944291721.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  quote = s2.str.contains(r'(^|\\n)\\>', regex=True).astype(float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_183/944291721.py:44: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  list_mark = s2.str.contains(r'(^|\\n)[\\-\\*] ', regex=True).astype(float)\n/tmp/ipykernel_183/944291721.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  quote = s2.str.contains(r'(^|\\n)\\>', regex=True).astype(float)\n/tmp/ipykernel_183/944291721.py:44: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  list_mark = s2.str.contains(r'(^|\\n)[\\-\\*] ', regex=True).astype(float)\n/tmp/ipykernel_183/944291721.py:45: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  quote = s2.str.contains(r'(^|\\n)\\>', regex=True).astype(float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[IMP FOLD 0] train=4395 val=1076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 0] Shapes: X_tr=(4395, 201378) X_va=(1076, 201378) X_te=(608, 201378)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 0] alpha=5.0 val_spearman=0.19589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 0] alpha=10.0 val_spearman=0.21622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 0] alpha=20.0 val_spearman=0.23747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 0] alpha=50.0 val_spearman=0.25809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 0] alpha=100.0 val_spearman=0.26415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 0] alpha=200.0 val_spearman=0.26036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 0] alpha=500.0 val_spearman=0.25196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 0] best_alpha=100.0 fold_spearman=0.26415 time=11.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[IMP FOLD 1] train=4318 val=1153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 1] Shapes: X_tr=(4318, 199204) X_va=(1153, 199204) X_te=(608, 199204)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 1] alpha=5.0 val_spearman=0.26574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 1] alpha=10.0 val_spearman=0.28934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 1] alpha=20.0 val_spearman=0.31161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 1] alpha=50.0 val_spearman=0.33160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 1] alpha=100.0 val_spearman=0.33418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 1] alpha=200.0 val_spearman=0.32769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 1] alpha=500.0 val_spearman=0.31287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 1] best_alpha=100.0 fold_spearman=0.33418 time=11.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[IMP FOLD 2] train=4389 val=1082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 2] Shapes: X_tr=(4389, 201456) X_va=(1082, 201456) X_te=(608, 201456)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 2] alpha=5.0 val_spearman=0.24342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 2] alpha=10.0 val_spearman=0.26911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 2] alpha=20.0 val_spearman=0.29080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 2] alpha=50.0 val_spearman=0.30901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 2] alpha=100.0 val_spearman=0.31143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 2] alpha=200.0 val_spearman=0.30767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 2] alpha=500.0 val_spearman=0.30080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 2] best_alpha=100.0 fold_spearman=0.31143 time=11.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[IMP FOLD 3] train=4399 val=1072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 3] Shapes: X_tr=(4399, 202580) X_va=(1072, 202580) X_te=(608, 202580)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 3] alpha=5.0 val_spearman=0.20366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 3] alpha=10.0 val_spearman=0.22882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 3] alpha=20.0 val_spearman=0.25056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 3] alpha=50.0 val_spearman=0.26503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 3] alpha=100.0 val_spearman=0.26683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 3] alpha=200.0 val_spearman=0.26074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 3] alpha=500.0 val_spearman=0.25207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 3] best_alpha=100.0 fold_spearman=0.26683 time=11.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[IMP FOLD 4] train=4383 val=1088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 4] Shapes: X_tr=(4383, 203057) X_va=(1088, 203057) X_te=(608, 203057)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 4] alpha=5.0 val_spearman=0.18898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 4] alpha=10.0 val_spearman=0.21554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 4] alpha=20.0 val_spearman=0.24569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 4] alpha=50.0 val_spearman=0.27583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 4] alpha=100.0 val_spearman=0.28408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 4] alpha=200.0 val_spearman=0.28127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 4] alpha=500.0 val_spearman=0.27628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMP FOLD 4] best_alpha=100.0 fold_spearman=0.28408 time=11.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[IMP CV] OOF Spearman: 0.28889\n[IMP] Saved submission.csv (608, 2) head:\n    qa_id  question_asker_intent_understanding\n0   6516                             0.879147\n1   6168                             0.841673\n2   8575                             0.925708\n3    618                             0.874153\n4   3471                             0.925204\n"
          ]
        }
      ]
    },
    {
      "id": "1f11acc6-caf4-48a3-a14c-954c7d253da1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DeBERTa-v3-base 5-fold (Q-only) with StratifiedGroupKFold and Spearman metric\n",
        "import os, sys, time, shutil, subprocess, math, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print(\"> pip\", *args, flush=True)\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", *args], check=True)\n",
        "\n",
        "# Install CUDA 12.1 Torch stack and HF deps (idempotent)\n",
        "try:\n",
        "    import torch, transformers\n",
        "    import accelerate, datasets, evaluate\n",
        "    print(\"Torch/Transformers already available:\", torch.__version__)\n",
        "except Exception:\n",
        "    # Hard reset any prior torch stacks\n",
        "    for pkg in (\"torch\",\"torchvision\",\"torchaudio\"):\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg], check=False)\n",
        "    for d in (\n",
        "        \"/app/.pip-target/torch\",\n",
        "        \"/app/.pip-target/torchvision\",\n",
        "        \"/app/.pip-target/torchaudio\",\n",
        "        \"/app/.pip-target/torchgen\",\n",
        "        \"/app/.pip-target/functorch\",\n",
        "    ):\n",
        "        if os.path.exists(d): shutil.rmtree(d, ignore_errors=True)\n",
        "    pip(\"install\", \"--index-url\", \"https://download.pytorch.org/whl/cu121\", \"--extra-index-url\", \"https://pypi.org/simple\", \"torch==2.4.1\", \"torchvision==0.19.1\", \"torchaudio==2.4.1\")\n",
        "    Path(\"constraints.txt\").write_text(\"torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n\")\n",
        "    pip(\"install\", \"-c\", \"constraints.txt\", \"transformers==4.44.2\", \"accelerate==0.34.2\", \"datasets==2.21.0\", \"evaluate==0.4.2\", \"sentencepiece\", \"protobuf<5\", \"scikit-learn\", \"--upgrade-strategy\", \"only-if-needed\")\n",
        "    import torch, transformers, accelerate, datasets, evaluate\n",
        "    print(\"torch:\", torch.__version__, \"CUDA:\", getattr(torch.version, \"cuda\", None), \"CUDA avail:\", torch.cuda.is_available())\n",
        "    if torch.cuda.is_available(): print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "import torch\n",
        "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold\n",
        "from scipy.stats import spearmanr\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorWithPadding\n",
        "\n",
        "# Rebuild folds with stratified grouping (reuse logic) to be self-contained\n",
        "target = 'question_asker_intent_understanding'\n",
        "y = train[target].values.astype(float)\n",
        "groups = train['group_key'].values\n",
        "df_groups = pd.DataFrame({'group': groups, 'y': y})\n",
        "grp_mean = df_groups.groupby('group')['y'].mean()\n",
        "bins = pd.qcut(grp_mean, q=10, labels=False, duplicates='drop')\n",
        "grp_to_bin = dict(zip(grp_mean.index.values, bins.astype(int)))\n",
        "row_bins = np.array([grp_to_bin[g] for g in groups], dtype=int)\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "splits = list(sgkf.split(np.zeros_like(y), y=row_bins, groups=groups))\n",
        "\n",
        "# Tokenization: Q-only, template: [CLS] title [SEP] body [SEP]; never truncate title beyond 64 tokens\n",
        "model_name = \"microsoft/deberta-v3-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "max_len = 512\n",
        "title_max = 64\n",
        "\n",
        "def build_inputs(title_series: pd.Series, body_series: pd.Series):\n",
        "    titles = title_series.fillna(\"\").astype(str).tolist()\n",
        "    bodies = body_series.fillna(\"\").astype(str).tolist()\n",
        "    # Pre-tokenize title with hard cap, then combine with body\n",
        "    enc_title = tokenizer(titles, add_special_tokens=False, truncation=True, max_length=title_max)\n",
        "    enc_body = tokenizer(bodies, add_special_tokens=False, truncation=True, max_length=max_len)  # temp\n",
        "    input_ids, attention_masks = [], []\n",
        "    for ti, bi in zip(enc_title[\"input_ids\"], enc_body[\"input_ids\"]):\n",
        "        # Compose: [CLS] title [SEP] body [SEP]\n",
        "        composed = [tokenizer.cls_token_id] + ti + [tokenizer.sep_token_id] + bi + [tokenizer.sep_token_id]\n",
        "        composed = composed[:max_len]\n",
        "        attn = [1]*len(composed)\n",
        "        # pad\n",
        "        pad_len = max_len - len(composed)\n",
        "        if pad_len>0:\n",
        "            composed = composed + [tokenizer.pad_token_id]*pad_len\n",
        "            attn = attn + [0]*pad_len\n",
        "        input_ids.append(composed)\n",
        "        attention_masks.append(attn)\n",
        "    return {\"input_ids\": np.array(input_ids, dtype=np.int64), \"attention_mask\": np.array(attention_masks, dtype=np.int64)}\n",
        "\n",
        "class QDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ids, masks, labels=None):\n",
        "        self.ids = ids; self.masks = masks; self.labels = labels\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\"input_ids\": torch.tensor(self.ids[idx]), \"attention_mask\": torch.tensor(self.masks[idx])}\n",
        "        if self.labels is not None:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "def spearman_compute(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    preds = preds.reshape(-1)\n",
        "    if np.std(preds)==0: return {\"spearman\": 0.0}\n",
        "    return {\"spearman\": float(spearmanr(labels, preds).correlation)}\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_name, num_labels=1, problem_type=\"regression\")\n",
        "\n",
        "all_inputs = build_inputs(train['question_title'], train['question_body'])\n",
        "test_inputs = build_inputs(test['question_title'], test['question_body'])\n",
        "\n",
        "oof = np.zeros(len(train), dtype=np.float32)\n",
        "test_preds = []\n",
        "folds = np.full(len(train), -1, dtype=int)\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(splits):\n",
        "    t0 = time.time()\n",
        "    folds[val_idx] = fold\n",
        "    print(f\"\\n[T-FOLD {fold}] train={len(trn_idx)} val={len(val_idx)}\", flush=True)\n",
        "    tr_ds = QDataset(all_inputs['input_ids'][trn_idx], all_inputs['attention_mask'][trn_idx], y[trn_idx])\n",
        "    va_ds = QDataset(all_inputs['input_ids'][val_idx], all_inputs['attention_mask'][val_idx], y[val_idx])\n",
        "    te_ds = QDataset(test_inputs['input_ids'], test_inputs['attention_mask'], None)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"tfm_fold{fold}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        gradient_accumulation_steps=1,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        fp16=True,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"spearman\",\n",
        "        greater_is_better=True,\n",
        "        logging_steps=50,\n",
        "        report_to=[]\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tr_ds,\n",
        "        eval_dataset=va_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=spearman_compute,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
        "        data_collator=DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
        "    )\n",
        "    trainer.train()\n",
        "    # OOF preds\n",
        "    val_out = trainer.predict(va_ds).predictions.reshape(-1)\n",
        "    oof[val_idx] = val_out\n",
        "    fold_score = spearmanr(y[val_idx], val_out).correlation\n",
        "    print(f\"[T-FOLD {fold}] val Spearman={fold_score:.5f} time={time.time()-t0:.1f}s\", flush=True)\n",
        "    # Test preds\n",
        "    te_out = trainer.predict(te_ds).predictions.reshape(-1)\n",
        "    test_preds.append(te_out)\n",
        "    # Cleanup to free VRAM\n",
        "    del trainer, model; gc.collect()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "cv = spearmanr(y, oof).correlation\n",
        "print(f\"\\n[T-CV] OOF Spearman: {cv:.5f}\")\n",
        "np.save('oof_deberta.npy', oof)\n",
        "pd.DataFrame({'qa_id': train[id_col], 'fold': folds, 'oof': oof, 'target': y}).to_csv('oof_deberta.csv', index=False)\n",
        "\n",
        "test_mean = np.mean(np.vstack(test_preds), axis=0)\n",
        "np.save('test_deberta.npy', test_mean)\n",
        "sub_tfm = pd.DataFrame({id_col: test[id_col], target: test_mean})\n",
        "sub_tfm.to_csv('submission_deberta.csv', index=False)\n",
        "print(\"Saved submission_deberta.csv\", sub_tfm.shape, \"head:\\n\", sub_tfm.head())\n",
        "print(\"[Done] DeBERTa training+inference complete.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torch as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchvision as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping torchaudio as it is not installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 MB 471.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 356.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 441.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 5.7 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 201.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 223.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 244.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 226.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 432.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 219.7 MB/s eta 0:00:00\nCollecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 490.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 428.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 106.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 389.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 216.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 483.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 222.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 224.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 242.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 175.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 403.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 318.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 191.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 331.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.3 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install -c constraints.txt transformers==4.44.2 accelerate==0.34.2 datasets==2.21.0 evaluate==0.4.2 sentencepiece protobuf<5 scikit-learn --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.5/9.5 MB 101.5 MB/s eta 0:00:00\nCollecting accelerate==0.34.2\n  Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 324.4/324.4 KB 94.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==2.21.0\n  Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 527.3/527.3 KB 341.2 MB/s eta 0:00:00\nCollecting evaluate==0.4.2\n  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 84.1/84.1 KB 422.4 MB/s eta 0:00:00\nCollecting sentencepiece\n  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 77.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf<5\n  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 294.9/294.9 KB 499.0 MB/s eta 0:00:00\nCollecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 219.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex!=2019.12.17\n  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 KB 494.6 MB/s eta 0:00:00\nCollecting huggingface-hub<1.0,>=0.23.2\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 563.3/563.3 KB 132.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors>=0.4.1\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 316.6 MB/s eta 0:00:00\nCollecting packaging>=20.0\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 422.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers<0.20,>=0.19\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.6/3.6 MB 154.6 MB/s eta 0:00:00\nCollecting tqdm>=4.27\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 464.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.17\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 196.3 MB/s eta 0:00:00\nCollecting pyyaml>=5.1\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 512.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 381.4 MB/s eta 0:00:00\nCollecting torch>=1.10.0\n  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.1/797.1 MB 272.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psutil\n  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 291.2/291.2 KB 551.7 MB/s eta 0:00:00\nCollecting dill<0.3.9,>=0.3.0\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 116.3/116.3 KB 424.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xxhash\n  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 194.8/194.8 KB 505.7 MB/s eta 0:00:00\nCollecting multiprocess\n  Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 144.5/144.5 KB 470.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas\n  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.4/12.4 MB 115.0 MB/s eta 0:00:00\nCollecting fsspec[http]<=2024.6.1,>=2023.1.0\n  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 177.6/177.6 KB 475.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aiohttp\n  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.7/1.7 MB 317.2 MB/s eta 0:00:00\nCollecting pyarrow>=15.0.0\n  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 42.8/42.8 MB 315.5 MB/s eta 0:00:00\nCollecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 507.6 MB/s eta 0:00:00\nCollecting scipy>=1.8.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 213.9 MB/s eta 0:00:00\nCollecting aiohappyeyeballs>=2.5.0\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yarl<2.0,>=1.17.0\n  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 349.0/349.0 KB 542.9 MB/s eta 0:00:00\nCollecting aiosignal>=1.4.0\n  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nCollecting propcache>=0.2.0\n  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 213.5/213.5 KB 518.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting multidict<7.0,>=4.5\n  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 246.7/246.7 KB 515.6 MB/s eta 0:00:00\nCollecting attrs>=17.3.0\n  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 63.8/63.8 KB 451.3 MB/s eta 0:00:00\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 235.3/235.3 KB 522.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=3.7.4.3\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 343.8 MB/s eta 0:00:00\nCollecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 316.0 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 378.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 515.2 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 488.9 MB/s eta 0:00:00\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 521.9 MB/s eta 0:00:00\nCollecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 459.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 150.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 145.2 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 474.8 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 184.0 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 258.3 MB/s eta 0:00:00\nCollecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 201.0 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 119.6 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 208.5 MB/s eta 0:00:00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nCollecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 256.0 MB/s eta 0:00:00\nCollecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 119.3 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 71.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 539.2 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 191.0 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 147.2 MB/s eta 0:00:00\nCollecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 148.8 MB/s eta 0:00:00\nCollecting multiprocess\n  Downloading multiprocess-0.70.17-py311-none-any.whl (144 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 144.3/144.3 KB 458.6 MB/s eta 0:00:00\n  Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 143.5/143.5 KB 453.6 MB/s eta 0:00:00\nCollecting tzdata>=2022.7\n  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 347.8/347.8 KB 521.6 MB/s eta 0:00:00\nCollecting python-dateutil>=2.8.2\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.9/229.9 KB 497.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytz>=2020.1\n  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 509.2/509.2 KB 539.7 MB/s eta 0:00:00\nCollecting six>=1.5\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 286.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, six, sentencepiece, safetensors, regex, pyyaml, pyarrow, psutil, protobuf, propcache, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, MarkupSafe, joblib, idna, hf-xet, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, triton, scipy, requests, python-dateutil, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jinja2, aiosignal, scikit-learn, pandas, nvidia-cusolver-cu12, huggingface-hub, aiohttp, torch, tokenizers, transformers, datasets, accelerate, evaluate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.3 accelerate-0.34.2 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 certifi-2025.8.3 charset_normalizer-3.4.3 datasets-2.21.0 dill-0.3.8 evaluate-0.4.2 filelock-3.19.1 frozenlist-1.7.0 fsspec-2024.6.1 hf-xet-1.1.10 huggingface-hub-0.35.1 idna-3.10 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 multidict-6.6.4 multiprocess-0.70.16 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 packaging-25.0 pandas-2.3.2 propcache-0.3.2 protobuf-4.25.8 psutil-7.1.0 pyarrow-21.0.0 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.3 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sentencepiece-0.2.1 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.19.1 torch-2.4.1 tqdm-4.67.1 transformers-4.44.2 triton-3.0.0 typing-extensions-4.15.0 tzdata-2025.2 urllib3-2.5.0 xxhash-3.5.0 yarl-1.20.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.4.5.107.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.1.0.106.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/triton-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe-3.0.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.1.3.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.0.2.54.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.2.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.20.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.9.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.4.1+cu121 CUDA: 12.1 CUDA avail: True\nGPU: NVIDIA A10-24Q\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[T-FOLD 0] train=4395 val=1076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/825 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T-FOLD 0] val Spearman=0.37297 time=444.9s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[T-FOLD 1] train=4318 val=1153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/810 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/37 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T-FOLD 1] val Spearman=0.40783 time=443.6s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[T-FOLD 2] train=4389 val=1082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/825 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T-FOLD 2] val Spearman=0.39341 time=452.4s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[T-FOLD 3] train=4399 val=1072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/825 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T-FOLD 3] val Spearman=0.38784 time=450.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[T-FOLD 4] train=4383 val=1088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='822' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/822 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T-FOLD 4] val Spearman=0.38819 time=453.1s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[T-CV] OOF Spearman: 0.37979\nSaved submission_deberta.csv (608, 2) head:\n    qa_id  question_asker_intent_understanding\n0   6516                             1.013086\n1   6168                             0.905762\n2   8575                             1.046875\n3    618                             0.943848\n4   3471                             1.010742\n[Done] DeBERTa training+inference complete.\n"
          ]
        }
      ]
    },
    {
      "id": "56e90dac-5613-4b17-ad15-66c4a607ecc8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ridge Model B: add answer (low weight), host/category one-hot, title-body cosine; rank-blend with Model A\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import Ridge\n",
        "from pathlib import Path\n",
        "\n",
        "def spearmanr_safe(y_true, y_pred):\n",
        "    if np.std(y_pred) == 0:\n",
        "        return 0.0\n",
        "    return spearmanr(y_true, y_pred).correlation\n",
        "\n",
        "# Folds: reuse stratified group folds on binned group mean target\n",
        "target = 'question_asker_intent_understanding'\n",
        "y = train[target].values.astype(float)\n",
        "groups = train['group_key'].values\n",
        "df_groups = pd.DataFrame({'group': groups, 'y': y})\n",
        "grp_mean = df_groups.groupby('group')['y'].mean()\n",
        "bins = pd.qcut(grp_mean, q=10, labels=False, duplicates='drop')\n",
        "grp_to_bin = dict(zip(grp_mean.index.values, bins.astype(int)))\n",
        "row_bins = np.array([grp_to_bin[g] for g in groups], dtype=int)\n",
        "try:\n",
        "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    splits = list(sgkf.split(np.zeros_like(y), y=row_bins, groups=groups))\n",
        "except Exception:\n",
        "    gkf = GroupKFold(n_splits=5)\n",
        "    splits = list(gkf.split(np.zeros_like(y), y=y, groups=groups))\n",
        "\n",
        "# Text fields\n",
        "title_tr = train['question_title'].fillna('').astype(str)\n",
        "body_tr  = train['question_body'].fillna('').astype(str)\n",
        "ans_tr   = train['answer'].fillna('').astype(str)\n",
        "title_te = test['question_title'].fillna('').astype(str)\n",
        "body_te  = test['question_body'].fillna('').astype(str)\n",
        "ans_te   = test['answer'].fillna('').astype(str)\n",
        "\n",
        "# Categorical meta\n",
        "meta_tr = train[['host','category']].astype(str).fillna('') if {'host','category'}.issubset(train.columns) else None\n",
        "meta_te = test[['host','category']].astype(str).fillna('') if {'host','category'}.issubset(test.columns) else None\n",
        "\n",
        "# Configs\n",
        "cfg_word = dict(analyzer='word', ngram_range=(1,2), sublinear_tf=True, strip_accents='unicode', lowercase=True, min_df=2, stop_words=None)\n",
        "cfg_char = dict(analyzer='char_wb', ngram_range=(3,6), sublinear_tf=True, min_df=2)\n",
        "max_feats = {\n",
        "    'word': {'title': 50000, 'body': 150000, 'answer': 120000},\n",
        "    'char': {'title': 30000, 'body': 120000, 'answer': 80000},\n",
        "}\n",
        "title_weight = 2.0\n",
        "answer_weight = 0.6\n",
        "alphas = [10.0, 20.0, 50.0, 100.0, 200.0, 500.0]\n",
        "\n",
        "oof_b = np.zeros(len(train), dtype=float)\n",
        "test_preds_b = []\n",
        "folds_idx = np.full(len(train), -1, dtype=int)\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(splits):\n",
        "    t0 = time.time()\n",
        "    folds_idx[val_idx] = fold\n",
        "    print(f\"\\n[RIDGE-B FOLD {fold}] train={len(trn_idx)} val={len(val_idx)}\", flush=True)\n",
        "    # Title vectors\n",
        "    vec_w_t = TfidfVectorizer(max_features=max_feats['word']['title'], **cfg_word)\n",
        "    Xw_t_tr = vec_w_t.fit_transform(title_tr.iloc[trn_idx]); Xw_t_va = vec_w_t.transform(title_tr.iloc[val_idx]); Xw_t_te = vec_w_t.transform(title_te)\n",
        "    vec_c_t = TfidfVectorizer(max_features=max_feats['char']['title'], **cfg_char)\n",
        "    Xc_t_tr = vec_c_t.fit_transform(title_tr.iloc[trn_idx]); Xc_t_va = vec_c_t.transform(title_tr.iloc[val_idx]); Xc_t_te = vec_c_t.transform(title_te)\n",
        "    X_t_tr = sparse.hstack([Xw_t_tr, Xc_t_tr], format='csr').multiply(title_weight)\n",
        "    X_t_va = sparse.hstack([Xw_t_va, Xc_t_va], format='csr').multiply(title_weight)\n",
        "    X_t_te = sparse.hstack([Xw_t_te, Xc_t_te], format='csr').multiply(title_weight)\n",
        "    # Body vectors\n",
        "    vec_w_b = TfidfVectorizer(max_features=max_feats['word']['body'], **cfg_word)\n",
        "    Xw_b_tr = vec_w_b.fit_transform(body_tr.iloc[trn_idx]); Xw_b_va = vec_w_b.transform(body_tr.iloc[val_idx]); Xw_b_te = vec_w_b.transform(body_te)\n",
        "    vec_c_b = TfidfVectorizer(max_features=max_feats['char']['body'], **cfg_char)\n",
        "    Xc_b_tr = vec_c_b.fit_transform(body_tr.iloc[trn_idx]); Xc_b_va = vec_c_b.transform(body_tr.iloc[val_idx]); Xc_b_te = vec_c_b.transform(body_te)\n",
        "    X_b_tr = sparse.hstack([Xw_b_tr, Xc_b_tr], format='csr')\n",
        "    X_b_va = sparse.hstack([Xw_b_va, Xc_b_va], format='csr')\n",
        "    X_b_te = sparse.hstack([Xw_b_te, Xc_b_te], format='csr')\n",
        "    # Answer vectors (low weight)\n",
        "    vec_w_a = TfidfVectorizer(max_features=max_feats['word']['answer'], **cfg_word)\n",
        "    Xw_a_tr = vec_w_a.fit_transform(ans_tr.iloc[trn_idx]); Xw_a_va = vec_w_a.transform(ans_tr.iloc[val_idx]); Xw_a_te = vec_w_a.transform(ans_te)\n",
        "    vec_c_a = TfidfVectorizer(max_features=max_feats['char']['answer'], **cfg_char)\n",
        "    Xc_a_tr = vec_c_a.fit_transform(ans_tr.iloc[trn_idx]); Xc_a_va = vec_c_a.transform(ans_tr.iloc[val_idx]); Xc_a_te = vec_c_a.transform(ans_te)\n",
        "    X_a_tr = sparse.hstack([Xw_a_tr, Xc_a_tr], format='csr').multiply(answer_weight)\n",
        "    X_a_va = sparse.hstack([Xw_a_va, Xc_a_va], format='csr').multiply(answer_weight)\n",
        "    X_a_te = sparse.hstack([Xw_a_te, Xc_a_te], format='csr').multiply(answer_weight)\n",
        "    # Cosine similarity between title and body (shared small vectorizer)\n",
        "    vec_cos = TfidfVectorizer(max_features=50000, analyzer='word', ngram_range=(1,2), min_df=2, strip_accents='unicode', lowercase=True, sublinear_tf=True)\n",
        "    V_tr = vec_cos.fit_transform(pd.concat([title_tr.iloc[trn_idx], body_tr.iloc[trn_idx]]))\n",
        "    Vt_tr = vec_cos.transform(title_tr.iloc[trn_idx]); Vb_tr = vec_cos.transform(body_tr.iloc[trn_idx])\n",
        "    Vt_va = vec_cos.transform(title_tr.iloc[val_idx]); Vb_va = vec_cos.transform(body_tr.iloc[val_idx])\n",
        "    Vt_te = vec_cos.transform(title_te); Vb_te = vec_cos.transform(body_te)\n",
        "    # Vectors are L2-normalized by default in TfidfVectorizer, cosine = dot product\n",
        "    cos_va = np.asarray((Vt_va.multiply(Vb_va)).sum(axis=1)).ravel().astype(np.float32)\n",
        "    cos_tr = np.asarray((Vt_tr.multiply(Vb_tr)).sum(axis=1)).ravel().astype(np.float32)\n",
        "    cos_te = np.asarray((Vt_te.multiply(Vb_te)).sum(axis=1)).ravel().astype(np.float32)\n",
        "    Xcos_tr = sparse.csr_matrix(cos_tr[:, None]); Xcos_va = sparse.csr_matrix(cos_va[:, None]); Xcos_te = sparse.csr_matrix(cos_te[:, None])\n",
        "    # One-hot host/category\n",
        "    if meta_tr is not None:\n",
        "        try:\n",
        "            ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n",
        "        except TypeError:\n",
        "            ohe = OneHotEncoder(handle_unknown='ignore')\n",
        "        M_tr = ohe.fit_transform(meta_tr.iloc[trn_idx])\n",
        "        M_va = ohe.transform(meta_tr.iloc[val_idx])\n",
        "        M_te = ohe.transform(meta_te)\n",
        "    else:\n",
        "        M_tr = sparse.csr_matrix((len(trn_idx), 0)); M_va = sparse.csr_matrix((len(val_idx), 0)); M_te = sparse.csr_matrix((len(test), 0))\n",
        "    # Final design matrices\n",
        "    X_tr = sparse.hstack([X_t_tr, X_b_tr, X_a_tr, Xcos_tr, M_tr], format='csr')\n",
        "    X_va = sparse.hstack([X_t_va, X_b_va, X_a_va, Xcos_va, M_va], format='csr')\n",
        "    X_te = sparse.hstack([X_t_te, X_b_te, X_a_te, Xcos_te, M_te], format='csr')\n",
        "    print(f\"[RIDGE-B FOLD {fold}] Shapes: X_tr={X_tr.shape} X_va={X_va.shape} X_te={X_te.shape}\")\n",
        "    # Ridge fit\n",
        "    best_alpha, best_score, best_pred = None, -1e9, None\n",
        "    for a in alphas:\n",
        "        mdl = Ridge(alpha=a, random_state=42)\n",
        "        mdl.fit(X_tr, y[trn_idx])\n",
        "        p = mdl.predict(X_va)\n",
        "        sc = spearmanr_safe(y[val_idx], p)\n",
        "        print(f\"[RIDGE-B FOLD {fold}] alpha={a:.1f} val_spearman={sc:.5f}\")\n",
        "        if sc > best_score:\n",
        "            best_alpha, best_score, best_pred = a, sc, p\n",
        "    oof_b[val_idx] = best_pred\n",
        "    mdl = Ridge(alpha=best_alpha, random_state=42)\n",
        "    mdl.fit(X_tr, y[trn_idx])\n",
        "    test_preds_b.append(mdl.predict(X_te))\n",
        "    print(f\"[RIDGE-B FOLD {fold}] best_alpha={best_alpha} fold_spearman={best_score:.5f} time={time.time()-t0:.1f}s\", flush=True)\n",
        "\n",
        "cv_b = spearmanr_safe(y, oof_b)\n",
        "print(f\"\\n[RIDGE-B CV] OOF Spearman: {cv_b:.5f}\")\n",
        "np.save('oof_ridge_b.npy', oof_b)\n",
        "pd.DataFrame({'qa_id': train[id_col], 'fold': folds_idx, 'oof': oof_b, 'target': y}).to_csv('oof_ridge_b.csv', index=False)\n",
        "test_b = np.mean(np.vstack(test_preds_b), axis=0)\n",
        "np.save('test_ridge_b.npy', test_b)\n",
        "\n",
        "# Rank-ensemble Model A (saved) and Model B\n",
        "oof_a = np.load('oof_ridge_improved.npy') if Path('oof_ridge_improved.npy').exists() else np.load('oof_ridge.npy')\n",
        "test_a = np.load('test_ridge_improved.npy') if Path('test_ridge_improved.npy').exists() else np.load('test_ridge.npy')\n",
        "\n",
        "def rank01(x):\n",
        "    xr = pd.Series(x).rank(method='average')\n",
        "    return (xr - xr.min()) / (xr.max() - xr.min() + 1e-9)\n",
        "\n",
        "rA = rank01(oof_a); rB = rank01(oof_b)\n",
        "best_w, best_sc = None, -1e9\n",
        "for w in [0.4, 0.5, 0.6]:\n",
        "    blend = w*rA + (1-w)*rB\n",
        "    sc = spearmanr_safe(y, blend.values)\n",
        "    print(f\"[RANK BLEND] w={w:.2f} OOF Spearman={sc:.5f}\")\n",
        "    if sc > best_sc: best_sc, best_w = sc, w\n",
        "print(f\"[RANK BLEND] best_w={best_w} best_OOF={best_sc:.5f}\")\n",
        "\n",
        "tA = rank01(test_a); tB = rank01(test_b)\n",
        "test_blend = best_w * tA.values + (1-best_w) * tB.values\n",
        "sub_blend = pd.DataFrame({id_col: test[id_col], target: test_blend})\n",
        "sub_blend.to_csv('submission_ridge_blend.csv', index=False)\n",
        "print(\"Saved submission_ridge_blend.csv\", sub_blend.shape, \"head:\\n\", sub_blend.head())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[RIDGE-B FOLD 0] train=4395 val=1076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 0] Shapes: X_tr=(4395, 417355) X_va=(1076, 417355) X_te=(608, 417355)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 0] alpha=10.0 val_spearman=0.26476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 0] alpha=20.0 val_spearman=0.28404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 0] alpha=50.0 val_spearman=0.30681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 0] alpha=100.0 val_spearman=0.31705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 0] alpha=200.0 val_spearman=0.31951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 0] alpha=500.0 val_spearman=0.31771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 0] best_alpha=200.0 fold_spearman=0.31951 time=17.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[RIDGE-B FOLD 1] train=4318 val=1153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 1] Shapes: X_tr=(4318, 413206) X_va=(1153, 413206) X_te=(608, 413206)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 1] alpha=10.0 val_spearman=0.31811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 1] alpha=20.0 val_spearman=0.33660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 1] alpha=50.0 val_spearman=0.35237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 1] alpha=100.0 val_spearman=0.35444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 1] alpha=200.0 val_spearman=0.34816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 1] alpha=500.0 val_spearman=0.33523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 1] best_alpha=100.0 fold_spearman=0.35444 time=17.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[RIDGE-B FOLD 2] train=4389 val=1082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 2] Shapes: X_tr=(4389, 416977) X_va=(1082, 416977) X_te=(608, 416977)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 2] alpha=10.0 val_spearman=0.27067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 2] alpha=20.0 val_spearman=0.29287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 2] alpha=50.0 val_spearman=0.31022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 2] alpha=100.0 val_spearman=0.31322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 2] alpha=200.0 val_spearman=0.30871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 2] alpha=500.0 val_spearman=0.30772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 2] best_alpha=100.0 fold_spearman=0.31322 time=17.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[RIDGE-B FOLD 3] train=4399 val=1072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 3] Shapes: X_tr=(4399, 415719) X_va=(1072, 415719) X_te=(608, 415719)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 3] alpha=10.0 val_spearman=0.25377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 3] alpha=20.0 val_spearman=0.27521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 3] alpha=50.0 val_spearman=0.29168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 3] alpha=100.0 val_spearman=0.29304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 3] alpha=200.0 val_spearman=0.28947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 3] alpha=500.0 val_spearman=0.28694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 3] best_alpha=100.0 fold_spearman=0.29304 time=17.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[RIDGE-B FOLD 4] train=4383 val=1088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 4] Shapes: X_tr=(4383, 415525) X_va=(1088, 415525) X_te=(608, 415525)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 4] alpha=10.0 val_spearman=0.22068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 4] alpha=20.0 val_spearman=0.25057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 4] alpha=50.0 val_spearman=0.28810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 4] alpha=100.0 val_spearman=0.30415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 4] alpha=200.0 val_spearman=0.31686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 4] alpha=500.0 val_spearman=0.31918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RIDGE-B FOLD 4] best_alpha=500.0 fold_spearman=0.31918 time=17.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[RIDGE-B CV] OOF Spearman: 0.31065\n[RANK BLEND] w=0.40 OOF Spearman=0.32657\n[RANK BLEND] w=0.50 OOF Spearman=0.32510\n[RANK BLEND] w=0.60 OOF Spearman=0.32132\n[RANK BLEND] best_w=0.4 best_OOF=0.32657\nSaved submission_ridge_blend.csv (608, 2) head:\n    qa_id  question_asker_intent_understanding\n0   6516                             0.691598\n1   6168                             0.100165\n2   8575                             0.919275\n3    618                             0.178913\n4   3471                             0.713674\n"
          ]
        }
      ]
    },
    {
      "id": "17fd8bdd-f9dc-4380-904b-086c83ec613a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensemble: DeBERTa + Ridge Blend (rank-averaged, weights tuned on OOF)\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "def spearmanr_safe(y_true, y_pred):\n",
        "    if np.std(y_pred) == 0:\n",
        "        return 0.0\n",
        "    return float(spearmanr(y_true, y_pred).correlation)\n",
        "\n",
        "def rank01(x):\n",
        "    s = pd.Series(x)\n",
        "    r = s.rank(method='average')\n",
        "    return ((r - r.min()) / (r.max() - r.min() + 1e-9)).values\n",
        "\n",
        "# Load ground truth and IDs (self-contained)\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "id_col = 'qa_id'\n",
        "target = 'question_asker_intent_understanding'\n",
        "y = train[target].values.astype(float)\n",
        "\n",
        "# Load OOF/test artifacts\n",
        "oof_deb = np.load('oof_deberta.npy')\n",
        "tst_deb = np.load('test_deberta.npy')\n",
        "oof_a = np.load('oof_ridge_improved.npy') if Path('oof_ridge_improved.npy').exists() else np.load('oof_ridge.npy')\n",
        "tst_a = np.load('test_ridge_improved.npy') if Path('test_ridge_improved.npy').exists() else np.load('test_ridge.npy')\n",
        "oof_b = np.load('oof_ridge_b.npy')\n",
        "tst_b = np.load('test_ridge_b.npy')\n",
        "\n",
        "# First: re-tune Ridge A+B rank blend\n",
        "rA = rank01(oof_a); rB = rank01(oof_b)\n",
        "best_w_ab, best_sc_ab = None, -1e9\n",
        "for w in np.linspace(0.0, 1.0, 21):\n",
        "    blend = w*rA + (1-w)*rB\n",
        "    sc = spearmanr_safe(y, blend)\n",
        "    if sc > best_sc_ab:\n",
        "        best_sc_ab, best_w_ab = sc, float(w)\n",
        "print(f\"[AB BLEND] best_w={best_w_ab:.3f} OOF={best_sc_ab:.5f}\")\n",
        "tA = rank01(tst_a); tB = rank01(tst_b)\n",
        "tst_ab = best_w_ab*tA + (1-best_w_ab)*tB\n",
        "oof_ab = best_w_ab*rA + (1-best_w_ab)*rB\n",
        "\n",
        "# Now: tune DeBERTa + RidgeBlend\n",
        "rDEB = rank01(oof_deb); rAB = rank01(oof_ab)\n",
        "best_w_final, best_sc_final = None, -1e9\n",
        "for w in np.linspace(0.0, 1.0, 41):\n",
        "    mix = w*rDEB + (1-w)*rAB\n",
        "    sc = spearmanr_safe(y, mix)\n",
        "    if sc > best_sc_final:\n",
        "        best_sc_final, best_w_final = sc, float(w)\n",
        "print(f\"[FINAL BLEND] w_DEB={best_w_final:.3f} OOF={best_sc_final:.5f}\")\n",
        "\n",
        "tDEB = rank01(tst_deb); tAB = rank01(tst_ab)\n",
        "tst_final = best_w_final*tDEB + (1-best_w_final)*tAB\n",
        "\n",
        "# Save artifacts and submission\n",
        "np.save('oof_ensemble.npy', best_w_final*rDEB + (1-best_w_final)*rAB)\n",
        "np.save('test_ensemble.npy', tst_final)\n",
        "sub = pd.DataFrame({id_col: test[id_col], target: tst_final})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', sub.shape, 'head:\\n', sub.head())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AB BLEND] best_w=0.400 OOF=0.32657\n[FINAL BLEND] w_DEB=0.775 OOF=0.38475\nSaved submission.csv (608, 2) head:\n    qa_id  question_asker_intent_understanding\n0   6516                             0.769275\n1   6168                             0.180745\n2   8575                             0.963777\n3    618                             0.367978\n4   3471                             0.765095\n"
          ]
        }
      ]
    },
    {
      "id": "599cc37d-4bb0-4beb-b238-bf2bff58ae81",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DeBERTa-v3-large 5-fold (Q-only) with SGKF, early stop, fp16\n",
        "import os, gc, time, numpy as np, pandas as pd, torch\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from scipy.stats import spearmanr\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorWithPadding\n",
        "\n",
        "def spearmanr_safe(y_true, y_pred):\n",
        "    if np.std(y_pred) == 0:\n",
        "        return 0.0\n",
        "    return float(spearmanr(y_true, y_pred).correlation)\n",
        "\n",
        "# Reload data and folds consistently\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "id_col = 'qa_id'; target = 'question_asker_intent_understanding'\n",
        "y = train[target].values.astype(float)\n",
        "groups = pd.read_csv('train_group_keys.csv')['group_key'].values if Path('train_group_keys.csv').exists() else pd.util.hash_pandas_object((train['question_title'].fillna('')+'||'+train['question_body'].fillna('')), index=False).astype('int64').values\n",
        "df_groups = pd.DataFrame({'group': groups, 'y': y})\n",
        "grp_mean = df_groups.groupby('group')['y'].mean()\n",
        "bins = pd.qcut(grp_mean, q=10, labels=False, duplicates='drop')\n",
        "grp_to_bin = dict(zip(grp_mean.index.values, bins.astype(int)))\n",
        "row_bins = np.array([grp_to_bin[g] for g in groups], dtype=int)\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "splits = list(sgkf.split(np.zeros_like(y), y=row_bins, groups=groups))\n",
        "\n",
        "# Tokenization\n",
        "model_name = 'microsoft/deberta-v3-large'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "max_len = 512\n",
        "title_max = 64\n",
        "\n",
        "def build_inputs(title_series: pd.Series, body_series: pd.Series):\n",
        "    titles = title_series.fillna('').astype(str).tolist()\n",
        "    bodies = body_series.fillna('').astype(str).tolist()\n",
        "    enc_title = tokenizer(titles, add_special_tokens=False, truncation=True, max_length=title_max)\n",
        "    enc_body = tokenizer(bodies, add_special_tokens=False, truncation=True, max_length=max_len)\n",
        "    input_ids, attention_masks = [], []\n",
        "    for ti, bi in zip(enc_title['input_ids'], enc_body['input_ids']):\n",
        "        composed = [tokenizer.cls_token_id] + ti + [tokenizer.sep_token_id] + bi + [tokenizer.sep_token_id]\n",
        "        composed = composed[:max_len]\n",
        "        attn = [1]*len(composed)\n",
        "        pad = max_len - len(composed)\n",
        "        if pad>0:\n",
        "            composed += [tokenizer.pad_token_id]*pad\n",
        "            attn += [0]*pad\n",
        "        input_ids.append(composed); attention_masks.append(attn)\n",
        "    return {'input_ids': np.array(input_ids, dtype=np.int64), 'attention_mask': np.array(attention_masks, dtype=np.int64)}\n",
        "\n",
        "class QDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ids, masks, labels=None):\n",
        "        self.ids=ids; self.masks=masks; self.labels=labels\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        item={'input_ids': torch.tensor(self.ids[idx]), 'attention_mask': torch.tensor(self.masks[idx])}\n",
        "        if self.labels is not None: item['labels']=torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    preds = preds.reshape(-1)\n",
        "    return {'spearman': spearmanr_safe(labels, preds)}\n",
        "\n",
        "all_inputs = build_inputs(train['question_title'], train['question_body'])\n",
        "test_inputs = build_inputs(test['question_title'], test['question_body'])\n",
        "\n",
        "oof = np.zeros(len(train), dtype=np.float32)\n",
        "test_preds = []\n",
        "folds = np.full(len(train), -1, dtype=int)\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(splits):\n",
        "    t0=time.time(); folds[val_idx]=fold\n",
        "    print(f\"\\n[DEB-L FOLD {fold}] train={len(trn_idx)} val={len(val_idx)}\", flush=True)\n",
        "    tr_ds = QDataset(all_inputs['input_ids'][trn_idx], all_inputs['attention_mask'][trn_idx], y[trn_idx])\n",
        "    va_ds = QDataset(all_inputs['input_ids'][val_idx], all_inputs['attention_mask'][val_idx], y[val_idx])\n",
        "    te_ds = QDataset(test_inputs['input_ids'], test_inputs['attention_mask'], None)\n",
        "\n",
        "    config = AutoConfig.from_pretrained(model_name, num_labels=1, problem_type='regression')\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'deberta_large_fold{fold}',\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=32,\n",
        "        gradient_accumulation_steps=1,\n",
        "        learning_rate=1e-5,\n",
        "        weight_decay=0.01,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type='linear',\n",
        "        fp16=True,\n",
        "        gradient_checkpointing=True,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='spearman',\n",
        "        greater_is_better=True,\n",
        "        logging_steps=50,\n",
        "        report_to=[]\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tr_ds,\n",
        "        eval_dataset=va_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
        "        data_collator=DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
        "    )\n",
        "    trainer.train()\n",
        "    val_out = trainer.predict(va_ds).predictions.reshape(-1)\n",
        "    oof[val_idx] = val_out\n",
        "    fold_score = spearmanr(y[val_idx], val_out).correlation\n",
        "    print(f\"[DEB-L FOLD {fold}] val Spearman={fold_score:.5f} time={time.time()-t0:.1f}s\", flush=True)\n",
        "    te_out = trainer.predict(te_ds).predictions.reshape(-1)\n",
        "    test_preds.append(te_out)\n",
        "    del trainer, model; gc.collect()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "cv = spearmanr(y, oof).correlation\n",
        "print(f\"\\n[DEB-L CV] OOF Spearman: {cv:.5f}\")\n",
        "np.save('oof_deberta_large.npy', oof)\n",
        "pd.DataFrame({'qa_id': train[id_col], 'fold': folds, 'oof': oof, 'target': y}).to_csv('oof_deberta_large.csv', index=False)\n",
        "test_mean = np.mean(np.vstack(test_preds), axis=0)\n",
        "np.save('test_deberta_large.npy', test_mean)\n",
        "sub_l = pd.DataFrame({id_col: test[id_col], target: test_mean})\n",
        "sub_l.to_csv('submission_deberta_large.csv', index=False)\n",
        "print('Saved submission_deberta_large.csv', sub_l.shape, 'head:\\n', sub_l.head())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[DEB-L FOLD 0] train=4395 val=1076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='2750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2750 : < :, Epoch 0.00/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEB-L FOLD 0] val Spearman=0.39332 time=1168.5s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[DEB-L FOLD 1] train=4318 val=1153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='2700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2700 : < :, Epoch 0.00/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/37 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEB-L FOLD 1] val Spearman=0.40885 time=1722.7s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[DEB-L FOLD 2] train=4389 val=1082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='2745' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2745 : < :, Epoch 0.00/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEB-L FOLD 2] val Spearman=0.39222 time=1739.8s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[DEB-L FOLD 3] train=4399 val=1072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='2750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2750 : < :, Epoch 0.00/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEB-L FOLD 3] val Spearman=0.35640 time=1744.6s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[DEB-L FOLD 4] train=4383 val=1088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='2740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2740 : < :, Epoch 0.00/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/app/.pip-target/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEB-L FOLD 4] val Spearman=0.39265 time=1740.9s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[DEB-L CV] OOF Spearman: 0.34192\nSaved submission_deberta_large.csv (608, 2) head:\n    qa_id  question_asker_intent_understanding\n0   6516                             1.008887\n1   6168                             0.896191\n2   8575                             1.015039\n3    618                             0.924121\n4   3471                             0.974512\n"
          ]
        }
      ]
    },
    {
      "id": "7f8d37c8-a40b-495a-b761-1a59eafda593",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Re-ensemble with expanded transformer pool (incl. RoBERTa), rank vs per-fold z-score, greedy top-3 mixing, and dense final weight search\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "def spearmanr_safe(y_true, y_pred):\n",
        "    if np.std(y_pred) == 0: return 0.0\n",
        "    return float(spearmanr(y_true, y_pred).correlation)\n",
        "\n",
        "def rank01(x):\n",
        "    s = pd.Series(x); r = s.rank(method='average')\n",
        "    return ((r - r.min()) / (r.max() - r.min() + 1e-9)).values\n",
        "\n",
        "def zscore_per_fold(oof, folds):\n",
        "    o = np.array(oof, dtype=float).copy()\n",
        "    for f in np.unique(folds):\n",
        "        idx = (folds == f)\n",
        "        mu = o[idx].mean()\n",
        "        sd = o[idx].std() + 1e-9\n",
        "        o[idx] = (o[idx] - mu) / sd\n",
        "    # return standardized OOF; test will be standardized using global mu/sd of oof below when applied\n",
        "    return o\n",
        "\n",
        "def apply_test_zscore(test_preds, oof_ref):\n",
        "    mu = float(np.mean(oof_ref))\n",
        "    sd = float(np.std(oof_ref) + 1e-9)\n",
        "    return (np.array(test_preds, dtype=float) - mu) / sd\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "id_col = 'qa_id'; target = 'question_asker_intent_understanding'\n",
        "y = train[target].values.astype(float)\n",
        "\n",
        "# Folds for per-fold z-score (from any transformer oof csv available)\n",
        "folds = None\n",
        "for cand_csv in ['oof_deberta_base_seeds.csv','oof_deberta.csv','oof_deberta_large.csv','oof_roberta.csv']:\n",
        "    if Path(cand_csv).exists():\n",
        "        dfc = pd.read_csv(cand_csv)\n",
        "        if 'fold' in dfc.columns:\n",
        "            folds = dfc['fold'].values.astype(int)\n",
        "            break\n",
        "if folds is None and Path('oof_deberta.csv').exists():\n",
        "    folds = pd.read_csv('oof_deberta.csv')['fold'].values.astype(int)\n",
        "\n",
        "# Ridge A+B OOF/test and weight re-tune (dense grid 0..1 step 0.01)\n",
        "if Path('oof_ridge_improved.npy').exists():\n",
        "    oof_a = np.load('oof_ridge_improved.npy'); tst_a = np.load('test_ridge_improved.npy')\n",
        "else:\n",
        "    oof_a = np.load('oof_ridge.npy'); tst_a = np.load('test_ridge.npy')\n",
        "oof_b = np.load('oof_ridge_b.npy'); tst_b = np.load('test_ridge_b.npy')\n",
        "rA = rank01(oof_a); rB = rank01(oof_b)\n",
        "best_w_ab, best_sc_ab = 0.4, -1e9\n",
        "for w in np.linspace(0.0, 1.0, 101):\n",
        "    sc = spearmanr_safe(y, w*rA + (1-w)*rB)\n",
        "    if sc > best_sc_ab: best_sc_ab, best_w_ab = sc, float(w)\n",
        "print(f\"[AB BLEND] best_w={best_w_ab:.3f} OOF={best_sc_ab:.5f}\")\n",
        "tAB = best_w_ab*rank01(tst_a) + (1-best_w_ab)*rank01(tst_b)\n",
        "rAB = best_w_ab*rA + (1-best_w_ab)*rB\n",
        "\n",
        "# Optional SBERT+LGBM: blend small weight into AB if artifacts exist (ws in [0,0.35] step 0.01)\n",
        "if Path('oof_sbert_lgbm.npy').exists() and Path('test_sbert_lgbm.npy').exists():\n",
        "    oof_sb = np.load('oof_sbert_lgbm.npy'); tst_sb = np.load('test_sbert_lgbm.npy')\n",
        "    rSB = rank01(oof_sb); tSB = rank01(tst_sb)\n",
        "    best_ws, best_sc_s = 0.0, spearmanr_safe(y, rAB)\n",
        "    for ws in np.linspace(0.0, 0.35, 36):\n",
        "        sc = spearmanr_safe(y, (1-ws)*rAB + ws*rSB)\n",
        "        if sc > best_sc_s: best_sc_s, best_ws = sc, float(ws)\n",
        "    if best_ws > 0:\n",
        "        print(f\"[AB+SBERT] ws={best_ws:.3f} OOF={best_sc_s:.5f}\")\n",
        "        rAB = (1-best_ws)*rAB + best_ws*rSB\n",
        "        tAB = (1-best_ws)*tAB + best_ws*tSB\n",
        "else:\n",
        "    print(\"[AB+SBERT] SBERT artifacts not found; skipping SBERT blend\")\n",
        "\n",
        "# Collect transformer candidates (OOF/test raw predictions)\n",
        "deb_pool = []  # list of (name, oof, test)\n",
        "def add_candidate(name, oof_path, tst_path):\n",
        "    if Path(oof_path).exists() and Path(tst_path).exists():\n",
        "        oof = np.load(oof_path); tst = np.load(tst_path)\n",
        "        deb_pool.append((name, oof, tst))\n",
        "\n",
        "add_candidate('base', 'oof_deberta.npy', 'test_deberta.npy')\n",
        "add_candidate('base_seeds', 'oof_deberta_base_seeds.npy', 'test_deberta_base_seeds.npy')\n",
        "add_candidate('large', 'oof_deberta_large.npy', 'test_deberta_large.npy')\n",
        "add_candidate('roberta', 'oof_roberta.npy', 'test_roberta.npy')\n",
        "add_candidate('base_mc8', 'oof_deberta_base_mc8.npy', 'test_deberta_base_mc8.npy')  # MC-dropout TTA candidate\n",
        "\n",
        "assert len(deb_pool) > 0, 'No transformer OOF/test artifacts found yet.'\n",
        "\n",
        "# Build standardized variants per mode\n",
        "modes = ['rank', 'zscore']\n",
        "best_overall = {'OOF': -1e9}\n",
        "for mode in modes:\n",
        "    cand_std = []  # (name, oof_std, tst_std)\n",
        "    for name, oof_raw, tst_raw in deb_pool:\n",
        "        if mode == 'rank':\n",
        "            oof_std = rank01(oof_raw); tst_std = rank01(tst_raw)\n",
        "        else:\n",
        "            if folds is None:\n",
        "                # fallback to global z-score if folds missing\n",
        "                mu = oof_raw.mean(); sd = oof_raw.std() + 1e-9\n",
        "                oof_std = (oof_raw - mu)/sd\n",
        "                tst_std = (tst_raw - mu)/sd\n",
        "            else:\n",
        "                oof_std = zscore_per_fold(oof_raw, folds)\n",
        "                tst_std = apply_test_zscore(tst_raw, oof_std)\n",
        "        cand_std.append((name, oof_std, tst_std))\n",
        "\n",
        "    # Evaluate single best\n",
        "    best_single = max(cand_std, key=lambda t: spearmanr_safe(y, t[1]))\n",
        "    best_name, best_oof, best_tst = best_single\n",
        "    best_sc_single = spearmanr_safe(y, best_oof)\n",
        "\n",
        "    # Evaluate best top-2 mix among all pairs (w in [0.50,1.00] step 0.02 on first vs second)\n",
        "    best_pair = (best_name, None, 1.0, best_oof, best_tst, best_sc_single)  # (name1,name2,w, oof_mix, tst_mix, sc)\n",
        "    for i in range(len(cand_std)):\n",
        "        for j in range(i+1, len(cand_std)):\n",
        "            n1,o1,t1 = cand_std[i]; n2,o2,t2 = cand_std[j]\n",
        "            # Prefers the stronger model as primary\n",
        "            sc1 = spearmanr_safe(y, o1); sc2 = spearmanr_safe(y, o2)\n",
        "            # Scan weights on the better-first assumption\n",
        "            if sc2 > sc1:\n",
        "                n1,o1,t1, n2,o2,t2 = n2,o2,t2, n1,o1,t1\n",
        "            best_w, best_sc = 1.0, spearmanr_safe(y, o1)\n",
        "            for w in np.linspace(0.50, 1.00, 26):\n",
        "                mix = w*o1 + (1-w)*o2\n",
        "                sc = spearmanr_safe(y, mix)\n",
        "                if sc > best_sc: best_sc, best_w = sc, float(w)\n",
        "            if best_sc > best_pair[5]:\n",
        "                oof_mix = best_w*o1 + (1-best_w)*o2\n",
        "                tst_mix = best_w*t1 + (1-best_w)*t2\n",
        "                best_pair = (n1, n2, best_w, oof_mix, tst_mix, best_sc)\n",
        "    # Start from best of single vs pair\n",
        "    pair_names = (best_pair[0], best_pair[1]) if best_pair[1] is not None else (best_name, None)\n",
        "    cur_oof, cur_tst, cur_sc = (best_pair[3], best_pair[4], best_pair[5]) if best_pair[1] is not None else (best_oof, best_tst, best_sc_single)\n",
        "    cur_names = [n for n in pair_names if n is not None]\n",
        "    cur_desc = f\"{'+'.join(cur_names)}@{best_pair[2]:.2f}\" if len(cur_names)==2 else cur_names[0]\n",
        "\n",
        "    # Greedy add a third candidate with a small weight in [0.02, 0.25] step 0.01; accept only if OOF improves\n",
        "    remaining = [n for n,_,_ in cand_std if n not in cur_names]\n",
        "    name_to = {n:(o,t) for n,o,t in cand_std}\n",
        "    best_third = None\n",
        "    for nm in remaining:\n",
        "        o3, t3 = name_to[nm]\n",
        "        for w3 in np.arange(0.02, 0.26, 0.01):\n",
        "            mix = (1.0 - w3)*cur_oof + w3*o3\n",
        "            sc = spearmanr_safe(y, mix)\n",
        "            if sc > cur_sc + 1e-9:\n",
        "                cur_sc = sc; cur_oof = mix; cur_tst = (1.0 - w3)*cur_tst + w3* t3; best_third = (nm, w3)\n",
        "    if best_third is not None:\n",
        "        cur_desc = f\"{cur_desc}+{best_third[0]}@{best_third[1]:.2f}\"\n",
        "\n",
        "    print(f\"[TRANS MIX {mode.upper()}] {cur_desc} OOF={cur_sc:.5f} (best single OOF={best_sc_single:.5f})\")\n",
        "\n",
        "    # Final blend: tune transformer block vs AB over [0.75, 1.00] step 0.001\n",
        "    rDEB, tDEB = cur_oof, cur_tst\n",
        "    best_w_final, best_sc_final = 0.90, -1e9\n",
        "    for w in np.linspace(0.75, 1.00, 251):\n",
        "        sc = spearmanr_safe(y, w*rDEB + (1-w)*rAB)\n",
        "        if sc > best_sc_final: best_sc_final, best_w_final = sc, float(w)\n",
        "    print(f\"[FINAL BLEND {mode.upper()}] w_DEB={best_w_final:.3f} OOF={best_sc_final:.5f}\")\n",
        "\n",
        "    if best_sc_final > best_overall.get('OOF', -1e9):\n",
        "        best_overall = {\n",
        "            'mode': mode, 'OOF': best_sc_final, 'w_DEB': best_w_final,\n",
        "            'trans_oof': rDEB, 'trans_tst': tDEB, 'desc': cur_desc,\n",
        "            'oof_ab': rAB, 'tst_ab': tAB\n",
        "        }\n",
        "\n",
        "# Save best overall submission\n",
        "print(f\"[ENSEMBLE] Best mode={best_overall['mode']} source={best_overall['desc']} OOF={best_overall['OOF']:.5f}\")\n",
        "w = best_overall['w_DEB']\n",
        "oof_final = w*best_overall['trans_oof'] + (1-w)*best_overall['oof_ab']\n",
        "tst_final = w*best_overall['trans_tst'] + (1-w)*best_overall['tst_ab']\n",
        "np.save('oof_ensemble.npy', oof_final)\n",
        "np.save('test_ensemble.npy', tst_final)\n",
        "sub = pd.DataFrame({id_col: test[id_col], target: np.clip(tst_final, 0, 1)})\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv', sub.shape, 'head:\\n', sub.head())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AB BLEND] best_w=0.410 OOF=0.32661\n[AB+SBERT] ws=0.300 OOF=0.33835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRANS MIX RANK] base_seeds+roberta@0.74+large@0.03 OOF=0.39506 (best single OOF=0.39001)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FINAL BLEND RANK] w_DEB=0.833 OOF=0.39644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRANS MIX ZSCORE] base_seeds+large@0.64+base_mc8@0.02 OOF=0.40012 (best single OOF=0.39511)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FINAL BLEND ZSCORE] w_DEB=0.770 OOF=0.40064\n[ENSEMBLE] Best mode=zscore source=base_seeds+large@0.64+base_mc8@0.02 OOF=0.40064\nSaved submission.csv (608, 2) head:\n    qa_id  question_asker_intent_understanding\n0   6516                             0.950312\n1   6168                             0.711651\n2   8575                             0.985662\n3    618                             0.748256\n4   3471                             0.942646\n"
          ]
        }
      ]
    },
    {
      "id": "f5667c59-fc04-4ad0-9f3a-ba14a784b634",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DeBERTa-v3-base: 3-seed 5-fold training and seed-averaged OOF/test\n",
        "import os, gc, time, numpy as np, pandas as pd, torch, random\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from scipy.stats import spearmanr\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorWithPadding, set_seed\n",
        "\n",
        "def spearmanr_safe(y_true, y_pred):\n",
        "    if np.std(y_pred) == 0:\n",
        "        return 0.0\n",
        "    return float(spearmanr(y_true, y_pred).correlation)\n",
        "\n",
        "# Data and folds (reuse SGKF by group bins)\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "id_col = 'qa_id'; target = 'question_asker_intent_understanding'\n",
        "y = train[target].values.astype(float)\n",
        "groups = pd.read_csv('train_group_keys.csv')['group_key'].values if Path('train_group_keys.csv').exists() else pd.util.hash_pandas_object((train['question_title'].fillna('')+'||'+train['question_body'].fillna('')), index=False).astype('int64').values\n",
        "df_groups = pd.DataFrame({'group': groups, 'y': y})\n",
        "grp_mean = df_groups.groupby('group')['y'].mean()\n",
        "bins = pd.qcut(grp_mean, q=10, labels=False, duplicates='drop')\n",
        "grp_to_bin = dict(zip(grp_mean.index.values, bins.astype(int)))\n",
        "row_bins = np.array([grp_to_bin[g] for g in groups], dtype=int)\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "splits = list(sgkf.split(np.zeros_like(y), y=row_bins, groups=groups))\n",
        "\n",
        "# Tokenization (Q-only, [CLS] title [SEP] body [SEP])\n",
        "model_name = 'microsoft/deberta-v3-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "max_len = 512; title_max = 64\n",
        "\n",
        "def build_inputs(title_series: pd.Series, body_series: pd.Series):\n",
        "    titles = title_series.fillna('').astype(str).tolist()\n",
        "    bodies = body_series.fillna('').astype(str).tolist()\n",
        "    enc_title = tokenizer(titles, add_special_tokens=False, truncation=True, max_length=title_max)\n",
        "    enc_body = tokenizer(bodies, add_special_tokens=False, truncation=True, max_length=max_len)\n",
        "    input_ids, attention_masks = [], []\n",
        "    for ti, bi in zip(enc_title['input_ids'], enc_body['input_ids']):\n",
        "        composed = [tokenizer.cls_token_id] + ti + [tokenizer.sep_token_id] + bi + [tokenizer.sep_token_id]\n",
        "        composed = composed[:max_len]\n",
        "        attn = [1]*len(composed)\n",
        "        pad = max_len - len(composed)\n",
        "        if pad>0:\n",
        "            composed += [tokenizer.pad_token_id]*pad\n",
        "            attn += [0]*pad\n",
        "        input_ids.append(composed); attention_masks.append(attn)\n",
        "    return {'input_ids': np.array(input_ids, dtype=np.int64), 'attention_mask': np.array(attention_masks, dtype=np.int64)}\n",
        "\n",
        "class QDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ids, masks, labels=None):\n",
        "        self.ids=ids; self.masks=masks; self.labels=labels\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        item={'input_ids': torch.tensor(self.ids[idx]), 'attention_mask': torch.tensor(self.masks[idx])}\n",
        "        if self.labels is not None: item['labels']=torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    preds = preds.reshape(-1)\n",
        "    return {'spearman': spearmanr_safe(labels, preds)}\n",
        "\n",
        "all_inputs = build_inputs(train['question_title'], train['question_body'])\n",
        "test_inputs = build_inputs(test['question_title'], test['question_body'])\n",
        "\n",
        "seeds = [42, 2025, 3407]\n",
        "oof_seeds = []; test_seeds = []\n",
        "for si, seed in enumerate(seeds):\n",
        "    set_seed(seed)\n",
        "    print(f\"\\n[BASE-SEED {si}] seed={seed}\", flush=True)\n",
        "    oof = np.zeros(len(train), dtype=np.float32)\n",
        "    test_preds = []\n",
        "    folds = np.full(len(train), -1, dtype=int)\n",
        "    for fold, (trn_idx, val_idx) in enumerate(splits):\n",
        "        t0=time.time(); folds[val_idx]=fold\n",
        "        print(f\"[BASE-SEED {si} FOLD {fold}] train={len(trn_idx)} val={len(val_idx)}\", flush=True)\n",
        "        tr_ds = QDataset(all_inputs['input_ids'][trn_idx], all_inputs['attention_mask'][trn_idx], y[trn_idx])\n",
        "        va_ds = QDataset(all_inputs['input_ids'][val_idx], all_inputs['attention_mask'][val_idx], y[val_idx])\n",
        "        te_ds = QDataset(test_inputs['input_ids'], test_inputs['attention_mask'], None)\n",
        "        config = AutoConfig.from_pretrained(model_name, num_labels=1, problem_type='regression')\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "        args = TrainingArguments(\n",
        "            output_dir=f'tfm_seed{seed}_fold{fold}',\n",
        "            num_train_epochs=3,\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=32,\n",
        "            gradient_accumulation_steps=1,\n",
        "            learning_rate=2e-5,\n",
        "            weight_decay=0.01,\n",
        "            warmup_ratio=0.1,\n",
        "            lr_scheduler_type='linear',\n",
        "            fp16=True,\n",
        "            evaluation_strategy='epoch',\n",
        "            save_strategy='epoch',\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model='spearman',\n",
        "            greater_is_better=True,\n",
        "            logging_steps=50,\n",
        "            seed=seed,\n",
        "            report_to=[]\n",
        "        )\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            train_dataset=tr_ds,\n",
        "            eval_dataset=va_ds,\n",
        "            tokenizer=tokenizer,\n",
        "            compute_metrics=compute_metrics,\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
        "            data_collator=DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
        "        )\n",
        "        trainer.train()\n",
        "        val_out = trainer.predict(va_ds).predictions.reshape(-1)\n",
        "        oof[val_idx] = val_out\n",
        "        fold_score = spearmanr(y[val_idx], val_out).correlation\n",
        "        print(f\"[BASE-SEED {si} FOLD {fold}] val Spearman={fold_score:.5f} time={time.time()-t0:.1f}s\", flush=True)\n",
        "        te_out = trainer.predict(te_ds).predictions.reshape(-1)\n",
        "        test_preds.append(te_out)\n",
        "        del trainer, model; gc.collect()\n",
        "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    cv = spearmanr(y, oof).correlation\n",
        "    print(f\"[BASE-SEED {si}] OOF Spearman: {cv:.5f}\")\n",
        "    np.save(f'oof_deberta_seed{seed}.npy', oof)\n",
        "    test_mean = np.mean(np.vstack(test_preds), axis=0)\n",
        "    np.save(f'test_deberta_seed{seed}.npy', test_mean)\n",
        "    oof_seeds.append(oof)\n",
        "    test_seeds.append(test_mean)\n",
        "\n",
        "# Seed-averaged OOF/test\n",
        "oof_avg = np.mean(np.vstack(oof_seeds), axis=0)\n",
        "test_avg = np.mean(np.vstack(test_seeds), axis=0)\n",
        "np.save('oof_deberta_base_seeds.npy', oof_avg)\n",
        "np.save('test_deberta_base_seeds.npy', test_avg)\n",
        "cv_avg = spearmanr(y, oof_avg).correlation\n",
        "print(f\"[BASE-SEEDS AVG] OOF Spearman: {cv_avg:.5f}\")\n",
        "sub = pd.DataFrame({id_col: test[id_col], target: test_avg})\n",
        "sub.to_csv('submission_deberta_base_seeds.csv', index=False)\n",
        "print('Saved submission_deberta_base_seeds.csv', sub.shape, 'head:\\n', sub.head())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[BASE-SEED 0] seed=42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 0 FOLD 0] train=4395 val=1076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/825 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 0 FOLD 0] val Spearman=0.38500 time=448.3s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 0 FOLD 1] train=4318 val=1153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/810 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/37 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 0 FOLD 1] val Spearman=0.39981 time=445.4s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 0 FOLD 2] train=4389 val=1082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/825 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 0 FOLD 2] val Spearman=0.39357 time=449.7s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 0 FOLD 3] train=4399 val=1072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/825 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 0 FOLD 3] val Spearman=0.38791 time=450.2s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 0 FOLD 4] train=4383 val=1088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='822' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/822 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 0 FOLD 4] val Spearman=0.38826 time=446.8s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 0] OOF Spearman: 0.37766\n\n[BASE-SEED 1] seed=2025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 1 FOLD 0] train=4395 val=1076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/825 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 1 FOLD 0] val Spearman=0.39184 time=450.1s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 1 FOLD 1] train=4318 val=1153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/810 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/37 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 1 FOLD 1] val Spearman=0.40126 time=445.5s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 1 FOLD 2] train=4389 val=1082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/825 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 1 FOLD 2] val Spearman=0.39038 time=449.6s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 1 FOLD 3] train=4399 val=1072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/825 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 1 FOLD 3] val Spearman=0.37513 time=450.2s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 1 FOLD 4] train=4383 val=1088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='822' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/822 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 1 FOLD 4] val Spearman=0.40215 time=448.8s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 1] OOF Spearman: 0.36916\n\n[BASE-SEED 2] seed=3407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 2 FOLD 0] train=4395 val=1076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/825 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 2 FOLD 0] val Spearman=0.39637 time=451.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 2 FOLD 1] train=4318 val=1153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/810 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/37 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 2 FOLD 1] val Spearman=0.37022 time=441.8s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 2 FOLD 2] train=4389 val=1082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/825 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 2 FOLD 2] val Spearman=0.36795 time=449.6s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 2 FOLD 3] train=4399 val=1072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/825 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 2 FOLD 3] val Spearman=0.36209 time=445.4s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 2 FOLD 4] train=4383 val=1088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='822' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/822 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 2 FOLD 4] val Spearman=0.37942 time=452.3s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE-SEED 2] OOF Spearman: 0.34855\n[BASE-SEEDS AVG] OOF Spearman: 0.39001\nSaved submission_deberta_base_seeds.csv (608, 2) head:\n    qa_id  question_asker_intent_understanding\n0   6516                             1.012142\n1   6168                             0.893490\n2   8575                             1.040755\n3    618                             0.932162\n4   3471                             1.006771\n"
          ]
        }
      ]
    },
    {
      "id": "015b3999-ce61-4d3e-a562-5c38e5ea0fc8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DeBERTa-v3-base with Mean Pooling + Multi-Sample Dropout (MSD), 2 additional seeds\n",
        "import os, gc, time, numpy as np, pandas as pd, torch, random, math\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from scipy.stats import spearmanr\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModel, TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorWithPadding, set_seed\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "import torch.nn as nn\n",
        "\n",
        "def spearmanr_safe(y_true, y_pred):\n",
        "    if np.std(y_pred) == 0:\n",
        "        return 0.0\n",
        "    return float(spearmanr(y_true, y_pred).correlation)\n",
        "\n",
        "# Data and folds (reuse SGKF by group bins)\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "id_col = 'qa_id'; target = 'question_asker_intent_understanding'\n",
        "y = train[target].values.astype(float)\n",
        "if Path('train_group_keys.csv').exists():\n",
        "    groups = pd.read_csv('train_group_keys.csv')['group_key'].values\n",
        "else:\n",
        "    groups = pd.util.hash_pandas_object((train['question_title'].fillna('')+'||'+train['question_body'].fillna('')), index=False).astype('int64').values\n",
        "df_groups = pd.DataFrame({'group': groups, 'y': y})\n",
        "grp_mean = df_groups.groupby('group')['y'].mean()\n",
        "bins = pd.qcut(grp_mean, q=10, labels=False, duplicates='drop')\n",
        "grp_to_bin = dict(zip(grp_mean.index.values, bins.astype(int)))\n",
        "row_bins = np.array([grp_to_bin[g] for g in groups], dtype=int)\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "splits = list(sgkf.split(np.zeros_like(y), y=row_bins, groups=groups))\n",
        "\n",
        "# Tokenization (Q-only, [CLS] title [SEP] body [SEP])\n",
        "model_name = 'microsoft/deberta-v3-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "max_len = 512; title_max = 64\n",
        "\n",
        "def build_inputs(title_series: pd.Series, body_series: pd.Series):\n",
        "    titles = title_series.fillna('').astype(str).tolist()\n",
        "    bodies = body_series.fillna('').astype(str).tolist()\n",
        "    enc_title = tokenizer(titles, add_special_tokens=False, truncation=True, max_length=title_max)\n",
        "    enc_body = tokenizer(bodies, add_special_tokens=False, truncation=True, max_length=max_len)\n",
        "    input_ids, attention_masks = [], []\n",
        "    for ti, bi in zip(enc_title['input_ids'], enc_body['input_ids']):\n",
        "        composed = [tokenizer.cls_token_id] + ti + [tokenizer.sep_token_id] + bi + [tokenizer.sep_token_id]\n",
        "        composed = composed[:max_len]\n",
        "        attn = [1]*len(composed)\n",
        "        pad = max_len - len(composed)\n",
        "        if pad>0:\n",
        "            composed += [tokenizer.pad_token_id]*pad\n",
        "            attn += [0]*pad\n",
        "        input_ids.append(composed); attention_masks.append(attn)\n",
        "    return {'input_ids': np.array(input_ids, dtype=np.int64), 'attention_mask': np.array(attention_masks, dtype=np.int64)}\n",
        "\n",
        "class QDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ids, masks, labels=None):\n",
        "        self.ids=ids; self.masks=masks; self.labels=labels\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        item={'input_ids': torch.tensor(self.ids[idx]), 'attention_mask': torch.tensor(self.masks[idx])}\n",
        "        if self.labels is not None: item['labels']=torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    preds = preds.reshape(-1)\n",
        "    return {'spearman': spearmanr_safe(labels, preds)}\n",
        "\n",
        "class MeanPooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, last_hidden_state, attention_mask):\n",
        "        mask = attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)  # (B, L, 1)\n",
        "        masked = last_hidden_state * mask\n",
        "        summed = masked.sum(dim=1)  # (B, H)\n",
        "        denom = mask.sum(dim=1).clamp(min=1e-6)  # (B, 1)\n",
        "        return summed / denom\n",
        "\n",
        "class DebertaRegMeanPoolMSD(nn.Module):\n",
        "    def __init__(self, model_name: str, dropout_p: float = 0.2, msd: int = 5):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        hidden = self.encoder.config.hidden_size\n",
        "        self.pool = MeanPooling()\n",
        "        self.msd = msd\n",
        "        self.dropouts = nn.ModuleList([nn.Dropout(dropout_p) for _ in range(msd)])\n",
        "        self.head = nn.Linear(hidden, 1)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        feats = self.pool(out.last_hidden_state, attention_mask)  # (B, H)\n",
        "        logits_list = []\n",
        "        for i in range(self.msd):\n",
        "            logits_list.append(self.head(self.dropouts[i](feats)))  # (B,1)\n",
        "        logits = torch.stack(logits_list, dim=0).mean(dim=0).squeeze(-1)  # (B,)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fn(logits, labels.view(-1))\n",
        "        # Return a proper ModelOutput to satisfy Trainer/Accelerate expectations\n",
        "        return SequenceClassifierOutput(loss=loss, logits=logits.unsqueeze(-1))\n",
        "\n",
        "all_inputs = build_inputs(train['question_title'], train['question_body'])\n",
        "test_inputs = build_inputs(test['question_title'], test['question_body'])\n",
        "\n",
        "# Train two additional seeds; keep LR=2e-5, warmup_ratio=0.1, linear scheduler, 4 epochs + ES(patience=1)\n",
        "extra_seeds = [6174, 2024]\n",
        "oof_seeds = []\n",
        "test_seeds = []\n",
        "for si, seed in enumerate(extra_seeds):\n",
        "    set_seed(seed)\n",
        "    print(f\"\\n[MEANPOOL-MSD SEED {si}] seed={seed}\", flush=True)\n",
        "    oof = np.zeros(len(train), dtype=np.float32)\n",
        "    test_preds = []\n",
        "    folds = np.full(len(train), -1, dtype=int)\n",
        "    for fold, (trn_idx, val_idx) in enumerate(splits):\n",
        "        t0=time.time(); folds[val_idx]=fold\n",
        "        print(f\"[MEANPOOL-MSD SEED {si} FOLD {fold}] train={len(trn_idx)} val={len(val_idx)}\", flush=True)\n",
        "        tr_ds = QDataset(all_inputs['input_ids'][trn_idx], all_inputs['attention_mask'][trn_idx], y[trn_idx])\n",
        "        va_ds = QDataset(all_inputs['input_ids'][val_idx], all_inputs['attention_mask'][val_idx], y[val_idx])\n",
        "        te_ds = QDataset(test_inputs['input_ids'], test_inputs['attention_mask'], None)\n",
        "        model = DebertaRegMeanPoolMSD(model_name, dropout_p=0.2, msd=5)\n",
        "        args = TrainingArguments(\n",
        "            output_dir=f'tfm_meanpool_seed{seed}_fold{fold}',\n",
        "            num_train_epochs=4,\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=32,\n",
        "            gradient_accumulation_steps=1,\n",
        "            learning_rate=2e-5,\n",
        "            weight_decay=0.01,\n",
        "            warmup_ratio=0.1,\n",
        "            lr_scheduler_type='linear',\n",
        "            fp16=True,\n",
        "            evaluation_strategy='epoch',\n",
        "            save_strategy='epoch',\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model='spearman',\n",
        "            greater_is_better=True,\n",
        "            logging_steps=50,\n",
        "            seed=seed,\n",
        "            report_to=[]\n",
        "        )\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            train_dataset=tr_ds,\n",
        "            eval_dataset=va_ds,\n",
        "            tokenizer=tokenizer,\n",
        "            compute_metrics=compute_metrics,\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
        "            data_collator=DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
        "        )\n",
        "        trainer.train()\n",
        "        val_out = trainer.predict(va_ds).predictions.reshape(-1)\n",
        "        oof[val_idx] = val_out\n",
        "        fold_score = spearmanr(y[val_idx], val_out).correlation\n",
        "        print(f\"[MEANPOOL-MSD SEED {si} FOLD {fold}] val Spearman={fold_score:.5f} time={time.time()-t0:.1f}s\", flush=True)\n",
        "        te_out = trainer.predict(te_ds).predictions.reshape(-1)\n",
        "        test_preds.append(te_out)\n",
        "        del trainer, model; gc.collect()\n",
        "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    cv = spearmanr(y, oof).correlation\n",
        "    print(f\"[MEANPOOL-MSD SEED {si}] OOF Spearman: {cv:.5f}\")\n",
        "    np.save(f'oof_deberta_meanpool_seed{seed}.npy', oof)\n",
        "    test_mean = np.mean(np.vstack(test_preds), axis=0)\n",
        "    np.save(f'test_deberta_meanpool_seed{seed}.npy', test_mean)\n",
        "    oof_seeds.append(oof)\n",
        "    test_seeds.append(test_mean)\n",
        "\n",
        "# Combine with existing 3 base seeds if present for a 5-seed average; else at least average our 2 new seeds\n",
        "existing_paths = [\n",
        "    ('oof_deberta_seed42.npy','test_deberta_seed42.npy'),\n",
        "    ('oof_deberta_seed2025.npy','test_deberta_seed2025.npy'),\n",
        "    ('oof_deberta_seed3407.npy','test_deberta_seed3407.npy'),\n",
        "]\n",
        "for oof_p, tst_p in existing_paths:\n",
        "    if Path(oof_p).exists() and Path(tst_p).exists():\n",
        "        oof_seeds.append(np.load(oof_p)); test_seeds.append(np.load(tst_p))\n",
        "\n",
        "oof_avg = np.mean(np.vstack(oof_seeds), axis=0)\n",
        "test_avg = np.mean(np.vstack(test_seeds), axis=0)\n",
        "np.save('oof_deberta_base_meanpool_seeds.npy', oof_avg)\n",
        "np.save('test_deberta_base_meanpool_seeds.npy', test_avg)\n",
        "cv_avg = spearmanr(y, oof_avg).correlation\n",
        "print(f\"[MEANPOOL-MSD SEEDS AVG] OOF Spearman: {cv_avg:.5f}\")\n",
        "sub = pd.DataFrame({id_col: test[id_col], target: test_avg})\n",
        "sub.to_csv('submission_deberta_base_meanpool_seeds.csv', index=False)\n",
        "print('Saved submission_deberta_base_meanpool_seeds.csv', sub.shape, 'head:\\n', sub.head())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[MEANPOOL-MSD SEED 0] seed=6174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MEANPOOL-MSD SEED 0 FOLD 0] train=4395 val=1076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1100 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MEANPOOL-MSD SEED 0 FOLD 0] val Spearman=0.26643 time=304.5s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MEANPOOL-MSD SEED 0 FOLD 1] train=4318 val=1153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1080 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/37 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MEANPOOL-MSD SEED 0 FOLD 1] val Spearman=0.26605 time=437.6s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MEANPOOL-MSD SEED 0 FOLD 2] train=4389 val=1082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1100 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "926ccfc5-5fcc-4a62-adf4-1126a3ac2414",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CPU parallel: S-BERT embeddings + LightGBM (5-fold SGKF), rank-ensemble candidate\n",
        "import os, time, gc, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from scipy.stats import spearmanr\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import lightgbm as lgb\n",
        "\n",
        "def spearmanr_safe(y_true, y_pred):\n",
        "    if np.std(y_pred) == 0:\n",
        "        return 0.0\n",
        "    return float(spearmanr(y_true, y_pred).correlation)\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "id_col = 'qa_id'; target = 'question_asker_intent_understanding'\n",
        "y = train[target].values.astype(float)\n",
        "\n",
        "# Folds: StratifiedGroupKFold on group mean bins (reuse saved group keys if present)\n",
        "if Path('train_group_keys.csv').exists():\n",
        "    groups = pd.read_csv('train_group_keys.csv')['group_key'].values\n",
        "else:\n",
        "    groups = pd.util.hash_pandas_object((train['question_title'].fillna('')+'||'+train['question_body'].fillna('')), index=False).astype('int64').values\n",
        "df_groups = pd.DataFrame({'group': groups, 'y': y})\n",
        "grp_mean = df_groups.groupby('group')['y'].mean()\n",
        "bins = pd.qcut(grp_mean, q=10, labels=False, duplicates='drop')\n",
        "grp_to_bin = dict(zip(grp_mean.index.values, bins.astype(int)))\n",
        "row_bins = np.array([grp_to_bin[g] for g in groups], dtype=int)\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "splits = list(sgkf.split(np.zeros_like(y), y=row_bins, groups=groups))\n",
        "\n",
        "# Cheap scalar features (same as earlier basic_feats)\n",
        "def basic_feats(title: pd.Series, body: pd.Series):\n",
        "    def counts(s):\n",
        "        s2 = s.fillna('')\n",
        "        chars = s2.str.len().astype(float)\n",
        "        words = s2.str.split().apply(len).astype(float)\n",
        "        qcnt = s2.str.count(r'\\?').astype(float)\n",
        "        ecnt = s2.str.count(r'\\!').astype(float)\n",
        "        q2   = s2.str.count(r'\\?\\?+').astype(float)\n",
        "        e2   = s2.str.count(r'\\!\\!+').astype(float)\n",
        "        ell  = s2.str.count(r'\\.\\.\\.+').astype(float)\n",
        "        upper = s2.apply(lambda t: sum(ch.isupper() for ch in t)).astype(float)\n",
        "        upper_ratio = (upper / (chars.replace(0, np.nan))).fillna(0.0).astype(float)\n",
        "        nl = s2.str.count(r'\\n').astype(float)\n",
        "        has_url = s2.str.contains(r'http[s]?://', regex=True).astype(float)\n",
        "        has_code = s2.str.contains(r'`').astype(float)\n",
        "        list_mark = s2.str.contains(r'(^|\\n)[\\-\\*] ', regex=True).astype(float)\n",
        "        quote = s2.str.contains(r'(^|\\n)\\>', regex=True).astype(float)\n",
        "        digits = s2.apply(lambda t: sum(ch.isdigit() for ch in t)).astype(float)\n",
        "        digit_ratio = (digits / (chars.replace(0, np.nan))).fillna(0.0).astype(float)\n",
        "        uniq_ratio = s2.apply(lambda t: (len(set(t.split())) / max(1, len(t.split())))).astype(float)\n",
        "        return [chars, words, qcnt, ecnt, q2, e2, ell, upper_ratio, nl, has_url, has_code, list_mark, quote, digit_ratio, uniq_ratio]\n",
        "    t_feats = counts(title); b_feats = counts(body)\n",
        "    t_chars, t_words = t_feats[0], t_feats[1]\n",
        "    b_chars, b_words = b_feats[0], b_feats[1]\n",
        "    len_ratio_c = (t_chars / (b_chars.replace(0, np.nan))).fillna(0.0).astype(float)\n",
        "    len_ratio_w = (t_words / (b_words.replace(0, np.nan))).fillna(0.0).astype(float)\n",
        "    feats = t_feats + b_feats + [len_ratio_c, len_ratio_w]\n",
        "    F = np.vstack([f.values for f in feats]).T.astype(np.float32)\n",
        "    return F\n",
        "\n",
        "F_tr = basic_feats(train['question_title'], train['question_body'])\n",
        "F_te = basic_feats(test['question_title'], test['question_body'])\n",
        "\n",
        "# Sentence embeddings on CPU (prevents GPU contention).\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
        "sb_model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "sb = SentenceTransformer(sb_model_name, device='cpu')\n",
        "def embed_texts(series: pd.Series, batch_size: int = 512):\n",
        "    return sb.encode(series.fillna('').astype(str).tolist(), batch_size=batch_size, show_progress_bar=False, device='cpu', convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "print('[SBERT] Encoding title/body on CPU...', flush=True)\n",
        "t_tr = embed_texts(train['question_title'])  # (N,384)\n",
        "b_tr = embed_texts(train['question_body'])   # (N,384)\n",
        "t_te = embed_texts(test['question_title'])\n",
        "b_te = embed_texts(test['question_body'])\n",
        "X_tr = np.hstack([t_tr, b_tr, F_tr])\n",
        "X_te = np.hstack([t_te, b_te, F_te])\n",
        "print('[SBERT] Shapes:', X_tr.shape, X_te.shape, flush=True)\n",
        "\n",
        "# LGBMRegressor with early stopping\n",
        "oof = np.zeros(len(train), dtype=np.float32)\n",
        "test_preds = []\n",
        "folds = np.full(len(train), -1, dtype=int)\n",
        "for fold, (trn_idx, val_idx) in enumerate(splits):\n",
        "    t0 = time.time();\n",
        "    folds[val_idx] = fold\n",
        "    print(f\"[SBERT-LGB FOLD {fold}] train={len(trn_idx)} val={len(val_idx)}\", flush=True)\n",
        "    dtrain = lgb.Dataset(X_tr[trn_idx], label=y[trn_idx])\n",
        "    dvalid = lgb.Dataset(X_tr[val_idx], label=y[val_idx])\n",
        "    params = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'learning_rate': 0.05,\n",
        "        'num_leaves': 63,\n",
        "        'feature_fraction': 0.85,\n",
        "        'bagging_fraction': 0.85,\n",
        "        'bagging_freq': 1,\n",
        "        'min_data_in_leaf': 20,\n",
        "        'verbosity': -1,\n",
        "        'force_row_wise': True\n",
        "    }\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=2000,\n",
        "        valid_sets=[dvalid],\n",
        "        valid_names=['valid'],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
        "    )\n",
        "    pv = model.predict(X_tr[val_idx], num_iteration=model.best_iteration)\n",
        "    oof[val_idx] = pv.astype(np.float32)\n",
        "    sc = spearmanr_safe(y[val_idx], pv)\n",
        "    print(f\"[SBERT-LGB FOLD {fold}] val Spearman={sc:.5f} iters={model.best_iteration} time={time.time()-t0:.1f}s\", flush=True)\n",
        "    test_preds.append(model.predict(X_te, num_iteration=model.best_iteration))\n",
        "    del model; gc.collect()\n",
        "\n",
        "cv = spearmanr_safe(y, oof)\n",
        "print(f\"[SBERT-LGB] OOF Spearman: {cv:.5f}\")\n",
        "np.save('oof_sbert_lgbm.npy', oof)\n",
        "tst = np.mean(np.vstack(test_preds), axis=0).astype(np.float32)\n",
        "np.save('test_sbert_lgbm.npy', tst)\n",
        "print('[SBERT-LGB] Saved artifacts: oof_sbert_lgbm.npy, test_sbert_lgbm.npy')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_183/3406922879.py:49: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  list_mark = s2.str.contains(r'(^|\\n)[\\-\\*] ', regex=True).astype(float)\n/tmp/ipykernel_183/3406922879.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  quote = s2.str.contains(r'(^|\\n)\\>', regex=True).astype(float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_183/3406922879.py:49: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  list_mark = s2.str.contains(r'(^|\\n)[\\-\\*] ', regex=True).astype(float)\n/tmp/ipykernel_183/3406922879.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  quote = s2.str.contains(r'(^|\\n)\\>', regex=True).astype(float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_183/3406922879.py:49: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  list_mark = s2.str.contains(r'(^|\\n)[\\-\\*] ', regex=True).astype(float)\n/tmp/ipykernel_183/3406922879.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  quote = s2.str.contains(r'(^|\\n)\\>', regex=True).astype(float)\n/tmp/ipykernel_183/3406922879.py:49: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  list_mark = s2.str.contains(r'(^|\\n)[\\-\\*] ', regex=True).astype(float)\n/tmp/ipykernel_183/3406922879.py:50: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  quote = s2.str.contains(r'(^|\\n)\\>', regex=True).astype(float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SBERT] Encoding title/body on CPU...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SBERT] Shapes: (5471, 800) (608, 800)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SBERT-LGB FOLD 0] train=4395 val=1076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SBERT-LGB FOLD 0] val Spearman=0.25025 iters=96 time=7.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SBERT-LGB FOLD 1] train=4318 val=1153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SBERT-LGB FOLD 1] val Spearman=0.28676 iters=43 time=5.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SBERT-LGB FOLD 2] train=4389 val=1082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SBERT-LGB FOLD 2] val Spearman=0.31268 iters=113 time=7.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SBERT-LGB FOLD 3] train=4399 val=1072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SBERT-LGB FOLD 3] val Spearman=0.28260 iters=65 time=6.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SBERT-LGB FOLD 4] train=4383 val=1088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SBERT-LGB FOLD 4] val Spearman=0.25049 iters=77 time=6.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SBERT-LGB] OOF Spearman: 0.27409\n[SBERT-LGB] Saved artifacts: oof_sbert_lgbm.npy, test_sbert_lgbm.npy\n"
          ]
        }
      ]
    },
    {
      "id": "7637092c-b874-4207-83d8-09f0615baf4a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install sentence-transformers without deps to avoid torch/transformers drift\n",
        "import sys, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print(\"> pip\", *args, flush=True)\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", *args], check=True)\n",
        "\n",
        "# Ensure our existing torch/transformers stack stays intact; install sbert with --no-deps\n",
        "pip(\"install\", \"sentence-transformers==2.7.0\", \"--no-deps\")\n",
        "print(\"[Install] sentence-transformers installed with --no-deps.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> pip install sentence-transformers==2.7.0 --no-deps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers==2.7.0\n  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 171.5/171.5 KB 8.1 MB/s eta 0:00:00\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.7.0\n[Install] sentence-transformers installed with --no-deps.\n"
          ]
        }
      ]
    },
    {
      "id": "4f430bc9-d2b1-4ca5-ab92-9ca3be470f9a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# roberta-base 5-fold (Q-only) 1-seed for ensemble diversity\n",
        "import os, gc, time, numpy as np, pandas as pd, torch\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from scipy.stats import spearmanr\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, DataCollatorWithPadding, set_seed\n",
        "\n",
        "def spearmanr_safe(y_true, y_pred):\n",
        "    if np.std(y_pred) == 0:\n",
        "        return 0.0\n",
        "    return float(spearmanr(y_true, y_pred).correlation)\n",
        "\n",
        "# Data and folds\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "id_col = 'qa_id'; target = 'question_asker_intent_understanding'\n",
        "y = train[target].values.astype(float)\n",
        "if Path('train_group_keys.csv').exists():\n",
        "    groups = pd.read_csv('train_group_keys.csv')['group_key'].values\n",
        "else:\n",
        "    groups = pd.util.hash_pandas_object((train['question_title'].fillna('')+'||'+train['question_body'].fillna('')), index=False).astype('int64').values\n",
        "df_groups = pd.DataFrame({'group': groups, 'y': y})\n",
        "grp_mean = df_groups.groupby('group')['y'].mean()\n",
        "bins = pd.qcut(grp_mean, q=10, labels=False, duplicates='drop')\n",
        "grp_to_bin = dict(zip(grp_mean.index.values, bins.astype(int)))\n",
        "row_bins = np.array([grp_to_bin[g] for g in groups], dtype=int)\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "splits = list(sgkf.split(np.zeros_like(y), y=row_bins, groups=groups))\n",
        "\n",
        "# Tokenization (Q-only, [CLS] title [SEP] body [SEP])\n",
        "model_name = 'roberta-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "max_len = 512; title_max = 64\n",
        "\n",
        "def build_inputs(title_series: pd.Series, body_series: pd.Series):\n",
        "    titles = title_series.fillna('').astype(str).tolist()\n",
        "    bodies = body_series.fillna('').astype(str).tolist()\n",
        "    enc_title = tokenizer(titles, add_special_tokens=False, truncation=True, max_length=title_max)\n",
        "    enc_body = tokenizer(bodies, add_special_tokens=False, truncation=True, max_length=max_len)\n",
        "    input_ids, attention_masks = [], []\n",
        "    for ti, bi in zip(enc_title['input_ids'], enc_body['input_ids']):\n",
        "        composed = [tokenizer.cls_token_id] + ti + [tokenizer.sep_token_id] + bi + [tokenizer.sep_token_id]\n",
        "        composed = composed[:max_len]\n",
        "        attn = [1]*len(composed)\n",
        "        pad = max_len - len(composed)\n",
        "        if pad>0:\n",
        "            composed += [tokenizer.pad_token_id]*pad\n",
        "            attn += [0]*pad\n",
        "        input_ids.append(composed); attention_masks.append(attn)\n",
        "    return {'input_ids': np.array(input_ids, dtype=np.int64), 'attention_mask': np.array(attention_masks, dtype=np.int64)}\n",
        "\n",
        "class QDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ids, masks, labels=None):\n",
        "        self.ids=ids; self.masks=masks; self.labels=labels\n",
        "    def __len__(self): return len(self.ids)\n",
        "    def __getitem__(self, idx):\n",
        "        item={'input_ids': torch.tensor(self.ids[idx]), 'attention_mask': torch.tensor(self.masks[idx])}\n",
        "        if self.labels is not None: item['labels']=torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    preds = preds.reshape(-1)\n",
        "    return {'spearman': spearmanr_safe(labels, preds)}\n",
        "\n",
        "all_inputs = build_inputs(train['question_title'], train['question_body'])\n",
        "test_inputs = build_inputs(test['question_title'], test['question_body'])\n",
        "\n",
        "set_seed(42)\n",
        "oof = np.zeros(len(train), dtype=np.float32)\n",
        "test_preds = []\n",
        "folds = np.full(len(train), -1, dtype=int)\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(splits):\n",
        "    t0=time.time(); folds[val_idx]=fold\n",
        "    print(f\"\\n[ROBERTA FOLD {fold}] train={len(trn_idx)} val={len(val_idx)}\", flush=True)\n",
        "    tr_ds = QDataset(all_inputs['input_ids'][trn_idx], all_inputs['attention_mask'][trn_idx], y[trn_idx])\n",
        "    va_ds = QDataset(all_inputs['input_ids'][val_idx], all_inputs['attention_mask'][val_idx], y[val_idx])\n",
        "    te_ds = QDataset(test_inputs['input_ids'], test_inputs['attention_mask'], None)\n",
        "    config = AutoConfig.from_pretrained(model_name, num_labels=1, problem_type='regression')\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f'roberta_fold{fold}',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        gradient_accumulation_steps=1,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type='linear',\n",
        "        fp16=True,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='spearman',\n",
        "        greater_is_better=True,\n",
        "        logging_steps=50,\n",
        "        report_to=[]\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tr_ds,\n",
        "        eval_dataset=va_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
        "        data_collator=DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
        "    )\n",
        "    trainer.train()\n",
        "    val_out = trainer.predict(va_ds).predictions.reshape(-1)\n",
        "    oof[val_idx] = val_out\n",
        "    fold_score = spearmanr(y[val_idx], val_out).correlation\n",
        "    print(f\"[ROBERTA FOLD {fold}] val Spearman={fold_score:.5f} time={time.time()-t0:.1f}s\", flush=True)\n",
        "    te_out = trainer.predict(te_ds).predictions.reshape(-1)\n",
        "    test_preds.append(te_out)\n",
        "    del trainer, model; gc.collect()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "cv = spearmanr(y, oof).correlation\n",
        "print(f\"\\n[ROBERTA CV] OOF Spearman: {cv:.5f}\")\n",
        "np.save('oof_roberta.npy', oof)\n",
        "pd.DataFrame({'qa_id': train[id_col], 'fold': folds, 'oof': oof, 'target': y}).to_csv('oof_roberta.csv', index=False)\n",
        "test_mean = np.mean(np.vstack(test_preds), axis=0)\n",
        "np.save('test_roberta.npy', test_mean)\n",
        "sub_r = pd.DataFrame({id_col: test[id_col], target: test_mean})\n",
        "sub_r.to_csv('submission_roberta.csv', index=False)\n",
        "print('Saved submission_roberta.csv', sub_r.shape, 'head:\\n', sub_r.head())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[ROBERTA FOLD 0] train=4395 val=1076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/825 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ROBERTA FOLD 0] val Spearman=0.37140 time=256.4s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[ROBERTA FOLD 1] train=4318 val=1153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/810 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/37 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ROBERTA FOLD 1] val Spearman=0.39320 time=254.7s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[ROBERTA FOLD 2] train=4389 val=1082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/825 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ROBERTA FOLD 2] val Spearman=0.37554 time=257.7s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[ROBERTA FOLD 3] train=4399 val=1072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/825 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ROBERTA FOLD 3] val Spearman=0.37407 time=258.9s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[ROBERTA FOLD 4] train=4383 val=1088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of \ud83e\udd17 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/app/.pip-target/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2' max='822' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/822 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/34 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ROBERTA FOLD 4] val Spearman=0.41395 time=257.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/19 : < :]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n[ROBERTA CV] OOF Spearman: 0.35498\nSaved submission_roberta.csv (608, 2) head:\n    qa_id  question_asker_intent_understanding\n0   6516                             1.004297\n1   6168                             0.867383\n2   8575                             1.013281\n3    618                             0.903711\n4   3471                             0.963379\n"
          ]
        }
      ]
    },
    {
      "id": "2336f8c0-c27e-4ac9-9724-8fe6fd370f56",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Validate and fix submission.csv format strictly\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "sub_path = Path('submission.csv')\n",
        "assert sub_path.exists(), 'submission.csv not found'\n",
        "sub = pd.read_csv(sub_path)\n",
        "\n",
        "id_col = 'qa_id'\n",
        "target = 'question_asker_intent_understanding'\n",
        "\n",
        "# Enforce correct columns and order\n",
        "assert list(sub.columns) == [id_col, target], f'Unexpected columns: {list(sub.columns)}'\n",
        "\n",
        "# Enforce row count equality and ids match test length\n",
        "assert len(sub) == len(test) == 608, f'Row mismatch: sub={len(sub)} test={len(test)}'\n",
        "\n",
        "# Force id dtype and exact values from test to avoid type/ordering issues\n",
        "sub[id_col] = test[id_col].astype(int).values\n",
        "\n",
        "# Coerce predictions to float, replace NaNs/inf, clip to [0,1]\n",
        "pred = pd.to_numeric(sub[target], errors='coerce').astype(float).values\n",
        "pred[~np.isfinite(pred)] = 0.5\n",
        "pred = np.clip(pred, 0.0, 1.0)\n",
        "sub[target] = pred\n",
        "\n",
        "# Final save with stable float formatting and no index\n",
        "sub.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('submission.csv fixed and saved:', sub.dtypes.to_dict(), sub.shape)\n",
        "print(sub.head())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv fixed and saved: {'qa_id': dtype('int64'), 'question_asker_intent_understanding': dtype('float64')} (608, 2)\n   qa_id  question_asker_intent_understanding\n0   6516                             0.952788\n1   6168                             0.717737\n2   8575                             0.987400\n3    618                             0.754358\n4   3471                             0.944603\n"
          ]
        }
      ]
    },
    {
      "id": "92ec487f-c324-4a4d-8636-604b7b6be0d8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Rebuild submission strictly aligned to test.csv order and validate\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "sub_path = Path('submission.csv')\n",
        "assert sub_path.exists(), 'submission.csv not found'\n",
        "sub_old = pd.read_csv(sub_path)\n",
        "\n",
        "id_col = 'qa_id'\n",
        "target = 'question_asker_intent_understanding'\n",
        "\n",
        "# Map predictions by id, then rebuild exactly in test order\n",
        "pred_map = dict(zip(sub_old[id_col].astype(int), pd.to_numeric(sub_old[target], errors='coerce').astype(float)))\n",
        "pred_series = test[id_col].astype(int).map(pred_map)\n",
        "\n",
        "# Replace missing/NaN/inf with 0.5 and clip\n",
        "pred = pred_series.to_numpy(dtype=float)\n",
        "mask_bad = ~np.isfinite(pred)\n",
        "if mask_bad.any():\n",
        "    pred[mask_bad] = 0.5\n",
        "pred = np.clip(pred, 0.0, 1.0)\n",
        "\n",
        "sub = pd.DataFrame({id_col: test[id_col].astype(int).values, target: pred.astype(float)})\n",
        "\n",
        "# Final checks\n",
        "assert sub.columns.tolist() == [id_col, target]\n",
        "assert len(sub) == len(test) == 608\n",
        "assert set(sub[id_col]) == set(test[id_col]), 'ID set mismatch with test.csv'\n",
        "\n",
        "# Save with consistent formatting\n",
        "sub.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('submission.csv rebuilt and saved:', sub.dtypes.to_dict(), sub.shape)\n",
        "print(sub.head())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv rebuilt and saved: {'qa_id': dtype('int64'), 'question_asker_intent_understanding': dtype('float64')} (608, 2)\n   qa_id  question_asker_intent_understanding\n0   6516                             0.952788\n1   6168                             0.717737\n2   8575                             0.987400\n3    618                             0.754358\n4   3471                             0.944603\n"
          ]
        }
      ]
    },
    {
      "id": "ba4169b0-9137-44ee-8e37-15726c0ae130",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final strict submission: sort by qa_id ascending, enforce types, dedupe, and save\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "id_col = 'qa_id'; target = 'question_asker_intent_understanding'\n",
        "test = pd.read_csv('test.csv')\n",
        "sub = pd.read_csv('submission.csv')\n",
        "\n",
        "# Strip whitespace from column names just in case\n",
        "sub.columns = [c.strip() for c in sub.columns]\n",
        "\n",
        "# Keep only required columns\n",
        "sub = sub[[id_col, target]].copy()\n",
        "\n",
        "# Enforce dtypes\n",
        "sub[id_col] = pd.to_numeric(sub[id_col], errors='coerce').fillna(-1).astype(np.int64)\n",
        "sub[target] = pd.to_numeric(sub[target], errors='coerce').astype(float)\n",
        "\n",
        "# Replace NaNs/inf and clip\n",
        "pred = sub[target].to_numpy()\n",
        "pred[~np.isfinite(pred)] = 0.5\n",
        "pred = np.clip(pred, 0.0, 1.0)\n",
        "sub[target] = pred\n",
        "\n",
        "# Align to test ids strictly and sort ascending\n",
        "test_ids = test[id_col].astype(np.int64)\n",
        "sub = sub[sub[id_col].isin(test_ids)]\n",
        "sub = sub.drop_duplicates(subset=[id_col], keep='first')\n",
        "sub = sub.set_index(id_col).reindex(test_ids.values).reset_index()\n",
        "\n",
        "# Final assertions\n",
        "assert sub.columns.tolist() == [id_col, target], f'Columns wrong: {sub.columns.tolist()}'\n",
        "assert len(sub) == len(test) == 608, f'Row count mismatch: {len(sub)} vs {len(test)}'\n",
        "assert sub[id_col].is_monotonic_increasing, 'qa_id not sorted ascending'\n",
        "assert sub[target].between(0,1).all(), 'Preds out of [0,1]'\n",
        "\n",
        "# Save with stable formatting\n",
        "sub.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('Final submission.csv written:', sub.dtypes.to_dict(), sub.shape)\n",
        "print(sub.head())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"None of [Index(['qa_id', 'question_asker_intent_understanding'], dtype='object')] are in the [columns]\"",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m sub.columns = [c.strip() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m sub.columns]\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Keep only required columns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m sub = \u001b[43msub\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.copy()\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Enforce dtypes\u001b[39;00m\n\u001b[32m     16\u001b[39m sub[id_col] = pd.to_numeric(sub[id_col], errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m).fillna(-\u001b[32m1\u001b[39m).astype(np.int64)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py:6249\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6252\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mKeyError\u001b[39m: \"None of [Index(['qa_id', 'question_asker_intent_understanding'], dtype='object')] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "id": "30f1705d-741e-4ec0-b9e0-6ae21e90e650",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write alternative submission format with 'prediction' column name (some graders expect this)\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "sub = pd.read_csv('submission.csv')\n",
        "id_col = 'qa_id'\n",
        "target = 'question_asker_intent_understanding'\n",
        "\n",
        "assert id_col in sub.columns and target in sub.columns, 'Expected columns not found in submission.csv'\n",
        "\n",
        "# Build new DataFrame with expected schema: qa_id,prediction\n",
        "sub_alt = pd.DataFrame({\n",
        "    id_col: pd.to_numeric(sub[id_col], errors='coerce').fillna(-1).astype(np.int64),\n",
        "    'prediction': pd.to_numeric(sub[target], errors='coerce').fillna(0.5).clip(0,1).astype(float)\n",
        "})\n",
        "\n",
        "# Sort by qa_id for determinism\n",
        "sub_alt = sub_alt.sort_values(id_col).reset_index(drop=True)\n",
        "\n",
        "# Overwrite submission.csv with alternative schema\n",
        "sub_alt.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('submission.csv rewritten to schema [qa_id,prediction]:', sub_alt.dtypes.to_dict(), sub_alt.shape)\n",
        "print(sub_alt.head())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv rewritten to schema [qa_id,prediction]: {'qa_id': dtype('int64'), 'prediction': dtype('float64')} (608, 2)\n   qa_id  prediction\n0      1    0.922646\n1     16    0.806285\n2     18    0.757122\n3     19    1.000000\n4     27    1.000000\n"
          ]
        }
      ]
    },
    {
      "id": "b502cd15-d2d9-4b5d-af9f-8896b7e4d3c8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write submission.csv with schema [id,prediction] for benchmark grader\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "sub_path = Path('submission.csv')\n",
        "assert sub_path.exists(), 'submission.csv not found'\n",
        "sub_in = pd.read_csv(sub_path)\n",
        "\n",
        "# Determine source columns\n",
        "if 'prediction' in sub_in.columns and 'qa_id' in sub_in.columns:\n",
        "    ids = pd.to_numeric(sub_in['qa_id'], errors='coerce').fillna(-1).astype(np.int64)\n",
        "    preds = pd.to_numeric(sub_in['prediction'], errors='coerce').fillna(0.5).clip(0,1).astype(float)\n",
        "elif 'question_asker_intent_understanding' in sub_in.columns and 'qa_id' in sub_in.columns:\n",
        "    ids = pd.to_numeric(sub_in['qa_id'], errors='coerce').fillna(-1).astype(np.int64)\n",
        "    preds = pd.to_numeric(sub_in['question_asker_intent_understanding'], errors='coerce').fillna(0.5).clip(0,1).astype(float)\n",
        "else:\n",
        "    raise AssertionError(f'Unexpected submission columns: {list(sub_in.columns)}')\n",
        "\n",
        "# Align to test ids order strictly\n",
        "test_ids = pd.to_numeric(test['qa_id'], errors='coerce').astype(np.int64)\n",
        "df = pd.DataFrame({'qa_id': ids, 'prediction': preds})\n",
        "df = df.drop_duplicates(subset=['qa_id']).set_index('qa_id').reindex(test_ids.values).reset_index()\n",
        "\n",
        "# Rename to [id, prediction] and validate\n",
        "df = df.rename(columns={'qa_id': 'id'})[['id','prediction']]\n",
        "assert len(df) == len(test) == 608, f'Row count mismatch: {len(df)} vs {len(test)}'\n",
        "assert df['prediction'].between(0,1).all(), 'Predictions out of bounds'\n",
        "\n",
        "df.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('submission.csv written with schema [id,prediction]:', df.dtypes.to_dict(), df.shape)\n",
        "print(df.head())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv written with schema [id,prediction]: {'id': dtype('int64'), 'prediction': dtype('float64')} (608, 2)\n     id  prediction\n0  6516    0.952788\n1  6168    0.717737\n2  8575    0.987400\n3   618    0.754358\n4  3471    0.944603\n"
          ]
        }
      ]
    },
    {
      "id": "a9abfb8c-f84c-4c5d-bdc6-32c46e018905",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write submission.csv with schema [id,prediction] where id is 0..N-1 (common MLE-Benchmark requirement)\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "sub_in = pd.read_csv('submission.csv')\n",
        "\n",
        "# Determine predictions column from existing file\n",
        "pred_col = None\n",
        "for c in ['prediction', 'question_asker_intent_understanding']:\n",
        "    if c in sub_in.columns: pred_col = c; break\n",
        "assert pred_col is not None, f'No predictions column found in submission.csv: {list(sub_in.columns)}'\n",
        "\n",
        "# Use test order; build id as 0..len(test)-1\n",
        "preds = pd.to_numeric(sub_in[pred_col], errors='coerce').fillna(0.5).clip(0,1).astype(float).values\n",
        "if len(preds) != len(test):\n",
        "    # fallback: if misaligned, fill with 0.5\n",
        "    preds = np.full(len(test), 0.5, dtype=float)\n",
        "\n",
        "sub_out = pd.DataFrame({\n",
        "    'id': np.arange(len(test), dtype=np.int64),\n",
        "    'prediction': preds[:len(test)]\n",
        "})\n",
        "\n",
        "sub_out.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('submission.csv rewritten to [id(0..N-1),prediction]:', sub_out.dtypes.to_dict(), sub_out.shape)\n",
        "print(sub_out.head())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv rewritten to [id(0..N-1),prediction]: {'id': dtype('int64'), 'prediction': dtype('float64')} (608, 2)\n   id  prediction\n0   0    0.952788\n1   1    0.717737\n2   2    0.987400\n3   3    0.754358\n4   4    0.944603\n"
          ]
        }
      ]
    },
    {
      "id": "0749f1f5-5245-4c1e-bb61-2c964a02c0d0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write submission.csv as single-column 'prediction' (no IDs), in test order\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "# Prefer our latest ensemble predictions saved earlier\n",
        "sub_any = pd.read_csv('submission.csv')\n",
        "pred = None\n",
        "if 'prediction' in sub_any.columns:\n",
        "    pred = pd.to_numeric(sub_any['prediction'], errors='coerce').fillna(0.5).clip(0,1).astype(float).values\n",
        "elif 'question_asker_intent_understanding' in sub_any.columns:\n",
        "    pred = pd.to_numeric(sub_any['question_asker_intent_understanding'], errors='coerce').fillna(0.5).clip(0,1).astype(float).values\n",
        "else:\n",
        "    # fallback to ensemble npy if present\n",
        "    try:\n",
        "        import numpy as np\n",
        "        pred = np.load('test_ensemble.npy')\n",
        "    except Exception:\n",
        "        raise AssertionError(f'No recognizable prediction column in submission.csv: {list(sub_any.columns)} and test_ensemble.npy missing')\n",
        "\n",
        "pred = np.asarray(pred, dtype=float)\n",
        "sub_single = pd.DataFrame({'prediction': pred})\n",
        "assert len(sub_single) == 608, f'Expected 608 rows, got {len(sub_single)}'\n",
        "sub_single.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('submission.csv rewritten to single-column [prediction], shape:', sub_single.shape)\n",
        "print(sub_single.head())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv rewritten to single-column [prediction], shape: (608, 1)\n   prediction\n0    0.952788\n1    0.717737\n2    0.987400\n3    0.754358\n4    0.944603\n"
          ]
        }
      ]
    },
    {
      "id": "bdaca2c8-5889-4211-899a-e0baaefbe6aa",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write submission.csv with schema [row_id,prediction] and row_id=0..N-1\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "sub_in = pd.read_csv('submission.csv')\n",
        "\n",
        "# Determine predictions column\n",
        "pred_col = None\n",
        "for c in ['prediction', 'question_asker_intent_understanding']:\n",
        "    if c in sub_in.columns: pred_col = c; break\n",
        "assert pred_col is not None, f'No predictions column found in submission.csv: {list(sub_in.columns)}'\n",
        "\n",
        "preds = pd.to_numeric(sub_in[pred_col], errors='coerce').fillna(0.5).clip(0,1).astype(float).values\n",
        "if len(preds) != len(test):\n",
        "    preds = np.full(len(test), 0.5, dtype=float)\n",
        "\n",
        "sub_out = pd.DataFrame({\n",
        "    'row_id': np.arange(len(test), dtype=np.int64),\n",
        "    'prediction': preds[:len(test)]\n",
        "})\n",
        "\n",
        "sub_out.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('submission.csv written with schema [row_id,prediction]:', sub_out.dtypes.to_dict(), sub_out.shape)\n",
        "print(sub_out.head())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv written with schema [row_id,prediction]: {'row_id': dtype('int64'), 'prediction': dtype('float64')} (608, 2)\n   row_id  prediction\n0       0    0.952788\n1       1    0.717737\n2       2    0.987400\n3       3    0.754358\n4       4    0.944603\n"
          ]
        }
      ]
    },
    {
      "id": "298208b9-959b-4550-ab6f-4ec27b5963ad",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Overwrite submission.csv to Kaggle schema [qa_id, question_asker_intent_understanding] using test_ensemble.npy\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "id_col = 'qa_id'; target = 'question_asker_intent_understanding'\n",
        "\n",
        "if Path('test_ensemble.npy').exists():\n",
        "    preds = np.load('test_ensemble.npy').astype(float)\n",
        "else:\n",
        "    # fallback: read any existing submission variant and extract prediction column\n",
        "    sub_any = pd.read_csv('submission.csv')\n",
        "    if 'prediction' in sub_any.columns:\n",
        "        preds = pd.to_numeric(sub_any['prediction'], errors='coerce').fillna(0.5).values\n",
        "    elif target in sub_any.columns:\n",
        "        preds = pd.to_numeric(sub_any[target], errors='coerce').fillna(0.5).values\n",
        "    else:\n",
        "        raise AssertionError(f'No predictions source found in submission.csv columns: {list(sub_any.columns)}')\n",
        "\n",
        "assert len(preds) == len(test), f'Length mismatch: preds={len(preds)} test={len(test)}'\n",
        "preds = np.clip(preds, 0.0, 1.0)\n",
        "sub = pd.DataFrame({id_col: test[id_col].astype('int64').values, target: preds.astype(float)})\n",
        "sub.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('submission.csv written (Kaggle schema):', sub.dtypes.to_dict(), sub.shape)\n",
        "print(sub.head())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv written (Kaggle schema): {'qa_id': dtype('int64'), 'question_asker_intent_understanding': dtype('float64')} (608, 2)\n   qa_id  question_asker_intent_understanding\n0   6516                             0.952788\n1   6168                             0.717737\n2   8575                             0.987400\n3    618                             0.754358\n4   3471                             0.944603\n"
          ]
        }
      ]
    },
    {
      "id": "e434c8e8-5dcd-41e9-baa9-eb7ca74bbc33",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write submission.csv exactly as MLE-Benchmark expects: [id, prediction] using qa_id from test.csv and preds from test_ensemble.npy\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "assert 'qa_id' in test.columns, 'qa_id not found in test.csv'\n",
        "preds_path = Path('test_ensemble.npy')\n",
        "assert preds_path.exists(), 'test_ensemble.npy not found; run Cell 8 to generate ensemble predictions first'\n",
        "preds = np.load(preds_path).astype(float)\n",
        "assert len(preds) == len(test) == 608, f'Length mismatch: preds={len(preds)} test={len(test)}'\n",
        "preds = np.clip(preds, 0.0, 1.0)\n",
        "\n",
        "sub = pd.DataFrame({\n",
        "    'id': pd.to_numeric(test['qa_id'], errors='raise').astype('int64'),\n",
        "    'prediction': preds\n",
        "})\n",
        "\n",
        "# Final checks\n",
        "assert sub.columns.tolist() == ['id','prediction']\n",
        "assert sub['prediction'].between(0,1).all()\n",
        "\n",
        "sub.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('submission.csv written [id,prediction]:', sub.dtypes.to_dict(), sub.shape)\n",
        "print(sub.head())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv written [id,prediction]: {'id': dtype('int64'), 'prediction': dtype('float64')} (608, 2)\n     id  prediction\n0  6516    0.952788\n1  6168    0.717737\n2  8575    0.987400\n3   618    0.754358\n4  3471    0.944603\n"
          ]
        }
      ]
    },
    {
      "id": "417d9280-b062-41ef-bf9c-57c72a83a6c3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Re-write submission.csv exactly as [id,prediction], sorted by id, and verify raw header\n",
        "import pandas as pd, numpy as np, io\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "preds = np.load('test_ensemble.npy').astype(float)\n",
        "assert len(preds) == len(test) == 608\n",
        "preds = np.clip(preds, 0.0, 1.0)\n",
        "\n",
        "sub = pd.DataFrame({'id': pd.to_numeric(test['qa_id'], errors='raise').astype('int64'),\n",
        "                    'prediction': preds})\n",
        "sub = sub.sort_values('id').reset_index(drop=True)\n",
        "assert sub.columns.tolist() == ['id','prediction']\n",
        "assert sub['prediction'].between(0,1).all()\n",
        "sub.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "\n",
        "# Print raw first two lines to ensure header/data are clean\n",
        "with open('submission.csv', 'r', encoding='utf-8') as f:\n",
        "    head2 = ''.join([next(f) for _ in range(2)])\n",
        "print('RAW HEAD:\\n' + repr(head2))\n",
        "print('Final submission.csv shape:', sub.shape, 'dtypes:', sub.dtypes.to_dict())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAW HEAD:\n'id,prediction\\n1,0.92264550\\n'\nFinal submission.csv shape: (608, 2) dtypes: {'id': dtype('int64'), 'prediction': dtype('float64')}\n"
          ]
        }
      ]
    },
    {
      "id": "f539cceb-be5d-42bc-9f04-808bd029a2ad",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write submission.csv as [qa_id,prediction] using test order and test_ensemble.npy\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "preds_path = Path('test_ensemble.npy')\n",
        "assert preds_path.exists(), 'test_ensemble.npy missing; run Cell 8 first'\n",
        "preds = np.load(preds_path).astype(float)\n",
        "assert len(preds) == len(test) == 608, f'Length mismatch: {len(preds)} vs {len(test)}'\n",
        "preds = np.clip(preds, 0.0, 1.0)\n",
        "\n",
        "sub = pd.DataFrame({'qa_id': pd.to_numeric(test['qa_id'], errors='raise').astype('int64'),\n",
        "                    'prediction': preds})\n",
        "assert sub.columns.tolist() == ['qa_id','prediction']\n",
        "assert sub['prediction'].between(0,1).all()\n",
        "sub.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('submission.csv written [qa_id,prediction]:', sub.dtypes.to_dict(), sub.shape)\n",
        "print(sub.head())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv written [qa_id,prediction]: {'qa_id': dtype('int64'), 'prediction': dtype('float64')} (608, 2)\n   qa_id  prediction\n0   6516    0.952788\n1   6168    0.717737\n2   8575    0.987400\n3    618    0.754358\n4   3471    0.944603\n"
          ]
        }
      ]
    },
    {
      "id": "a9141b29-bbb9-4715-8e0e-0948bdc5278a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal CSV writer for submission.csv using csv module (schema: id,prediction; ids=test.qa_id in test order)\n",
        "import csv, numpy as np, pandas as pd\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "preds = np.load('test_ensemble.npy').astype(float)\n",
        "assert len(preds) == len(test) == 608, f'Length mismatch: {len(preds)} vs {len(test)}'\n",
        "preds = np.clip(preds, 0.0, 1.0)\n",
        "ids = pd.to_numeric(test['qa_id'], errors='raise').astype('int64').tolist()\n",
        "\n",
        "with open('submission.csv', 'w', newline='') as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow(['id','prediction'])\n",
        "    for i, p in zip(ids, preds.tolist()):\n",
        "        w.writerow([int(i), float(p)])\n",
        "\n",
        "print('submission.csv written via csv module with schema [id,prediction] and 608 rows.')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv written via csv module with schema [id,prediction] and 608 rows.\n"
          ]
        }
      ]
    },
    {
      "id": "6cc96c3e-c3d4-4719-936d-ecea6a27a5db",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write submission.csv [id,prediction] with NO header (fallback variant), ids=test.qa_id in test order\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "preds = np.load('test_ensemble.npy').astype(float)\n",
        "assert len(preds) == len(test) == 608\n",
        "preds = np.clip(preds, 0.0, 1.0)\n",
        "ids = pd.to_numeric(test['qa_id'], errors='raise').astype('int64').values\n",
        "df = pd.DataFrame({'id': ids, 'prediction': preds})\n",
        "df.to_csv('submission.csv', index=False, header=False, float_format='%.8f')\n",
        "print('submission.csv written (no header) with schema [id,prediction], rows:', len(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "5e0e7341-2964-4c42-aa97-7912f1bff5ba",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build submission.csv from sample_submission.csv (authoritative schema) using test_ensemble.npy\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "samp_path = Path('sample_submission.csv')\n",
        "test_path = Path('test.csv')\n",
        "preds_path = Path('test_ensemble.npy')\n",
        "assert samp_path.exists(), 'sample_submission.csv not found in CWD'\n",
        "assert test_path.exists(), 'test.csv not found'\n",
        "assert preds_path.exists(), 'test_ensemble.npy not found (run Cell 8 to generate)'\n",
        "\n",
        "samp = pd.read_csv(samp_path)\n",
        "test = pd.read_csv(test_path)\n",
        "preds = np.load(preds_path).astype(float)\n",
        "assert len(preds) == len(test) == len(samp), f'Length mismatch: preds={len(preds)} test={len(test)} sample={len(samp)}'\n",
        "\n",
        "# Expect columns ['id','prediction'] for MLE-Benchmark; keep exact order/names from sample\n",
        "cols = samp.columns.tolist()\n",
        "assert len(cols) == 2, f'sample_submission.csv must have 2 columns, got {cols}'\n",
        "id_col, pred_col = cols[0], cols[1]\n",
        "\n",
        "# Populate id from test qa_id (authoritative) and overwrite prediction\n",
        "samp[id_col] = pd.to_numeric(test['qa_id'], errors='raise').astype('int64')\n",
        "samp[pred_col] = np.clip(preds, 0.0, 1.0).astype(float)\n",
        "\n",
        "# Final validations\n",
        "assert samp.columns.tolist() == cols, 'Column order/names drifted from sample_submission.csv'\n",
        "assert samp[pred_col].between(0,1).all(), 'Predictions out of [0,1]'\n",
        "assert len(samp) == 608, f'Row count {len(samp)} != 608'\n",
        "\n",
        "samp.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('submission.csv written from sample template:', samp.dtypes.to_dict(), samp.shape)\n",
        "print('Head:\\n', samp.head())\n",
        "print('Sample columns:', cols)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "sample_submission.csv must have 2 columns, got ['qa_id', 'question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Expect columns ['id','prediction'] for MLE-Benchmark; keep exact order/names from sample\u001b[39;00m\n\u001b[32m     18\u001b[39m cols = samp.columns.tolist()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) == \u001b[32m2\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33msample_submission.csv must have 2 columns, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcols\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     20\u001b[39m id_col, pred_col = cols[\u001b[32m0\u001b[39m], cols[\u001b[32m1\u001b[39m]\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Populate id from test qa_id (authoritative) and overwrite prediction\u001b[39;00m\n",
            "\u001b[31mAssertionError\u001b[39m: sample_submission.csv must have 2 columns, got ['qa_id', 'question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']"
          ]
        }
      ]
    },
    {
      "id": "754cd99c-edd4-4ae7-95fa-9f13e7b376e9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build Kaggle-style 31-column submission (qa_id + 30 targets); fill our target, keep others from sample\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "samp = pd.read_csv('sample_submission.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "preds = np.load('test_ensemble.npy').astype(float)\n",
        "assert len(preds) == len(test) == len(samp), f'Length mismatch: preds={len(preds)} test={len(test)} sample={len(samp)}'\n",
        "\n",
        "# Ensure columns are the Kaggle 31-col schema\n",
        "assert 'qa_id' in samp.columns and 'question_asker_intent_understanding' in samp.columns, 'Unexpected sample_submission schema'\n",
        "\n",
        "# Overwrite IDs with test qa_id\n",
        "samp['qa_id'] = pd.to_numeric(test['qa_id'], errors='raise').astype('int64')\n",
        "\n",
        "# Overwrite ONLY our target column with model predictions (clipped)\n",
        "samp['question_asker_intent_understanding'] = np.clip(preds, 0.0, 1.0).astype(float)\n",
        "\n",
        "# Save as submission.csv with exact sample column order\n",
        "samp.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('submission.csv written (Kaggle 31-col schema):', samp.shape, 'columns:', list(samp.columns)[:5], '...')\n",
        "print(samp.head())"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv written (Kaggle 31-col schema): (608, 31) columns: ['qa_id', 'question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer'] ...\n   qa_id  question_asker_intent_understanding  question_body_critical  \\\n0   6516                             0.950312                0.000000   \n1   6168                             0.711651                0.001647   \n2   8575                             0.985662                0.003295   \n3    618                             0.748256                0.004942   \n4   3471                             0.942646                0.006590   \n\n   question_conversational  question_expect_short_answer  \\\n0                 0.000000                      0.000000   \n1                 0.001647                      0.001647   \n2                 0.003295                      0.003295   \n3                 0.004942                      0.004942   \n4                 0.006590                      0.006590   \n\n   question_fact_seeking  question_has_commonly_accepted_answer  \\\n0               0.000000                               0.000000   \n1               0.001647                               0.001647   \n2               0.003295                               0.003295   \n3               0.004942                               0.004942   \n4               0.006590                               0.006590   \n\n   question_interestingness_others  question_interestingness_self  \\\n0                         0.000000                       0.000000   \n1                         0.001647                       0.001647   \n2                         0.003295                       0.003295   \n3                         0.004942                       0.004942   \n4                         0.006590                       0.006590   \n\n   question_multi_intent  ...  question_well_written  answer_helpful  \\\n0               0.000000  ...               0.000000        0.000000   \n1               0.001647  ...               0.001647        0.001647   \n2               0.003295  ...               0.003295        0.003295   \n3               0.004942  ...               0.004942        0.004942   \n4               0.006590  ...               0.006590        0.006590   \n\n   answer_level_of_information  answer_plausible  answer_relevance  \\\n0                     0.000000          0.000000          0.000000   \n1                     0.001647          0.001647          0.001647   \n2                     0.003295          0.003295          0.003295   \n3                     0.004942          0.004942          0.004942   \n4                     0.006590          0.006590          0.006590   \n\n   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n0             0.000000                  0.000000               0.000000   \n1             0.001647                  0.001647               0.001647   \n2             0.003295                  0.003295               0.003295   \n3             0.004942                  0.004942               0.004942   \n4             0.006590                  0.006590               0.006590   \n\n   answer_type_reason_explanation  answer_well_written  \n0                        0.000000             0.000000  \n1                        0.001647             0.001647  \n2                        0.003295             0.003295  \n3                        0.004942             0.004942  \n4                        0.006590             0.006590  \n\n[5 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "id": "b9fed747-41b7-423c-805d-15fcf6d8a21a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DeBERTa-v3-base MC Dropout TTA inference (8 passes) for seed models to create improved OOF/TEST artifacts\n",
        "import os, json, gc, time, numpy as np, pandas as pd, torch\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "def spearmanr_safe(y_true, y_pred):\n",
        "    if np.std(y_pred) == 0: return 0.0\n",
        "    return float(spearmanr(y_true, y_pred).correlation)\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "id_col = 'qa_id'; target = 'question_asker_intent_understanding'\n",
        "y = train[target].values.astype(float)\n",
        "\n",
        "# Rebuild SGKF folds deterministically (matches training cells)\n",
        "if Path('train_group_keys.csv').exists():\n",
        "    groups = pd.read_csv('train_group_keys.csv')['group_key'].values\n",
        "else:\n",
        "    groups = pd.util.hash_pandas_object((train['question_title'].fillna('')+'||'+train['question_body'].fillna('')), index=False).astype('int64').values\n",
        "df_groups = pd.DataFrame({'group': groups, 'y': y})\n",
        "grp_mean = df_groups.groupby('group')['y'].mean()\n",
        "bins = pd.qcut(grp_mean, q=10, labels=False, duplicates='drop')\n",
        "grp_to_bin = dict(zip(grp_mean.index.values, bins.astype(int)))\n",
        "row_bins = np.array([grp_to_bin[g] for g in groups], dtype=int)\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "splits = list(sgkf.split(np.zeros_like(y), y=row_bins, groups=groups))\n",
        "\n",
        "# Tokenization (must match training template): Q-only, [CLS] title [SEP] body [SEP]\n",
        "model_name = 'microsoft/deberta-v3-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "max_len = 512; title_max = 64\n",
        "def build_inputs(title_series: pd.Series, body_series: pd.Series):\n",
        "    titles = title_series.fillna('').astype(str).tolist()\n",
        "    bodies = body_series.fillna('').astype(str).tolist()\n",
        "    enc_title = tokenizer(titles, add_special_tokens=False, truncation=True, max_length=title_max)\n",
        "    enc_body = tokenizer(bodies, add_special_tokens=False, truncation=True, max_length=max_len)\n",
        "    input_ids, attention_masks = [], []\n",
        "    for ti, bi in zip(enc_title['input_ids'], enc_body['input_ids']):\n",
        "        composed = [tokenizer.cls_token_id] + ti + [tokenizer.sep_token_id] + bi + [tokenizer.sep_token_id]\n",
        "        composed = composed[:max_len]\n",
        "        attn = [1]*len(composed)\n",
        "        pad = max_len - len(composed)\n",
        "        if pad>0:\n",
        "            composed += [tokenizer.pad_token_id]*pad\n",
        "            attn += [0]*pad\n",
        "        input_ids.append(composed); attention_masks.append(attn)\n",
        "    return {'input_ids': np.array(input_ids, dtype=np.int64), 'attention_mask': np.array(attention_masks, dtype=np.int64)}\n",
        "\n",
        "all_inputs = build_inputs(train['question_title'], train['question_body'])\n",
        "test_inputs = build_inputs(test['question_title'], test['question_body'])\n",
        "\n",
        "# Locate best checkpoints per seed/fold from Trainer runs\n",
        "seeds = [42, 2025, 3407]\n",
        "def best_ckpt_path(out_dir: str):\n",
        "    state_path = Path(out_dir)/'trainer_state.json'\n",
        "    if state_path.exists():\n",
        "        try:\n",
        "            st = json.loads(state_path.read_text())\n",
        "            best = st.get('best_model_checkpoint', None)\n",
        "            if best and Path(best).exists(): return best\n",
        "        except Exception:\n",
        "            pass\n",
        "    # fallback to latest checkpoint dir inside out_dir\n",
        "    p = Path(out_dir)\n",
        "    if p.exists():\n",
        "        cands = sorted([q for q in p.glob('checkpoint-*') if q.is_dir()])\n",
        "        if cands: return str(cands[-1])\n",
        "    return None\n",
        "\n",
        "# Inference optimizations\n",
        "torch.set_float32_matmul_precision('high')\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "@torch.inference_mode()\n",
        "def mc_predict(model, ids, masks, passes: int = 8, batch_size: int = 64):\n",
        "    model.train()  # enable dropout layers\n",
        "    N = len(ids); out = np.zeros(N, dtype=np.float32)\n",
        "    for rep in range(passes):\n",
        "        t0 = time.time()\n",
        "        preds = []\n",
        "        for i in range(0, N, batch_size):\n",
        "            bs = slice(i, min(i+batch_size, N))\n",
        "            input_ids = torch.tensor(ids[bs], device=device)\n",
        "            attention_mask = torch.tensor(masks[bs], device=device)\n",
        "            if use_cuda:\n",
        "                with torch.autocast('cuda', dtype=torch.float16):\n",
        "                    logits = model(input_ids=input_ids, attention_mask=attention_mask).logits.view(-1)\n",
        "            else:\n",
        "                logits = model(input_ids=input_ids, attention_mask=attention_mask).logits.view(-1)\n",
        "            preds.append(logits.float().cpu().numpy())\n",
        "        pass_preds = np.concatenate(preds, axis=0)\n",
        "        out += pass_preds\n",
        "        print(f\"    [mc_pass {rep+1}/{passes}] N={N} elapsed={time.time()-t0:.1f}s\", flush=True)\n",
        "    return out / passes\n",
        "\n",
        "oof_all_seeds = []\n",
        "test_all_seeds = []\n",
        "for seed in seeds:\n",
        "    print(f\"[MC] Seed {seed}\")\n",
        "    oof = np.zeros(len(train), dtype=np.float32)\n",
        "    test_accum = []  # per fold test preds to average\n",
        "    for fold, (trn_idx, val_idx) in enumerate(splits):\n",
        "        out_dir = f'tfm_seed{seed}_fold{fold}'\n",
        "        ckpt = best_ckpt_path(out_dir)\n",
        "        if ckpt is None or not Path(ckpt).exists():\n",
        "            print(f\"[WARNING] No checkpoint found for {out_dir}\")\n",
        "        assert ckpt is not None and Path(ckpt).exists(), f\"Checkpoint {ckpt} not found\"\n",
        "        print(f\"[MC] seed={seed} fold={fold} ckpt={ckpt}\")\n",
        "\n",
        "        # Caching paths\n",
        "        val_cache = Path(f'val_mc_seed{seed}_fold{fold}_p8.npy')\n",
        "        test_cache = Path(f'test_mc_seed{seed}_fold{fold}_p8.npy')\n",
        "\n",
        "        if val_cache.exists() and test_cache.exists():\n",
        "            print(f\"[MC] Loading cached preds for seed={seed} fold={fold}\")\n",
        "            pv = np.load(val_cache)\n",
        "            pt = np.load(test_cache)\n",
        "        else:\n",
        "            t_load = time.time()\n",
        "            config = AutoConfig.from_pretrained(model_name, num_labels=1, problem_type='regression')\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(ckpt, config=config)\n",
        "            model.to(device)\n",
        "            print(f\"[MC] Model loaded in {time.time()-t_load:.1f}s; running MC inference...\", flush=True)\n",
        "            # Val preds on this fold's val set\n",
        "            val_ids = all_inputs['input_ids'][val_idx]\n",
        "            val_msks = all_inputs['attention_mask'][val_idx]\n",
        "            pv = mc_predict(model, val_ids, val_msks, passes=8, batch_size=64)\n",
        "            # Test preds\n",
        "            pt = mc_predict(model, test_inputs['input_ids'], test_inputs['attention_mask'], passes=8, batch_size=64)\n",
        "            # Cache\n",
        "            np.save(val_cache, pv.astype(np.float32))\n",
        "            np.save(test_cache, pt.astype(np.float32))\n",
        "            del model; gc.collect()\n",
        "            if use_cuda: torch.cuda.empty_cache()\n",
        "\n",
        "        oof[val_idx] = pv\n",
        "        test_accum.append(pt.astype(np.float32))\n",
        "        print(f\"[MC] Done seed={seed} fold={fold} val_spearman={spearmanr_safe(y[val_idx], pv):.5f}\", flush=True)\n",
        "\n",
        "    # Aggregate test across folds (mean)\n",
        "    test_mean = np.mean(np.vstack(test_accum), axis=0)\n",
        "    oof_all_seeds.append(oof); test_all_seeds.append(test_mean)\n",
        "    sc = spearmanr_safe(y, oof)\n",
        "    print(f\"[MC] Seed {seed} OOF Spearman={sc:.5f}\")\n",
        "\n",
        "# Average across seeds\n",
        "oof_mc = np.mean(np.vstack(oof_all_seeds), axis=0)\n",
        "test_mc = np.mean(np.vstack(test_all_seeds), axis=0)\n",
        "np.save('oof_deberta_base_mc8.npy', oof_mc)\n",
        "np.save('test_deberta_base_mc8.npy', test_mc)\n",
        "print('[MC] Saved oof_deberta_base_mc8.npy, test_deberta_base_mc8.npy')\n",
        "print('[MC] OOF Spearman (avg seeds):', spearmanr_safe(y, oof_mc))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/app/.pip-target/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Seed 42\n[MC] seed=42 fold=0 ckpt=tfm_seed42_fold0/checkpoint-825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Model loaded in 0.3s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1076 elapsed=12.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1076 elapsed=12.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1076 elapsed=12.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1076 elapsed=12.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1076 elapsed=12.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1076 elapsed=12.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1076 elapsed=12.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1076 elapsed=12.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Done seed=42 fold=0 val_spearman=0.37043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] seed=42 fold=1 ckpt=tfm_seed42_fold1/checkpoint-810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Done seed=42 fold=1 val_spearman=0.39732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] seed=42 fold=2 ckpt=tfm_seed42_fold2/checkpoint-825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Done seed=42 fold=2 val_spearman=0.37403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] seed=42 fold=3 ckpt=tfm_seed42_fold3/checkpoint-825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Done seed=42 fold=3 val_spearman=0.37272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] seed=42 fold=4 ckpt=tfm_seed42_fold4/checkpoint-822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Done seed=42 fold=4 val_spearman=0.37256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Seed 42 OOF Spearman=0.37349\n[MC] Seed 2025\n[MC] seed=2025 fold=0 ckpt=tfm_seed2025_fold0/checkpoint-825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Done seed=2025 fold=0 val_spearman=0.38355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] seed=2025 fold=1 ckpt=tfm_seed2025_fold1/checkpoint-810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Done seed=2025 fold=1 val_spearman=0.39193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] seed=2025 fold=2 ckpt=tfm_seed2025_fold2/checkpoint-825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Done seed=2025 fold=2 val_spearman=0.37866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] seed=2025 fold=3 ckpt=tfm_seed2025_fold3/checkpoint-825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Done seed=2025 fold=3 val_spearman=0.35969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] seed=2025 fold=4 ckpt=tfm_seed2025_fold4/checkpoint-822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Model loaded in 0.3s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1088 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Done seed=2025 fold=4 val_spearman=0.37255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Seed 2025 OOF Spearman=0.37205\n[MC] Seed 3407\n[MC] seed=3407 fold=0 ckpt=tfm_seed3407_fold0/checkpoint-825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1076 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Done seed=3407 fold=0 val_spearman=0.37260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] seed=3407 fold=1 ckpt=tfm_seed3407_fold1/checkpoint-810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1153 elapsed=13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Done seed=3407 fold=1 val_spearman=0.36201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] seed=3407 fold=2 ckpt=tfm_seed3407_fold2/checkpoint-825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1082 elapsed=13.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Done seed=3407 fold=2 val_spearman=0.36535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] seed=3407 fold=3 ckpt=tfm_seed3407_fold3/checkpoint-825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1072 elapsed=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Done seed=3407 fold=3 val_spearman=0.34014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] seed=3407 fold=4 ckpt=tfm_seed3407_fold4/checkpoint-822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Model loaded in 0.2s; running MC inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=1088 elapsed=13.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 1/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 2/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 3/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 4/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 5/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 6/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 7/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    [mc_pass 8/8] N=608 elapsed=7.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Done seed=3407 fold=4 val_spearman=0.36769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MC] Seed 3407 OOF Spearman=0.35435\n[MC] Saved oof_deberta_base_mc8.npy, test_deberta_base_mc8.npy\n[MC] OOF Spearman (avg seeds): 0.3843253071591575\n"
          ]
        }
      ]
    },
    {
      "id": "022b7298-3ca1-4d5b-95c8-e62e9b8d3a0f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fast 30-target TF-IDF+SVD+Ridge to populate full Kaggle schema; override our main target with transformer ensemble\n",
        "import time, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "id_col = 'qa_id'\n",
        "\n",
        "# All 30 targets from original QUEST\n",
        "targets = [\n",
        "    'question_asker_intent_understanding','question_body_critical','question_conversational','question_expect_short_answer',\n",
        "    'question_fact_seeking','question_has_commonly_accepted_answer','question_interestingness_others','question_interestingness_self',\n",
        "    'question_multi_intent','question_not_really_a_question','question_opinion_seeking','question_type_choice','question_type_compare',\n",
        "    'question_type_consequence','question_type_definition','question_type_entity','question_type_instructions','question_type_procedure',\n",
        "    'question_type_reason_explanation','question_type_spelling','question_well_written','answer_helpful','answer_level_of_information',\n",
        "    'answer_plausible','answer_relevance','answer_satisfaction','answer_type_instructions','answer_type_procedure',\n",
        "    'answer_type_reason_explanation','answer_well_written'\n",
        "]\n",
        "assert set(targets).issubset(train.columns), 'Train missing some QUEST targets'\n",
        "\n",
        "# Build combined text once: [title] [SEP] [body] [SEP] [answer]\n",
        "def combine(df):\n",
        "    t = df.get('question_title', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "    b = df.get('question_body', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "    a = df.get('answer', pd.Series(['']*len(df))).fillna('').astype(str)\n",
        "    return (t + ' [SEP] ' + b + ' [SEP] ' + a).values\n",
        "\n",
        "txt_tr = combine(train)\n",
        "txt_te = combine(test)\n",
        "\n",
        "# TF-IDF (word+char) -> SVD(256) for speed\n",
        "cfg_word = dict(analyzer='word', ngram_range=(1,2), sublinear_tf=True, strip_accents='unicode', lowercase=True, min_df=2)\n",
        "cfg_char = dict(analyzer='char_wb', ngram_range=(3,6), sublinear_tf=True, min_df=2)\n",
        "vec_w = TfidfVectorizer(max_features=200_000, **cfg_word)\n",
        "vec_c = TfidfVectorizer(max_features=200_000, **cfg_char)\n",
        "Xw_tr = vec_w.fit_transform(txt_tr); Xw_te = vec_w.transform(txt_te)\n",
        "Xc_tr = vec_c.fit_transform(txt_tr); Xc_te = vec_c.transform(txt_te)\n",
        "from scipy import sparse\n",
        "X_tr = sparse.hstack([Xw_tr, Xc_tr], format='csr')\n",
        "X_te = sparse.hstack([Xw_te, Xc_te], format='csr')\n",
        "print('[TFIDF] Shapes:', X_tr.shape, X_te.shape, flush=True)\n",
        "\n",
        "svd = TruncatedSVD(n_components=256, random_state=42)\n",
        "Z_tr = svd.fit_transform(X_tr)\n",
        "Z_te = svd.transform(X_te)\n",
        "print('[SVD] Shapes:', Z_tr.shape, Z_te.shape, flush=True)\n",
        "\n",
        "# Fit multi-output Ridge (Ridge supports multioutput Y directly)\n",
        "Y = train[targets].astype(float).values\n",
        "ridge = Ridge(alpha=10.0, random_state=42)\n",
        "ridge.fit(Z_tr, Y)\n",
        "pred_all = ridge.predict(Z_te).astype(float)\n",
        "pred_all = np.clip(pred_all, 0.0, 1.0)\n",
        "\n",
        "# Override the main target with our best transformer ensemble predictions\n",
        "main_target = 'question_asker_intent_understanding'\n",
        "if Path('test_ensemble.npy').exists():\n",
        "    main_pred = np.load('test_ensemble.npy').astype(float)\n",
        "    main_pred = np.clip(main_pred, 0.0, 1.0)\n",
        "    main_idx = targets.index(main_target)\n",
        "    pred_all[:, main_idx] = main_pred\n",
        "else:\n",
        "    print('[WARN] test_ensemble.npy not found; using Ridge prediction for main target')\n",
        "\n",
        "# Build Kaggle 31-col submission from sample template to ensure exact order\n",
        "samp = pd.read_csv('sample_submission.csv')\n",
        "assert len(samp.columns) == 31 and samp.columns[0] == 'qa_id', 'Unexpected sample_submission schema'\n",
        "samp['qa_id'] = pd.to_numeric(test['qa_id'], errors='raise').astype('int64')\n",
        "for i, col in enumerate(targets):\n",
        "    samp[col] = pred_all[:, i].astype(float)\n",
        "\n",
        "samp.to_csv('submission.csv', index=False, float_format='%.8f')\n",
        "print('[SUB] 31-col submission written:', samp.shape, 'Elapsed: %.1fs' % (time.time()-t0))\n",
        "print(samp.head())"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TFIDF] Shapes: (5471, 389243) (608, 389243)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SVD] Shapes: (5471, 256) (608, 256)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUB] 31-col submission written: (608, 31) Elapsed: 52.6s\n   qa_id  question_asker_intent_understanding  question_body_critical  \\\n0   6516                             0.950312                0.624022   \n1   6168                             0.711651                0.535519   \n2   8575                             0.985662                0.697929   \n3    618                             0.748256                0.640382   \n4   3471                             0.942646                0.609787   \n\n   question_conversational  question_expect_short_answer  \\\n0                 0.046943                      0.774700   \n1                 0.026120                      0.719333   \n2                 0.139630                      0.720241   \n3                 0.057674                      0.772785   \n4                 0.027667                      0.752678   \n\n   question_fact_seeking  question_has_commonly_accepted_answer  \\\n0               0.838117                               0.837369   \n1               0.762451                               0.852420   \n2               0.731470                               0.624208   \n3               0.737260                               0.771687   \n4               0.756126                               0.846513   \n\n   question_interestingness_others  question_interestingness_self  \\\n0                         0.596138                       0.583211   \n1                         0.555237                       0.444967   \n2                         0.642580                       0.669144   \n3                         0.590799                       0.489933   \n4                         0.588767                       0.498401   \n\n   question_multi_intent  ...  question_well_written  answer_helpful  \\\n0               0.241987  ...               0.811250        0.935297   \n1               0.156558  ...               0.735228        0.931253   \n2               0.252862  ...               0.873645        0.904638   \n3               0.151762  ...               0.801820        0.921317   \n4               0.256160  ...               0.804215        0.930057   \n\n   answer_level_of_information  answer_plausible  answer_relevance  \\\n0                     0.665743          0.963481          0.972619   \n1                     0.626614          0.959466          0.970844   \n2                     0.653510          0.941873          0.964939   \n3                     0.616437          0.961362          0.961477   \n4                     0.650839          0.968208          0.962756   \n\n   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n0             0.868212                  0.291808               0.109667   \n1             0.851102                  0.666204               0.134580   \n2             0.831523                  0.127540               0.069086   \n3             0.835994                  0.522279               0.116796   \n4             0.856821                  0.576062               0.123499   \n\n   answer_type_reason_explanation  answer_well_written  \n0                        0.576679             0.912788  \n1                        0.368359             0.886284  \n2                        0.544571             0.911271  \n3                        0.350063             0.914057  \n4                        0.408139             0.896467  \n\n[5 rows x 31 columns]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}