{
  "cells": [
    {
      "id": "8f1e04d4-0b51-4fe5-9e43-619aef66280d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "# Clean conflicting system and user packages\n",
        "pip_targets = [\n",
        "    '/app/.pip-target',\n",
        "    os.path.expanduser('~/.pip-target')\n",
        "]\n",
        "for target in pip_targets:\n",
        "    if os.path.exists(target):\n",
        "        print(f'Removing pip target: {target}')\n",
        "        shutil.rmtree(target, ignore_errors=True)\n",
        "\n",
        "# Create local writable directory for packages in CWD\n",
        "LOCAL_PKGS = Path.cwd() / 'pkgs'\n",
        "LOCAL_PKGS.mkdir(exist_ok=True)\n",
        "print(f'Local pkgs dir: {LOCAL_PKGS}')\n",
        "\n",
        "# Install PyTorch cu121 only if not already installed\n",
        "if not (LOCAL_PKGS / 'torch').exists():\n",
        "    print('Installing PyTorch cu121...')\n",
        "    subprocess.check_call([\n",
        "        sys.executable, '-m', 'pip', 'install',\n",
        "        '--index-url', 'https://download.pytorch.org/whl/cu121',\n",
        "        '--extra-index-url', 'https://pypi.org/simple',\n",
        "        '--target', str(LOCAL_PKGS),\n",
        "        '--no-cache-dir',\n",
        "        'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1'\n",
        "    ])\n",
        "else:\n",
        "    print('PyTorch already installed in local pkgs, skipping.')\n",
        "\n",
        "# Install transformers and all key dependencies to local dir (with --upgrade)\n",
        "print('Installing transformers and deps...')\n",
        "hf_deps = [\n",
        "    'transformers==4.44.2',\n",
        "    'tokenizers==0.19.1',\n",
        "    'huggingface-hub==0.24.6',\n",
        "    'safetensors>=0.4.3',\n",
        "    'accelerate==0.34.2',\n",
        "    'datasets==2.21.0',\n",
        "    'evaluate==0.4.2',\n",
        "    'sentencepiece==0.2.0',\n",
        "    'protobuf<=4.36.0',\n",
        "    'scikit-learn'\n",
        "]\n",
        "subprocess.check_call([\n",
        "    sys.executable, '-m', 'pip', 'install',\n",
        "    '--target', str(LOCAL_PKGS),\n",
        "    '--no-cache-dir',\n",
        "    '--upgrade',\n",
        "    '--no-deps',\n",
        "    *hf_deps\n",
        "])\n",
        "# Install minimal deps with compatible versions, including certifi\n",
        "subprocess.check_call([\n",
        "    sys.executable, '-m', 'pip', 'install',\n",
        "    '--target', str(LOCAL_PKGS),\n",
        "    '--no-cache-dir',\n",
        "    '--upgrade',\n",
        "    'numpy>=1.17',\n",
        "    'filelock',\n",
        "    'fsspec==2024.6.1',\n",
        "    'requests',\n",
        "    'tqdm',\n",
        "    'pyyaml',\n",
        "    'packaging',\n",
        "    'regex!=2019.12.17',\n",
        "    'certifi>=2023.7.22',\n",
        "    'urllib3<3,>=1.21.1',\n",
        "    'charset-normalizer<4,>=2'\n",
        "])\n",
        "\n",
        "# Prepend local pkgs to sys.path (before system paths)\n",
        "if str(LOCAL_PKGS) not in sys.path:\n",
        "    sys.path.insert(0, str(LOCAL_PKGS))\n",
        "print('Added local pkgs to sys.path')\n",
        "\n",
        "# Patch to fix import issues (before sanity check)\n",
        "import importlib\n",
        "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
        "os.environ['TRANSFORMERS_NO_FLAX'] = '1'\n",
        "os.environ['HF_HOME'] = str(LOCAL_PKGS / 'hf_cache')\n",
        "os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'\n",
        "os.environ['CURL_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'\n",
        "\n",
        "# Clean sys.path further\n",
        "sys.path = [p for p in sys.path if '.pip-target' not in p]\n",
        "sys.path.insert(0, str(LOCAL_PKGS))\n",
        "\n",
        "# Purge modules more aggressively\n",
        "modules_to_purge = ['transformers', 'tokenizers', 'huggingface_hub', 'safetensors', 'accelerate', 'requests', 'certifi', 'urllib3', 'charset_normalizer', 'idna', 'torch']\n",
        "for mod in modules_to_purge:\n",
        "    if mod in sys.modules:\n",
        "        del sys.modules[mod]\n",
        "    for k in list(sys.modules.keys()):\n",
        "        if k.startswith(mod + '.') or k == mod:\n",
        "            del sys.modules[k]\n",
        "importlib.invalidate_caches()\n",
        "\n",
        "# Re-import torch after purge to ensure local version\n",
        "import torch\n",
        "print(f'torch: {torch.__version__} built CUDA: {getattr(torch.version, \"cuda\", None)}')\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "assert torch.cuda.is_available(), 'CUDA not available'\n",
        "assert str(getattr(torch.version, 'cuda', '')).startswith('12.1'), f'Wrong CUDA: {torch.version.cuda}'\n",
        "print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "# Test torch.utils.checkpoint\n",
        "import torch.utils.checkpoint\n",
        "print('torch.utils.checkpoint imported successfully.')\n",
        "\n",
        "import transformers\n",
        "print('transformers version:', transformers.__version__)\n",
        "print('transformers file:', transformers.__file__)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
        "print('Tokenizer loaded successfully from local install.')\n",
        "print('Installation complete and verified (model load skipped to avoid checkpoint issue).')\n",
        "\n",
        "# Clean up memory\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing pip target: /app/.pip-target\nRemoving pip target: /app/.pip-target\nLocal pkgs dir: /var/lib/simon/agent_run_states/google-quest-challenge-20250928-161614/pkgs\nPyTorch already installed in local pkgs, skipping.\nInstalling transformers and deps...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.5/9.5 MB 118.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers==0.19.1\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.6/3.6 MB 167.2 MB/s eta 0:00:00\nCollecting huggingface-hub==0.24.6\n  Downloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 417.5/417.5 KB 511.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors>=0.4.3\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 528.3 MB/s eta 0:00:00\nCollecting accelerate==0.34.2\n  Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 324.4/324.4 KB 501.4 MB/s eta 0:00:00\nCollecting datasets==2.21.0\n  Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 527.3/527.3 KB 277.7 MB/s eta 0:00:00\nCollecting evaluate==0.4.2\n  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 84.1/84.1 KB 179.8 MB/s eta 0:00:00\nCollecting sentencepiece==0.2.0\n  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.3/1.3 MB 160.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf<=4.36.0\n  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 294.9/294.9 KB 455.6 MB/s eta 0:00:00\nCollecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 345.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: sentencepiece, transformers, tokenizers, scikit-learn, safetensors, protobuf, huggingface-hub, evaluate, datasets, accelerate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed accelerate-0.34.2 datasets-2.21.0 evaluate-0.4.2 huggingface-hub-0.24.6 protobuf-4.25.8 safetensors-0.6.2 scikit-learn-1.7.2 sentencepiece-0.2.0 tokenizers-0.19.1 transformers-4.44.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.17\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 251.5 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec==2024.6.1\n  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 177.6/177.6 KB 461.0 MB/s eta 0:00:00\nCollecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 416.9 MB/s eta 0:00:00\nCollecting tqdm\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 444.1 MB/s eta 0:00:00\nCollecting pyyaml\n  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 806.6/806.6 KB 444.2 MB/s eta 0:00:00\nCollecting packaging\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 412.9 MB/s eta 0:00:00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex!=2019.12.17\n  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 KB 505.1 MB/s eta 0:00:00\nCollecting certifi>=2023.7.22\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 467.2 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 452.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset-normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 480.9 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 422.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: urllib3, tqdm, regex, pyyaml, packaging, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed certifi-2025.8.3 charset-normalizer-3.4.3 filelock-3.19.1 fsspec-2024.6.1 idna-3.10 numpy-1.26.4 packaging-25.0 pyyaml-6.0.3 regex-2025.9.18 requests-2.32.5 tqdm-4.67.1 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added local pkgs to sys.path\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.4.1+cu121 built CUDA: 12.1\nCUDA available: True\nGPU: NVIDIA A10-24Q\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.utils.checkpoint imported successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/var/lib/simon/agent_run_states/google-quest-challenge-20250928-161614/pkgs/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers version: 4.44.2\ntransformers file: /var/lib/simon/agent_run_states/google-quest-challenge-20250928-161614/pkgs/transformers/__init__.py\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded successfully from local install.\nInstallation complete and verified (model load skipped to avoid checkpoint issue).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/var/lib/simon/agent_run_states/google-quest-challenge-20250928-161614/pkgs/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "id": "5ef3c5cf-bf7b-44f9-b969-04065eceb8f2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Data Loading and Preparation for Transformer Baseline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import spearmanr\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import gc\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Define target columns (30)\n",
        "target_cols = [\n",
        "    'question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n",
        "    'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n",
        "    'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n",
        "    'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n",
        "    'question_type_compare', 'question_type_consequence', 'question_type_definition',\n",
        "    'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n",
        "    'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n",
        "    'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
        "    'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n",
        "    'answer_type_reason_explanation', 'answer_well_written'\n",
        "]\n",
        "\n",
        "y_train = train[target_cols].values\n",
        "print('Train shape:', train.shape, 'Targets shape:', y_train.shape)\n",
        "\n",
        "# Groups for CV: by url (stricter to avoid leakage)\n",
        "train['url_group'] = pd.factorize(train['url'])[0]\n",
        "groups = train['url_group'].values\n",
        "print('Number of url groups:', len(np.unique(groups)))\n",
        "\n",
        "# Tokenizer\n",
        "MODEL_NAME = 'roberta-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "MAX_LEN = 512\n",
        "sep = f' {tokenizer.sep_token} '\n",
        "\n",
        "# Prepare input texts using tokenizer's sep_token\n",
        "train['input_text'] = train['question_title'] + sep + train['question_body'] + sep + train['answer']\n",
        "test['input_text'] = test['question_title'] + sep + test['question_body'] + sep + test['answer']\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_texts(texts):\n",
        "    return tokenizer(\n",
        "        texts.tolist(),\n",
        "        max_length=MAX_LEN,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt',\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "# Example tokenization (full will be in dataset)\n",
        "train_enc = tokenize_texts(train['input_text'])\n",
        "print('Tokenized shapes:', {k: v.shape for k, v in train_enc.items()})\n",
        "\n",
        "# Custom Dataset (fixed: squeeze single-sample tensors)\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, texts, targets=None, tokenizer=None, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts.iloc[idx])\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        item = {\n",
        "            key: val.squeeze(0)\n",
        "            for key, val in encoding.items()\n",
        "        }\n",
        "        if self.targets is not None:\n",
        "            item['targets'] = torch.tensor(self.targets[idx], dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "# NaN-safe Spearman scorer\n",
        "def column_spearman_scorer(y_true, y_pred):\n",
        "    scores = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        s, _ = spearmanr(y_true[:, i], y_pred[:, i])\n",
        "        scores.append(0.0 if np.isnan(s) else s)\n",
        "    return np.mean(scores)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (5471, 41) Targets shape: (5471, 30)\nNumber of url groups: 3392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/var/lib/simon/agent_run_states/google-quest-challenge-20250928-161614/pkgs/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized shapes: {'input_ids': torch.Size([5471, 512]), 'attention_mask': torch.Size([5471, 512])}\n"
          ]
        }
      ]
    },
    {
      "id": "68d64f47-d974-499e-9770-e037d1c77856",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch.nn as nn\n",
        "from transformers import RobertaModel, get_linear_schedule_with_warmup\n",
        "from torch.amp import autocast, GradScaler\n",
        "import os\n",
        "import torch.nn.utils\n",
        "\n",
        "# Set env to avoid tokenizers warning\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "class RobertaRegression(nn.Module):\n",
        "    def __init__(self, model_name, num_targets=30, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.encoder = RobertaModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.regressor = nn.Linear(768, num_targets)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden = outputs.last_hidden_state\n",
        "        mask = attention_mask.unsqueeze(-1).float()\n",
        "        pooled = (last_hidden * mask).sum(1) / mask.sum(1).clamp(min=1e-6)\n",
        "        pooled = self.dropout(pooled)\n",
        "        logits = self.regressor(pooled)\n",
        "        return logits\n",
        "\n",
        "# Training setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "NUM_FOLDS = 5\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "sigmoid = nn.Sigmoid()\n",
        "\n",
        "# Initialize OOF and test preds\n",
        "oof_preds = np.zeros((len(train), len(target_cols)))\n",
        "test_preds = np.zeros((len(test), len(target_cols)))\n",
        "\n",
        "# GroupKFold by url_group\n",
        "gkf = GroupKFold(n_splits=NUM_FOLDS)\n",
        "\n",
        "for fold, (tr_idx, val_idx) in enumerate(gkf.split(train, y_train, groups=groups)):\n",
        "    print(f'Fold {fold+1}/{NUM_FOLDS}')\n",
        "    tr_texts = train.iloc[tr_idx]['input_text']\n",
        "    val_texts = train.iloc[val_idx]['input_text']\n",
        "    tr_targets = y_train[tr_idx]\n",
        "    val_targets = y_train[val_idx]\n",
        "    \n",
        "    # Datasets and loaders\n",
        "    tr_dataset = QADataset(tr_texts, tr_targets, tokenizer, MAX_LEN)\n",
        "    val_dataset = QADataset(val_texts, val_targets, tokenizer, MAX_LEN)\n",
        "    test_dataset = QADataset(test['input_text'], None, tokenizer, MAX_LEN)\n",
        "    \n",
        "    tr_loader = DataLoader(tr_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "    \n",
        "    # Model\n",
        "    model = RobertaRegression(MODEL_NAME).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    \n",
        "    # Scheduler\n",
        "    num_training_steps = len(tr_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1 * num_training_steps),\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "    \n",
        "    # Training loop with FP16\n",
        "    scaler = GradScaler('cuda')\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        for batch in tr_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            targets = batch['targets'].to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            with autocast('cuda'):\n",
        "                logits = model(input_ids, attention_mask)\n",
        "                loss = criterion(logits, targets)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(tr_loader):.4f}')\n",
        "    \n",
        "    # Validation preds\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            with autocast('cuda'):\n",
        "                logits = model(input_ids, attention_mask)\n",
        "            preds = torch.clip(sigmoid(logits), 0, 1).cpu().numpy()\n",
        "            val_preds.append(preds)\n",
        "    val_preds = np.vstack(val_preds)\n",
        "    oof_preds[val_idx] = val_preds\n",
        "    \n",
        "    # Fold score\n",
        "    fold_score = column_spearman_scorer(val_targets, val_preds)\n",
        "    print(f'Fold {fold+1} Spearman: {fold_score:.4f}')\n",
        "    \n",
        "    # Test preds\n",
        "    test_fold_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            with autocast('cuda'):\n",
        "                logits = model(input_ids, attention_mask)\n",
        "            preds = torch.clip(sigmoid(logits), 0, 1).cpu().numpy()\n",
        "            test_fold_preds.append(preds)\n",
        "    test_fold = np.vstack(test_fold_preds)\n",
        "    test_preds += test_fold / NUM_FOLDS\n",
        "    \n",
        "    # Clean up\n",
        "    del model, tr_loader, val_loader, test_loader\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Overall CV score\n",
        "cv_score = column_spearman_scorer(y_train, oof_preds)\n",
        "print(f'\\nMean CV Spearman: {cv_score:.4f}')\n",
        "\n",
        "# Save OOF and test preds\n",
        "np.save('roberta_oof.npy', oof_preds)\n",
        "np.save('roberta_test.npy', test_preds)\n",
        "\n",
        "# Clip test preds\n",
        "test_preds = np.clip(test_preds, 0, 1)\n",
        "\n",
        "# Submission\n",
        "sub_df = pd.DataFrame(test_preds, columns=target_cols)\n",
        "sub_df.insert(0, 'qa_id', test['qa_id'])\n",
        "sub_df.to_csv('submission_roberta.csv', index=False)\n",
        "print('\\nRoBERTa submission saved. Shape:', sub_df.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\nFold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.4412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Loss: 0.3793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Loss: 0.3673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Spearman: 0.3608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.4390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Loss: 0.3788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Loss: 0.3674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 Spearman: 0.3475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.4467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Loss: 0.3826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Loss: 0.3704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 Spearman: 0.3425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_3012/1776503429.py:97: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n  s, _ = spearmanr(y_true[:, i], y_pred[:, i])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.4467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Loss: 0.3814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Loss: 0.3692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Spearman: 0.3586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.4396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Loss: 0.3782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Loss: 0.3667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 Spearman: 0.3552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nMean CV Spearman: 0.3514\n\nRoBERTa submission saved. Shape: (608, 31)\n"
          ]
        }
      ]
    },
    {
      "id": "75b90f32-7ddf-495e-acf3-5dcb1ad610ed",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick check after kernel restart\n",
        "import transformers, tokenizers, torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "print('transformers', transformers.__version__, '| tokenizers', tokenizers.__version__)\n",
        "print('torch', torch.__version__, 'CUDA', torch.version.cuda, 'GPU OK?', torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "efb6a927-b521-4e2b-9231-6981bb873639",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blending RoBERTa + TF-IDF for Final Submission (with weight tuning)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Load saved predictions\n",
        "roberta_oof = np.load('roberta_oof.npy')\n",
        "roberta_test = np.load('roberta_test.npy')\n",
        "tfidf_oof = np.load('tfidf_oof_v2.npy')\n",
        "tfidf_test = np.load('tfidf_test_v2.npy')\n",
        "\n",
        "# Load y_train for CV scoring\n",
        "train = pd.read_csv('train.csv')\n",
        "target_cols = [\n",
        "    'question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n",
        "    'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n",
        "    'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n",
        "    'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n",
        "    'question_type_compare', 'question_type_consequence', 'question_type_definition',\n",
        "    'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n",
        "    'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n",
        "    'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
        "    'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n",
        "    'answer_type_reason_explanation', 'answer_well_written'\n",
        "]\n",
        "y_train = train[target_cols].values\n",
        "\n",
        "# Compute individual CV scores\n",
        "def column_spearman_scorer(y_true, y_pred):\n",
        "    scores = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        s, _ = spearmanr(y_true[:, i], y_pred[:, i])\n",
        "        scores.append(0.0 if np.isnan(s) else s)\n",
        "    return np.mean(scores)\n",
        "\n",
        "roberta_cv = column_spearman_scorer(y_train, roberta_oof)\n",
        "tfidf_cv = column_spearman_scorer(y_train, tfidf_oof)\n",
        "print(f'RoBERTa CV: {roberta_cv:.4f}')\n",
        "print(f'TF-IDF CV: {tfidf_cv:.4f}')\n",
        "\n",
        "# Tune blend weight: grid search on OOF for best w_roberta (0.6 to 0.95)\n",
        "weights = np.arange(0.60, 0.96, 0.05)\n",
        "best_weight = 0.85\n",
        "best_cv = -1\n",
        "for w_roberta in weights:\n",
        "    w_tfidf = 1 - w_roberta\n",
        "    blend_oof = w_roberta * roberta_oof + w_tfidf * tfidf_oof\n",
        "    blend_oof = np.clip(blend_oof, 0, 1)\n",
        "    blend_cv = column_spearman_scorer(y_train, blend_oof)\n",
        "    print(f'w_roberta={w_roberta:.2f}, CV: {blend_cv:.4f}')\n",
        "    if blend_cv > best_cv:\n",
        "        best_cv = blend_cv\n",
        "        best_weight = w_roberta\n",
        "\n",
        "print(f'\\nBest w_roberta: {best_weight:.2f}, Best CV: {best_cv:.4f}')\n",
        "\n",
        "# Use best weights for test blend\n",
        "w_roberta = best_weight\n",
        "w_tfidf = 1 - w_roberta\n",
        "blend_test = w_roberta * roberta_test + w_tfidf * tfidf_test\n",
        "blend_test = np.clip(blend_test, 0, 1)\n",
        "\n",
        "# If best CV >= 0.39597 (Silver), use for submission\n",
        "test = pd.read_csv('test.csv')\n",
        "sub_df = pd.DataFrame(blend_test, columns=target_cols)\n",
        "sub_df.insert(0, 'qa_id', test['qa_id'])\n",
        "sub_df.to_csv('submission.csv', index=False)\n",
        "print(f'\\nBlended submission saved with w_roberta={w_roberta:.2f}. Shape: {sub_df.shape}')\n",
        "if best_cv >= 0.39597:\n",
        "    print('CV meets Silver threshold (>=0.39597). Ready for submit_final_answer.')\n",
        "else:\n",
        "    print('CV below Silver. Consider pure RoBERTa or further improvements.')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa CV: 0.3514\nTF-IDF CV: 0.2984\nw_roberta=0.60, CV: 0.3640\nw_roberta=0.65, CV: 0.3661\nw_roberta=0.70, CV: 0.3675\nw_roberta=0.75, CV: 0.3679\nw_roberta=0.80, CV: 0.3672\nw_roberta=0.85, CV: 0.3654\nw_roberta=0.90, CV: 0.3622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_roberta=0.95, CV: 0.3576\n\nBest w_roberta: 0.75, Best CV: 0.3679\n\nBlended submission saved with w_roberta=0.75. Shape: (608, 31)\nCV below Silver. Consider pure RoBERTa or further improvements.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}