{
  "cells": [
    {
      "id": "0fc19e83-32f1-44d7-b0e4-d093caabfc8d",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spooky Author Identification: Plan\n",
        "\n",
        "Goals:\n",
        "- Establish a strong baseline fast and iterate to medal.\n",
        "- Metric: multi-class log loss; target: author (3 classes).\n",
        "\n",
        "Environment & Discipline:\n",
        "- Verify GPU availability immediately (nvidia-smi). If unavailable, exit.\n",
        "- Single, deterministic CV: StratifiedKFold (n_splits=5, shuffle=True, seed=42).\n",
        "- Cache vectorized matrices (TF-IDF) once; slice per fold.\n",
        "- Log progress and timing per fold; save OOF and test logits for ensembling.\n",
        "\n",
        "Baseline v1:\n",
        "- Text-only linear models with TF-IDF:\n",
        "  - Word n-grams (1\u20132), Character n-grams (3\u20135).\n",
        "  - Model: Logistic Regression (saga or liblinear), and LinearSVC+Platt or SGDClassifier(log).\n",
        "  - NB-SVM style log-count ratio variant for comparison.\n",
        "- Expect strong baseline (classic for this comp) with CV logloss ~0.27\u20130.30; push to \u22640.27 for silver.\n",
        "\n",
        "Feature Engineering v2:\n",
        "- Tune TF-IDF ranges, min_df, sublinear_tf, normalization.\n",
        "- Combine word + char spaces (FeatureUnion or hstack).\n",
        "- Add simple lexical features (length, punctuation, capitalization ratios) to a tree model (CatBoost/XGBoost) and/or concatenate to linear.\n",
        "\n",
        "Modeling v2/v3:\n",
        "- Try multinomial Naive Bayes, Logistic Regression (C sweep), SGD (alpha sweep).\n",
        "- Calibrate scores (cv=5, method='isotonic' or 'sigmoid') if needed for logloss.\n",
        "- Optional: Light CatBoost text (CPU) or XGBoost on sparse (GPU) if helpful.\n",
        "\n",
        "Ensembling:\n",
        "- Blend diverse OOFs (word vs char models, NB-SVM vs LR vs SGD).\n",
        "- Weight by CV logloss; simple weighted average.\n",
        "\n",
        "Validation & Tracking:\n",
        "- Save and reuse: folds, vectorizers, OOF preds, test preds.\n",
        "- Inspect per-class confusion and confidence bins to guide tweaks.\n",
        "\n",
        "Next steps:\n",
        "1) Env check + quick data EDA (size, lengths, class balance).\n",
        "2) Implement TF-IDF (word+char) + Logistic Regression baseline with 5-fold CV, cache OOF/test.\n",
        "3) Iterate: tuning + add char/word unions; evaluate; then blend.\n",
        "4) Generate submission and aim for \u22640.27 CV.\n",
        "\n",
        "We will request expert review after environment check + baseline CV results, and before committing longer training runs."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "353c2663-119d-467c-a210-5ea00b61d858",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment check + quick EDA\n",
        "import os, sys, time, shutil, subprocess, json, math, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n",
        "\n",
        "# 1) GPU check (nvidia-smi); exit early if no GPU\n",
        "log(\"Checking GPU availability via nvidia-smi...\")\n",
        "try:\n",
        "    out = subprocess.run([\"bash\",\"-lc\",\"nvidia-smi || true\"], capture_output=True, text=True)\n",
        "    print(out.stdout)\n",
        "    if 'NVIDIA-SMI' not in out.stdout:\n",
        "        log(\"WARNING: GPU not detected. Proceeding with CPU (OK for linear TF-IDF models).\")\n",
        "    else:\n",
        "        log(\"GPU detected.\")\n",
        "except Exception as e:\n",
        "    log(f\"nvidia-smi check failed: {e}\")\n",
        "\n",
        "# 2) Data load\n",
        "t0=time.time()\n",
        "log(\"Loading train.csv and test.csv...\")\n",
        "train_path = 'train.csv'\n",
        "test_path = 'test.csv'\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "log(f\"Loaded train: {train.shape}, test: {test.shape} in {time.time()-t0:.2f}s\")\n",
        "\n",
        "# 3) Basic schema\n",
        "log(\"Train head:\")\n",
        "print(train.head(3))\n",
        "log(\"Train columns:\")\n",
        "print(train.columns.tolist())\n",
        "log(\"Null summary:\")\n",
        "print(train.isnull().mean())\n",
        "\n",
        "# 4) Target distribution\n",
        "if 'author' in train.columns:\n",
        "    vc = train['author'].value_counts().sort_index()\n",
        "    log(\"Author distribution (counts / proportions):\")\n",
        "    print(vc)\n",
        "    print((vc / vc.sum()).round(4))\n",
        "else:\n",
        "    log(\"ERROR: 'author' not found in train.\")\n",
        "\n",
        "# 5) Text length stats\n",
        "text_col = 'text' if 'text' in train.columns else None\n",
        "if text_col:\n",
        "    train['len'] = train[text_col].astype(str).str.len()\n",
        "    train['wc'] = train[text_col].astype(str).str.split().apply(len)\n",
        "    log(\"Length stats (char):\")\n",
        "    print(train['len'].describe())\n",
        "    log(\"Length stats (words):\")\n",
        "    print(train['wc'].describe())\n",
        "    log(\"Sample texts:\")\n",
        "    for i in range(3):\n",
        "        r = train.sample(1, random_state=42+i).iloc[0]\n",
        "        print(f\"id={r['id']} author={r['author']} len={r['len']} wc={r['wc']}\\n{r[text_col][:200]}...\")\n",
        "else:\n",
        "    log(\"ERROR: 'text' column not found in train.\")\n",
        "\n",
        "# 6) Sanity: test has id+text\n",
        "missing_test_cols = [c for c in ['id','text'] if c not in test.columns]\n",
        "if missing_test_cols:\n",
        "    log(f\"ERROR: test is missing columns: {missing_test_cols}\")\n",
        "else:\n",
        "    log(\"Test columns OK: ['id','text'] present.\")\n",
        "\n",
        "log(\"EDA complete. Next: implement TF-IDF + Logistic Regression baseline with 5-fold CV.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01:58:54] Checking GPU availability via nvidia-smi...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 29 01:58:54 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.06             Driver Version: 550.144.06     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A10-24Q                 On  |   00000002:00:00.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |     128MiB /  24512MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n\n[01:58:54] GPU detected.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01:58:54] Loading train.csv and test.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01:58:54] Loaded train: (17621, 3), test: (1958, 2) in 0.03s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01:58:54] Train head:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id                                               text author\n0  id06121  So I did not abandon the search until I had be...    EAP\n1  id01074  He had promised to spend some hours with me on...    MWS\n2  id12145  In the end he give him a funny kind o' thingum...    HPL\n[01:58:54] Train columns:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['id', 'text', 'author']\n[01:58:54] Null summary:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id        0.0\ntext      0.0\nauthor    0.0\ndtype: float64\n[01:58:54] Author distribution (counts / proportions):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "author\nEAP    7090\nHPL    5074\nMWS    5457\nName: count, dtype: int64\nauthor\nEAP    0.4024\nHPL    0.2880\nMWS    0.3097\nName: count, dtype: float64\n[01:58:54] Length stats (char):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    17621.000000\nmean       148.750752\nstd        107.652448\nmin         21.000000\n25%         81.000000\n50%        128.000000\n75%        191.000000\nmax       4663.000000\nName: len, dtype: float64\n[01:58:54] Length stats (words):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    17621.000000\nmean        26.682651\nstd         19.226686\nmin          2.000000\n25%         15.000000\n50%         23.000000\n75%         34.000000\nmax        861.000000\nName: wc, dtype: float64\n[01:58:54] Sample texts:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id=id22723 author=EAP len=60 wc=10\nThe frame was oval, richly gilded and filigreed in Moresque....\nid=id19992 author=HPL len=145 wc=26\nI would, of course, soon have to shift from Washington to some other southward street; for that party from the hotel would doubtless be after me....\nid=id21137 author=EAP len=168 wc=32\nJupiter and myself are going upon an expedition into the hills, upon the main land, and, in this expedition we shall need the aid of some person in whom we can confide....\n[01:58:54] Test columns OK: ['id','text'] present.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01:58:54] EDA complete. Next: implement TF-IDF + Logistic Regression baseline with 5-fold CV.\n"
          ]
        }
      ]
    },
    {
      "id": "5cea7aa9-ef6d-4afe-a84c-e9038bf244ac",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Baseline v1: TF-IDF (word 1-2 + char 3-5) + Logistic Regression (multinomial) with 5-fold CV\n",
        "import time, gc\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42\n",
        "N_FOLDS = 5\n",
        "np.random.seed(SEED)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test_text = test['text'].astype(str).values\n",
        "\n",
        "# Vectorizer configs (expert defaults)\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), min_df=2, max_df=0.9, lowercase=True,\n",
        "                   strip_accents='unicode', token_pattern=r'(?u)\\b\\w+\\b', sublinear_tf=True,\n",
        "                   smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "char_params = dict(analyzer='char', ngram_range=(3,5), min_df=2, lowercase=True,\n",
        "                   sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_fold_features(x_tr, x_val, x_test) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix]:\n",
        "    v_word = TfidfVectorizer(**word_params)\n",
        "    v_char = TfidfVectorizer(**char_params)\n",
        "    Xw_tr = v_word.fit_transform(x_tr)\n",
        "    Xw_val = v_word.transform(x_val)\n",
        "    Xw_test = v_word.transform(x_test)\n",
        "    Xc_tr = v_char.fit_transform(x_tr)\n",
        "    Xc_val = v_char.transform(x_val)\n",
        "    Xc_test = v_char.transform(x_test)\n",
        "    X_tr = sparse.hstack([Xw_tr, Xc_tr], format='csr')\n",
        "    X_val = sparse.hstack([Xw_val, Xc_val], format='csr')\n",
        "    X_te = sparse.hstack([Xw_test, Xc_test], format='csr')\n",
        "    return X_tr, X_val, X_te\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "# Prepare arrays\n",
        "classes = np.unique(y)\n",
        "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
        "oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "\n",
        "fold_losses = []\n",
        "t0_all = time.time()\n",
        "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "    t0 = time.time()\n",
        "    print(f\"[CV] Fold {fold}/{N_FOLDS} start: tr={len(tr_idx)} val={len(val_idx)}\", flush=True)\n",
        "    x_tr, x_val = X_text[tr_idx], X_text[val_idx]\n",
        "    y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "\n",
        "    # Build features per fold to avoid leakage\n",
        "    X_tr, X_val, X_te = build_fold_features(x_tr, x_val, X_test_text)\n",
        "    print(f\"[CV] Fold {fold} features: X_tr={X_tr.shape} X_val={X_val.shape} X_te={X_te.shape}\", flush=True)\n",
        "\n",
        "    # Model: Logistic Regression (multinomial) saga\n",
        "    clf = LogisticRegression(solver='saga', penalty='l2', multi_class='multinomial',\n",
        "                             C=4.0, max_iter=5000, tol=1e-3, n_jobs=-1, random_state=SEED)\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    proba_val = clf.predict_proba(X_val).astype(np.float32)\n",
        "    proba_test = clf.predict_proba(X_te).astype(np.float32)\n",
        "\n",
        "    # Map to fixed class order\n",
        "    # Ensure probabilities align by column to 'classes' order\n",
        "    clf_order = list(clf.classes_)\n",
        "    order_idx = [clf_order.index(c) for c in classes]\n",
        "    proba_val = proba_val[:, order_idx]\n",
        "    proba_test = proba_test[:, order_idx]\n",
        "\n",
        "    # Store\n",
        "    oof[val_idx] = proba_val\n",
        "    test_pred += proba_test / N_FOLDS\n",
        "\n",
        "    # Fold logloss\n",
        "    loss = log_loss(y_val, proba_val, labels=list(classes))\n",
        "    fold_losses.append(loss)\n",
        "    dt = time.time() - t0\n",
        "    print(f\"[CV] Fold {fold} logloss={loss:.5f} elapsed={dt:.2f}s\", flush=True)\n",
        "\n",
        "    # Cleanup\n",
        "    del X_tr, X_val, X_te, proba_val, proba_test, clf\n",
        "    gc.collect()\n",
        "\n",
        "oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "print(f\"[CV] OOF logloss={oof_loss:.5f}; folds={fold_losses}; total_elapsed={time.time()-t0_all:.2f}s\", flush=True)\n",
        "\n",
        "# Save artifacts\n",
        "np.save('oof_lr_wordchar.npy', oof)\n",
        "np.save('test_lr_wordchar.npy', test_pred)\n",
        "\n",
        "# Build submission with correct column order ['EAP','HPL','MWS']\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "submit_order = submit_cols  # desired order\n",
        "\n",
        "# Ensure classes cover exactly these and reorder\n",
        "assert set(classes) == set(submit_cols), f\"Classes mismatch: {classes}\"\n",
        "idx_map = [list(classes).index(c) for c in submit_order]\n",
        "probs = test_pred[:, idx_map]\n",
        "\n",
        "# Numerical safety: clip and renormalize per row\n",
        "eps = 1e-9\n",
        "probs = np.clip(probs, eps, 1 - eps)\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "sub = pd.DataFrame(probs, columns=submit_order)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(\"Saved submission.csv. Head:\\n\", sub.head(), flush=True)\n",
        "\n",
        "# Quick sanity: per-row sums \u2248 1\n",
        "row_sums = sub[submit_cols].sum(axis=1).values\n",
        "print(\"Row sums (first 5):\", row_sums[:5], flush=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 1/5 start: tr=14096 val=3525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 1 features: X_tr=(14096, 173289) X_val=(3525, 173289) X_te=(1958, 173289)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 1 logloss=0.39005 elapsed=6.35s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 2/5 start: tr=14097 val=3524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 2 features: X_tr=(14097, 174021) X_val=(3524, 174021) X_te=(1958, 174021)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 2 logloss=0.39548 elapsed=6.15s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 3/5 start: tr=14097 val=3524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 3 features: X_tr=(14097, 173478) X_val=(3524, 173478) X_te=(1958, 173478)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 3 logloss=0.40560 elapsed=6.36s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 4/5 start: tr=14097 val=3524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 4 features: X_tr=(14097, 173723) X_val=(3524, 173723) X_te=(1958, 173723)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 4 logloss=0.39183 elapsed=6.24s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 5/5 start: tr=14097 val=3524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 5 features: X_tr=(14097, 173699) X_val=(3524, 173699) X_te=(1958, 173699)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] Fold 5 logloss=0.38725 elapsed=6.23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] OOF logloss=0.39404; folds=[0.3900476689274559, 0.39547997709809957, 0.40559739327647654, 0.39182913231648786, 0.38725023417017657]; total_elapsed=31.60s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv. Head:\n         id       EAP       HPL       MWS\n0  id27251  0.023297  0.963772  0.012931\n1  id09612  0.234541  0.186852  0.578606\n2  id11943  0.031630  0.009772  0.958598\n3  id19526  0.018826  0.058122  0.923052\n4  id12931  0.076831  0.049408  0.873761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row sums (first 5): [0.99999994 1.         1.         1.         0.99999994]\n"
          ]
        }
      ]
    },
    {
      "id": "fd436f96-98e9-4808-b99b-f00703e36519",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Models v2: char-only LR (char_wb 3-6), word-only LR (1-3), SGD(word+char) + OOF-weighted blend\n",
        "import time, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, Tuple, List\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42\n",
        "N_FOLDS = 5\n",
        "np.random.seed(SEED)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test_text = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols), f\"Classes mismatch: {classes}\"\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "def cv_model_single_vectorizer(vectorizer: TfidfVectorizer,\n",
        "                               build_on_each_fold: bool,\n",
        "                               clf_builder,\n",
        "                               clf_param_grid: List,\n",
        "                               name: str) -> Tuple[np.ndarray, np.ndarray, float, Dict]:\n",
        "    t_all = time.time()\n",
        "    best = dict(loss=1e9, params=None, oof=None, test=None)\n",
        "    for params in clf_param_grid:\n",
        "        oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "        test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "        fold_losses = []\n",
        "        print(f\"[{name}] Params: {params}\", flush=True)\n",
        "        for fold, (tr_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "            t0=time.time()\n",
        "            x_tr, x_val = X_text[tr_idx], X_text[val_idx]\n",
        "            y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "            if build_on_each_fold:\n",
        "                vec = vectorizer\n",
        "                X_tr = vec.fit_transform(x_tr)\n",
        "                X_val = vec.transform(x_val)\n",
        "                X_te  = vec.transform(X_test_text)\n",
        "            else:\n",
        "                # Fit vectorizer once on full train (not recommended for OOF honesty).\n",
        "                vec = vectorizer\n",
        "                X_tr = vec.fit_transform(X_text[tr_idx])\n",
        "                X_val = vec.transform(X_text[val_idx])\n",
        "                X_te  = vec.transform(X_test_text)\n",
        "            clf = clf_builder(**params)\n",
        "            clf.fit(X_tr, y_tr)\n",
        "            proba_val = clf.predict_proba(X_val).astype(np.float32)\n",
        "            proba_test = clf.predict_proba(X_te).astype(np.float32)\n",
        "            # reorder to classes\n",
        "            order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "            proba_val = proba_val[:, order_idx]\n",
        "            proba_test = proba_test[:, order_idx]\n",
        "            oof[val_idx] = proba_val\n",
        "            test_pred += proba_test / N_FOLDS\n",
        "            loss = log_loss(y_val, proba_val, labels=list(classes))\n",
        "            fold_losses.append(loss)\n",
        "            print(f\"[{name}] Fold {fold} loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "            del X_tr, X_val, X_te, proba_val, proba_test, clf\n",
        "            gc.collect()\n",
        "        oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "        print(f\"[{name}] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t_all:.2f}s\", flush=True)\n",
        "        if oof_loss < best['loss']:\n",
        "            best.update(loss=oof_loss, params=params, oof=oof, test=test_pred)\n",
        "    return best['oof'], best['test'], best['loss'], best\n",
        "\n",
        "def cv_model_two_vectorizers(vec_word: TfidfVectorizer, vec_char: TfidfVectorizer,\n",
        "                             clf_builder, clf_param_grid: List, name: str) -> Tuple[np.ndarray, np.ndarray, float, Dict]:\n",
        "    t_all = time.time()\n",
        "    best = dict(loss=1e9, params=None, oof=None, test=None)\n",
        "    for params in clf_param_grid:\n",
        "        oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "        test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "        fold_losses = []\n",
        "        print(f\"[{name}] Params: {params}\", flush=True)\n",
        "        for fold, (tr_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "            t0=time.time()\n",
        "            x_tr, x_val = X_text[tr_idx], X_text[val_idx]\n",
        "            y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "            vw = vec_word\n",
        "            vc = vec_char\n",
        "            Xw_tr = vw.fit_transform(x_tr); Xw_val = vw.transform(x_val); Xw_te = vw.transform(X_test_text)\n",
        "            Xc_tr = vc.fit_transform(x_tr); Xc_val = vc.transform(x_val); Xc_te = vc.transform(X_test_text)\n",
        "            X_tr = sparse.hstack([Xw_tr, Xc_tr], format='csr')\n",
        "            X_val = sparse.hstack([Xw_val, Xc_val], format='csr')\n",
        "            X_te  = sparse.hstack([Xw_te, Xc_te], format='csr')\n",
        "            clf = clf_builder(**params)\n",
        "            clf.fit(X_tr, y_tr)\n",
        "            proba_val = clf.predict_proba(X_val).astype(np.float32)\n",
        "            proba_test = clf.predict_proba(X_te).astype(np.float32)\n",
        "            order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "            proba_val = proba_val[:, order_idx]\n",
        "            proba_test = proba_test[:, order_idx]\n",
        "            oof[val_idx] = proba_val\n",
        "            test_pred += proba_test / N_FOLDS\n",
        "            loss = log_loss(y_val, proba_val, labels=list(classes))\n",
        "            fold_losses.append(loss)\n",
        "            print(f\"[{name}] Fold {fold} loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "            del Xw_tr, Xw_val, Xw_te, Xc_tr, Xc_val, Xc_te, X_tr, X_val, X_te, proba_val, proba_test, clf\n",
        "            gc.collect()\n",
        "        oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "        print(f\"[{name}] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t_all:.2f}s\", flush=True)\n",
        "        if oof_loss < best['loss']:\n",
        "            best.update(loss=oof_loss, params=params, oof=oof, test=test_pred)\n",
        "    return best['oof'], best['test'], best['loss'], best\n",
        "\n",
        "# Vectorizers per expert defaults\n",
        "vec_char_wb = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,6), min_df=2, lowercase=True,\n",
        "                              sublinear_tf=True, dtype=np.float32)\n",
        "vec_char = TfidfVectorizer(analyzer='char', ngram_range=(3,5), min_df=2, lowercase=True,\n",
        "                           sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "vec_word13 = TfidfVectorizer(analyzer='word', ngram_range=(1,3), min_df=2, max_df=0.9, lowercase=True,\n",
        "                             strip_accents='unicode', token_pattern=r'(?u)\\b\\w+\\b', sublinear_tf=True,\n",
        "                             smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "vec_word12 = TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df=2, max_df=0.9, lowercase=True,\n",
        "                             strip_accents='unicode', token_pattern=r'(?u)\\b\\w+\\b', sublinear_tf=True,\n",
        "                             smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "# 1) LR on char-only (char_wb 3-6), sweep C\n",
        "lr_builder = lambda **kw: LogisticRegression(solver='saga', penalty='l2', multi_class='multinomial',\n",
        "                                             max_iter=5000, tol=1e-3, n_jobs=-1, random_state=SEED, **kw)\n",
        "lr_C_grid = [ {'C': c} for c in [2.0, 4.0, 8.0] ]\n",
        "oof_char, test_char, loss_char, best_char = cv_model_single_vectorizer(vec_char_wb, True, lr_builder, lr_C_grid, name='LR_char_wb')\n",
        "np.save('oof_lr_char.npy', oof_char); np.save('test_lr_char.npy', test_char)\n",
        "\n",
        "# 2) LR on word-only (1-3), sweep C\n",
        "oof_word, test_word, loss_word, best_word = cv_model_single_vectorizer(vec_word13, True, lr_builder, lr_C_grid, name='LR_word_1_3')\n",
        "np.save('oof_lr_word.npy', oof_word); np.save('test_lr_word.npy', test_word)\n",
        "\n",
        "# 3) SGD on word+char (word 1-2 + char 3-5), sweep alpha\n",
        "sgd_builder = lambda **kw: SGDClassifier(loss='log_loss', penalty='l2', max_iter=2000, tol=1e-4,\n",
        "                                         early_stopping=True, validation_fraction=0.1, n_iter_no_change=5,\n",
        "                                         average=True, random_state=SEED, **kw)\n",
        "alpha_grid = [ {'alpha': a} for a in [1e-6, 3e-6, 1e-5] ]\n",
        "oof_sgd, test_sgd, loss_sgd, best_sgd = cv_model_two_vectorizers(vec_word12, vec_char, sgd_builder, alpha_grid, name='SGD_wordchar')\n",
        "np.save('oof_sgd_wordchar.npy', oof_sgd); np.save('test_sgd_wordchar.npy', test_sgd)\n",
        "\n",
        "print(f\"Best OOF losses -> char:{loss_char:.5f} word:{loss_word:.5f} sgd:{loss_sgd:.5f}\", flush=True)\n",
        "\n",
        "# Blend (weights per expert starting point): 0.50 char, 0.35 word, 0.15 sgd\n",
        "w_char, w_word, w_sgd = 0.50, 0.35, 0.15\n",
        "oof_blend = (w_char*oof_char + w_word*oof_word + w_sgd*oof_sgd).astype(np.float32)\n",
        "test_blend = (w_char*test_char + w_word*test_word + w_sgd*test_sgd).astype(np.float32)\n",
        "oof_blend_loss = log_loss(y, oof_blend, labels=list(classes))\n",
        "print(f\"[BLEND] OOF logloss={oof_blend_loss:.5f}\", flush=True)\n",
        "\n",
        "# Save blended submission\n",
        "idx_map = [list(classes).index(c) for c in submit_cols]\n",
        "probs = test_blend[:, idx_map]\n",
        "eps = 1e-9\n",
        "probs = np.clip(probs, eps, 1-eps)\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print(\"Saved submission.csv (blend). Head:\\n\", sub.head(), flush=True)\n",
        "\n",
        "# Log params chosen\n",
        "print(\"Best params:\")\n",
        "print(\"  LR_char_wb:\", best_char['params'])\n",
        "print(\"  LR_word_1_3:\", best_word['params'])\n",
        "print(\"  SGD_wordchar:\", best_sgd['params'])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Params: {'C': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Fold 1 loss=0.45683 elapsed=3.53s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Fold 2 loss=0.46149 elapsed=3.57s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Fold 3 loss=0.46741 elapsed=3.66s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Fold 4 loss=0.46121 elapsed=3.74s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Fold 5 loss=0.45641 elapsed=3.75s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] OOF=0.46067; folds=[0.45683, 0.46149, 0.46741, 0.46121, 0.45641] total=18.52s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Params: {'C': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Fold 1 loss=0.42375 elapsed=3.83s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Fold 2 loss=0.42454 elapsed=3.83s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Fold 3 loss=0.43573 elapsed=3.77s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Fold 4 loss=0.42745 elapsed=3.76s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Fold 5 loss=0.42344 elapsed=3.78s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] OOF=0.42698; folds=[0.42375, 0.42454, 0.43573, 0.42745, 0.42344] total=37.76s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Params: {'C': 8.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Fold 1 loss=0.40969 elapsed=4.29s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Fold 2 loss=0.40529 elapsed=4.28s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Fold 3 loss=0.42235 elapsed=4.38s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Fold 4 loss=0.41161 elapsed=4.37s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] Fold 5 loss=0.40872 elapsed=4.33s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_wb] OOF=0.41153; folds=[0.40969, 0.40529, 0.42235, 0.41161, 0.40872] total=59.68s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Params: {'C': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Fold 1 loss=0.51518 elapsed=1.19s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Fold 2 loss=0.52073 elapsed=1.19s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Fold 3 loss=0.52547 elapsed=1.23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Fold 4 loss=0.51224 elapsed=1.21s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Fold 5 loss=0.51230 elapsed=1.24s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] OOF=0.51718; folds=[0.51518, 0.52073, 0.52547, 0.51224, 0.5123] total=6.33s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Params: {'C': 4.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Fold 1 loss=0.46807 elapsed=1.25s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Fold 2 loss=0.47426 elapsed=1.29s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Fold 3 loss=0.48028 elapsed=1.28s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Fold 4 loss=0.46474 elapsed=1.27s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Fold 5 loss=0.46666 elapsed=1.27s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] OOF=0.47080; folds=[0.46807, 0.47426, 0.48028, 0.46474, 0.46666] total=12.97s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Params: {'C': 8.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Fold 1 loss=0.43805 elapsed=1.38s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Fold 2 loss=0.44438 elapsed=1.43s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Fold 3 loss=0.45196 elapsed=1.37s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Fold 4 loss=0.43415 elapsed=1.38s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] Fold 5 loss=0.43811 elapsed=1.37s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word_1_3] OOF=0.44133; folds=[0.43805, 0.44438, 0.45196, 0.43415, 0.43811] total=20.19s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_wordchar] Params: {'alpha': 1e-06}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_wordchar] Fold 1 loss=0.82708 elapsed=3.80s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_wordchar] Fold 2 loss=0.82626 elapsed=3.77s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_wordchar] Fold 3 loss=0.93218 elapsed=3.84s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_wordchar] Fold 4 loss=0.89054 elapsed=3.84s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_wordchar] Fold 5 loss=0.80539 elapsed=3.88s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_wordchar] OOF=0.85629; folds=[0.82708, 0.82626, 0.93218, 0.89054, 0.80539] total=19.47s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_wordchar] Params: {'alpha': 3e-06}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_wordchar] Fold 1 loss=0.46374 elapsed=3.98s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_wordchar] Fold 2 loss=0.50019 elapsed=3.77s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_wordchar] Fold 3 loss=0.50480 elapsed=4.09s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_wordchar] Fold 4 loss=0.49524 elapsed=3.81s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_wordchar] Fold 5 loss=0.47229 elapsed=3.87s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_wordchar] OOF=0.48725; folds=[0.46374, 0.50019, 0.5048, 0.49524, 0.47229] total=39.37s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_wordchar] Params: {'alpha': 1e-05}\n"
          ]
        }
      ]
    },
    {
      "id": "e42621b7-5d66-4f9a-a785-cbb979e91bd5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fix per expert: char analyzer (2-6, lowercase=False) + word max_df=1.0; LR C sweep with 5-fold CV\n",
        "import time, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Tuple\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42\n",
        "N_FOLDS = 5\n",
        "np.random.seed(SEED)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test_text = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols), f\"Classes mismatch: {classes}\"\n",
        "\n",
        "# Vectorizer params (known-good, with apostrophes and hyphens kept for words; no accent strip on char)\n",
        "word_params = dict(analyzer='word', ngram_range=(1,2), min_df=2, max_df=1.0, lowercase=True,\n",
        "                   strip_accents='unicode', token_pattern=r\"(?u)\\b[-\\w']+\\b\", sublinear_tf=True,\n",
        "                   smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "char_params = dict(analyzer='char', ngram_range=(2,6), min_df=2, lowercase=False,\n",
        "                   strip_accents=None, sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_fold_features(x_tr, x_val, x_test) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int, int]:\n",
        "    v_word = TfidfVectorizer(**word_params)\n",
        "    v_char = TfidfVectorizer(**char_params)\n",
        "    Xw_tr = v_word.fit_transform(x_tr); Xw_val = v_word.transform(x_val); Xw_test = v_word.transform(x_test)\n",
        "    Xc_tr = v_char.fit_transform(x_tr); Xc_val = v_char.transform(x_val); Xc_test = v_char.transform(x_test)\n",
        "    X_tr = sparse.hstack([Xw_tr, Xc_tr], format='csr')\n",
        "    X_val = sparse.hstack([Xw_val, Xc_val], format='csr')\n",
        "    X_te  = sparse.hstack([Xw_test, Xc_test], format='csr')\n",
        "    return X_tr, X_val, X_te, Xw_tr.shape[1], Xc_tr.shape[1]\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "best = {'loss': 1e9, 'C': None, 'oof': None, 'test': None}\n",
        "for C in [2.0, 4.0, 8.0, 12.0]:\n",
        "    print(f\"[RUN] C={C}\", flush=True)\n",
        "    oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "    test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "    fold_losses = []\n",
        "    t0_all = time.time()\n",
        "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "        t0 = time.time()\n",
        "        x_tr, x_val = X_text[tr_idx], X_text[val_idx]\n",
        "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "        X_tr, X_val, X_te, vw_dim, vc_dim = build_fold_features(x_tr, x_val, X_test_text)\n",
        "        print(f\"  [Fold {fold}] shapes: X_tr={X_tr.shape}, X_val={X_val.shape}, X_te={X_te.shape}; vocab(word,char)=({vw_dim},{vc_dim})\", flush=True)\n",
        "        clf = LogisticRegression(solver='saga', multi_class='multinomial', penalty='l2',\n",
        "                                 C=C, max_iter=10000, tol=1e-4, n_jobs=-1, random_state=SEED)\n",
        "        clf.fit(X_tr, y_tr)\n",
        "        proba_val = clf.predict_proba(X_val).astype(np.float32)\n",
        "        proba_test = clf.predict_proba(X_te).astype(np.float32)\n",
        "        order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "        proba_val = proba_val[:, order_idx]\n",
        "        proba_test = proba_test[:, order_idx]\n",
        "        oof[val_idx] = proba_val\n",
        "        test_pred += proba_test / N_FOLDS\n",
        "        loss = log_loss(y_val, proba_val)\n",
        "        fold_losses.append(loss)\n",
        "        print(f\"  [Fold {fold}] logloss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "        del X_tr, X_val, X_te, proba_val, proba_test, clf\n",
        "        gc.collect()\n",
        "    oof_loss = log_loss(y, oof)\n",
        "    print(f\"[RUN] C={C} OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total_elapsed={time.time()-t0_all:.2f}s\", flush=True)\n",
        "    if oof_loss < best['loss']:\n",
        "        best.update(loss=oof_loss, C=C, oof=oof, test=test_pred)\n",
        "\n",
        "print(f\"[BEST] OOF={best['loss']:.5f} at C={best['C']}\", flush=True)\n",
        "np.save('oof_lr_wordchar_fixed.npy', best['oof'])\n",
        "np.save('test_lr_wordchar_fixed.npy', best['test'])\n",
        "\n",
        "# Build submission\n",
        "idx_map = [list(classes).index(c) for c in submit_cols]\n",
        "probs = best['test'][:, idx_map]\n",
        "eps = 1e-9\n",
        "probs = np.clip(probs, eps, 1-eps)\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (fixed recipe with hyphen-aware tokenization). Head:\\n', sub.head(), flush=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RUN] C=2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] shapes: X_tr=(14096, 345620), X_val=(3525, 345620), X_te=(1958, 345620); vocab(word,char)=(52523,293097)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] logloss=0.42226 elapsed=10.31s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] shapes: X_tr=(14097, 346840), X_val=(3524, 346840), X_te=(1958, 346840); vocab(word,char)=(52659,294181)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] logloss=0.42466 elapsed=18.73s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] shapes: X_tr=(14097, 345772), X_val=(3524, 345772), X_te=(1958, 345772); vocab(word,char)=(52462,293310)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] logloss=0.43415 elapsed=16.35s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] shapes: X_tr=(14097, 346267), X_val=(3524, 346267), X_te=(1958, 346267); vocab(word,char)=(52519,293748)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] logloss=0.42090 elapsed=12.70s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] shapes: X_tr=(14097, 346805), X_val=(3524, 346805), X_te=(1958, 346805); vocab(word,char)=(52356,294449)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] logloss=0.41732 elapsed=18.53s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RUN] C=2.0 OOF=0.42386; folds=[0.42226, 0.42466, 0.43415, 0.4209, 0.41732] total_elapsed=76.98s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RUN] C=4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] shapes: X_tr=(14096, 345620), X_val=(3525, 345620), X_te=(1958, 345620); vocab(word,char)=(52523,293097)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] logloss=0.39074 elapsed=19.86s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] shapes: X_tr=(14097, 346840), X_val=(3524, 346840), X_te=(1958, 346840); vocab(word,char)=(52659,294181)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] logloss=0.39204 elapsed=28.25s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] shapes: X_tr=(14097, 345772), X_val=(3524, 345772), X_te=(1958, 345772); vocab(word,char)=(52462,293310)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] logloss=0.40400 elapsed=26.15s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] shapes: X_tr=(14097, 346267), X_val=(3524, 346267), X_te=(1958, 346267); vocab(word,char)=(52519,293748)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] logloss=0.38968 elapsed=22.54s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] shapes: X_tr=(14097, 346805), X_val=(3524, 346805), X_te=(1958, 346805); vocab(word,char)=(52356,294449)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] logloss=0.38616 elapsed=27.79s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RUN] C=4.0 OOF=0.39252; folds=[0.39074, 0.39204, 0.404, 0.38968, 0.38616] total_elapsed=124.95s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RUN] C=8.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] shapes: X_tr=(14096, 345620), X_val=(3525, 345620), X_te=(1958, 345620); vocab(word,char)=(52523,293097)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  [Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] shapes: X_tr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_tr.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, X_val=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_val.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, X_te=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_te.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m; vocab(word,char)=(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvw_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvc_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     57\u001b[39m clf = LogisticRegression(solver=\u001b[33m'\u001b[39m\u001b[33msaga\u001b[39m\u001b[33m'\u001b[39m, multi_class=\u001b[33m'\u001b[39m\u001b[33mmultinomial\u001b[39m\u001b[33m'\u001b[39m, penalty=\u001b[33m'\u001b[39m\u001b[33ml2\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     58\u001b[39m                          C=C, max_iter=\u001b[32m10000\u001b[39m, tol=\u001b[32m1e-4\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m, random_state=SEED)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m proba_val = clf.predict_proba(X_val).astype(np.float32)\n\u001b[32m     61\u001b[39m proba_test = clf.predict_proba(X_te).astype(np.float32)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1473\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m     estimator._validate_params()\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1469\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1470\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1471\u001b[39m     )\n\u001b[32m   1472\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1350\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1348\u001b[39m     n_threads = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1350\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m fold_coefs_, _, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n\u001b[32m   1376\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, \u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py:74\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     69\u001b[39m config = get_config()\n\u001b[32m     70\u001b[39m iterable_with_config = (\n\u001b[32m     71\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     73\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1952\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1946\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   1947\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   1948\u001b[39m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[32m   1949\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   1950\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1952\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1595\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1592\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1594\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1595\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1597\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1598\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1599\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1600\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1601\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1707\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1702\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1703\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1705\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1706\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1707\u001b[39m     time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1708\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1710\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1711\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1712\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "a9a76d65-425b-4ffd-bc1f-300b72417cb4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Char-only model: Calibrated LinearSVC on TF-IDF char (1-6, min_df=1, lowercase=False, isotonic)\n",
        "import time, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Dict\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42\n",
        "N_FOLDS = 5\n",
        "np.random.seed(SEED)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test_text = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols), f\"Classes mismatch: {classes}\"\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "# Char vectorizer per expert: analyzer='char', ngram_range=(1,6), min_df=1, lowercase=False, strip_accents=None\n",
        "char_params = dict(analyzer='char', ngram_range=(1,6), min_df=1, lowercase=False,\n",
        "                   strip_accents=None, sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_char_fold(x_tr, x_val, x_test) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int]:\n",
        "    v = TfidfVectorizer(**char_params)\n",
        "    X_tr = v.fit_transform(x_tr)\n",
        "    X_val = v.transform(x_val)\n",
        "    X_te  = v.transform(x_test)\n",
        "    return X_tr, X_val, X_te, X_tr.shape[1]\n",
        "\n",
        "def cv_char_svc(C_grid: List[float], name: str='CalibSVC_char_1_6_iso') -> Tuple[np.ndarray, np.ndarray, float, Dict]:\n",
        "    best = dict(loss=1e9, C=None, oof=None, test=None)\n",
        "    for C in C_grid:\n",
        "        oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "        test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "        fold_losses = []\n",
        "        t0_all = time.time()\n",
        "        print(f\"[{name}] C={C}\", flush=True)\n",
        "        for fold, (tr_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "            t0 = time.time()\n",
        "            x_tr, x_val = X_text[tr_idx], X_text[val_idx]\n",
        "            y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "            X_tr, X_val, X_te, vdim = build_char_fold(x_tr, x_val, X_test_text)\n",
        "            print(f\"  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}\", flush=True)\n",
        "            base = LinearSVC(C=C, tol=1e-4, max_iter=10000, random_state=SEED, dual=True)\n",
        "            clf = CalibratedClassifierCV(estimator=base, method='isotonic', cv=5, n_jobs=-1)\n",
        "            clf.fit(X_tr, y_tr)\n",
        "            proba_val = clf.predict_proba(X_val).astype(np.float32)\n",
        "            proba_test = clf.predict_proba(X_te).astype(np.float32)\n",
        "            order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "            proba_val = proba_val[:, order_idx]\n",
        "            proba_test = proba_test[:, order_idx]\n",
        "            oof[val_idx] = proba_val\n",
        "            test_pred += proba_test / N_FOLDS\n",
        "            loss = log_loss(y_val, proba_val, labels=list(classes))\n",
        "            fold_losses.append(loss)\n",
        "            print(f\"  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "            del X_tr, X_val, X_te, proba_val, proba_test, clf, base\n",
        "            gc.collect()\n",
        "        oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "        print(f\"[{name}] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s\", flush=True)\n",
        "        if oof_loss < best['loss']:\n",
        "            best.update(loss=oof_loss, C=C, oof=oof, test=test_pred)\n",
        "    return best['oof'], best['test'], best['loss'], best\n",
        "\n",
        "# Run Calibrated LinearSVC char-only (isotonic, includes 1-grams)\n",
        "svc_C_grid = [0.5, 1.0, 2.0]\n",
        "oof_svc_char16_iso, test_svc_char16_iso, loss_svc_char16_iso, best_svc_char16_iso = cv_char_svc(svc_C_grid, name='CalibSVC_char_1_6_iso')\n",
        "np.save('oof_svc_char_1_6_iso.npy', oof_svc_char16_iso); np.save('test_svc_char_1_6_iso.npy', test_svc_char16_iso)\n",
        "print(f\"[CalibSVC_char_1_6_iso] BEST OOF={loss_svc_char16_iso:.5f} with C={best_svc_char16_iso['C']}\", flush=True)\n",
        "\n",
        "# Build submission from best SVC char-only\n",
        "idx_map = [list(classes).index(c) for c in submit_cols]\n",
        "probs = best_svc_char16_iso['test'][:, idx_map]\n",
        "eps = 1e-9\n",
        "probs = np.clip(probs, eps, 1-eps)\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (CalibSVC char 1-6 isotonic). Head:\\n', sub.head(), flush=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibSVC_char_1_6_iso] C=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 512933) vdim=512933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.40480 elapsed=7.73s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 515302) vdim=515302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.39344 elapsed=7.76s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 515756) vdim=515756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.40903 elapsed=7.71s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 513663) vdim=513663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.40520 elapsed=7.60s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 514470) vdim=514470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.38461 elapsed=7.62s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibSVC_char_1_6_iso] OOF=0.39942; folds=[0.4048, 0.39344, 0.40903, 0.4052, 0.38461] total=38.80s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibSVC_char_1_6_iso] C=1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 512933) vdim=512933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.38945 elapsed=7.88s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 515302) vdim=515302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.38524 elapsed=7.87s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 515756) vdim=515756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.40065 elapsed=7.83s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 513663) vdim=513663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.39636 elapsed=7.37s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 514470) vdim=514470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.37740 elapsed=7.36s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibSVC_char_1_6_iso] OOF=0.38982; folds=[0.38945, 0.38524, 0.40065, 0.39636, 0.3774] total=38.66s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibSVC_char_1_6_iso] C=2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 512933) vdim=512933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.38765 elapsed=8.02s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 515302) vdim=515302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.38231 elapsed=7.88s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 515756) vdim=515756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.39823 elapsed=7.85s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 513663) vdim=513663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.39746 elapsed=7.98s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 514470) vdim=514470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.37686 elapsed=7.74s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibSVC_char_1_6_iso] OOF=0.38850; folds=[0.38765, 0.38231, 0.39823, 0.39746, 0.37686] total=39.82s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibSVC_char_1_6_iso] BEST OOF=0.38850 with C=2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (CalibSVC char 1-6 isotonic). Head:\n         id       EAP       HPL       MWS\n0  id27251  0.013289  0.966162  0.020548\n1  id09612  0.173638  0.204327  0.622035\n2  id11943  0.036458  0.004822  0.958720\n3  id19526  0.002769  0.217847  0.779384\n4  id12931  0.029037  0.060493  0.910470\n"
          ]
        }
      ]
    },
    {
      "id": "2356a67c-a2b4-4009-ae1f-1d4ce58ef530",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fast NB-SVM (log-count ratio) for word(1-2) only, then OOF-weighted blend with LR(word+char fixed)\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import List, Tuple, Dict\n",
        "from pathlib import Path\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42\n",
        "N_FOLDS = 5\n",
        "np.random.seed(SEED)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test_text = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols), f\"Classes mismatch: {classes}\"\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "# Faster vectorizer config (reduced vdim): word 1-2, min_df=2, keep apostrophes\n",
        "vec_word_params = dict(analyzer='word', ngram_range=(1,2), min_df=2, max_df=1.0, lowercase=True,\n",
        "                        strip_accents='unicode', token_pattern=r\"(?u)\\b[\\w']+\\b\", dtype=np.float32)\n",
        "\n",
        "def _log_count_ratio(X: sparse.csr_matrix, y_bin: np.ndarray, alpha: float = 0.5) -> np.ndarray:\n",
        "    X_pos = X[y_bin == 1]\n",
        "    X_neg = X[y_bin == 0]\n",
        "    p = np.asarray(X_pos.sum(axis=0)).ravel() + alpha\n",
        "    q = np.asarray(X_neg.sum(axis=0)).ravel() + alpha\n",
        "    r = np.log(p / q)\n",
        "    return r.astype(np.float32)\n",
        "\n",
        "def _apply_r(X: sparse.csr_matrix, r: np.ndarray) -> sparse.csr_matrix:\n",
        "    return X.multiply(r)\n",
        "\n",
        "def cv_nbsvm_word(vec: CountVectorizer, alpha: float, C: float, name: str):\n",
        "    t0_all = time.time()\n",
        "    oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "    test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "    fold_losses = []\n",
        "    print(f\"[{name}] C={C} alpha={alpha}\", flush=True)\n",
        "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "        t0 = time.time()\n",
        "        x_tr, x_val = X_text[tr_idx], X_text[val_idx]\n",
        "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "        v = vec\n",
        "        X_tr = v.fit_transform(x_tr)\n",
        "        X_val = v.transform(x_val)\n",
        "        X_te  = v.transform(X_test_text)\n",
        "        vdim = X_tr.shape[1]\n",
        "        print(f\"  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}\", flush=True)\n",
        "        proba_val = np.zeros((len(val_idx), len(classes)), dtype=np.float32)\n",
        "        proba_te  = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "        for ci, c in enumerate(classes):\n",
        "            y_bin_tr = (y_tr == c).astype(np.int8)\n",
        "            r = _log_count_ratio(X_tr, y_bin_tr, alpha=alpha)\n",
        "            Xr_tr = _apply_r(X_tr, r)\n",
        "            Xr_val = _apply_r(X_val, r)\n",
        "            Xr_te  = _apply_r(X_te, r)\n",
        "            # Faster binary LR: liblinear (probability=True) on reduced vdim\n",
        "            clf = LogisticRegression(solver='liblinear', penalty='l2', max_iter=2000, tol=1e-4,\n",
        "                                     C=C, n_jobs=1, random_state=SEED)\n",
        "            clf.fit(Xr_tr, y_bin_tr)\n",
        "            proba_val[:, ci] = clf.predict_proba(Xr_val)[:, 1].astype(np.float32)\n",
        "            proba_te[:,  ci] = clf.predict_proba(Xr_te)[:, 1].astype(np.float32)\n",
        "            del y_bin_tr, r, Xr_tr, Xr_val, Xr_te, clf\n",
        "            gc.collect()\n",
        "        eps = 1e-9\n",
        "        proba_val = np.clip(proba_val, eps, 1 - eps)\n",
        "        proba_val = proba_val / proba_val.sum(axis=1, keepdims=True)\n",
        "        proba_te  = np.clip(proba_te,  eps, 1 - eps)\n",
        "        proba_te  = proba_te  / proba_te.sum(axis=1,  keepdims=True)\n",
        "        oof[val_idx] = proba_val\n",
        "        test_pred += proba_te / N_FOLDS\n",
        "        loss = log_loss(y_val, proba_val, labels=list(classes))\n",
        "        fold_losses.append(loss)\n",
        "        print(f\"  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "        del X_tr, X_val, X_te, proba_val, proba_te, v\n",
        "        gc.collect()\n",
        "    oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "    print(f\"[{name}] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s\", flush=True)\n",
        "    return oof, test_pred, oof_loss\n",
        "\n",
        "# Run fast NB-SVM word-only\n",
        "word_vec = CountVectorizer(**vec_word_params)\n",
        "alpha = 0.5\n",
        "C = 12.0\n",
        "oof_nb_word, test_nb_word, loss_nb_word = cv_nbsvm_word(word_vec, alpha, C, name='NBSVM_word_1_2_min2_fast')\n",
        "np.save('oof_nbsvm_word.npy', oof_nb_word); np.save('test_nbsvm_word.npy', test_nb_word)\n",
        "print(f\"[NBSVM_word_fast] OOF={loss_nb_word:.5f} C={C}\", flush=True)\n",
        "\n",
        "# Load LR(word+char fixed recipe) OOF/test\n",
        "oof_lr_wc = np.load('oof_lr_wordchar_fixed.npy') if (Path('oof_lr_wordchar_fixed.npy').exists()) else None\n",
        "test_lr_wc = np.load('test_lr_wordchar_fixed.npy') if (Path('test_lr_wordchar_fixed.npy').exists()) else None\n",
        "\n",
        "# Simple OOF-weighted blend over a small grid (non-negative, sum to 1) between NBSVM_word and LR(word+char)\n",
        "best_blend = (1e9, None, None)\n",
        "if oof_lr_wc is not None:\n",
        "    for w in np.linspace(0.1, 0.9, 9):\n",
        "        oof_bl = (w * oof_nb_word + (1.0 - w) * oof_lr_wc).astype(np.float32)\n",
        "        loss = log_loss(y, oof_bl, labels=list(classes))\n",
        "        if loss < best_blend[0]:\n",
        "            best_blend = (loss, w, oof_bl)\n",
        "    print(f\"[BLEND word+lr_wc] best OOF={best_blend[0]:.5f} w_word={best_blend[1]:.2f} w_lr_wc={1-best_blend[1]:.2f}\", flush=True)\n",
        "    # Build blended submission\n",
        "    w = best_blend[1]\n",
        "    test_bl = (w * test_nb_word + (1.0 - w) * test_lr_wc).astype(np.float32)\n",
        "    idx_map = [list(classes).index(c) for c in submit_cols]\n",
        "    probs = test_bl[:, idx_map]\n",
        "    eps = 1e-9\n",
        "    probs = np.clip(probs, eps, 1-eps)\n",
        "    probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "    sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "    sub.insert(0, 'id', test['id'].values)\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Saved submission.csv (Fast NB-SVM word + LR blend). Head:\\n', sub.head(), flush=True)\n",
        "else:\n",
        "    print('Skipped blend: missing LR word+char fixed preds.', flush=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_word_1_2_min2_fast] C=12.0 alpha=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 52523) vdim=52523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.51822 elapsed=2.28s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 52659) vdim=52659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.50560 elapsed=1.97s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 52462) vdim=52462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.53824 elapsed=1.95s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 52519) vdim=52519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.49216 elapsed=1.93s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 52356) vdim=52356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.51517 elapsed=2.06s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_word_1_2_min2_fast] OOF=0.51388; folds=[0.51822, 0.5056, 0.53824, 0.49216, 0.51517] total=10.48s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_word_fast] OOF=0.51388 C=12.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BLEND word+lr_wc] best OOF=0.36143 w_word=0.20 w_lr_wc=0.80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (Fast NB-SVM word + LR blend). Head:\n         id       EAP       HPL       MWS\n0  id27251  0.005360  0.992360  0.002280\n1  id09612  0.161116  0.145306  0.693578\n2  id11943  0.012770  0.004396  0.982834\n3  id19526  0.011659  0.042058  0.946283\n4  id12931  0.052283  0.034653  0.913063\n"
          ]
        }
      ]
    },
    {
      "id": "ee16788e-92c4-4e84-b3b6-e2b4269bd8ac",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Blend existing OOFs: LR(word+char fixed) + NBSVM(word fast) + CalibSVC(char 1-6 isotonic) + LR(char 1-7) + CNB(word)\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "y = train['author'].values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "\n",
        "def load_or_none(p):\n",
        "    return np.load(p) if Path(p).exists() else None\n",
        "\n",
        "oof_lr = load_or_none('oof_lr_wordchar_fixed.npy')\n",
        "test_lr = load_or_none('test_lr_wordchar_fixed.npy')\n",
        "oof_nbw = load_or_none('oof_nbsvm_word.npy')\n",
        "test_nbw = load_or_none('test_nbsvm_word.npy')\n",
        "oof_svc_iso = load_or_none('oof_svc_char_1_6_iso.npy')\n",
        "test_svc_iso = load_or_none('test_svc_char_1_6_iso.npy')\n",
        "oof_lr_char17 = load_or_none('oof_lr_char_1_7.npy')\n",
        "test_lr_char17 = load_or_none('test_lr_char_1_7.npy')\n",
        "oof_cnb_word = load_or_none('oof_cnb_word.npy')\n",
        "test_cnb_word = load_or_none('test_cnb_word.npy')\n",
        "\n",
        "avail = [\n",
        "    ('lr_wc', oof_lr, test_lr),\n",
        "    ('nbw', oof_nbw, test_nbw),\n",
        "    ('svc_iso', oof_svc_iso, test_svc_iso),\n",
        "    ('lr_char17', oof_lr_char17, test_lr_char17),\n",
        "    ('cnb_word', oof_cnb_word, test_cnb_word),\n",
        "]\n",
        "avail = [(n,o,t) for n,o,t in avail if o is not None and t is not None]\n",
        "print('Available models:', [n for n,_,_ in avail])\n",
        "\n",
        "assert any(n=='lr_wc' for n,_,_ in avail), 'LR(word+char fixed) required'\n",
        "\n",
        "# Grid search non-negative weights that sum to 1 for up to 5 models (coarse grid to keep runtime tractable)\n",
        "best = (1e9, None, None)\n",
        "ws = np.round(np.arange(0.0, 1.0001, 0.05), 5)\n",
        "names = [n for n,_,_ in avail]\n",
        "\n",
        "def try_weights(ws_sel):\n",
        "    s = sum(ws_sel)\n",
        "    if abs(s - 1.0) > 1e-9: return None\n",
        "    weights = {names[i]: ws_sel[i] for i in range(len(names))}\n",
        "    blend = None\n",
        "    for i,(name,oof,_) in enumerate(avail):\n",
        "        w = weights.get(name, 0.0)\n",
        "        if w == 0.0: continue\n",
        "        blend = (oof * w) if blend is None else (blend + oof * w)\n",
        "    eps = 1e-9\n",
        "    blend = np.clip(blend, eps, 1 - eps)\n",
        "    blend = blend / blend.sum(axis=1, keepdims=True)\n",
        "    loss = log_loss(y, blend, labels=list(classes))\n",
        "    return loss, weights, blend\n",
        "\n",
        "L = len(avail)\n",
        "if L >= 2:\n",
        "    # Nested loops up to 5 models; keep step coarse\n",
        "    if L == 5:\n",
        "        for w1 in ws:\n",
        "            for w2 in ws:\n",
        "                for w3 in ws:\n",
        "                    for w4 in ws:\n",
        "                        w5 = 1.0 - w1 - w2 - w3 - w4\n",
        "                        if w5 < 0 or w5 > 1: continue\n",
        "                        res = try_weights([w1,w2,w3,w4,w5])\n",
        "                        if res is None: continue\n",
        "                        loss, weights, blend = res\n",
        "                        if loss < best[0]: best = (loss, weights.copy(), blend.copy())\n",
        "    elif L == 4:\n",
        "        for w1 in ws:\n",
        "            for w2 in ws:\n",
        "                for w3 in ws:\n",
        "                    w4 = 1.0 - w1 - w2 - w3\n",
        "                    if w4 < 0 or w4 > 1: continue\n",
        "                    res = try_weights([w1,w2,w3,w4])\n",
        "                    if res is None: continue\n",
        "                    loss, weights, blend = res\n",
        "                    if loss < best[0]: best = (loss, weights.copy(), blend.copy())\n",
        "    elif L == 3:\n",
        "        for w1 in ws:\n",
        "            for w2 in ws:\n",
        "                w3 = 1.0 - w1 - w2\n",
        "                if w3 < 0 or w3 > 1: continue\n",
        "                res = try_weights([w1,w2,w3])\n",
        "                if res is None: continue\n",
        "                loss, weights, blend = res\n",
        "                if loss < best[0]: best = (loss, weights.copy(), blend.copy())\n",
        "    else:  # L == 2\n",
        "        for w1 in ws:\n",
        "            w2 = 1.0 - w1\n",
        "            res = try_weights([w1,w2])\n",
        "            if res is None: continue\n",
        "            loss, weights, blend = res\n",
        "            if loss < best[0]: best = (loss, weights.copy(), blend.copy())\n",
        "\n",
        "print(f\"[BLEND SEARCH] best OOF={best[0]:.5f} weights={best[1]}\")\n",
        "\n",
        "# Build blended test preds\n",
        "weights = best[1]\n",
        "test_blend = None\n",
        "for name, _, tpred in avail:\n",
        "    w = weights.get(name, 0.0)\n",
        "    if w == 0.0:\n",
        "        continue\n",
        "    test_blend = (tpred * w) if test_blend is None else (test_blend + tpred * w)\n",
        "eps = 1e-9\n",
        "test_blend = np.clip(test_blend, eps, 1 - eps)\n",
        "test_blend = test_blend / test_blend.sum(axis=1, keepdims=True)\n",
        "\n",
        "# Save submission\n",
        "idx_map = [list(classes).index(c) for c in submit_cols]\n",
        "probs = test_blend[:, idx_map]\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (optimized blend incl. isotonic SVC, char LR, CNB). Head:\\n', sub.head())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available models: ['lr_wc', 'nbw', 'svc_iso', 'lr_char17', 'cnb_word']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BLEND SEARCH] best OOF=0.34115 weights={'lr_wc': 0.05, 'nbw': 0.15, 'svc_iso': 0.0, 'lr_char17': 0.45, 'cnb_word': 0.3499999999999999}\nSaved submission.csv (optimized blend incl. isotonic SVC, char LR, CNB). Head:\n         id       EAP       HPL       MWS\n0  id27251  0.002815  0.994643  0.002542\n1  id09612  0.111937  0.146228  0.741835\n2  id11943  0.011674  0.003286  0.985039\n3  id19526  0.009523  0.055012  0.935465\n4  id12931  0.037538  0.029420  0.933042\n"
          ]
        }
      ]
    },
    {
      "id": "7d2fc08e-fada-4c20-9740-6de4e7bc679e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Proper NB-SVM (word+char presence) with softmax margins\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import List, Tuple, Dict\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42\n",
        "N_FOLDS = 5\n",
        "np.random.seed(SEED)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test_text = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols), f\"Classes mismatch: {classes}\"\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "# Vectorizers per expert: presence (binary=True)\n",
        "word_params = dict(analyzer='word', ngram_range=(1,3), min_df=2, max_df=1.0, lowercase=True,\n",
        "                   strip_accents='unicode', token_pattern=r\"(?u)\\b[\\w']+\\b\", binary=True, dtype=np.float32)\n",
        "char_params = dict(analyzer='char', ngram_range=(2,6), min_df=2, lowercase=False,\n",
        "                   strip_accents=None, binary=True, dtype=np.float32)\n",
        "\n",
        "def _log_count_ratio(X: sparse.csr_matrix, y_bin: np.ndarray, alpha: float = 1.0) -> np.ndarray:\n",
        "    X_pos = X[y_bin == 1]\n",
        "    X_neg = X[y_bin == 0]\n",
        "    p = np.asarray(X_pos.sum(axis=0)).ravel() + alpha\n",
        "    q = np.asarray(X_neg.sum(axis=0)).ravel() + alpha\n",
        "    r = np.log(p / q)\n",
        "    return r.astype(np.float32)\n",
        "\n",
        "def _apply_r(X: sparse.csr_matrix, r: np.ndarray) -> sparse.csr_matrix:\n",
        "    return X.multiply(r)\n",
        "\n",
        "def _softmax(m: np.ndarray, axis: int = 1) -> np.ndarray:\n",
        "    m = m - m.max(axis=axis, keepdims=True)\n",
        "    expm = np.exp(m, dtype=np.float32)\n",
        "    s = expm.sum(axis=axis, keepdims=True)\n",
        "    return expm / s\n",
        "\n",
        "def cv_nbsvm_wordchar(alpha: float = 1.0, C: float = 30.0, name: str = 'NBSVM_wc_word1_3_char2_6_bin'):\n",
        "    t0_all = time.time()\n",
        "    oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "    test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "    fold_losses = []\n",
        "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "        t0 = time.time()\n",
        "        x_tr, x_val = X_text[tr_idx], X_text[val_idx]\n",
        "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "        vw = CountVectorizer(**word_params)\n",
        "        vc = CountVectorizer(**char_params)\n",
        "        Xw_tr = vw.fit_transform(x_tr); Xw_val = vw.transform(x_val); Xw_te = vw.transform(X_test_text)\n",
        "        Xc_tr = vc.fit_transform(x_tr); Xc_val = vc.transform(x_val); Xc_te = vc.transform(X_test_text)\n",
        "        X_tr = sparse.hstack([Xw_tr, Xc_tr], format='csr')\n",
        "        X_val = sparse.hstack([Xw_val, Xc_val], format='csr')\n",
        "        X_te  = sparse.hstack([Xw_te, Xc_te], format='csr')\n",
        "        vdim = X_tr.shape[1]\n",
        "        print(f\"[${name}] Fold {fold} X_tr={X_tr.shape} vdim={vdim}\", flush=True)\n",
        "        # margins for each class\n",
        "        margins_val = np.zeros((len(val_idx), len(classes)), dtype=np.float32)\n",
        "        margins_te  = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "        for ci, c in enumerate(classes):\n",
        "            y_bin_tr = (y_tr == c).astype(np.int8)\n",
        "            r = _log_count_ratio(X_tr, y_bin_tr, alpha=alpha)\n",
        "            Xr_tr = _apply_r(X_tr, r)\n",
        "            Xr_val = _apply_r(X_val, r)\n",
        "            Xr_te  = _apply_r(X_te, r)\n",
        "            clf = LogisticRegression(solver='liblinear', penalty='l2', max_iter=2000, tol=1e-4,\n",
        "                                     C=C, n_jobs=1, random_state=SEED)\n",
        "            clf.fit(Xr_tr, y_bin_tr)\n",
        "            margins_val[:, ci] = clf.decision_function(Xr_val).astype(np.float32)\n",
        "            margins_te[:,  ci] = clf.decision_function(Xr_te).astype(np.float32)\n",
        "            del y_bin_tr, r, Xr_tr, Xr_val, Xr_te, clf\n",
        "            gc.collect()\n",
        "        proba_val = _softmax(margins_val, axis=1).astype(np.float32)\n",
        "        proba_te  = _softmax(margins_te,  axis=1).astype(np.float32)\n",
        "        oof[val_idx] = proba_val\n",
        "        test_pred += proba_te / N_FOLDS\n",
        "        loss = log_loss(y_val, proba_val, labels=list(classes))\n",
        "        fold_losses.append(loss)\n",
        "        print(f\"[{name}] Fold {fold} loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "        del X_tr, X_val, X_te, Xw_tr, Xw_val, Xw_te, Xc_tr, Xc_val, Xc_te, margins_val, margins_te, proba_val, proba_te\n",
        "        gc.collect()\n",
        "    oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "    print(f\"[{name}] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s\", flush=True)\n",
        "    return oof, test_pred, oof_loss\n",
        "\n",
        "# Run NB-SVM word+char\n",
        "alpha = 1.0\n",
        "C = 30.0\n",
        "oof_nbwc, test_nbwc, loss_nbwc = cv_nbsvm_wordchar(alpha=alpha, C=C, name='NBSVM_wc_word1_3_char2_6_bin')\n",
        "np.save('oof_nbsvm_wordchar.npy', oof_nbwc); np.save('test_nbsvm_wordchar.npy', test_nbwc)\n",
        "print(f\"[NBSVM_wc] OOF={loss_nbwc:.5f} C={C}\", flush=True)\n",
        "\n",
        "# Build submission from NB-SVM wc\n",
        "idx_map = [list(classes).index(c) for c in submit_cols]\n",
        "probs = test_nbwc[:, idx_map]\n",
        "eps = 1e-9\n",
        "probs = np.clip(probs, eps, 1-eps)\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (NBSVM wc). Head:\\n', sub.head(), flush=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[$NBSVM_wc_word1_3_char2_6_bin] Fold 1 X_tr=(14096, 368484) vdim=368484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_wc_word1_3_char2_6_bin] Fold 1 loss=0.83395 elapsed=17.66s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[$NBSVM_wc_word1_3_char2_6_bin] Fold 2 X_tr=(14097, 369846) vdim=369846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_wc_word1_3_char2_6_bin] Fold 2 loss=0.79148 elapsed=19.07s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[$NBSVM_wc_word1_3_char2_6_bin] Fold 3 X_tr=(14097, 368577) vdim=368577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_wc_word1_3_char2_6_bin] Fold 3 loss=0.84791 elapsed=18.89s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[$NBSVM_wc_word1_3_char2_6_bin] Fold 4 X_tr=(14097, 369281) vdim=369281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_wc_word1_3_char2_6_bin] Fold 4 loss=0.76345 elapsed=18.66s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[$NBSVM_wc_word1_3_char2_6_bin] Fold 5 X_tr=(14097, 369678) vdim=369678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_wc_word1_3_char2_6_bin] Fold 5 loss=0.80935 elapsed=19.03s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_wc_word1_3_char2_6_bin] OOF=0.80923; folds=[0.83395, 0.79148, 0.84791, 0.76345, 0.80935] total=93.61s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_wc] OOF=0.80923 C=30.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (NBSVM wc). Head:\n         id           EAP           HPL           MWS\n0  id27251  1.639839e-08  1.000000e+00  1.000000e-09\n1  id09612  2.211119e-02  3.770798e-01  6.008090e-01\n2  id11943  1.000000e-09  1.000000e-09  1.000000e+00\n3  id19526  4.798707e-07  3.557995e-06  9.999959e-01\n4  id12931  1.356093e-09  4.184225e-07  9.999996e-01\n"
          ]
        }
      ]
    },
    {
      "id": "2f7edcf7-f6d3-4668-8dda-ab7f12c09030",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Strong char-only LR: TF-IDF char (1-7), lowercase=False, high C sweep\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import List, Tuple, Dict\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42\n",
        "N_FOLDS = 5\n",
        "np.random.seed(SEED)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test_text = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols), f\"Classes mismatch: {classes}\"\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "# Char TF-IDF: include 1-grams, keep case and punctuation; no accent strip\n",
        "char_params = dict(analyzer='char', ngram_range=(1,7), min_df=1, lowercase=False,\n",
        "                   strip_accents=None, sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_char_fold(x_tr, x_val, x_test) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int]:\n",
        "    v = TfidfVectorizer(**char_params)\n",
        "    X_tr = v.fit_transform(x_tr)\n",
        "    X_val = v.transform(x_val)\n",
        "    X_te  = v.transform(x_test)\n",
        "    return X_tr, X_val, X_te, X_tr.shape[1]\n",
        "\n",
        "def cv_char_lr(C_grid: List[float], name: str='LR_char_1_7') -> Tuple[np.ndarray, np.ndarray, float, Dict]:\n",
        "    best = dict(loss=1e9, C=None, oof=None, test=None)\n",
        "    for C in C_grid:\n",
        "        oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "        test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "        fold_losses = []\n",
        "        t0_all = time.time()\n",
        "        print(f\"[{name}] C={C}\", flush=True)\n",
        "        for fold, (tr_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "            t0 = time.time()\n",
        "            x_tr, x_val = X_text[tr_idx], X_text[val_idx]\n",
        "            y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "            X_tr, X_val, X_te, vdim = build_char_fold(x_tr, x_val, X_test_text)\n",
        "            print(f\"  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}\", flush=True)\n",
        "            clf = LogisticRegression(solver='saga', multi_class='multinomial', penalty='l2',\n",
        "                                     C=C, max_iter=12000, tol=1e-4, n_jobs=-1, random_state=SEED)\n",
        "            clf.fit(X_tr, y_tr)\n",
        "            proba_val = clf.predict_proba(X_val).astype(np.float32)\n",
        "            proba_test = clf.predict_proba(X_te).astype(np.float32)\n",
        "            order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "            proba_val = proba_val[:, order_idx]\n",
        "            proba_test = proba_test[:, order_idx]\n",
        "            oof[val_idx] = proba_val\n",
        "            test_pred += proba_test / N_FOLDS\n",
        "            loss = log_loss(y_val, proba_val, labels=list(classes))\n",
        "            fold_losses.append(loss)\n",
        "            print(f\"  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "            del X_tr, X_val, X_te, proba_val, proba_test, clf\n",
        "            gc.collect()\n",
        "        oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "        print(f\"[{name}] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s\", flush=True)\n",
        "        if oof_loss < best['loss']:\n",
        "            best.update(loss=oof_loss, C=C, oof=oof.copy(), test=test_pred.copy())\n",
        "    return best['oof'], best['test'], best['loss'], best\n",
        "\n",
        "# Run char LR with high-C sweep\n",
        "C_grid = [16.0, 24.0, 32.0]\n",
        "oof_lr_char17, test_lr_char17, loss_lr_char17, best_lr_char17 = cv_char_lr(C_grid, name='LR_char_1_7')\n",
        "np.save('oof_lr_char_1_7.npy', oof_lr_char17); np.save('test_lr_char_1_7.npy', test_lr_char17)\n",
        "print(f\"[LR_char_1_7] BEST OOF={loss_lr_char17:.5f} with C={best_lr_char17['C']}\", flush=True)\n",
        "\n",
        "# Build submission from best char LR\n",
        "idx_map = [list(classes).index(c) for c in submit_cols]\n",
        "probs = best_lr_char17['test'][:, idx_map]\n",
        "eps = 1e-9\n",
        "probs = np.clip(probs, eps, 1-eps)\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (LR char 1-7). Head:\\n', sub.head(), flush=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_1_7] C=16.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 1047048) vdim=1047048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.38691 elapsed=75.43s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 1051824) vdim=1051824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.38322 elapsed=85.09s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 1052194) vdim=1052194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.39852 elapsed=86.69s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 1049129) vdim=1049129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.38787 elapsed=71.06s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 1049968) vdim=1049968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.37650 elapsed=85.12s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_1_7] OOF=0.38660; folds=[0.38691, 0.38322, 0.39852, 0.38787, 0.3765] total=403.79s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_1_7] C=24.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 1047048) vdim=1047048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.37900 elapsed=74.71s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 1051824) vdim=1051824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.37448 elapsed=92.76s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 1052194) vdim=1052194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.39122 elapsed=94.76s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 1049129) vdim=1049129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.38062 elapsed=83.48s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 1049968) vdim=1049968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.36824 elapsed=95.58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_1_7] OOF=0.37871; folds=[0.379, 0.37448, 0.39122, 0.38062, 0.36824] total=441.68s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_1_7] C=32.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 1047048) vdim=1047048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.37493 elapsed=84.32s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 1051824) vdim=1051824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.36982 elapsed=105.92s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 1052194) vdim=1052194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.38758 elapsed=109.78s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 1049129) vdim=1049129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.37705 elapsed=93.48s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 1049968) vdim=1049968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.36392 elapsed=109.18s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_1_7] OOF=0.37466; folds=[0.37493, 0.36982, 0.38758, 0.37705, 0.36392] total=503.07s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_1_7] BEST OOF=0.37466 with C=32.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (LR char 1-7). Head:\n         id       EAP       HPL       MWS\n0  id27251  0.004349  0.990329  0.005322\n1  id09612  0.193648  0.212092  0.594261\n2  id11943  0.024085  0.006677  0.969238\n3  id19526  0.008216  0.081952  0.909832\n4  id12931  0.038221  0.034926  0.926853\n"
          ]
        }
      ]
    },
    {
      "id": "e02b5fae-65d1-417e-b220-1f6cf8031478",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fast ComplementNB word-only (1-3) with apostrophes; add to blend\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import List, Tuple, Dict\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.metrics import log_loss\n",
        "from pathlib import Path\n",
        "\n",
        "SEED = 42\n",
        "N_FOLDS = 5\n",
        "np.random.seed(SEED)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test_text = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols), f\"Classes mismatch: {classes}\"\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "vec_word_params = dict(analyzer='word', ngram_range=(1,3), min_df=2, max_df=1.0, lowercase=True,\n",
        "                        strip_accents='unicode', token_pattern=r\"(?u)\\b[\\w']+\\b\", dtype=np.float32)\n",
        "\n",
        "def cv_cnb_word(alpha_grid: List[float], name: str='CNB_word_1_3') -> Tuple[np.ndarray, np.ndarray, float, Dict]:\n",
        "    best = dict(loss=1e9, alpha=None, oof=None, test=None)\n",
        "    for alpha in alpha_grid:\n",
        "        oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "        test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "        fold_losses = []\n",
        "        t0_all = time.time()\n",
        "        print(f\"[{name}] alpha={alpha}\", flush=True)\n",
        "        for fold, (tr_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "            t0 = time.time()\n",
        "            x_tr, x_val = X_text[tr_idx], X_text[val_idx]\n",
        "            y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "            v = CountVectorizer(**vec_word_params)\n",
        "            X_tr = v.fit_transform(x_tr)\n",
        "            X_val = v.transform(x_val)\n",
        "            X_te  = v.transform(X_test_text)\n",
        "            clf = ComplementNB(alpha=alpha)\n",
        "            clf.fit(X_tr, y_tr)\n",
        "            proba_val = clf.predict_proba(X_val).astype(np.float32)\n",
        "            proba_test = clf.predict_proba(X_te).astype(np.float32)\n",
        "            order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "            proba_val = proba_val[:, order_idx]\n",
        "            proba_test = proba_test[:, order_idx]\n",
        "            oof[val_idx] = proba_val\n",
        "            test_pred += proba_test / N_FOLDS\n",
        "            loss = log_loss(y_val, proba_val, labels=list(classes))\n",
        "            fold_losses.append(loss)\n",
        "            print(f\"  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "            del X_tr, X_val, X_te, proba_val, proba_test, clf, v\n",
        "            gc.collect()\n",
        "        oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "        print(f\"[{name}] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s\", flush=True)\n",
        "        if oof_loss < best['loss']:\n",
        "            best.update(loss=oof_loss, alpha=alpha, oof=oof.copy(), test=test_pred.copy())\n",
        "    return best['oof'], best['test'], best['loss'], best\n",
        "\n",
        "# Run CNB word-only quickly\n",
        "alpha_grid = [0.2, 0.5, 1.0]\n",
        "oof_cnb_word, test_cnb_word, loss_cnb_word, best_cnb_word = cv_cnb_word(alpha_grid, name='CNB_word_1_3')\n",
        "np.save('oof_cnb_word.npy', oof_cnb_word); np.save('test_cnb_word.npy', test_cnb_word)\n",
        "print(f\"[CNB_word] BEST OOF={loss_cnb_word:.5f} alpha={best_cnb_word['alpha']}\", flush=True)\n",
        "\n",
        "# Build submission from CNB (for inspection)\n",
        "idx_map = [list(classes).index(c) for c in submit_cols]\n",
        "probs = best_cnb_word['test'][:, idx_map]\n",
        "eps = 1e-9\n",
        "probs = np.clip(probs, eps, 1-eps)\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (CNB word). Head:\\n', sub.head(), flush=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNB_word_1_3] alpha=0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.68287 elapsed=0.94s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.64267 elapsed=0.94s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.70578 elapsed=0.94s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.65235 elapsed=0.94s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.60621 elapsed=0.94s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.57018 elapsed=0.95s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.62249 elapsed=0.95s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.57917 elapsed=0.95s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.58839 elapsed=0.94s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNB_word_1_3] OOF=0.59329; folds=[0.60621, 0.57018, 0.62249, 0.57917, 0.58839] total=5.11s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNB_word_1_3] alpha=1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.55913 elapsed=0.96s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.52643 elapsed=0.96s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.57037 elapsed=0.95s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.53416 elapsed=0.94s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.54351 elapsed=0.95s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNB_word_1_3] OOF=0.54672; folds=[0.55913, 0.52643, 0.57037, 0.53416, 0.54351] total=5.12s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CNB_word] BEST OOF=0.54672 alpha=1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (CNB word). Head:\n         id           EAP           HPL       MWS\n0  id27251  5.201592e-08  9.999968e-01  0.000003\n1  id09612  2.594946e-03  8.419059e-02  0.913214\n2  id11943  8.744872e-06  5.705953e-07  0.999991\n3  id19526  2.069279e-03  1.644513e-02  0.981486\n4  id12931  3.491907e-02  6.544923e-04  0.964426\n"
          ]
        }
      ]
    },
    {
      "id": "b90dd14f-49c7-479a-998e-0bf18ff1eb41",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Correct NB-SVM (word+char presence, row L2-normalized, softmax margins) per expert\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED=42; N_FOLDS=5\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "\n",
        "word_params=dict(analyzer='word', ngram_range=(1,3), min_df=2, max_df=1.0, lowercase=True,\n",
        "                 strip_accents='unicode', token_pattern=r\"(?u)\\b[-\\w']+\\b\", binary=True, dtype=np.float32)\n",
        "char_params=dict(analyzer='char', ngram_range=(2,6), min_df=2, lowercase=False,\n",
        "                 strip_accents=None, binary=True, dtype=np.float32)\n",
        "\n",
        "def _r(X, yb, a=1.0):\n",
        "    p = np.asarray(X[yb==1].sum(axis=0)).ravel() + a\n",
        "    q = np.asarray(X[yb==0].sum(axis=0)).ravel() + a\n",
        "    return np.log(p/q).astype(np.float32)\n",
        "\n",
        "def _softmax(m):\n",
        "    m = m - m.max(axis=1, keepdims=True)\n",
        "    e = np.exp(m); return e / e.sum(axis=1, keepdims=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "oof = np.zeros((len(train), len(classes)), np.float32)\n",
        "test_pred = np.zeros((len(test), len(classes)), np.float32)\n",
        "t0_all = time.time()\n",
        "for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
        "    t0 = time.time()\n",
        "    x_tr, x_va = X_text[tr], X_text[va]\n",
        "    y_tr, y_va = y[tr], y[va]\n",
        "    vw = CountVectorizer(**word_params); vc = CountVectorizer(**char_params)\n",
        "    Xw_tr = vw.fit_transform(x_tr); Xw_va = vw.transform(x_va); Xw_te = vw.transform(X_test)\n",
        "    Xc_tr = vc.fit_transform(x_tr); Xc_va = vc.transform(x_va); Xc_te = vc.transform(X_test)\n",
        "    X_tr = sparse.hstack([Xw_tr, Xc_tr], format='csr')\n",
        "    X_va = sparse.hstack([Xw_va, Xc_va], format='csr')\n",
        "    X_te = sparse.hstack([Xw_te, Xc_te], format='csr')\n",
        "    print(f\"[NB-SVM] Fold {fold}: X_tr={X_tr.shape}\", flush=True)\n",
        "    margins_va = np.zeros((len(va), len(classes)), np.float32)\n",
        "    margins_te = np.zeros((len(test), len(classes)), np.float32)\n",
        "    for ci, c in enumerate(classes):\n",
        "        yb = (y_tr == c).astype(np.int8)\n",
        "        r = _r(X_tr, yb, a=1.0)\n",
        "        Xr_tr = normalize(X_tr.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        Xr_va = normalize(X_va.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        Xr_te = normalize(X_te.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        clf = LogisticRegression(solver='liblinear', C=30.0, max_iter=2000, random_state=SEED)\n",
        "        clf.fit(Xr_tr, yb)\n",
        "        margins_va[:, ci] = clf.decision_function(Xr_va).astype(np.float32)\n",
        "        margins_te[:, ci] = clf.decision_function(Xr_te).astype(np.float32)\n",
        "        del yb, r, Xr_tr, Xr_va, Xr_te, clf\n",
        "        gc.collect()\n",
        "    P_va = _softmax(margins_va).astype(np.float32)\n",
        "    P_te = _softmax(margins_te).astype(np.float32)\n",
        "    oof[va] = P_va\n",
        "    test_pred += P_te / N_FOLDS\n",
        "    loss = log_loss(y_va, P_va, labels=list(classes))\n",
        "    print(f\"[NB-SVM] Fold {fold} logloss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "    del X_tr, X_va, X_te, Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, margins_va, margins_te, P_va, P_te\n",
        "    gc.collect()\n",
        "\n",
        "oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "print(f\"NB-SVM (wc presence) OOF: {oof_loss:.5f}; total_elapsed={time.time()-t0_all:.2f}s\", flush=True)\n",
        "np.save('oof_nbsvm_wc_fixed.npy', oof)\n",
        "np.save('test_nbsvm_wc_fixed.npy', test_pred)\n",
        "\n",
        "# No submission here; will use in optimized blend next\n",
        "print('Saved oof_nbsvm_wc_fixed.npy and test_nbsvm_wc_fixed.npy', flush=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM] Fold 1: X_tr=(14096, 368484)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM] Fold 1 logloss=0.38046 elapsed=14.89s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM] Fold 2: X_tr=(14097, 369846)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM] Fold 2 logloss=0.37418 elapsed=14.55s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM] Fold 3: X_tr=(14097, 368577)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM] Fold 3 logloss=0.38723 elapsed=14.55s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM] Fold 4: X_tr=(14097, 369281)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM] Fold 4 logloss=0.36116 elapsed=14.19s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM] Fold 5: X_tr=(14097, 369678)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NB-SVM] Fold 5 logloss=0.36744 elapsed=14.47s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM (wc presence) OOF: 0.37409; total_elapsed=72.93s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_nbsvm_wc_fixed.npy and test_nbsvm_wc_fixed.npy\n"
          ]
        }
      ]
    },
    {
      "id": "403143c0-c14b-4b53-bb2c-68e18f88bb3a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, time, gc\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import log_loss\n",
        "from scipy.optimize import minimize, minimize_scalar\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "y = train['author'].values\n",
        "classes = np.unique(y).tolist()\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "\n",
        "def load(p): return np.load(p) if Path(p).exists() else None\n",
        "\n",
        "# Pruned diverse set (drop weaker/duplicative bases):\n",
        "cands = [\n",
        "    ('nbsvm_wc_tweaked',          load('oof_nbsvm_wc_tweaked.npy'),          load('test_nbsvm_wc_tweaked.npy')),\n",
        "    ('lr_wc_fixed',               load('oof_lr_wordchar_fixed.npy'),         load('test_lr_wordchar_fixed.npy')),\n",
        "    ('nbsvm_char_2_6_counts',     load('oof_nbsvm_char_2_6_counts.npy'),     load('test_nbsvm_char_2_6_counts.npy')),\n",
        "    ('nbsvm_char_2_7_presence',   load('oof_nbsvm_char_2_7_presence.npy'),   load('test_nbsvm_char_2_7_presence.npy')),\n",
        "    ('lr_char_1_7',               load('oof_lr_char_1_7.npy'),               load('test_lr_char_1_7.npy')),\n",
        "    ('lr_char_1_8_fast',          load('oof_lr_char_1_8.npy'),               load('test_lr_char_1_8.npy')),\n",
        "    ('lr_char_1_8_hero',          load('oof_lr_char_1_8_hero.npy'),          load('test_lr_char_1_8_hero.npy')),\n",
        "    ('svc_charwb_1_6_sig',        load('oof_svc_charwb_1_6_sig.npy'),        load('test_svc_charwb_1_6_sig.npy')),\n",
        "    ('lr_charwb_4_8',             load('oof_lr_charwb_4_8.npy'),             load('test_lr_charwb_4_8.npy')),\n",
        "    # Removed: 'svc_word_uni_iso', 'svc_char_1_6_iso', 'sgd_char_3_7_hinge_sig' (too weak / redundant)\n",
        "]\n",
        "cands = [(n,o,t) for n,o,t in cands if (o is not None and t is not None)]\n",
        "names = [n for n,_,_ in cands]\n",
        "OOFs_raw  = [np.clip(o,1e-12,1-1e-12)/o.sum(axis=1,keepdims=True) for _,o,_ in cands]\n",
        "TESTs_raw = [np.clip(t,1e-12,1-1e-12)/t.sum(axis=1,keepdims=True) for _,_,t in cands]\n",
        "K = len(names); assert K>=2, f'Need >=2 models, got {K}'\n",
        "print('Blending models:', names)\n",
        "\n",
        "def scale_probs(P, T):\n",
        "    S = np.clip(P, 1e-12, 1-1e-12) ** (1.0/float(T))\n",
        "    return S / S.sum(axis=1, keepdims=True)\n",
        "\n",
        "# Per-model temperature calibration on OOF\n",
        "per_model_T = []\n",
        "OOFs = []\n",
        "TESTs = []\n",
        "for i in range(K):\n",
        "    Pi = OOFs_raw[i]\n",
        "    def loss_Ti(T):\n",
        "        return log_loss(y, scale_probs(Pi, T), labels=classes)\n",
        "    resTi = minimize_scalar(loss_Ti, bounds=(0.5, 5.0), method='bounded')\n",
        "    Ti = float(resTi.x)\n",
        "    per_model_T.append(Ti)\n",
        "    OOFs.append(scale_probs(OOFs_raw[i], Ti))\n",
        "    TESTs.append(scale_probs(TESTs_raw[i], Ti))\n",
        "print('Per-model T:', {names[i]: round(per_model_T[i],3) for i in range(K)})\n",
        "\n",
        "# Diagnostics: Per-model OOFs after calibration\n",
        "per_oof = {names[i]: log_loss(y, OOFs[i], labels=classes) for i in range(K)}\n",
        "print('Per-model OOF (post-cal):', {k: round(v,5) for k,v in per_oof.items()})\n",
        "\n",
        "def geo_pool_log(stacks, w):\n",
        "    A = np.zeros_like(stacks[0], dtype=np.float64)\n",
        "    for k in range(K):\n",
        "        if w[k] == 0.0: continue\n",
        "        A += w[k] * np.log(stacks[k])\n",
        "    A -= A.max(axis=1, keepdims=True)\n",
        "    P = np.exp(A); P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "def softmax(z):\n",
        "    z = z - z.max()\n",
        "    e = np.exp(z); return e / e.sum()\n",
        "\n",
        "# Objective in log-prob space; small entropy to avoid collapse\n",
        "lambda_ent = 0.005\n",
        "def obj(theta):\n",
        "    w = softmax(theta)\n",
        "    P = geo_pool_log(OOFs, w)\n",
        "    reg = lambda_ent * float(np.sum(w * (np.log(w + 1e-12))))\n",
        "    return log_loss(y, P, labels=classes) + reg\n",
        "\n",
        "# Deterministic multi-start L-BFGS (32 starts)\n",
        "best = (1e9, None, None)\n",
        "rng = np.random.RandomState(42)\n",
        "starts = [np.zeros(K)] + [rng.normal(0, 0.5, size=K) for _ in range(31)]\n",
        "for si, theta0 in enumerate(starts):\n",
        "    res = minimize(obj, theta0, method='L-BFGS-B')\n",
        "    w_try = softmax(res.x)\n",
        "    val = float(res.fun)\n",
        "    if val < best[0]:\n",
        "        best = (val, w_try.copy(), res.x.copy())\n",
        "w_raw = best[1]\n",
        "print('Best obj:', round(best[0],5))\n",
        "\n",
        "# Caps/pruning\n",
        "min_weight = 0.00\n",
        "global_cap = 0.60\n",
        "nb_cap = 0.60\n",
        "weak_cap = 0.12  # optional cap for weaker models\n",
        "w = w_raw.copy()\n",
        "w[w < min_weight] = 0.0\n",
        "w = np.minimum(w, global_cap)\n",
        "# NB-family cap\n",
        "nb_mask = np.array([n.startswith('nbsvm_') for n in names], bool)\n",
        "nb_sum = w[nb_mask].sum()\n",
        "if nb_sum > nb_cap and nb_sum > 0:\n",
        "    w[nb_mask] *= (nb_cap / nb_sum)\n",
        "# Cap very weak models (OOF > 0.45) lightly if present\n",
        "for i,n in enumerate(names):\n",
        "    try:\n",
        "        if per_oof[n] > 0.45:\n",
        "            w[i] = min(w[i], weak_cap)\n",
        "    except KeyError:\n",
        "        pass\n",
        "# Normalize capped/pruned weights\n",
        "s = w.sum()\n",
        "if s == 0:\n",
        "    w = np.ones_like(w) / len(w)\n",
        "else:\n",
        "    w = w / s\n",
        "print('Final weights:', {names[i]: round(w[i],3) for i in range(K)})\n",
        "\n",
        "# Blend OOF/Test\n",
        "P_oof = geo_pool_log(OOFs, w)\n",
        "P_test = geo_pool_log(TESTs, w)\n",
        "oof_preT = log_loss(y, P_oof, labels=classes)\n",
        "print('Blend OOF (pre-temp):', round(oof_preT,5))\n",
        "\n",
        "# Final temperature (single global)\n",
        "def scale(P,T):\n",
        "    S = np.clip(P,1e-12,1-1e-12) ** (1.0/float(T))\n",
        "    return S / S.sum(axis=1, keepdims=True)\n",
        "def loss_T(T): return log_loss(y, scale(P_oof,T), labels=classes)\n",
        "resT = minimize_scalar(loss_T, bounds=(0.5,5.0), method='bounded')\n",
        "T_opt = float(resT.x)\n",
        "P_oof_scaled = scale(P_oof, T_opt)\n",
        "oof_final = log_loss(y, P_oof_scaled, labels=classes)\n",
        "print('Final T:', round(T_opt,4), 'Final OOF:', round(oof_final,5))\n",
        "\n",
        "# Apply to test and save\n",
        "P_test_scaled = scale(P_test, T_opt)\n",
        "probs = P_test_scaled[:, [classes.index(c) for c in submit_cols]]\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0,'id',test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; elapsed=%.2fs' % (time.time()-t0))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blending models: ['nbsvm_wc_tweaked', 'lr_wc_fixed', 'nbsvm_char_2_6_counts', 'nbsvm_char_2_7_presence', 'lr_char_1_7', 'lr_char_1_8_fast', 'lr_char_1_8_hero', 'svc_charwb_1_6_sig', 'lr_charwb_4_8']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-model T: {'nbsvm_wc_tweaked': 1.55, 'lr_wc_fixed': 0.877, 'nbsvm_char_2_6_counts': 1.579, 'nbsvm_char_2_7_presence': 1.477, 'lr_char_1_7': 0.88, 'lr_char_1_8_fast': 0.825, 'lr_char_1_8_hero': 0.871, 'svc_charwb_1_6_sig': 0.803, 'lr_charwb_4_8': 1.036}\nPer-model OOF (post-cal): {'nbsvm_wc_tweaked': 0.32156, 'lr_wc_fixed': 0.36343, 'nbsvm_char_2_6_counts': 0.33019, 'nbsvm_char_2_7_presence': 0.338, 'lr_char_1_7': 0.37154, 'lr_char_1_8_fast': 0.37566, 'lr_char_1_8_hero': 0.36987, 'svc_charwb_1_6_sig': 0.39657, 'lr_charwb_4_8': 0.40984}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best obj: 0.31216\nFinal weights: {'nbsvm_wc_tweaked': 0.556, 'lr_wc_fixed': 0.084, 'nbsvm_char_2_6_counts': 0.027, 'nbsvm_char_2_7_presence': 0.037, 'lr_char_1_7': 0.093, 'lr_char_1_8_fast': 0.035, 'lr_char_1_8_hero': 0.062, 'svc_charwb_1_6_sig': 0.074, 'lr_charwb_4_8': 0.031}\nBlend OOF (pre-temp): 0.32009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final T: 0.9493 Final OOF: 0.31961\nSaved submission.csv; elapsed=91.95s\n"
          ]
        }
      ]
    },
    {
      "id": "7a9e831d-90b3-4ad4-b8fd-3b33e6d66870",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fast char-only LR: TF-IDF char (2-6), lowercase=False, min_df=2, sublinear_tf=True; C sweep\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import List, Tuple, Dict\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42\n",
        "N_FOLDS = 5\n",
        "np.random.seed(SEED)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test_text = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols), f\"Classes mismatch: {classes}\"\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "char_params = dict(analyzer='char', ngram_range=(2,6), min_df=2, lowercase=False,\n",
        "                   strip_accents=None, sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_char_fold(x_tr, x_val, x_test) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int]:\n",
        "    v = TfidfVectorizer(**char_params)\n",
        "    X_tr = v.fit_transform(x_tr)\n",
        "    X_val = v.transform(x_val)\n",
        "    X_te  = v.transform(x_test)\n",
        "    return X_tr, X_val, X_te, X_tr.shape[1]\n",
        "\n",
        "def cv_char26_lr(C_grid: List[float], name: str='LR_char_2_6') -> Tuple[np.ndarray, np.ndarray, float, Dict]:\n",
        "    best = dict(loss=1e9, C=None, oof=None, test=None)\n",
        "    for C in C_grid:\n",
        "        oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "        test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "        fold_losses = []\n",
        "        t0_all = time.time()\n",
        "        print(f\"[{name}] C={C}\", flush=True)\n",
        "        for fold, (tr_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "            t0 = time.time()\n",
        "            x_tr, x_val = X_text[tr_idx], X_text[val_idx]\n",
        "            y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "            X_tr, X_val, X_te, vdim = build_char_fold(x_tr, x_val, X_test_text)\n",
        "            print(f\"  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}\", flush=True)\n",
        "            clf = LogisticRegression(solver='saga', multi_class='multinomial', penalty='l2',\n",
        "                                     C=C, max_iter=8000, tol=1e-4, n_jobs=-1, random_state=SEED)\n",
        "            clf.fit(X_tr, y_tr)\n",
        "            proba_val = clf.predict_proba(X_val).astype(np.float32)\n",
        "            proba_test = clf.predict_proba(X_te).astype(np.float32)\n",
        "            order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "            proba_val = proba_val[:, order_idx]\n",
        "            proba_test = proba_test[:, order_idx]\n",
        "            oof[val_idx] = proba_val\n",
        "            test_pred += proba_test / N_FOLDS\n",
        "            loss = log_loss(y_val, proba_val, labels=list(classes))\n",
        "            fold_losses.append(loss)\n",
        "            print(f\"  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "            del X_tr, X_val, X_te, proba_val, proba_test, clf\n",
        "            gc.collect()\n",
        "        oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "        print(f\"[{name}] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s\", flush=True)\n",
        "        if oof_loss < best['loss']:\n",
        "            best.update(loss=oof_loss, C=C, oof=oof.copy(), test=test_pred.copy())\n",
        "    return best['oof'], best['test'], best['loss'], best\n",
        "\n",
        "# Run char(2,6) LR with C grid as per expert quick win\n",
        "C_grid = [12.0, 20.0, 32.0]\n",
        "oof_lr_char26, test_lr_char26, loss_lr_char26, best_lr_char26 = cv_char26_lr(C_grid, name='LR_char_2_6')\n",
        "np.save('oof_lr_char_2_6.npy', oof_lr_char26); np.save('test_lr_char_2_6.npy', test_lr_char26)\n",
        "print(f\"[LR_char_2_6] BEST OOF={loss_lr_char26:.5f} with C={best_lr_char26['C']}\", flush=True)\n",
        "\n",
        "# Optional: quick submission from this model\n",
        "idx_map = [list(classes).index(c) for c in submit_cols]\n",
        "probs = best_lr_char26['test'][:, idx_map]\n",
        "eps = 1e-9\n",
        "probs = np.clip(probs, eps, 1-eps)\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (LR char 2-6). Head:\\n', sub.head(), flush=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_2_6] C=12.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 293097) vdim=293097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.39090 elapsed=28.49s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 294181) vdim=294181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.38840 elapsed=34.08s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 293310) vdim=293310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.40300 elapsed=33.66s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 293748) vdim=293748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.39411 elapsed=27.29s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 294449) vdim=294449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.38137 elapsed=34.40s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_2_6] OOF=0.39155; folds=[0.3909, 0.3884, 0.403, 0.39411, 0.38137] total=158.27s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_2_6] C=20.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 293097) vdim=293097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.38241 elapsed=36.36s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 294181) vdim=294181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Run char(2,6) LR with C grid as per expert quick win\u001b[39;00m\n\u001b[32m     71\u001b[39m C_grid = [\u001b[32m12.0\u001b[39m, \u001b[32m20.0\u001b[39m, \u001b[32m32.0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m oof_lr_char26, test_lr_char26, loss_lr_char26, best_lr_char26 = \u001b[43mcv_char26_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLR_char_2_6\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m np.save(\u001b[33m'\u001b[39m\u001b[33moof_lr_char_2_6.npy\u001b[39m\u001b[33m'\u001b[39m, oof_lr_char26); np.save(\u001b[33m'\u001b[39m\u001b[33mtest_lr_char_2_6.npy\u001b[39m\u001b[33m'\u001b[39m, test_lr_char26)\n\u001b[32m     74\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LR_char_2_6] BEST OOF=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_lr_char26\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with C=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_lr_char26[\u001b[33m'\u001b[39m\u001b[33mC\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mcv_char26_lr\u001b[39m\u001b[34m(C_grid, name)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  [Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] X_tr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_tr.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m vdim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     49\u001b[39m clf = LogisticRegression(solver=\u001b[33m'\u001b[39m\u001b[33msaga\u001b[39m\u001b[33m'\u001b[39m, multi_class=\u001b[33m'\u001b[39m\u001b[33mmultinomial\u001b[39m\u001b[33m'\u001b[39m, penalty=\u001b[33m'\u001b[39m\u001b[33ml2\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     50\u001b[39m                          C=C, max_iter=\u001b[32m8000\u001b[39m, tol=\u001b[32m1e-4\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m, random_state=SEED)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m proba_val = clf.predict_proba(X_val).astype(np.float32)\n\u001b[32m     53\u001b[39m proba_test = clf.predict_proba(X_te).astype(np.float32)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1473\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m     estimator._validate_params()\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1469\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1470\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1471\u001b[39m     )\n\u001b[32m   1472\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1350\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1348\u001b[39m     n_threads = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1350\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m fold_coefs_, _, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n\u001b[32m   1376\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, \u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py:74\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     69\u001b[39m config = get_config()\n\u001b[32m     70\u001b[39m iterable_with_config = (\n\u001b[32m     71\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     73\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1952\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1946\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   1947\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   1948\u001b[39m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[32m   1949\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   1950\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1952\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1595\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1592\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1594\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1595\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1597\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1598\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1599\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1600\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1601\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1707\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1702\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1703\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1705\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1706\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1707\u001b[39m     time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1708\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1710\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1711\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1712\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "eb9c8034-50a9-4beb-af53-34f2a511a14e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Level-2 Stacking: Multinomial Logistic Regression on base-model OOF probabilities\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "y = train['author'].values\n",
        "classes = np.unique(y).tolist()\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "\n",
        "def load(p):\n",
        "    return np.load(p) if Path(p).exists() else None\n",
        "\n",
        "# Load base OOF/Test predictions (only those available will be used)\n",
        "bases = [\n",
        "    ('lr_wc_fixed', load('oof_lr_wordchar_fixed.npy'), load('test_lr_wordchar_fixed.npy')),\n",
        "    ('nbsvm_wc_fixed', load('oof_nbsvm_wc_fixed.npy'), load('test_nbsvm_wc_fixed.npy')),\n",
        "    ('lr_char_1_7', load('oof_lr_char_1_7.npy'), load('test_lr_char_1_7.npy')),\n",
        "    ('svc_char_1_6_iso', load('oof_svc_char_1_6_iso.npy'), load('test_svc_char_1_6_iso.npy')),\n",
        "    ('cnb_word', load('oof_cnb_word.npy'), load('test_cnb_word.npy')),\n",
        "    ('nbsvm_word_fast', load('oof_nbsvm_word.npy'), load('test_nbsvm_word.npy')),\n",
        "]\n",
        "bases = [(n,o,t) for n,o,t in bases if (o is not None and t is not None)]\n",
        "names = [n for n,_,_ in bases]\n",
        "assert len(bases) >= 2, 'Need at least two base models for stacking'\n",
        "print('Stacking base models:', names)\n",
        "\n",
        "# Build meta features: concatenate probabilities from each base model (order as in 'classes')\n",
        "def reorder_cols(P: np.ndarray, current_order, target_order):\n",
        "    idx = [list(current_order).index(c) for c in target_order]\n",
        "    return P[:, idx]\n",
        "\n",
        "OOFs = []\n",
        "TESTs = []\n",
        "for n,o,t in bases:\n",
        "    # assume columns are in classes order already; if unsure, clip & renorm\n",
        "    o = np.clip(o, 1e-12, 1-1e-12); o = o / o.sum(axis=1, keepdims=True)\n",
        "    t = np.clip(t, 1e-12, 1-1e-12); t = t / t.sum(axis=1, keepdims=True)\n",
        "    OOFs.append(o.astype(np.float32))\n",
        "    TESTs.append(t.astype(np.float32))\n",
        "\n",
        "X_meta = np.concatenate(OOFs, axis=1)  # shape (n_train, 3*K)\n",
        "X_test_meta = np.concatenate(TESTs, axis=1)  # shape (n_test, 3*K)\n",
        "print('Meta feature shapes:', X_meta.shape, X_test_meta.shape)\n",
        "\n",
        "# 5-fold meta CV\n",
        "SEED=42; N_FOLDS=5\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "meta_oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "meta_test = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "fold_losses = []\n",
        "t0_all = time.time()\n",
        "for fold, (tr, va) in enumerate(skf.split(X_meta, y), 1):\n",
        "    t0 = time.time()\n",
        "    X_tr, X_va = X_meta[tr], X_meta[va]\n",
        "    y_tr, y_va = y[tr], y[va]\n",
        "    clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', C=0.5, max_iter=2000, n_jobs=-1, random_state=SEED)\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    P_va = clf.predict_proba(X_va).astype(np.float32)\n",
        "    P_te = clf.predict_proba(X_test_meta).astype(np.float32)\n",
        "    # reorder to fixed class order\n",
        "    order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "    P_va = P_va[:, order_idx]; P_te = P_te[:, order_idx]\n",
        "    meta_oof[va] = P_va\n",
        "    meta_test += P_te / N_FOLDS\n",
        "    loss = log_loss(y_va, P_va, labels=classes)\n",
        "    fold_losses.append(loss)\n",
        "    print(f\"[META] Fold {fold} loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "    del X_tr, X_va, y_tr, y_va, clf, P_va, P_te; gc.collect()\n",
        "\n",
        "oof_loss = log_loss(y, meta_oof, labels=classes)\n",
        "print(f\"[META] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s\", flush=True)\n",
        "np.save('oof_meta_stack.npy', meta_oof); np.save('test_meta_stack.npy', meta_test)\n",
        "\n",
        "# Build submission\n",
        "idx_map = [classes.index(c) for c in submit_cols]\n",
        "probs = meta_test[:, idx_map]\n",
        "probs = np.clip(probs, 1e-12, 1-1e-12); probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (meta stack). Head:\\n', sub.head())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking base models: ['lr_wc_fixed', 'nbsvm_wc_fixed', 'lr_char_1_7', 'svc_char_1_6_iso', 'cnb_word', 'nbsvm_word_fast']\nMeta feature shapes: (17621, 18) (1958, 18)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[META] Fold 1 loss=0.32405 elapsed=0.86s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[META] Fold 2 loss=0.32416 elapsed=0.72s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[META] Fold 3 loss=0.33462 elapsed=0.72s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[META] Fold 4 loss=0.32360 elapsed=0.74s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[META] Fold 5 loss=0.32435 elapsed=0.73s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[META] OOF=0.32616; folds=[0.32405, 0.32416, 0.33462, 0.3236, 0.32435] total=4.12s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (meta stack). Head:\n         id       EAP       HPL       MWS\n0  id27251  0.009436  0.984666  0.005898\n1  id09612  0.068307  0.051661  0.880032\n2  id11943  0.021022  0.007304  0.971674\n3  id19526  0.031051  0.019808  0.949141\n4  id12931  0.027187  0.011376  0.961437\n"
          ]
        }
      ]
    },
    {
      "id": "71f043cb-cab0-4757-a2a5-ad2197cd10bf",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Temperature scaling for meta stack predictions\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import log_loss\n",
        "from scipy.optimize import minimize_scalar\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "y = train['author'].values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "\n",
        "oof_meta = np.load('oof_meta_stack.npy')\n",
        "test_meta = np.load('test_meta_stack.npy')\n",
        "\n",
        "def scale_probs(P, T):\n",
        "    P = np.clip(P, 1e-12, 1-1e-12)\n",
        "    S = P ** (1.0 / T)\n",
        "    return S / S.sum(axis=1, keepdims=True)\n",
        "\n",
        "def loss_T(T):\n",
        "    return log_loss(y, scale_probs(oof_meta, T), labels=list(classes))\n",
        "\n",
        "resT = minimize_scalar(loss_T, bounds=(0.5, 3.0), method='bounded')\n",
        "T_opt = float(resT.x)\n",
        "oof_scaled = scale_probs(oof_meta, T_opt)\n",
        "test_scaled = scale_probs(test_meta, T_opt)\n",
        "oof_loss_scaled = log_loss(y, oof_scaled, labels=list(classes))\n",
        "print(f\"[META TEMP] T={T_opt:.4f} OOF_scaled={oof_loss_scaled:.5f}\")\n",
        "\n",
        "# Save final submission using temperature-scaled meta predictions\n",
        "idx_map = [list(classes).index(c) for c in submit_cols]\n",
        "probs = test_scaled[:, idx_map]\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (meta stack + temperature scaling). Head:\\n', sub.head())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[META TEMP] T=0.9985 OOF_scaled=0.32616\nSaved submission.csv (meta stack + temperature scaling). Head:\n         id       EAP       HPL       MWS\n0  id27251  0.009374  0.984772  0.005854\n1  id09612  0.068085  0.051472  0.880443\n2  id11943  0.020908  0.007253  0.971838\n3  id19526  0.030905  0.019701  0.949394\n4  id12931  0.027051  0.011305  0.961644\n"
          ]
        }
      ]
    },
    {
      "id": "337bf0b9-29e9-402e-a4c5-d62e2f6dd0b6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# NB-SVM tweaked: word(1,2)+char(2,6); presence vs counts (alpha=0.5), C in [30,50]; pick best and save\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from scipy import sparse\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED=42; N_FOLDS=5\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "\n",
        "word_params_base = dict(analyzer='word', ngram_range=(1,2), min_df=2, max_df=1.0, lowercase=True,\n",
        "                        strip_accents='unicode', token_pattern=r\"(?u)\\b[-\\w']+\\b\", dtype=np.float32)\n",
        "char_params_base = dict(analyzer='char', ngram_range=(2,6), min_df=2, lowercase=False,\n",
        "                        strip_accents=None, dtype=np.float32)\n",
        "\n",
        "def _r_presence(X, yb, a=0.5):\n",
        "    p = np.asarray(X[yb==1].sum(axis=0)).ravel() + a\n",
        "    q = np.asarray(X[yb==0].sum(axis=0)).ravel() + a\n",
        "    return np.log(p/q).astype(np.float32)\n",
        "\n",
        "def _r_counts_normed(X, yb, a=0.5):\n",
        "    p = np.asarray(X[yb==1].sum(axis=0)).ravel() + a\n",
        "    q = np.asarray(X[yb==0].sum(axis=0)).ravel() + a\n",
        "    p = p / (p.sum() + a)\n",
        "    q = q / (q.sum() + a)\n",
        "    return np.log(p/q).astype(np.float32)\n",
        "\n",
        "def _softmax(m):\n",
        "    m = m - m.max(axis=1, keepdims=True)\n",
        "    e = np.exp(m); return e / e.sum(axis=1, keepdims=True)\n",
        "\n",
        "def run_nbsvm(variant='presence', C=30.0, alpha=0.5):\n",
        "    wp = word_params_base.copy(); cp = char_params_base.copy()\n",
        "    if variant == 'presence':\n",
        "        wp.update(binary=True); cp.update(binary=True)\n",
        "        r_func = _r_presence\n",
        "    elif variant == 'counts':\n",
        "        wp.update(binary=False); cp.update(binary=False)\n",
        "        r_func = _r_counts_normed\n",
        "    else:\n",
        "        raise ValueError('Unknown variant')\n",
        "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "    oof = np.zeros((len(train), len(classes)), np.float32)\n",
        "    test_pred = np.zeros((len(test), len(classes)), np.float32)\n",
        "    fold_losses = []\n",
        "    t0_all = time.time()\n",
        "    for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
        "        t0 = time.time()\n",
        "        x_tr, x_va = X_text[tr], X_text[va]\n",
        "        y_tr, y_va = y[tr], y[va]\n",
        "        vw = CountVectorizer(**wp); vc = CountVectorizer(**cp)\n",
        "        Xw_tr = vw.fit_transform(x_tr); Xw_va = vw.transform(x_va); Xw_te = vw.transform(X_test)\n",
        "        Xc_tr = vc.fit_transform(x_tr); Xc_va = vc.transform(x_va); Xc_te = vc.transform(X_test)\n",
        "        X_tr = sparse.hstack([Xw_tr, Xc_tr], format='csr')\n",
        "        X_va = sparse.hstack([Xw_va, Xc_va], format='csr')\n",
        "        X_te = sparse.hstack([Xw_te, Xc_te], format='csr')\n",
        "        margins_va = np.zeros((len(va), len(classes)), np.float32)\n",
        "        margins_te = np.zeros((len(test), len(classes)), np.float32)\n",
        "        for ci, c in enumerate(classes):\n",
        "            yb = (y_tr == c).astype(np.int8)\n",
        "            r = r_func(X_tr, yb, a=alpha)\n",
        "            Xr_tr = normalize(X_tr.multiply(r), norm='l2', axis=1, copy=False)\n",
        "            Xr_va = normalize(X_va.multiply(r), norm='l2', axis=1, copy=False)\n",
        "            Xr_te = normalize(X_te.multiply(r), norm='l2', axis=1, copy=False)\n",
        "            clf = LogisticRegression(solver='liblinear', C=C, max_iter=2000, random_state=SEED)\n",
        "            clf.fit(Xr_tr, yb)\n",
        "            margins_va[:, ci] = clf.decision_function(Xr_va).astype(np.float32)\n",
        "            margins_te[:, ci] = clf.decision_function(Xr_te).astype(np.float32)\n",
        "            del yb, r, Xr_tr, Xr_va, Xr_te, clf\n",
        "            gc.collect()\n",
        "        P_va = _softmax(margins_va).astype(np.float32)\n",
        "        P_te = _softmax(margins_te).astype(np.float32)\n",
        "        oof[va] = P_va\n",
        "        test_pred += P_te / N_FOLDS\n",
        "        loss = log_loss(y_va, P_va, labels=list(classes))\n",
        "        fold_losses.append(loss)\n",
        "        print(f\"[NBSVM {variant} C={C}] Fold {fold} loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "        del X_tr, X_va, X_te, Xw_tr, Xw_va, Xw_te, Xc_tr, Xc_va, Xc_te, margins_va, margins_te, P_va, P_te\n",
        "        gc.collect()\n",
        "    oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "    print(f\"[NBSVM {variant} C={C}] OOF={oof_loss:.5f}; total={time.time()-t0_all:.2f}s\", flush=True)\n",
        "    return oof, test_pred, oof_loss\n",
        "\n",
        "# Grid over variants and C; pick best\n",
        "configs = [(v, C) for v in ('presence','counts') for C in (30.0, 50.0)]\n",
        "best = dict(loss=1e9, variant=None, C=None, oof=None, test=None)\n",
        "for variant, C in configs:\n",
        "    oof_, test_, loss_ = run_nbsvm(variant=variant, C=C, alpha=0.5)\n",
        "    if loss_ < best['loss']:\n",
        "        best.update(loss=loss_, variant=variant, C=C, oof=oof_, test=test_)\n",
        "\n",
        "print(f\"[NBSVM BEST] variant={best['variant']} C={best['C']} OOF={best['loss']:.5f}\")\n",
        "np.save('oof_nbsvm_wc_tweaked.npy', best['oof'])\n",
        "np.save('test_nbsvm_wc_tweaked.npy', best['test'])\n",
        "print('Saved oof_nbsvm_wc_tweaked.npy and test_nbsvm_wc_tweaked.npy')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM presence C=30.0] Fold 1 loss=0.36812 elapsed=14.29s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM presence C=30.0] Fold 2 loss=0.36319 elapsed=14.19s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM presence C=30.0] Fold 3 loss=0.37374 elapsed=14.18s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM presence C=30.0] Fold 4 loss=0.34871 elapsed=14.17s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM presence C=30.0] Fold 5 loss=0.35646 elapsed=14.05s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM presence C=30.0] OOF=0.36204; total=71.18s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM presence C=50.0] Fold 1 loss=0.38758 elapsed=13.96s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM presence C=50.0] Fold 2 loss=0.38165 elapsed=14.05s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM presence C=50.0] Fold 3 loss=0.39283 elapsed=13.84s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM presence C=50.0] Fold 4 loss=0.36632 elapsed=13.72s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM presence C=50.0] Fold 5 loss=0.37434 elapsed=14.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM presence C=50.0] OOF=0.38054; total=69.87s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM counts C=30.0] Fold 1 loss=0.36347 elapsed=13.43s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM counts C=30.0] Fold 2 loss=0.34677 elapsed=13.37s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM counts C=30.0] Fold 3 loss=0.37334 elapsed=13.64s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM counts C=30.0] Fold 4 loss=0.35811 elapsed=13.51s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM counts C=30.0] Fold 5 loss=0.34077 elapsed=13.09s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM counts C=30.0] OOF=0.35649; total=67.33s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM counts C=50.0] Fold 1 loss=0.38089 elapsed=13.40s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM counts C=50.0] Fold 2 loss=0.36180 elapsed=13.30s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM counts C=50.0] Fold 3 loss=0.39125 elapsed=13.41s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM counts C=50.0] Fold 4 loss=0.37514 elapsed=13.47s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM counts C=50.0] Fold 5 loss=0.35629 elapsed=13.49s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM counts C=50.0] OOF=0.37307; total=67.37s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM BEST] variant=counts C=30.0 OOF=0.35649\nSaved oof_nbsvm_wc_tweaked.npy and test_nbsvm_wc_tweaked.npy\n"
          ]
        }
      ]
    },
    {
      "id": "461eb720-693b-491e-9059-9d70b1bbf1ee",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Char-only LR: TF-IDF char (2-6), lowercase=True (diverse), min_df=2, sublinear_tf=True; C sweep\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import List, Tuple, Dict\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42\n",
        "N_FOLDS = 5\n",
        "np.random.seed(SEED)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test_text = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols), f\"Classes mismatch: {classes}\"\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "char_params_lower = dict(analyzer='char', ngram_range=(2,6), min_df=2, lowercase=True,\n",
        "                         strip_accents='unicode', sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_char_fold_lower(x_tr, x_val, x_test) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int]:\n",
        "    v = TfidfVectorizer(**char_params_lower)\n",
        "    X_tr = v.fit_transform(x_tr)\n",
        "    X_val = v.transform(x_val)\n",
        "    X_te  = v.transform(x_test)\n",
        "    return X_tr, X_val, X_te, X_tr.shape[1]\n",
        "\n",
        "def cv_char26_lr_lower(C_grid: List[float], name: str='LR_char_2_6_lower') -> Tuple[np.ndarray, np.ndarray, float, Dict]:\n",
        "    best = dict(loss=1e9, C=None, oof=None, test=None)\n",
        "    for C in C_grid:\n",
        "        oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "        test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "        fold_losses = []\n",
        "        t0_all = time.time()\n",
        "        print(f\"[{name}] C={C}\", flush=True)\n",
        "        for fold, (tr_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "            t0 = time.time()\n",
        "            x_tr, x_val = X_text[tr_idx], X_text[val_idx]\n",
        "            y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "            X_tr, X_val, X_te, vdim = build_char_fold_lower(x_tr, x_val, X_test_text)\n",
        "            print(f\"  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}\", flush=True)\n",
        "            clf = LogisticRegression(solver='saga', multi_class='multinomial', penalty='l2',\n",
        "                                     C=C, max_iter=8000, tol=1e-4, n_jobs=-1, random_state=SEED)\n",
        "            clf.fit(X_tr, y_tr)\n",
        "            proba_val = clf.predict_proba(X_val).astype(np.float32)\n",
        "            proba_test = clf.predict_proba(X_te).astype(np.float32)\n",
        "            order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "            proba_val = proba_val[:, order_idx]\n",
        "            proba_test = proba_test[:, order_idx]\n",
        "            oof[val_idx] = proba_val\n",
        "            test_pred += proba_test / N_FOLDS\n",
        "            loss = log_loss(y_val, proba_val, labels=list(classes))\n",
        "            fold_losses.append(loss)\n",
        "            print(f\"  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "            del X_tr, X_val, X_te, proba_val, proba_test, clf\n",
        "            gc.collect()\n",
        "        oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "        print(f\"[{name}] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s\", flush=True)\n",
        "        if oof_loss < best['loss']:\n",
        "            best.update(loss=oof_loss, C=C, oof=oof.copy(), test=test_pred.copy())\n",
        "    return best['oof'], best['test'], best['loss'], best\n",
        "\n",
        "# Run char(2,6) lowercase LR with C grid\n",
        "C_grid = [12.0, 20.0, 32.0]\n",
        "oof_lr_char26_lower, test_lr_char26_lower, loss_lr_char26_lower, best_lr_char26_lower = cv_char26_lr_lower(C_grid, name='LR_char_2_6_lower')\n",
        "np.save('oof_lr_char_2_6_lower.npy', oof_lr_char26_lower); np.save('test_lr_char_2_6_lower.npy', test_lr_char26_lower)\n",
        "print(f\"[LR_char_2_6_lower] BEST OOF={loss_lr_char26_lower:.5f} with C={best_lr_char26_lower['C']}\", flush=True)\n",
        "\n",
        "# Optional: quick submission from this model\n",
        "idx_map = [list(classes).index(c) for c in submit_cols]\n",
        "probs = best_lr_char26_lower['test'][:, idx_map]\n",
        "eps = 1e-9\n",
        "probs = np.clip(probs, eps, 1-eps)\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (LR char 2-6 lowercase). Head:\\n', sub.head(), flush=True)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_2_6_lower] C=12.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 277345) vdim=277345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.39519 elapsed=28.89s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 278200) vdim=278200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.39413 elapsed=34.48s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 277563) vdim=277563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.40774 elapsed=33.88s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 278079) vdim=278079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.40166 elapsed=27.30s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 278322) vdim=278322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.38833 elapsed=35.07s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_2_6_lower] OOF=0.39741; folds=[0.39519, 0.39413, 0.40774, 0.40166, 0.38833] total=159.97s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_2_6_lower] C=20.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 277345) vdim=277345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.38702 elapsed=36.28s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 278200) vdim=278200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.38482 elapsed=42.30s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 277563) vdim=277563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.40072 elapsed=41.88s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 278079) vdim=278079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.39484 elapsed=35.11s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 278322) vdim=278322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.38012 elapsed=43.59s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_2_6_lower] OOF=0.38950; folds=[0.38702, 0.38482, 0.40072, 0.39484, 0.38012] total=199.51s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_2_6_lower] C=32.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 277345) vdim=277345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.38383 elapsed=40.21s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 278200) vdim=278200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.38064 elapsed=51.35s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 277563) vdim=277563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.39858 elapsed=51.24s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 278079) vdim=278079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.39294 elapsed=43.96s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 278322) vdim=278322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.37691 elapsed=50.40s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_2_6_lower] OOF=0.38658; folds=[0.38383, 0.38064, 0.39858, 0.39294, 0.37691] total=237.51s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_2_6_lower] BEST OOF=0.38658 with C=32.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (LR char 2-6 lowercase). Head:\n         id       EAP       HPL       MWS\n0  id27251  0.008741  0.985476  0.005784\n1  id09612  0.185323  0.209027  0.605651\n2  id11943  0.024677  0.003342  0.971981\n3  id19526  0.003913  0.037732  0.958355\n4  id12931  0.023281  0.040654  0.936065\n"
          ]
        }
      ]
    },
    {
      "id": "c8ff8ca4-af02-4987-a149-7451afe94243",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# New base model: LR with analyzer='char_wb' ngram_range=(3,6), lowercase=False; C sweep [8,16,24]\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import List, Tuple, Dict\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42\n",
        "N_FOLDS = 5\n",
        "np.random.seed(SEED)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test_text = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols), f\"Classes mismatch: {classes}\"\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "charwb_params = dict(analyzer='char_wb', ngram_range=(3,6), min_df=2, lowercase=False,\n",
        "                     strip_accents=None, sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_charwb_fold(x_tr, x_val, x_test) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int]:\n",
        "    v = TfidfVectorizer(**charwb_params)\n",
        "    X_tr = v.fit_transform(x_tr)\n",
        "    X_val = v.transform(x_val)\n",
        "    X_te  = v.transform(x_test)\n",
        "    return X_tr, X_val, X_te, X_tr.shape[1]\n",
        "\n",
        "def cv_charwb_lr(C_grid: List[float], name: str='LR_charwb_3_6') -> Tuple[np.ndarray, np.ndarray, float, Dict]:\n",
        "    best = dict(loss=1e9, C=None, oof=None, test=None)\n",
        "    for C in C_grid:\n",
        "        oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "        test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "        fold_losses = []\n",
        "        t0_all = time.time()\n",
        "        print(f\"[{name}] C={C}\", flush=True)\n",
        "        for fold, (tr_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "            t0 = time.time()\n",
        "            x_tr, x_val = X_text[tr_idx], X_text[val_idx]\n",
        "            y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "            X_tr, X_val, X_te, vdim = build_charwb_fold(x_tr, x_val, X_test_text)\n",
        "            print(f\"  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}\", flush=True)\n",
        "            clf = LogisticRegression(solver='saga', multi_class='multinomial', penalty='l2',\n",
        "                                     C=C, max_iter=10000, tol=1e-4, n_jobs=-1, random_state=SEED)\n",
        "            clf.fit(X_tr, y_tr)\n",
        "            proba_val = clf.predict_proba(X_val).astype(np.float32)\n",
        "            proba_test = clf.predict_proba(X_te).astype(np.float32)\n",
        "            order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "            proba_val = proba_val[:, order_idx]\n",
        "            proba_test = proba_test[:, order_idx]\n",
        "            oof[val_idx] = proba_val\n",
        "            test_pred += proba_test / N_FOLDS\n",
        "            loss = log_loss(y_val, proba_val, labels=list(classes))\n",
        "            fold_losses.append(loss)\n",
        "            print(f\"  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "            del X_tr, X_val, X_te, proba_val, proba_test, clf\n",
        "            gc.collect()\n",
        "        oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "        print(f\"[{name}] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s\", flush=True)\n",
        "        if oof_loss < best['loss']:\n",
        "            best.update(loss=oof_loss, C=C, oof=oof.copy(), test=test_pred.copy())\n",
        "    return best['oof'], best['test'], best['loss'], best\n",
        "\n",
        "# Run LR char_wb(3,6) with C sweep\n",
        "C_grid = [8.0, 16.0, 24.0]\n",
        "oof_lr_charwb_36, test_lr_charwb_36, loss_lr_charwb_36, best_charwb_36 = cv_charwb_lr(C_grid, name='LR_charwb_3_6')\n",
        "np.save('oof_lr_charwb_3_6.npy', oof_lr_charwb_36); np.save('test_lr_charwb_3_6.npy', test_lr_charwb_36)\n",
        "print(f\"[LR_charwb_3_6] BEST OOF={loss_lr_charwb_36:.5f} with C={best_charwb_36['C']}\", flush=True)\n",
        "\n",
        "# Quick submission from best char_wb model\n",
        "idx_map = [list(classes).index(c) for c in submit_cols]\n",
        "probs = best_charwb_36['test'][:, idx_map]\n",
        "eps = 1e-9\n",
        "probs = np.clip(probs, eps, 1-eps)\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (LR char_wb 3-6). Head:\\n', sub.head(), flush=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_3_6] C=8.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 117398) vdim=117398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.39976 elapsed=5.94s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 117813) vdim=117813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.39901 elapsed=9.49s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 117656) vdim=117656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.41587 elapsed=8.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 117909) vdim=117909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.40470 elapsed=5.99s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 117984) vdim=117984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.39950 elapsed=8.62s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_3_6] OOF=0.40377; folds=[0.39976, 0.39901, 0.41587, 0.4047, 0.3995] total=38.39s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_3_6] C=16.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 117398) vdim=117398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.39988 elapsed=8.63s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 117813) vdim=117813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.39451 elapsed=13.72s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 117656) vdim=117656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.41683 elapsed=12.49s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 117909) vdim=117909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.40362 elapsed=8.03s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 117984) vdim=117984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.39843 elapsed=13.01s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_3_6] OOF=0.40266; folds=[0.39988, 0.39451, 0.41683, 0.40362, 0.39843] total=56.22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_3_6] C=24.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 117398) vdim=117398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.40588 elapsed=11.50s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 117813) vdim=117813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.39783 elapsed=16.77s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 117656) vdim=117656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.42332 elapsed=15.37s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 117909) vdim=117909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.40890 elapsed=11.33s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 117984) vdim=117984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.40362 elapsed=15.80s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_3_6] OOF=0.40791; folds=[0.40588, 0.39783, 0.42332, 0.4089, 0.40362] total=71.11s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_3_6] BEST OOF=0.40266 with C=16.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (LR char_wb 3-6). Head:\n         id       EAP       HPL       MWS\n0  id27251  0.006199  0.992111  0.001690\n1  id09612  0.232643  0.064667  0.702690\n2  id11943  0.006258  0.000503  0.993239\n3  id19526  0.019338  0.070578  0.910085\n4  id12931  0.011832  0.023729  0.964439\n"
          ]
        }
      ]
    },
    {
      "id": "81480d48-ce87-4ba9-b9b3-f6733cc796fe",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# New base: Calibrated RidgeClassifier on word TF-IDF (1-3), token pattern keeps apostrophes/hyphens\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import List, Tuple, Dict\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42\n",
        "N_FOLDS = 5\n",
        "np.random.seed(SEED)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test_text = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols), f\"Classes mismatch: {classes}\"\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "word_params = dict(analyzer='word', ngram_range=(1,3), min_df=2, lowercase=True,\n",
        "                   strip_accents='unicode', token_pattern=r\"(?u)\\b[-\\w']+\\b\",\n",
        "                   sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_word_fold(x_tr, x_val, x_test) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int]:\n",
        "    v = TfidfVectorizer(**word_params)\n",
        "    X_tr = v.fit_transform(x_tr)\n",
        "    X_val = v.transform(x_val)\n",
        "    X_te  = v.transform(x_test)\n",
        "    return X_tr, X_val, X_te, X_tr.shape[1]\n",
        "\n",
        "def cv_calibrated_ridge(alpha_grid: List[float], name: str='CalibRidge_word_1_3') -> Tuple[np.ndarray, np.ndarray, float, Dict]:\n",
        "    best = dict(loss=1e9, alpha=None, oof=None, test=None)\n",
        "    for alpha in alpha_grid:\n",
        "        oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "        test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "        fold_losses = []\n",
        "        t0_all = time.time()\n",
        "        print(f\"[{name}] alpha={alpha}\", flush=True)\n",
        "        for fold, (tr_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "            t0 = time.time()\n",
        "            x_tr, x_val = X_text[tr_idx], X_text[val_idx]\n",
        "            y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "            X_tr, X_val, X_te, vdim = build_word_fold(x_tr, x_val, X_test_text)\n",
        "            print(f\"  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}\", flush=True)\n",
        "            base = RidgeClassifier(alpha=alpha, random_state=SEED)\n",
        "            # sklearn >=1.4 uses 'estimator' instead of 'base_estimator'\n",
        "            clf = CalibratedClassifierCV(estimator=base, method='sigmoid', cv=5, n_jobs=-1)\n",
        "            clf.fit(X_tr, y_tr)\n",
        "            proba_val = clf.predict_proba(X_val).astype(np.float32)\n",
        "            proba_test = clf.predict_proba(X_te).astype(np.float32)\n",
        "            order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "            proba_val = proba_val[:, order_idx]\n",
        "            proba_test = proba_test[:, order_idx]\n",
        "            oof[val_idx] = proba_val\n",
        "            test_pred += proba_test / N_FOLDS\n",
        "            loss = log_loss(y_val, proba_val, labels=list(classes))\n",
        "            fold_losses.append(loss)\n",
        "            print(f\"  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "            del X_tr, X_val, X_te, proba_val, proba_test, clf, base\n",
        "            gc.collect()\n",
        "        oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "        print(f\"[{name}] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s\", flush=True)\n",
        "        if oof_loss < best['loss']:\n",
        "            best.update(loss=oof_loss, alpha=alpha, oof=oof.copy(), test=test_pred.copy())\n",
        "    return best['oof'], best['test'], best['loss'], best\n",
        "\n",
        "# Run calibrated Ridge with small alpha grid\n",
        "alpha_grid = [2.0, 1.0, 4.0]\n",
        "oof_ridge_word, test_ridge_word, loss_ridge_word, best_ridge = cv_calibrated_ridge(alpha_grid, name='CalibRidge_word_1_3')\n",
        "np.save('oof_ridge_word.npy', oof_ridge_word); np.save('test_ridge_word.npy', test_ridge_word)\n",
        "print(f\"[CalibRidge_word_1_3] BEST OOF={loss_ridge_word:.5f} alpha={best_ridge['alpha']}\", flush=True)\n",
        "\n",
        "# Optional quick submission from calibrated Ridge\n",
        "idx_map = [list(classes).index(c) for c in submit_cols]\n",
        "probs = best_ridge['test'][:, idx_map]\n",
        "eps = 1e-9\n",
        "probs = np.clip(probs, eps, 1-eps)\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Calibrated Ridge word 1-3). Head:\\n', sub.head(), flush=True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibRidge_word_1_3] alpha=2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 75387) vdim=75387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.46513 elapsed=2.15s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 75665) vdim=75665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.46835 elapsed=1.78s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 75267) vdim=75267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.47315 elapsed=1.73s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 75533) vdim=75533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.46238 elapsed=1.73s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 75229) vdim=75229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.46752 elapsed=1.72s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibRidge_word_1_3] OOF=0.46730; folds=[0.46513, 0.46835, 0.47315, 0.46238, 0.46752] total=9.66s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibRidge_word_1_3] alpha=1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 75387) vdim=75387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.43918 elapsed=1.73s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 75665) vdim=75665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.44207 elapsed=1.75s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 75267) vdim=75267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.44729 elapsed=1.75s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 75533) vdim=75533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.43559 elapsed=1.20s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 75229) vdim=75229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.44300 elapsed=1.22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibRidge_word_1_3] OOF=0.44143; folds=[0.43918, 0.44207, 0.44729, 0.43559, 0.443] total=8.15s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibRidge_word_1_3] alpha=4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 75387) vdim=75387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.50802 elapsed=1.21s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 75665) vdim=75665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.50931 elapsed=1.19s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 75267) vdim=75267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.51530 elapsed=1.17s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 75533) vdim=75533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.50476 elapsed=1.20s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 75229) vdim=75229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.50717 elapsed=1.21s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibRidge_word_1_3] OOF=0.50891; folds=[0.50802, 0.50931, 0.5153, 0.50476, 0.50717] total=6.47s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibRidge_word_1_3] BEST OOF=0.44143 alpha=1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (Calibrated Ridge word 1-3). Head:\n         id       EAP       HPL       MWS\n0  id27251  0.014615  0.970435  0.014950\n1  id09612  0.155445  0.115151  0.729403\n2  id11943  0.025075  0.012276  0.962649\n3  id19526  0.037478  0.061502  0.901020\n4  id12931  0.155919  0.026078  0.818003\n"
          ]
        }
      ]
    },
    {
      "id": "d6468a22-14bc-4965-9cc4-357ac22ebffb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# New base: Char-only NB-SVM (counts) with char ngrams (2,6), min_df=2, lowercase=False; alpha=0.5, C=30\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from scipy import sparse\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED=42; N_FOLDS=5\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "\n",
        "char_params = dict(analyzer='char', ngram_range=(2,6), min_df=2, lowercase=False,\n",
        "                   strip_accents=None, binary=False, dtype=np.float32)\n",
        "\n",
        "def _r_counts_normed(X, yb, a=0.5):\n",
        "    p = np.asarray(X[yb==1].sum(axis=0)).ravel() + a\n",
        "    q = np.asarray(X[yb==0].sum(axis=0)).ravel() + a\n",
        "    p = p / (p.sum() + a)\n",
        "    q = q / (q.sum() + a)\n",
        "    return np.log(p/q).astype(np.float32)\n",
        "\n",
        "def _softmax(m):\n",
        "    m = m - m.max(axis=1, keepdims=True)\n",
        "    e = np.exp(m); return e / e.sum(axis=1, keepdims=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "oof = np.zeros((len(train), len(classes)), np.float32)\n",
        "test_pred = np.zeros((len(test), len(classes)), np.float32)\n",
        "fold_losses = []\n",
        "t0_all = time.time()\n",
        "for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
        "    t0 = time.time()\n",
        "    x_tr, x_va = X_text[tr], X_text[va]\n",
        "    y_tr, y_va = y[tr], y[va]\n",
        "    vc = CountVectorizer(**char_params)\n",
        "    Xc_tr = vc.fit_transform(x_tr); Xc_va = vc.transform(x_va); Xc_te = vc.transform(X_test)\n",
        "    print(f\"[NBSVM_char_counts] Fold {fold} X_tr={Xc_tr.shape}\", flush=True)\n",
        "    margins_va = np.zeros((len(va), len(classes)), np.float32)\n",
        "    margins_te = np.zeros((len(test), len(classes)), np.float32)\n",
        "    for ci, c in enumerate(classes):\n",
        "        yb = (y_tr == c).astype(np.int8)\n",
        "        r = _r_counts_normed(Xc_tr, yb, a=0.5)\n",
        "        Xr_tr = normalize(Xc_tr.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        Xr_va = normalize(Xc_va.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        Xr_te = normalize(Xc_te.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        clf = LogisticRegression(solver='liblinear', C=30.0, max_iter=2000, random_state=SEED)\n",
        "        clf.fit(Xr_tr, yb)\n",
        "        margins_va[:, ci] = clf.decision_function(Xr_va).astype(np.float32)\n",
        "        margins_te[:, ci] = clf.decision_function(Xr_te).astype(np.float32)\n",
        "        del yb, r, Xr_tr, Xr_va, Xr_te, clf\n",
        "        gc.collect()\n",
        "    P_va = _softmax(margins_va).astype(np.float32)\n",
        "    P_te = _softmax(margins_te).astype(np.float32)\n",
        "    oof[va] = P_va\n",
        "    test_pred += P_te / N_FOLDS\n",
        "    loss = log_loss(y_va, P_va, labels=list(classes))\n",
        "    fold_losses.append(loss)\n",
        "    print(f\"[NBSVM_char_counts] Fold {fold} loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "    del Xc_tr, Xc_va, Xc_te, margins_va, margins_te, P_va, P_te, vc\n",
        "    gc.collect()\n",
        "\n",
        "oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "print(f\"[NBSVM_char_counts] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s\", flush=True)\n",
        "np.save('oof_nbsvm_char_2_6_counts.npy', oof)\n",
        "np.save('test_nbsvm_char_2_6_counts.npy', test_pred)\n",
        "print('Saved oof_nbsvm_char_2_6_counts.npy and test_nbsvm_char_2_6_counts.npy', flush=True)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_char_counts] Fold 1 X_tr=(14096, 293097)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_char_counts] Fold 1 loss=0.37768 elapsed=12.24s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_char_counts] Fold 2 X_tr=(14097, 294181)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_char_counts] Fold 2 loss=0.35898 elapsed=12.19s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_char_counts] Fold 3 X_tr=(14097, 293310)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_char_counts] Fold 3 loss=0.38402 elapsed=11.65s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_char_counts] Fold 4 X_tr=(14097, 293748)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_char_counts] Fold 4 loss=0.37070 elapsed=11.91s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_char_counts] Fold 5 X_tr=(14097, 294449)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_char_counts] Fold 5 loss=0.35348 elapsed=12.25s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_char_counts] OOF=0.36897; folds=[0.37768, 0.35898, 0.38402, 0.3707, 0.35348] total=60.76s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_nbsvm_char_2_6_counts.npy and test_nbsvm_char_2_6_counts.npy\n"
          ]
        }
      ]
    },
    {
      "id": "1315ac72-0a39-43d9-9173-1765a288c810",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# New base: Strong Char-only LR (1-8), lowercase=False, sublinear_tf=True; faster variant (min_df=2, OvR/liblinear, single C=32)\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import List, Tuple, Dict\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42\n",
        "N_FOLDS = 5\n",
        "np.random.seed(SEED)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test_text = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols), f\"Classes mismatch: {classes}\"\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "# Speed tweak: min_df=2 to reduce vdim; keep case/punct; (1,8) char range\n",
        "char_params = dict(analyzer='char', ngram_range=(1,8), min_df=2, lowercase=False,\n",
        "                   strip_accents=None, sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_char_fold(x_tr, x_val, x_test) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int]:\n",
        "    v = TfidfVectorizer(**char_params)\n",
        "    X_tr = v.fit_transform(x_tr)\n",
        "    X_val = v.transform(x_val)\n",
        "    X_te  = v.transform(x_test)\n",
        "    return X_tr, X_val, X_te, X_tr.shape[1]\n",
        "\n",
        "def cv_char18_lr_fast(C: float = 32.0, name: str='LR_char_1_8_fast') -> Tuple[np.ndarray, np.ndarray, float, Dict]:\n",
        "    oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "    test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "    fold_losses = []\n",
        "    t0_all = time.time()\n",
        "    print(f\"[{name}] C={C}\", flush=True)\n",
        "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "        t0 = time.time()\n",
        "        x_tr, x_val = X_text[tr_idx], X_text[val_idx]\n",
        "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "        X_tr, X_val, X_te, vdim = build_char_fold(x_tr, x_val, X_test_text)\n",
        "        print(f\"  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}\", flush=True)\n",
        "        # Faster on huge vdim: OvR/liblinear instead of multinomial/saga\n",
        "        clf = LogisticRegression(solver='liblinear', multi_class='ovr', penalty='l2',\n",
        "                                 C=C, max_iter=5000, tol=1e-4, n_jobs=1, random_state=SEED)\n",
        "        clf.fit(X_tr, y_tr)\n",
        "        proba_val = clf.predict_proba(X_val).astype(np.float32)\n",
        "        proba_test = clf.predict_proba(X_te).astype(np.float32)\n",
        "        order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "        proba_val = proba_val[:, order_idx]\n",
        "        proba_test = proba_test[:, order_idx]\n",
        "        oof[val_idx] = proba_val\n",
        "        test_pred += proba_test / N_FOLDS\n",
        "        loss = log_loss(y_val, proba_val, labels=list(classes))\n",
        "        fold_losses.append(loss)\n",
        "        print(f\"  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "        del X_tr, X_val, X_te, proba_val, proba_test, clf\n",
        "        gc.collect()\n",
        "    oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "    print(f\"[{name}] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s\", flush=True)\n",
        "    meta = dict(C=C, folds=fold_losses, oof=oof_loss)\n",
        "    return oof, test_pred, oof_loss, meta\n",
        "\n",
        "# Train single strong C to keep runtime reasonable\n",
        "oof_lr_char18, test_lr_char18, loss_lr_char18, meta_lr_char18 = cv_char18_lr_fast(C=32.0, name='LR_char_1_8_fast')\n",
        "np.save('oof_lr_char_1_8.npy', oof_lr_char18); np.save('test_lr_char_1_8.npy', test_lr_char18)\n",
        "print(f\"[LR_char_1_8_fast] OOF={loss_lr_char18:.5f} with C={meta_lr_char18['C']}\", flush=True)\n",
        "\n",
        "# Optional: quick submission preview\n",
        "idx_map = [list(classes).index(c) for c in submit_cols]\n",
        "probs = test_lr_char18[:, idx_map]\n",
        "eps = 1e-9\n",
        "probs = np.clip(probs, eps, 1-eps)\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (LR char 1-8 fast). Head:\\n', sub.head(), flush=True)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_1_8_fast] C=32.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 768122) vdim=768122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.38229 elapsed=19.34s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 770255) vdim=770255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.37923 elapsed=18.41s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 767867) vdim=767867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.39507 elapsed=18.41s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 769029) vdim=769029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.38627 elapsed=18.34s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 769390) vdim=769390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.37175 elapsed=17.77s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_1_8_fast] OOF=0.38292; folds=[0.38229, 0.37923, 0.39507, 0.38627, 0.37175] total=92.78s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_1_8_fast] OOF=0.38292 with C=32.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv (LR char 1-8 fast). Head:\n         id       EAP       HPL       MWS\n0  id27251  0.009801  0.973037  0.017163\n1  id09612  0.149320  0.213424  0.637257\n2  id11943  0.033257  0.010454  0.956289\n3  id19526  0.007851  0.194168  0.797982\n4  id12931  0.046046  0.045911  0.908043\n"
          ]
        }
      ]
    },
    {
      "id": "7f69cd80-ac35-48ea-ae16-ea93fd3250af",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# New base (fast): NB-SVM char presence (2,7), alpha=0.75, liblinear OvR per-class\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED=42; N_FOLDS=5\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "\n",
        "char_params = dict(analyzer='char', ngram_range=(2,7), min_df=2, lowercase=False,\n",
        "                   strip_accents=None, binary=True, dtype=np.float32)\n",
        "\n",
        "def _r_presence(X, yb, a=0.75):\n",
        "    p = np.asarray(X[yb==1].sum(axis=0)).ravel() + a\n",
        "    q = np.asarray(X[yb==0].sum(axis=0)).ravel() + a\n",
        "    return np.log(p/q).astype(np.float32)\n",
        "\n",
        "def _softmax(m):\n",
        "    m = m - m.max(axis=1, keepdims=True)\n",
        "    e = np.exp(m); return e / e.sum(axis=1, keepdims=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "oof = np.zeros((len(train), len(classes)), np.float32)\n",
        "test_pred = np.zeros((len(test), len(classes)), np.float32)\n",
        "fold_losses = []\n",
        "t0_all = time.time()\n",
        "print('[NBSVM_char_presence_2_7] alpha=0.75 C=25.0', flush=True)\n",
        "for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
        "    t0 = time.time()\n",
        "    x_tr, x_va = X_text[tr], X_text[va]\n",
        "    y_tr, y_va = y[tr], y[va]\n",
        "    vc = CountVectorizer(**char_params)\n",
        "    Xc_tr = vc.fit_transform(x_tr); Xc_va = vc.transform(x_va); Xc_te = vc.transform(X_test)\n",
        "    print(f'  [Fold {fold}] X_tr={Xc_tr.shape}', flush=True)\n",
        "    margins_va = np.zeros((len(va), len(classes)), np.float32)\n",
        "    margins_te = np.zeros((len(test), len(classes)), np.float32)\n",
        "    for ci, c in enumerate(classes):\n",
        "        yb = (y_tr == c).astype(np.int8)\n",
        "        r = _r_presence(Xc_tr, yb, a=0.75)\n",
        "        Xr_tr = normalize(Xc_tr.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        Xr_va = normalize(Xc_va.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        Xr_te = normalize(Xc_te.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        clf = LogisticRegression(solver='liblinear', C=25.0, max_iter=2000, random_state=SEED)\n",
        "        clf.fit(Xr_tr, yb)\n",
        "        margins_va[:, ci] = clf.decision_function(Xr_va).astype(np.float32)\n",
        "        margins_te[:, ci] = clf.decision_function(Xr_te).astype(np.float32)\n",
        "        del yb, r, Xr_tr, Xr_va, Xr_te, clf; gc.collect()\n",
        "    P_va = _softmax(margins_va).astype(np.float32)\n",
        "    P_te = _softmax(margins_te).astype(np.float32)\n",
        "    oof[va] = P_va\n",
        "    test_pred += P_te / N_FOLDS\n",
        "    loss = log_loss(y_va, P_va, labels=list(classes))\n",
        "    fold_losses.append(loss)\n",
        "    print(f'  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s', flush=True)\n",
        "    del Xc_tr, Xc_va, Xc_te, margins_va, margins_te, P_va, P_te, vc; gc.collect()\n",
        "\n",
        "oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "print(f'[NBSVM_char_presence_2_7] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s', flush=True)\n",
        "np.save('oof_nbsvm_char_2_7_presence.npy', oof)\n",
        "np.save('test_nbsvm_char_2_7_presence.npy', test_pred)\n",
        "print('Saved oof_nbsvm_char_2_7_presence.npy and test_nbsvm_char_2_7_presence.npy')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_char_presence_2_7] alpha=0.75 C=25.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 512978)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.37087 elapsed=15.68s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 514363)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.37088 elapsed=15.37s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 512905)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.37714 elapsed=16.20s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 513617)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.35531 elapsed=16.69s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 514414)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.35912 elapsed=16.83s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_char_presence_2_7] OOF=0.36667; folds=[0.37087, 0.37088, 0.37714, 0.35531, 0.35912] total=81.39s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_nbsvm_char_2_7_presence.npy and test_nbsvm_char_2_7_presence.npy\n"
          ]
        }
      ]
    },
    {
      "id": "cdcfb90e-9088-4427-8bcd-79757cfbb230",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# New base: LinearSVC on char_wb(1,6) with sigmoid calibration (fast, diverse)\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import Tuple\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED=42; N_FOLDS=5\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "\n",
        "charwb_params = dict(analyzer='char_wb', ngram_range=(1,6), min_df=1, lowercase=False,\n",
        "                     strip_accents=None, sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_fold(x_tr, x_va, x_te) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int]:\n",
        "    v = TfidfVectorizer(**charwb_params)\n",
        "    X_tr = v.fit_transform(x_tr); X_va = v.transform(x_va); X_te = v.transform(x_te)\n",
        "    return X_tr, X_va, X_te, X_tr.shape[1]\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "oof = np.zeros((len(train), len(classes)), np.float32)\n",
        "test_pred = np.zeros((len(test), len(classes)), np.float32)\n",
        "fold_losses = []\n",
        "t0_all = time.time()\n",
        "C = 1.5\n",
        "print(f'[CalibSVC_charwb_1_6_sig] C={C}', flush=True)\n",
        "for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
        "    t0 = time.time()\n",
        "    x_tr, x_va = X_text[tr], X_text[va]\n",
        "    y_tr, y_va = y[tr], y[va]\n",
        "    X_tr, X_va, X_te, vdim = build_fold(x_tr, x_va, X_test)\n",
        "    print(f'  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}', flush=True)\n",
        "    base = LinearSVC(C=C, tol=1e-4, max_iter=10000, random_state=SEED, dual=True)\n",
        "    clf = CalibratedClassifierCV(estimator=base, method='sigmoid', cv=5, n_jobs=-1)\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    P_va = clf.predict_proba(X_va).astype(np.float32)\n",
        "    P_te = clf.predict_proba(X_te).astype(np.float32)\n",
        "    # reorder to fixed class order\n",
        "    order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "    P_va = P_va[:, order_idx]; P_te = P_te[:, order_idx]\n",
        "    # clip+renorm\n",
        "    P_va = np.clip(P_va, 1e-12, 1-1e-12); P_va /= P_va.sum(axis=1, keepdims=True)\n",
        "    P_te = np.clip(P_te, 1e-12, 1-1e-12); P_te /= P_te.sum(axis=1, keepdims=True)\n",
        "    oof[va] = P_va\n",
        "    test_pred += P_te / N_FOLDS\n",
        "    loss = log_loss(y_va, P_va, labels=list(classes))\n",
        "    fold_losses.append(loss)\n",
        "    print(f'  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s', flush=True)\n",
        "    del X_tr, X_va, X_te, P_va, P_te, clf, base; gc.collect()\n",
        "\n",
        "oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "print(f'[CalibSVC_charwb_1_6_sig] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s', flush=True)\n",
        "np.save('oof_svc_charwb_1_6_sig.npy', oof)\n",
        "np.save('test_svc_charwb_1_6_sig.npy', test_pred)\n",
        "print('Saved oof_svc_charwb_1_6_sig.npy and test_svc_charwb_1_6_sig.npy')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibSVC_charwb_1_6_sig] C=1.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 184581) vdim=184581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.40118 elapsed=5.93s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 185066) vdim=185066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.39867 elapsed=5.64s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 185603) vdim=185603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.41855 elapsed=5.64s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 184468) vdim=184468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.40914 elapsed=5.64s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 185025) vdim=185025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.40124 elapsed=5.55s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibSVC_charwb_1_6_sig] OOF=0.40576; folds=[0.40118, 0.39867, 0.41855, 0.40914, 0.40124] total=28.94s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_svc_charwb_1_6_sig.npy and test_svc_charwb_1_6_sig.npy\n"
          ]
        }
      ]
    },
    {
      "id": "a4baed71-c7ce-497e-b211-9058f3edeea4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# New base: Calibrated LinearSVC on word unigrams (case-sensitive) with isotonic calibration\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import Tuple\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED=42; N_FOLDS=5\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "\n",
        "word_params = dict(analyzer='word', ngram_range=(1,1), min_df=1, max_df=0.95, lowercase=False,\n",
        "                   strip_accents=None, token_pattern=r\"(?u)\\b[-\\w']+\\b\",\n",
        "                   sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_fold(x_tr, x_va, x_te) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int]:\n",
        "    v = TfidfVectorizer(**word_params)\n",
        "    X_tr = v.fit_transform(x_tr); X_va = v.transform(x_va); X_te = v.transform(x_te)\n",
        "    return X_tr, X_va, X_te, X_tr.shape[1]\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "oof = np.zeros((len(train), len(classes)), np.float32)\n",
        "test_pred = np.zeros((len(test), len(classes)), np.float32)\n",
        "fold_losses = []\n",
        "t0_all = time.time()\n",
        "C = 2.0\n",
        "print(f'[CalibSVC_word_uni_iso] C={C}', flush=True)\n",
        "for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
        "    t0 = time.time()\n",
        "    x_tr, x_va = X_text[tr], X_text[va]\n",
        "    y_tr, y_va = y[tr], y[va]\n",
        "    X_tr, X_va, X_te, vdim = build_fold(x_tr, x_va, X_test)\n",
        "    print(f'  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}', flush=True)\n",
        "    base = LinearSVC(C=C, tol=1e-4, max_iter=3000, random_state=SEED, dual=True)\n",
        "    clf = CalibratedClassifierCV(estimator=base, method='isotonic', cv=5, n_jobs=-1)\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    P_va = clf.predict_proba(X_va).astype(np.float32)\n",
        "    P_te = clf.predict_proba(X_te).astype(np.float32)\n",
        "    order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "    P_va = P_va[:, order_idx]; P_te = P_te[:, order_idx]\n",
        "    P_va = np.clip(P_va, 1e-12, 1-1e-12); P_va /= P_va.sum(axis=1, keepdims=True)\n",
        "    P_te = np.clip(P_te, 1e-12, 1-1e-12); P_te /= P_te.sum(axis=1, keepdims=True)\n",
        "    oof[va] = P_va\n",
        "    test_pred += P_te / N_FOLDS\n",
        "    loss = log_loss(y_va, P_va, labels=list(classes))\n",
        "    fold_losses.append(loss)\n",
        "    print(f'  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s', flush=True)\n",
        "    del X_tr, X_va, X_te, P_va, P_te, clf, base; gc.collect()\n",
        "\n",
        "oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "print(f'[CalibSVC_word_uni_iso] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s', flush=True)\n",
        "np.save('oof_svc_word_uni_iso.npy', oof)\n",
        "np.save('test_svc_word_uni_iso.npy', test_pred)\n",
        "print('Saved oof_svc_word_uni_iso.npy and test_svc_word_uni_iso.npy')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibSVC_word_uni_iso] C=2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 24405) vdim=24405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.45176 elapsed=1.06s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 24488) vdim=24488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.44446 elapsed=1.07s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 24624) vdim=24624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.46200 elapsed=1.04s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 24443) vdim=24443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.43980 elapsed=0.55s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 24594) vdim=24594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.44370 elapsed=0.55s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CalibSVC_word_uni_iso] OOF=0.44834; folds=[0.45176, 0.44446, 0.462, 0.4398, 0.4437] total=4.80s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_svc_word_uni_iso.npy and test_svc_word_uni_iso.npy\n"
          ]
        }
      ]
    },
    {
      "id": "262c016d-6c8e-4797-9f51-3958b2525928",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# New base (fast): SGD hinge on char counts (3,7) + sigmoid calibration\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import Tuple\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED=42; N_FOLDS=5\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "\n",
        "char_params = dict(analyzer='char', ngram_range=(3,7), min_df=2, lowercase=False,\n",
        "                   strip_accents=None, binary=False, dtype=np.float32)\n",
        "\n",
        "def build_fold(x_tr, x_va, x_te) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int]:\n",
        "    v = CountVectorizer(**char_params)\n",
        "    X_tr = v.fit_transform(x_tr); X_va = v.transform(x_va); X_te = v.transform(x_te)\n",
        "    return X_tr, X_va, X_te, X_tr.shape[1]\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "oof = np.zeros((len(train), len(classes)), np.float32)\n",
        "test_pred = np.zeros((len(test), len(classes)), np.float32)\n",
        "fold_losses = []\n",
        "t0_all = time.time()\n",
        "alpha = 1e-5\n",
        "print(f'[SGD_char_3_7_hinge_sig] alpha={alpha}', flush=True)\n",
        "for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
        "    t0 = time.time()\n",
        "    x_tr, x_va = X_text[tr], X_text[va]\n",
        "    y_tr, y_va = y[tr], y[va]\n",
        "    X_tr, X_va, X_te, vdim = build_fold(x_tr, x_va, X_test)\n",
        "    print(f'  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}', flush=True)\n",
        "    base = SGDClassifier(loss='hinge', penalty='l2', alpha=alpha, max_iter=2000, tol=1e-4,\n",
        "                         early_stopping=True, validation_fraction=0.1, n_iter_no_change=5,\n",
        "                         random_state=SEED)\n",
        "    clf = CalibratedClassifierCV(estimator=base, method='sigmoid', cv=3, n_jobs=-1)\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    P_va = clf.predict_proba(X_va).astype(np.float32)\n",
        "    P_te = clf.predict_proba(X_te).astype(np.float32)\n",
        "    # reorder to fixed class order\n",
        "    order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "    P_va = P_va[:, order_idx]; P_te = P_te[:, order_idx]\n",
        "    # clip+renorm\n",
        "    P_va = np.clip(P_va, 1e-12, 1-1e-12); P_va /= P_va.sum(axis=1, keepdims=True)\n",
        "    P_te = np.clip(P_te, 1e-12, 1-1e-12); P_te /= P_te.sum(axis=1, keepdims=True)\n",
        "    oof[va] = P_va\n",
        "    test_pred += P_te / N_FOLDS\n",
        "    loss = log_loss(y_va, P_va, labels=list(classes))\n",
        "    fold_losses.append(loss)\n",
        "    print(f'  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s', flush=True)\n",
        "    del X_tr, X_va, X_te, P_va, P_te, clf, base; gc.collect()\n",
        "\n",
        "oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "print(f'[SGD_char_3_7_hinge_sig] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s', flush=True)\n",
        "np.save('oof_sgd_char_3_7_hinge_sig.npy', oof)\n",
        "np.save('test_sgd_char_3_7_hinge_sig.npy', test_pred)\n",
        "print('Saved oof_sgd_char_3_7_hinge_sig.npy and test_sgd_char_3_7_hinge_sig.npy')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_char_3_7_hinge_sig] alpha=1e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 511677) vdim=511677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.46576 elapsed=7.86s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 513064) vdim=513064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.47545 elapsed=7.40s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 511604) vdim=511604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.47131 elapsed=7.58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 512304) vdim=512304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.47301 elapsed=7.48s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 513095) vdim=513095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.45768 elapsed=7.51s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SGD_char_3_7_hinge_sig] OOF=0.46864; folds=[0.46576, 0.47545, 0.47131, 0.47301, 0.45768] total=38.36s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_sgd_char_3_7_hinge_sig.npy and test_sgd_char_3_7_hinge_sig.npy\n"
          ]
        }
      ]
    },
    {
      "id": "7df44231-68b0-4e02-bdfc-d871fdcc26bc",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# New base: LR char_wb(4,8) multinomial (expected strong/diverse, relatively fast)\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import List, Tuple, Dict\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42\n",
        "N_FOLDS = 5\n",
        "np.random.seed(SEED)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test_text = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols), f\"Classes mismatch: {classes}\"\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "# Boundary-aware character n-grams; fewer features than plain char(1,8)\n",
        "charwb_params = dict(analyzer='char_wb', ngram_range=(4,8), min_df=1, lowercase=True,\n",
        "                     strip_accents='unicode', sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_charwb_fold(x_tr, x_val, x_test) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int]:\n",
        "    v = TfidfVectorizer(**charwb_params)\n",
        "    X_tr = v.fit_transform(x_tr)\n",
        "    X_val = v.transform(x_val)\n",
        "    X_te  = v.transform(x_test)\n",
        "    return X_tr, X_val, X_te, X_tr.shape[1]\n",
        "\n",
        "def cv_charwb_lr_strong(C: float = 20.0, name: str='LR_charwb_4_8_strong') -> Tuple[np.ndarray, np.ndarray, float, Dict]:\n",
        "    oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "    test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "    fold_losses = []\n",
        "    t0_all = time.time()\n",
        "    print(f\"[{name}] C={C}\", flush=True)\n",
        "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_text, y), 1):\n",
        "        t0 = time.time()\n",
        "        x_tr, x_val = X_text[tr_idx], X_text[val_idx]\n",
        "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "        X_tr, X_val, X_te, vdim = build_charwb_fold(x_tr, x_val, X_test_text)\n",
        "        print(f\"  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}\", flush=True)\n",
        "        clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', penalty='l2',\n",
        "                                 C=C, max_iter=10000, tol=1e-4, n_jobs=-1, random_state=SEED)\n",
        "        clf.fit(X_tr, y_tr)\n",
        "        proba_val = clf.predict_proba(X_val).astype(np.float32)\n",
        "        proba_test = clf.predict_proba(X_te).astype(np.float32)\n",
        "        order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "        proba_val = proba_val[:, order_idx]\n",
        "        proba_test = proba_test[:, order_idx]\n",
        "        oof[val_idx] = proba_val\n",
        "        test_pred += proba_test / N_FOLDS\n",
        "        loss = log_loss(y_val, proba_val, labels=list(classes))\n",
        "        fold_losses.append(loss)\n",
        "        print(f\"  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "        del X_tr, X_val, X_te, proba_val, proba_test, clf\n",
        "        gc.collect()\n",
        "    oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "    print(f\"[{name}] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s\", flush=True)\n",
        "    meta = dict(C=C, folds=fold_losses, oof=oof_loss)\n",
        "    return oof, test_pred, oof_loss, meta\n",
        "\n",
        "# Single strong C per expert; adjust if needed\n",
        "oof_lr_charwb_48, test_lr_charwb_48, loss_lr_charwb_48, meta_lr_charwb_48 = cv_charwb_lr_strong(C=20.0, name='LR_charwb_4_8_strong')\n",
        "np.save('oof_lr_charwb_4_8.npy', oof_lr_charwb_48); np.save('test_lr_charwb_4_8.npy', test_lr_charwb_48)\n",
        "print(f\"[LR_charwb_4_8_strong] OOF={loss_lr_charwb_48:.5f} with C={meta_lr_charwb_48['C']}\", flush=True)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_4_8_strong] C=20.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 307266) vdim=307266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.41165 elapsed=10.95s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 308405) vdim=308405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.40260 elapsed=8.15s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 308856) vdim=308856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.41427 elapsed=11.23s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 307582) vdim=307582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.41366 elapsed=10.52s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 308469) vdim=308469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.40829 elapsed=11.87s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_4_8_strong] OOF=0.41009; folds=[0.41165, 0.4026, 0.41427, 0.41366, 0.40829] total=53.26s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_4_8_strong] OOF=0.41009 with C=20.0\n"
          ]
        }
      ]
    },
    {
      "id": "8e17ee45-dc2e-4f60-9da9-a7b3f7401055",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# New base (fast): NB-SVM char presence (1,5), lowercase=True, alpha=1.0, C=50\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED=42; N_FOLDS=5\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "\n",
        "char_params = dict(analyzer='char', ngram_range=(1,5), min_df=1, lowercase=True,\n",
        "                   strip_accents='unicode', binary=True, dtype=np.float32)\n",
        "\n",
        "def _r_presence(X, yb, a=1.0):\n",
        "    p = np.asarray(X[yb==1].sum(axis=0)).ravel() + a\n",
        "    q = np.asarray(X[yb==0].sum(axis=0)).ravel() + a\n",
        "    return np.log(p/q).astype(np.float32)\n",
        "\n",
        "def _softmax(m):\n",
        "    m = m - m.max(axis=1, keepdims=True)\n",
        "    e = np.exp(m); return e / e.sum(axis=1, keepdims=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "oof = np.zeros((len(train), len(classes)), np.float32)\n",
        "test_pred = np.zeros((len(test), len(classes)), np.float32)\n",
        "fold_losses = []\n",
        "t0_all = time.time()\n",
        "print('[NBSVM_char_presence_1_5_lc] alpha=1.0 C=50.0', flush=True)\n",
        "for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
        "    t0 = time.time()\n",
        "    x_tr, x_va = X_text[tr], X_text[va]\n",
        "    y_tr, y_va = y[tr], y[va]\n",
        "    vc = CountVectorizer(**char_params)\n",
        "    Xc_tr = vc.fit_transform(x_tr); Xc_va = vc.transform(x_va); Xc_te = vc.transform(X_test)\n",
        "    print(f'  [Fold {fold}] X_tr={Xc_tr.shape}', flush=True)\n",
        "    margins_va = np.zeros((len(va), len(classes)), np.float32)\n",
        "    margins_te = np.zeros((len(test), len(classes)), np.float32)\n",
        "    for ci, c in enumerate(classes):\n",
        "        yb = (y_tr == c).astype(np.int8)\n",
        "        r = _r_presence(Xc_tr, yb, a=1.0)\n",
        "        Xr_tr = normalize(Xc_tr.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        Xr_va = normalize(Xc_va.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        Xr_te = normalize(Xc_te.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        clf = LogisticRegression(solver='liblinear', C=50.0, max_iter=2000, random_state=SEED)\n",
        "        clf.fit(Xr_tr, yb)\n",
        "        margins_va[:, ci] = clf.decision_function(Xr_va).astype(np.float32)\n",
        "        margins_te[:, ci] = clf.decision_function(Xr_te).astype(np.float32)\n",
        "        del yb, r, Xr_tr, Xr_va, Xr_te, clf; gc.collect()\n",
        "    P_va = _softmax(margins_va).astype(np.float32)\n",
        "    P_te = _softmax(margins_te).astype(np.float32)\n",
        "    oof[va] = P_va\n",
        "    test_pred += P_te / N_FOLDS\n",
        "    loss = log_loss(y_va, P_va, labels=list(classes))\n",
        "    fold_losses.append(loss)\n",
        "    print(f'  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s', flush=True)\n",
        "    del Xc_tr, Xc_va, Xc_te, margins_va, margins_te, P_va, P_te, vc; gc.collect()\n",
        "\n",
        "oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "print(f'[NBSVM_char_presence_1_5_lc] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s', flush=True)\n",
        "np.save('oof_nbsvm_char_1_5_presence_lc.npy', oof)\n",
        "np.save('test_nbsvm_char_1_5_presence_lc.npy', test_pred)\n",
        "print('Saved oof_nbsvm_char_1_5_presence_lc.npy and test_nbsvm_char_1_5_presence_lc.npy')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_char_presence_1_5_lc] alpha=1.0 C=50.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 175400)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.45363 elapsed=11.60s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 175847)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.44355 elapsed=11.48s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 176099)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.45335 elapsed=10.49s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 175579)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.44176 elapsed=10.99s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 175671)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.44731 elapsed=10.63s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_char_presence_1_5_lc] OOF=0.44792; folds=[0.45363, 0.44355, 0.45335, 0.44176, 0.44731] total=55.69s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_nbsvm_char_1_5_presence_lc.npy and test_nbsvm_char_1_5_presence_lc.npy\n"
          ]
        }
      ]
    },
    {
      "id": "80e13e94-2500-4d10-b301-51d024e9048a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Hero base: LR char(1,8) multinomial, min_df=1, lowercase=False, sublinear_tf=True (saga), single strong C\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import Tuple\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42; N_FOLDS = 5\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "\n",
        "char_params = dict(analyzer='char', ngram_range=(1,8), min_df=1, lowercase=False,\n",
        "                   strip_accents=None, sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_fold(x_tr, x_va, x_te) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int]:\n",
        "    v = TfidfVectorizer(**char_params)\n",
        "    X_tr = v.fit_transform(x_tr); X_va = v.transform(x_va); X_te = v.transform(x_te)\n",
        "    return X_tr, X_va, X_te, X_tr.shape[1]\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "oof = np.zeros((len(train), len(classes)), np.float32)\n",
        "test_pred = np.zeros((len(test), len(classes)), np.float32)\n",
        "fold_losses = []\n",
        "t0_all = time.time()\n",
        "C = 40.0\n",
        "print(f\"[LR_char_1_8_hero] C={C}\", flush=True)\n",
        "for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
        "    t0 = time.time()\n",
        "    x_tr, x_va = X_text[tr], X_text[va]\n",
        "    y_tr, y_va = y[tr], y[va]\n",
        "    X_tr, X_va, X_te, vdim = build_fold(x_tr, x_va, X_test)\n",
        "    print(f\"  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}\", flush=True)\n",
        "    clf = LogisticRegression(solver='saga', multi_class='multinomial', penalty='l2',\n",
        "                             C=C, max_iter=15000, tol=1e-4, n_jobs=-1, random_state=SEED)\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    P_va = clf.predict_proba(X_va).astype(np.float32)\n",
        "    P_te = clf.predict_proba(X_te).astype(np.float32)\n",
        "    order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "    P_va = P_va[:, order_idx]; P_te = P_te[:, order_idx]\n",
        "    oof[va] = P_va\n",
        "    test_pred += P_te / N_FOLDS\n",
        "    loss = log_loss(y_va, P_va, labels=list(classes))\n",
        "    fold_losses.append(loss)\n",
        "    print(f\"  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s\", flush=True)\n",
        "    del X_tr, X_va, X_te, P_va, P_te, clf; gc.collect()\n",
        "\n",
        "oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "print(f\"[LR_char_1_8_hero] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s\", flush=True)\n",
        "np.save('oof_lr_char_1_8_hero.npy', oof)\n",
        "np.save('test_lr_char_1_8_hero.npy', test_pred)\n",
        "print('Saved oof_lr_char_1_8_hero.npy and test_lr_char_1_8_hero.npy', flush=True)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_1_8_hero] C=40.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 1833412) vdim=1833412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.37428 elapsed=132.07s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 1841692) vdim=1841692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.36944 elapsed=154.61s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 1841174) vdim=1841174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.38547 elapsed=176.34s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 1837079) vdim=1837079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.37512 elapsed=124.40s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 1837993) vdim=1837993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.36295 elapsed=170.56s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_char_1_8_hero] OOF=0.37345; folds=[0.37428, 0.36944, 0.38547, 0.37512, 0.36295] total=758.57s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_lr_char_1_8_hero.npy and test_lr_char_1_8_hero.npy\n"
          ]
        }
      ]
    },
    {
      "id": "988ac82a-d2f0-40bf-8ee6-ec3e7da38527",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# A1) NBSVM word-only presence (1,3), alpha=0.75, liblinear C=25; save OOF/TEST\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED=42; N_FOLDS=5\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes)==set(submit_cols)\n",
        "\n",
        "vec_params = dict(analyzer='word', ngram_range=(1,3), min_df=2, max_df=1.0, lowercase=True,\n",
        "                  strip_accents='unicode', token_pattern=r\"(?u)\\b[-\\w']+\\b\", binary=True, dtype=np.float32)\n",
        "\n",
        "def _r_presence(X, yb, a=0.75):\n",
        "    p = np.asarray(X[yb==1].sum(axis=0)).ravel() + a\n",
        "    q = np.asarray(X[yb==0].sum(axis=0)).ravel() + a\n",
        "    return np.log(p/q).astype(np.float32)\n",
        "\n",
        "def _softmax(m):\n",
        "    m = m - m.max(axis=1, keepdims=True)\n",
        "    e = np.exp(m); return e / e.sum(axis=1, keepdims=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "oof = np.zeros((len(train), len(classes)), np.float32)\n",
        "test_pred = np.zeros((len(test), len(classes)), np.float32)\n",
        "fold_losses = []\n",
        "t0_all = time.time()\n",
        "print('[NBSVM_word_1_3_presence] alpha=0.75 C=25', flush=True)\n",
        "for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
        "    t0 = time.time()\n",
        "    x_tr, x_va = X_text[tr], X_text[va]\n",
        "    y_tr, y_va = y[tr], y[va]\n",
        "    vw = CountVectorizer(**vec_params)\n",
        "    Xw_tr = vw.fit_transform(x_tr); Xw_va = vw.transform(x_va); Xw_te = vw.transform(X_test)\n",
        "    print(f'  [Fold {fold}] X_tr={Xw_tr.shape}', flush=True)\n",
        "    margins_va = np.zeros((len(va), len(classes)), np.float32)\n",
        "    margins_te = np.zeros((len(test), len(classes)), np.float32)\n",
        "    for ci, c in enumerate(classes):\n",
        "        yb = (y_tr == c).astype(np.int8)\n",
        "        r = _r_presence(Xw_tr, yb, a=0.75)\n",
        "        Xr_tr = normalize(Xw_tr.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        Xr_va = normalize(Xw_va.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        Xr_te = normalize(Xw_te.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        clf = LogisticRegression(solver='liblinear', C=25.0, max_iter=2000, random_state=SEED)\n",
        "        clf.fit(Xr_tr, yb)\n",
        "        margins_va[:, ci] = clf.decision_function(Xr_va).astype(np.float32)\n",
        "        margins_te[:, ci] = clf.decision_function(Xr_te).astype(np.float32)\n",
        "        del yb, r, Xr_tr, Xr_va, Xr_te, clf; gc.collect()\n",
        "    P_va = _softmax(margins_va).astype(np.float32)\n",
        "    P_te = _softmax(margins_te).astype(np.float32)\n",
        "    oof[va] = P_va\n",
        "    test_pred += P_te / N_FOLDS\n",
        "    loss = log_loss(y_va, P_va, labels=list(classes))\n",
        "    fold_losses.append(loss)\n",
        "    print(f'  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s', flush=True)\n",
        "    del Xw_tr, Xw_va, Xw_te, margins_va, margins_te, P_va, P_te, vw; gc.collect()\n",
        "\n",
        "oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "print(f'[NBSVM_word_1_3_presence] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s', flush=True)\n",
        "np.save('oof_nbsvm_word_1_3_presence.npy', oof)\n",
        "np.save('test_nbsvm_word_1_3_presence.npy', test_pred)\n",
        "print('Saved oof_nbsvm_word_1_3_presence.npy and test_nbsvm_word_1_3_presence.npy', flush=True)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_word_1_3_presence] alpha=0.75 C=25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 75387)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.44227 elapsed=2.03s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 75665)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.43865 elapsed=1.95s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.46112 elapsed=1.97s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 75533)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.43584 elapsed=2.08s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 75229)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.44982 elapsed=1.95s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_word_1_3_presence] OOF=0.44554; folds=[0.44227, 0.43865, 0.46112, 0.43584, 0.44982] total=10.42s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_nbsvm_word_1_3_presence.npy and test_nbsvm_word_1_3_presence.npy\n"
          ]
        }
      ]
    },
    {
      "id": "cd57800c-2eea-45f5-a19d-735e3c6220fe",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# A2) NBSVM char_wb presence (2,7), alpha=0.75, liblinear C=30; save OOF/TEST\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED=42; N_FOLDS=5\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes)==set(submit_cols)\n",
        "\n",
        "vec_params = dict(analyzer='char_wb', ngram_range=(2,7), min_df=1, lowercase=False,\n",
        "                  strip_accents=None, binary=True, dtype=np.float32)\n",
        "\n",
        "def _r_presence(X, yb, a=0.75):\n",
        "    p = np.asarray(X[yb==1].sum(axis=0)).ravel() + a\n",
        "    q = np.asarray(X[yb==0].sum(axis=0)).ravel() + a\n",
        "    return np.log(p/q).astype(np.float32)\n",
        "\n",
        "def _softmax(m):\n",
        "    m = m - m.max(axis=1, keepdims=True)\n",
        "    e = np.exp(m); return e / e.sum(axis=1, keepdims=True)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "oof = np.zeros((len(train), len(classes)), np.float32)\n",
        "test_pred = np.zeros((len(test), len(classes)), np.float32)\n",
        "fold_losses = []\n",
        "t0_all = time.time()\n",
        "print('[NBSVM_charwb_2_7_presence] alpha=0.75 C=30', flush=True)\n",
        "for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
        "    t0 = time.time()\n",
        "    x_tr, x_va = X_text[tr], X_text[va]\n",
        "    y_tr, y_va = y[tr], y[va]\n",
        "    v = CountVectorizer(**vec_params)\n",
        "    X_tr = v.fit_transform(x_tr); X_va = v.transform(x_va); X_te = v.transform(X_test)\n",
        "    print(f'  [Fold {fold}] X_tr={X_tr.shape}', flush=True)\n",
        "    margins_va = np.zeros((len(va), len(classes)), np.float32)\n",
        "    margins_te = np.zeros((len(test), len(classes)), np.float32)\n",
        "    for ci, c in enumerate(classes):\n",
        "        yb = (y_tr == c).astype(np.int8)\n",
        "        r = _r_presence(X_tr, yb, a=0.75)\n",
        "        Xr_tr = normalize(X_tr.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        Xr_va = normalize(X_va.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        Xr_te = normalize(X_te.multiply(r), norm='l2', axis=1, copy=False)\n",
        "        clf = LogisticRegression(solver='liblinear', C=30.0, max_iter=2000, random_state=SEED)\n",
        "        clf.fit(Xr_tr, yb)\n",
        "        margins_va[:, ci] = clf.decision_function(Xr_va).astype(np.float32)\n",
        "        margins_te[:, ci] = clf.decision_function(Xr_te).astype(np.float32)\n",
        "        del yb, r, Xr_tr, Xr_va, Xr_te, clf; gc.collect()\n",
        "    P_va = _softmax(margins_va).astype(np.float32)\n",
        "    P_te = _softmax(margins_te).astype(np.float32)\n",
        "    oof[va] = P_va\n",
        "    test_pred += P_te / N_FOLDS\n",
        "    loss = log_loss(y_va, P_va, labels=list(classes))\n",
        "    fold_losses.append(loss)\n",
        "    print(f'  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s', flush=True)\n",
        "    del X_tr, X_va, X_te, margins_va, margins_te, P_va, P_te, v; gc.collect()\n",
        "\n",
        "oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "print(f'[NBSVM_charwb_2_7_presence] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s', flush=True)\n",
        "np.save('oof_nbsvm_charwb_2_7_presence.npy', oof)\n",
        "np.save('test_nbsvm_charwb_2_7_presence.npy', test_pred)\n",
        "print('Saved oof_nbsvm_charwb_2_7_presence.npy and test_nbsvm_charwb_2_7_presence.npy', flush=True)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_charwb_2_7_presence] alpha=0.75 C=30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 264846)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.45101 elapsed=9.64s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 265778)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.43024 elapsed=9.86s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 266406)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.45530 elapsed=9.54s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 264832)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.42798 elapsed=9.58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 265685)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.43758 elapsed=9.38s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NBSVM_charwb_2_7_presence] OOF=0.44042; folds=[0.45101, 0.43024, 0.4553, 0.42798, 0.43758] total=48.50s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_nbsvm_charwb_2_7_presence.npy and test_nbsvm_charwb_2_7_presence.npy\n"
          ]
        }
      ]
    },
    {
      "id": "fb516971-6f46-44d8-b3fb-462c9cc5579c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# A3) Hybrid TF-IDF LR: word(1,3) + char_wb(3,6), multinomial saga; save OOF/TEST\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import Tuple\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED=42; N_FOLDS=5\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes)==set(submit_cols)\n",
        "\n",
        "vec_word = TfidfVectorizer(analyzer='word', ngram_range=(1,3), min_df=3, max_df=0.95,\n",
        "                           lowercase=True, strip_accents='unicode', token_pattern=r\"(?u)\\b[-\\w']+\\b\",\n",
        "                           sublinear_tf=True, norm='l2', dtype=np.float32)\n",
        "vec_charwb = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,6), min_df=2, lowercase=False,\n",
        "                             strip_accents=None, sublinear_tf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_fold(x_tr, x_va, x_te) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix]:\n",
        "    Xw_tr = vec_word.fit_transform(x_tr); Xw_va = vec_word.transform(x_va); Xw_te = vec_word.transform(x_te)\n",
        "    Xc_tr = vec_charwb.fit_transform(x_tr); Xc_va = vec_charwb.transform(x_va); Xc_te = vec_charwb.transform(x_te)\n",
        "    X_tr = sparse.hstack([Xw_tr, Xc_tr], format='csr')\n",
        "    X_va = sparse.hstack([Xw_va, Xc_va], format='csr')\n",
        "    X_te = sparse.hstack([Xw_te, Xc_te], format='csr')\n",
        "    return X_tr, X_va, X_te\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "best = dict(loss=1e9, C=None, oof=None, test=None)\n",
        "for C in [12.0, 16.0]:\n",
        "    print(f'[LR_word13_charwb36] C={C}', flush=True)\n",
        "    oof = np.zeros((len(train), len(classes)), np.float32)\n",
        "    test_pred = np.zeros((len(test), len(classes)), np.float32)\n",
        "    fold_losses = []\n",
        "    t0_all = time.time()\n",
        "    for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
        "        t0 = time.time()\n",
        "        x_tr, x_va = X_text[tr], X_text[va]\n",
        "        y_tr, y_va = y[tr], y[va]\n",
        "        X_tr, X_va, X_te = build_fold(x_tr, x_va, X_test)\n",
        "        print(f'  [Fold {fold}] X_tr={X_tr.shape}', flush=True)\n",
        "        clf = LogisticRegression(solver='saga', multi_class='multinomial', C=C,\n",
        "                                 max_iter=8000, tol=1e-4, n_jobs=-1, random_state=SEED)\n",
        "        clf.fit(X_tr, y_tr)\n",
        "        P_va = clf.predict_proba(X_va).astype(np.float32)\n",
        "        P_te = clf.predict_proba(X_te).astype(np.float32)\n",
        "        order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "        P_va = P_va[:, order_idx]; P_te = P_te[:, order_idx]\n",
        "        oof[va] = P_va\n",
        "        test_pred += P_te / N_FOLDS\n",
        "        loss = log_loss(y_va, P_va, labels=list(classes))\n",
        "        fold_losses.append(loss)\n",
        "        print(f'  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s', flush=True)\n",
        "        del X_tr, X_va, X_te, P_va, P_te, clf; gc.collect()\n",
        "    oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "    print(f'[LR_word13_charwb36] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s', flush=True)\n",
        "    if oof_loss < best['loss']:\n",
        "        best.update(loss=oof_loss, C=C, oof=oof.copy(), test=test_pred.copy())\n",
        "\n",
        "print(f\"[LR_word13_charwb36] BEST OOF={best['loss']:.5f} with C={best['C']}\", flush=True)\n",
        "np.save('oof_lr_word13_charwb36.npy', best['oof'])\n",
        "np.save('test_lr_word13_charwb36.npy', best['test'])\n",
        "print('Saved oof_lr_word13_charwb36.npy and test_lr_word13_charwb36.npy', flush=True)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word13_charwb36] C=12.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 155680)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.36414 elapsed=13.47s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 156093)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.36485 elapsed=19.56s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 155966)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.38251 elapsed=17.77s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 156197)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.36746 elapsed=14.66s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 156270)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.36449 elapsed=19.03s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word13_charwb36] OOF=0.36869; folds=[0.36414, 0.36485, 0.38251, 0.36746, 0.36449] total=85.03s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word13_charwb36] C=16.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 155680)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.36385 elapsed=15.59s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 156093)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.36379 elapsed=21.96s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 155966)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.38255 elapsed=20.10s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 156197)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.36711 elapsed=17.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 156270)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.36401 elapsed=21.25s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word13_charwb36] OOF=0.36826; folds=[0.36385, 0.36379, 0.38255, 0.36711, 0.36401] total=96.44s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_word13_charwb36] BEST OOF=0.36826 with C=16.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_lr_word13_charwb36.npy and test_lr_word13_charwb36.npy\n"
          ]
        }
      ]
    },
    {
      "id": "8696bae3-8259-4661-b15f-6d97d5227dd3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Classwise Log-Opinion Pool (LOP) blender (9 models), per-model scalar temps, classwise weights, simple caps (revert for recovery)\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import log_loss\n",
        "from scipy.optimize import minimize, minimize_scalar\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "y = train['author'].values\n",
        "classes = np.unique(y).tolist()\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "C = len(classes)\n",
        "\n",
        "def load(p): return np.load(p) if Path(p).exists() else None\n",
        "\n",
        "# Candidate set: original 9-core (no MNB here for recovery)\n",
        "cands = [\n",
        "    ('nbsvm_wc_tweaked',        'oof_nbsvm_wc_tweaked.npy',        'test_nbsvm_wc_tweaked.npy'),\n",
        "    ('nbsvm_char_2_6_counts',   'oof_nbsvm_char_2_6_counts.npy',   'test_nbsvm_char_2_6_counts.npy'),\n",
        "    ('nbsvm_wc_fixed',          'oof_nbsvm_wc_fixed.npy',          'test_nbsvm_wc_fixed.npy'),\n",
        "    ('nbsvm_char_2_7_presence', 'oof_nbsvm_char_2_7_presence.npy', 'test_nbsvm_char_2_7_presence.npy'),\n",
        "    ('lr_wc_fixed',             'oof_lr_wordchar_fixed.npy',       'test_lr_wordchar_fixed.npy'),\n",
        "    ('lr_word13_charwb36',      'oof_lr_word13_charwb36.npy',      'test_lr_word13_charwb36.npy'),\n",
        "    ('lr_char_1_8_hero',        'oof_lr_char_1_8_hero.npy',        'test_lr_char_1_8_hero.npy'),\n",
        "    ('svc_charwb_1_6_sig',      'oof_svc_charwb_1_6_sig.npy',      'test_svc_charwb_1_6_sig.npy'),\n",
        "    ('lr_wordpunct_1_3',        'oof_lr_wordpunct_1_3.npy',        'test_lr_wordpunct_1_3.npy'),\n",
        "]\n",
        "\n",
        "loaded = []\n",
        "for name, oofp, tsp in cands:\n",
        "    o = load(oofp); t = load(tsp)\n",
        "    if o is None or t is None:\n",
        "        continue\n",
        "    o = np.clip(o, 1e-12, 1-1e-12); o = o / o.sum(axis=1, keepdims=True)\n",
        "    t = np.clip(t, 1e-12, 1-1e-12); t = t / t.sum(axis=1, keepdims=True)\n",
        "    loaded.append((name, o.astype(np.float64), t.astype(np.float64)))\n",
        "\n",
        "assert len(loaded) == 9, f'Need exactly 9 core bases; got {len(loaded)}'\n",
        "names = [n for n,_,_ in loaded]\n",
        "K = len(names)\n",
        "print('LOP candidates:', names, flush=True)\n",
        "\n",
        "# Scalar temperature per model\n",
        "def scale_probs_scalar(P, T):\n",
        "    S = np.clip(P, 1e-12, 1-1e-12) ** (1.0/float(T))\n",
        "    return S / S.sum(axis=1, keepdims=True)\n",
        "\n",
        "OOFs_raw = [o for _,o,_ in loaded]\n",
        "TESTs_raw = [t for _,_,t in loaded]\n",
        "per_model_T = []; OOFs = []; TESTs = []\n",
        "for i in range(K):\n",
        "    Pi = OOFs_raw[i]\n",
        "    def loss_Ti(T): return log_loss(y, scale_probs_scalar(Pi, T), labels=classes)\n",
        "    resTi = minimize_scalar(loss_Ti, bounds=(0.5, 5.0), method='bounded')\n",
        "    Ti = float(resTi.x)\n",
        "    per_model_T.append(Ti)\n",
        "    OOFs.append(scale_probs_scalar(OOFs_raw[i], Ti))\n",
        "    TESTs.append(scale_probs_scalar(TESTs_raw[i], Ti))\n",
        "print('Per-model scalar T:', {names[i]: round(per_model_T[i],3) for i in range(K)})\n",
        "\n",
        "per_oof = {names[i]: log_loss(y, OOFs[i], labels=classes) for i in range(K)}\n",
        "print('Per-model OOF (post scalar cal):', {k: round(v,5) for k,v in per_oof.items()})\n",
        "\n",
        "# LOP with classwise weights\n",
        "def geo_pool_log_classwise(stacks, W):\n",
        "    n = stacks[0].shape[0]\n",
        "    A = np.zeros((n, C), dtype=np.float64)\n",
        "    for k in range(K):\n",
        "        A += np.log(stacks[k]) * W[k][None, :]\n",
        "    A -= A.max(axis=1, keepdims=True)\n",
        "    P = np.exp(A); P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "def softmax_cols(Z):\n",
        "    W = np.zeros_like(Z)\n",
        "    for j in range(C):\n",
        "        z = Z[:, j]\n",
        "        z = z - z.max()\n",
        "        e = np.exp(z)\n",
        "        s = e.sum()\n",
        "        W[:, j] = e / (s if s>0 else 1.0)\n",
        "    return W\n",
        "\n",
        "# Recovery params (simple caps/renorm that gave ~0.31184)\n",
        "lambda_ent = 0.0025\n",
        "starts = 128\n",
        "global_cap = 0.55\n",
        "nb_cap = 0.62\n",
        "weak_cap = 0.09\n",
        "tiny_prune_thresh = 0.00\n",
        "explicit_caps = {\n",
        "    'svc_charwb_1_6_sig': 0.06,\n",
        "    'lr_wordpunct_1_3': 0.05,\n",
        "}\n",
        "\n",
        "nb_mask = np.array([n.startswith('nbsvm_') for n in names], dtype=bool)\n",
        "name_to_idx = {n:i for i,n in enumerate(names)}\n",
        "\n",
        "# Simple apply_caps (prior version that achieved 0.31184): entry caps -> NB family cap -> per-class renorm\n",
        "def apply_caps(W):\n",
        "    Wc = W.copy()\n",
        "    # Global cap per entry\n",
        "    Wc = np.minimum(Wc, global_cap)\n",
        "    # Explicit per-model caps\n",
        "    for n, cap in explicit_caps.items():\n",
        "        if n in name_to_idx:\n",
        "            i = name_to_idx[n]\n",
        "            Wc[i, :] = np.minimum(Wc[i, :], cap)\n",
        "    # Weak cap based on per-model OOF\n",
        "    for i, n in enumerate(names):\n",
        "        if per_oof[n] > 0.40:\n",
        "            Wc[i, :] = np.minimum(Wc[i, :], weak_cap)\n",
        "    # NB family cap per class\n",
        "    for j in range(C):\n",
        "        s_nb = Wc[nb_mask, j].sum()\n",
        "        if s_nb > nb_cap and s_nb > 0:\n",
        "            Wc[nb_mask, j] *= (nb_cap / s_nb)\n",
        "    # Prune tiny (disabled here) and renormalize per class\n",
        "    for j in range(C):\n",
        "        col = Wc[:, j]\n",
        "        if tiny_prune_thresh > 0:\n",
        "            col[col < tiny_prune_thresh] = 0.0\n",
        "        s = col.sum()\n",
        "        if s == 0:\n",
        "            col[:] = 1.0 / K\n",
        "        else:\n",
        "            col[:] = col / s\n",
        "        Wc[:, j] = col\n",
        "    return Wc\n",
        "\n",
        "def objective(Z):\n",
        "    W0 = softmax_cols(Z)\n",
        "    Wc = apply_caps(W0)\n",
        "    P = geo_pool_log_classwise(OOFs, Wc)\n",
        "    ent = 0.0\n",
        "    for j in range(C):\n",
        "        wj = np.clip(Wc[:, j], 1e-12, 1.0)\n",
        "        ent += float(np.sum(wj * np.log(wj)))\n",
        "    reg = lambda_ent * ent\n",
        "    return log_loss(y, P, labels=classes) + reg\n",
        "\n",
        "# Multi-start optimization\n",
        "best = (1e9, None, None)\n",
        "rng = np.random.RandomState(42)\n",
        "inits = [np.zeros((K, C))] + [rng.normal(0, 0.5, size=(K, C)) for _ in range(starts-1)]\n",
        "for si, Z0 in enumerate(inits, 1):\n",
        "    res = minimize(lambda z: objective(z.reshape(K, C)), Z0.ravel(), method='L-BFGS-B')\n",
        "    val = float(res.fun)\n",
        "    if val < best[0]:\n",
        "        best = (val, res.x.reshape(K, C).copy(), res)\n",
        "    if si % 16 == 0:\n",
        "        print(f'  [start {si}/{starts}] best_obj={best[0]:.5f}', flush=True)\n",
        "\n",
        "Z_star = best[1]\n",
        "W0 = softmax_cols(Z_star)\n",
        "Wc = apply_caps(W0)\n",
        "print('Best obj:', round(best[0],5))\n",
        "\n",
        "# Report final per-class weights\n",
        "print('Final per-class weights (sum=1 each class):')\n",
        "for j, cls in enumerate(classes):\n",
        "    wj = {names[i]: round(float(Wc[i, j]), 3) for i in range(K)}\n",
        "    print(cls, wj)\n",
        "\n",
        "# Blend OOF/Test with classwise weights\n",
        "P_oof = geo_pool_log_classwise(OOFs, Wc)\n",
        "P_test = geo_pool_log_classwise(TESTs, Wc)\n",
        "oof_pre = log_loss(y, P_oof, labels=classes)\n",
        "print('LOP Blend OOF (pre-temp):', round(oof_pre,5))\n",
        "\n",
        "# Per-class final temperatures\n",
        "def scale_classwise(P, Tvec):\n",
        "    T = np.asarray(Tvec, dtype=np.float64)\n",
        "    S = np.clip(P, 1e-12, 1-1e-12) ** (1.0 / T[None, :])\n",
        "    return S / S.sum(axis=1, keepdims=True)\n",
        "\n",
        "from scipy.optimize import minimize as _minimize\n",
        "bounds = [(0.5, 2.0)] * C\n",
        "x0 = np.ones(C, dtype=np.float64)\n",
        "resTc = _minimize(lambda t: log_loss(y, scale_classwise(P_oof, t), labels=classes),\n",
        "                  x0, method='L-BFGS-B', bounds=bounds)\n",
        "T_class = resTc.x\n",
        "P_oof_scaled = scale_classwise(P_oof, T_class)\n",
        "oof_final = log_loss(y, P_oof_scaled, labels=classes)\n",
        "print('Classwise T:', np.round(T_class, 4), 'Final OOF:', round(oof_final,5))\n",
        "\n",
        "# Save submission using classwise temperature scaling\n",
        "P_test_scaled = scale_classwise(P_test, T_class)\n",
        "probs = P_test_scaled[:, [classes.index(c) for c in submit_cols]]\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv; elapsed=%.2fs' % (time.time()-t0), flush=True)\n",
        "\n",
        "# Free\n",
        "del OOFs_raw, TESTs_raw, OOFs, TESTs; gc.collect()"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOP candidates: ['nbsvm_wc_tweaked', 'nbsvm_char_2_6_counts', 'nbsvm_wc_fixed', 'nbsvm_char_2_7_presence', 'lr_wc_fixed', 'lr_word13_charwb36', 'lr_char_1_8_hero', 'svc_charwb_1_6_sig', 'lr_wordpunct_1_3']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-model scalar T: {'nbsvm_wc_tweaked': 1.55, 'nbsvm_char_2_6_counts': 1.579, 'nbsvm_wc_fixed': 1.567, 'nbsvm_char_2_7_presence': 1.477, 'lr_wc_fixed': 0.877, 'lr_word13_charwb36': 1.006, 'lr_char_1_8_hero': 0.871, 'svc_charwb_1_6_sig': 0.803, 'lr_wordpunct_1_3': 0.771}\nPer-model OOF (post scalar cal): {'nbsvm_wc_tweaked': 0.32156, 'nbsvm_char_2_6_counts': 0.33019, 'nbsvm_wc_fixed': 0.33633, 'nbsvm_char_2_7_presence': 0.338, 'lr_wc_fixed': 0.36343, 'lr_word13_charwb36': 0.36825, 'lr_char_1_8_hero': 0.36987, 'svc_charwb_1_6_sig': 0.39657, 'lr_wordpunct_1_3': 0.40368}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [start 16/128] best_obj=0.29898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [start 32/128] best_obj=0.29898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [start 48/128] best_obj=0.29898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [start 64/128] best_obj=0.29898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [start 80/128] best_obj=0.29898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [start 96/128] best_obj=0.29898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [start 112/128] best_obj=0.29898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [start 128/128] best_obj=0.29881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best obj: 0.29881\nFinal per-class weights (sum=1 each class):\nEAP {'nbsvm_wc_tweaked': 0.234, 'nbsvm_char_2_6_counts': 0.105, 'nbsvm_wc_fixed': 0.178, 'nbsvm_char_2_7_presence': 0.159, 'lr_wc_fixed': 0.034, 'lr_word13_charwb36': 0.053, 'lr_char_1_8_hero': 0.022, 'svc_charwb_1_6_sig': 0.057, 'lr_wordpunct_1_3': 0.157}\nHPL {'nbsvm_wc_tweaked': 0.346, 'nbsvm_char_2_6_counts': 0.164, 'nbsvm_wc_fixed': 0.145, 'nbsvm_char_2_7_presence': 0.145, 'lr_wc_fixed': 0.023, 'lr_word13_charwb36': 0.024, 'lr_char_1_8_hero': 0.01, 'svc_charwb_1_6_sig': 0.077, 'lr_wordpunct_1_3': 0.065}\nMWS {'nbsvm_wc_tweaked': 0.291, 'nbsvm_char_2_6_counts': 0.127, 'nbsvm_wc_fixed': 0.069, 'nbsvm_char_2_7_presence': 0.107, 'lr_wc_fixed': 0.031, 'lr_word13_charwb36': 0.075, 'lr_char_1_8_hero': 0.014, 'svc_charwb_1_6_sig': 0.064, 'lr_wordpunct_1_3': 0.222}\nLOP Blend OOF (pre-temp): 0.31313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classwise T: [0.8854 0.9071 0.9523] Final OOF: 0.31158\nSaved submission.csv; elapsed=1033.75s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "644"
          },
          "metadata": {}
        }
      ]
    },
    {
      "id": "b0a8b32c-c980-4e3e-848f-642930dea7fb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# A5) LR char_wb(1,6) multinomial (often strong); sweep C in [16, 20, 24]; save OOF/TEST\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import List, Tuple, Dict\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED = 42; N_FOLDS = 5\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "\n",
        "charwb_params = dict(analyzer='char_wb', ngram_range=(1,6), min_df=1, lowercase=False,\n",
        "                     strip_accents=None, sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_fold(x_tr, x_va, x_te) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int]:\n",
        "    v = TfidfVectorizer(**charwb_params)\n",
        "    X_tr = v.fit_transform(x_tr); X_va = v.transform(x_va); X_te = v.transform(x_te)\n",
        "    return X_tr, X_va, X_te, X_tr.shape[1]\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "best = dict(loss=1e9, C=None, oof=None, test=None)\n",
        "for C in [16.0, 20.0, 24.0]:\n",
        "    print(f'[LR_charwb_1_6] C={C}', flush=True)\n",
        "    oof = np.zeros((len(train), len(classes)), dtype=np.float32)\n",
        "    test_pred = np.zeros((len(test), len(classes)), dtype=np.float32)\n",
        "    fold_losses = []\n",
        "    t0_all = time.time()\n",
        "    for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
        "        t0 = time.time()\n",
        "        x_tr, x_va = X_text[tr], X_text[va]\n",
        "        y_tr, y_va = y[tr], y[va]\n",
        "        X_tr, X_va, X_te, vdim = build_fold(x_tr, x_va, X_test)\n",
        "        print(f'  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}', flush=True)\n",
        "        clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', penalty='l2',\n",
        "                                 C=C, max_iter=12000, tol=1e-4, n_jobs=-1, random_state=SEED)\n",
        "        clf.fit(X_tr, y_tr)\n",
        "        P_va = clf.predict_proba(X_va).astype(np.float32)\n",
        "        P_te = clf.predict_proba(X_te).astype(np.float32)\n",
        "        order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "        P_va = P_va[:, order_idx]; P_te = P_te[:, order_idx]\n",
        "        oof[va] = P_va\n",
        "        test_pred += P_te / N_FOLDS\n",
        "        loss = log_loss(y_va, P_va, labels=list(classes))\n",
        "        fold_losses.append(loss)\n",
        "        print(f'  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s', flush=True)\n",
        "        del X_tr, X_va, X_te, P_va, P_te, clf; gc.collect()\n",
        "    oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "    print(f'[LR_charwb_1_6] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s', flush=True)\n",
        "    if oof_loss < best['loss']:\n",
        "        best.update(loss=oof_loss, C=C, oof=oof.copy(), test=test_pred.copy())\n",
        "\n",
        "print(f\"[LR_charwb_1_6] BEST OOF={best['loss']:.5f} with C={best['C']}\", flush=True)\n",
        "np.save('oof_lr_charwb_1_6.npy', best['oof'])\n",
        "np.save('test_lr_charwb_1_6.npy', best['test'])\n",
        "print('Saved oof_lr_charwb_1_6.npy and test_lr_charwb_1_6.npy', flush=True)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_1_6] C=16.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 184581) vdim=184581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.39544 elapsed=10.73s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 185066) vdim=185066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.38354 elapsed=14.48s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 185603) vdim=185603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.41368 elapsed=11.54s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 184468) vdim=184468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.40293 elapsed=9.68s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 185025) vdim=185025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.39141 elapsed=9.12s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_1_6] OOF=0.39740; folds=[0.39544, 0.38354, 0.41368, 0.40293, 0.39141] total=56.10s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_1_6] C=20.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 184581) vdim=184581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.39692 elapsed=14.45s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 185066) vdim=185066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.38451 elapsed=10.69s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 185603) vdim=185603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.41620 elapsed=12.31s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 184468) vdim=184468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.39989 elapsed=13.24s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 185025) vdim=185025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.39742 elapsed=11.22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_1_6] OOF=0.39899; folds=[0.39692, 0.38451, 0.4162, 0.39989, 0.39742] total=62.44s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_1_6] C=24.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 184581) vdim=184581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.39886 elapsed=13.85s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 185066) vdim=185066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.38602 elapsed=12.42s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 185603) vdim=185603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.41828 elapsed=14.21s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 184468) vdim=184468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.40106 elapsed=13.02s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 185025) vdim=185025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.39747 elapsed=9.51s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_1_6] OOF=0.40034; folds=[0.39886, 0.38602, 0.41828, 0.40106, 0.39747] total=63.53s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_charwb_1_6] BEST OOF=0.39740 with C=16.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_lr_charwb_1_6.npy and test_lr_charwb_1_6.npy\n"
          ]
        }
      ]
    },
    {
      "id": "601642fd-ed58-46b0-bdba-bf780559d8f7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# A4) Word+punct TF-IDF LR: word(1,3) with punctuation tokens; multinomial saga; save OOF/TEST\n",
        "import time, gc, re, numpy as np, pandas as pd\n",
        "from typing import Tuple\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED=42; N_FOLDS=5\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes)==set(submit_cols)\n",
        "\n",
        "# Token pattern keeps words (with apostrophes/hyphens) and standalone punctuation tokens\n",
        "punct_pat = r\"[\\.,;:!\\?\u2014\u201d\u201c\\\"'()\\-\u2026]\"  # includes em-dash and ellipsis\n",
        "tok_pat = rf\"(?u)\\b[-\\w']+\\b|{punct_pat}\"\n",
        "vec = TfidfVectorizer(analyzer='word', ngram_range=(1,3), min_df=2, max_df=0.95, lowercase=False,\n",
        "                      strip_accents=None, token_pattern=tok_pat, sublinear_tf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_fold(x_tr, x_va, x_te) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int]:\n",
        "    X_tr = vec.fit_transform(x_tr); X_va = vec.transform(x_va); X_te = vec.transform(x_te)\n",
        "    return X_tr, X_va, X_te, X_tr.shape[1]\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "C = 10.0\n",
        "print(f'[LR_wordpunct_1_3] C={C}', flush=True)\n",
        "oof = np.zeros((len(train), len(classes)), np.float32)\n",
        "test_pred = np.zeros((len(test), len(classes)), np.float32)\n",
        "fold_losses = []\n",
        "t0_all = time.time()\n",
        "for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
        "    t0 = time.time()\n",
        "    x_tr, x_va = X_text[tr], X_text[va]\n",
        "    y_tr, y_va = y[tr], y[va]\n",
        "    X_tr, X_va, X_te, vdim = build_fold(x_tr, x_va, X_test)\n",
        "    print(f'  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}', flush=True)\n",
        "    clf = LogisticRegression(solver='saga', multi_class='multinomial', C=C,\n",
        "                             max_iter=8000, tol=1e-4, n_jobs=-1, random_state=SEED)\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    P_va = clf.predict_proba(X_va).astype(np.float32)\n",
        "    P_te = clf.predict_proba(X_te).astype(np.float32)\n",
        "    order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "    P_va = P_va[:, order_idx]; P_te = P_te[:, order_idx]\n",
        "    oof[va] = P_va\n",
        "    test_pred += P_te / N_FOLDS\n",
        "    loss = log_loss(y_va, P_va, labels=list(classes))\n",
        "    fold_losses.append(loss)\n",
        "    print(f'  [Fold {fold}] loss={loss:.5f} elapsed={time.time()-t0:.2f}s', flush=True)\n",
        "    del X_tr, X_va, X_te, P_va, P_te, clf; gc.collect()\n",
        "\n",
        "oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "print(f'[LR_wordpunct_1_3] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s', flush=True)\n",
        "np.save('oof_lr_wordpunct_1_3.npy', oof)\n",
        "np.save('test_lr_wordpunct_1_3.npy', test_pred)\n",
        "print('Saved oof_lr_wordpunct_1_3.npy and test_lr_wordpunct_1_3.npy', flush=True)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_wordpunct_1_3] C=10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 87868) vdim=87868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] loss=0.41118 elapsed=2.02s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 88062) vdim=88062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] loss=0.41795 elapsed=2.22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] loss=0.42880 elapsed=2.03s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 88161) vdim=88161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] loss=0.41522 elapsed=1.97s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 87824) vdim=87824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] loss=0.41173 elapsed=2.05s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LR_wordpunct_1_3] OOF=0.41697; folds=[0.41118, 0.41795, 0.4288, 0.41522, 0.41173] total=10.85s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_lr_wordpunct_1_3.npy and test_lr_wordpunct_1_3.npy\n"
          ]
        }
      ]
    },
    {
      "id": "ac669d1c-5f03-46ea-adf4-882c75f21df1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# New base (cheap, diverse): MultinomialNB on TF-IDF char (2,6), alpha=0.1, lowercase=True, sublinear_tf=True; save OOF/TEST\n",
        "import time, gc, numpy as np, pandas as pd\n",
        "from typing import Tuple\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "SEED=42; N_FOLDS=5\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test = test['text'].astype(str).values\n",
        "classes = np.unique(y)\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes)==set(submit_cols)\n",
        "\n",
        "char_params = dict(analyzer='char', ngram_range=(2,6), min_df=2, lowercase=True,\n",
        "                   strip_accents='unicode', sublinear_tf=True, smooth_idf=True, norm='l2', dtype=np.float32)\n",
        "\n",
        "def build_fold(x_tr, x_va, x_te) -> Tuple[sparse.csr_matrix, sparse.csr_matrix, sparse.csr_matrix, int]:\n",
        "    v = TfidfVectorizer(**char_params)\n",
        "    X_tr = v.fit_transform(x_tr); X_va = v.transform(x_va); X_te = v.transform(x_te)\n",
        "    return X_tr, X_va, X_te, X_tr.shape[1]\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "alpha = 0.1\n",
        "oof = np.zeros((len(train), len(classes)), np.float32)\n",
        "test_pred = np.zeros((len(test), len(classes)), np.float32)\n",
        "fold_losses = []; t0_all=time.time()\n",
        "print(f'[MNB_char_tfidf_2_6] alpha={alpha}', flush=True)\n",
        "for fold,(tr,va) in enumerate(skf.split(X_text, y),1):\n",
        "    t0=time.time()\n",
        "    x_tr,x_va = X_text[tr], X_text[va]\n",
        "    y_tr,y_va = y[tr], y[va]\n",
        "    X_tr, X_va, X_te, vdim = build_fold(x_tr, x_va, X_test)\n",
        "    print(f'  [Fold {fold}] X_tr={X_tr.shape} vdim={vdim}', flush=True)\n",
        "    clf = MultinomialNB(alpha=alpha)\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    P_va = clf.predict_proba(X_va).astype(np.float32)\n",
        "    P_te = clf.predict_proba(X_te).astype(np.float32)\n",
        "    # reorder to fixed class order\n",
        "    order_idx = [list(clf.classes_).index(c) for c in classes]\n",
        "    P_va = P_va[:, order_idx]; P_te = P_te[:, order_idx]\n",
        "    # clip+renorm\n",
        "    P_va = np.clip(P_va, 1e-12, 1-1e-12); P_va /= P_va.sum(axis=1, keepdims=True)\n",
        "    P_te = np.clip(P_te, 1e-12, 1-1e-12); P_te /= P_te.sum(axis=1, keepdims=True)\n",
        "    oof[va] = P_va\n",
        "    test_pred += P_te / N_FOLDS\n",
        "    loss = log_loss(y_va, P_va, labels=list(classes))\n",
        "    fold_losses.append(loss)\n",
        "    print(f'    loss={loss:.5f} elapsed={time.time()-t0:.2f}s', flush=True)\n",
        "    del X_tr, X_va, X_te, P_va, P_te, clf; gc.collect()\n",
        "\n",
        "oof_loss = log_loss(y, oof, labels=list(classes))\n",
        "print(f'[MNB_char_tfidf_2_6] OOF={oof_loss:.5f}; folds={np.round(fold_losses,5).tolist()} total={time.time()-t0_all:.2f}s', flush=True)\n",
        "np.save('oof_mnb_char_2_6.npy', oof)\n",
        "np.save('test_mnb_char_2_6.npy', test_pred)\n",
        "print('Saved oof_mnb_char_2_6.npy and test_mnb_char_2_6.npy', flush=True)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MNB_char_tfidf_2_6] alpha=0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 1] X_tr=(14096, 277345) vdim=277345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    loss=0.39844 elapsed=4.77s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 2] X_tr=(14097, 278200) vdim=278200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    loss=0.39749 elapsed=4.78s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 3] X_tr=(14097, 277563) vdim=277563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    loss=0.40645 elapsed=4.78s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 4] X_tr=(14097, 278079) vdim=278079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    loss=0.40916 elapsed=4.80s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [Fold 5] X_tr=(14097, 278322) vdim=278322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    loss=0.38659 elapsed=4.77s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MNB_char_tfidf_2_6] OOF=0.39963; folds=[0.39844, 0.39749, 0.40645, 0.40916, 0.38659] total=24.42s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_mnb_char_2_6.npy and test_mnb_char_2_6.npy\n"
          ]
        }
      ]
    },
    {
      "id": "5fa2cb99-e3e3-4381-ba4b-7afadc94c66d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3-bin length-gated Classwise LOP (Run A): add MNB tiny-cap, stabilize temps, caps tweaks, confidence overlay\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from pathlib import Path\n",
        "from scipy.optimize import minimize, minimize_scalar\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "y = train['author'].values\n",
        "classes = np.unique(y).tolist()\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "C = len(classes)\n",
        "\n",
        "def load(p):\n",
        "    return np.load(p) if Path(p).exists() else None\n",
        "\n",
        "# Use 9-core + add mnb_char_2_6 with ultra-tight per-bin cap\n",
        "cands = [\n",
        "    ('nbsvm_wc_tweaked',        'oof_nbsvm_wc_tweaked.npy',        'test_nbsvm_wc_tweaked.npy'),\n",
        "    ('nbsvm_char_2_6_counts',   'oof_nbsvm_char_2_6_counts.npy',   'test_nbsvm_char_2_6_counts.npy'),\n",
        "    ('nbsvm_wc_fixed',          'oof_nbsvm_wc_fixed.npy',          'test_nbsvm_wc_fixed.npy'),\n",
        "    ('nbsvm_char_2_7_presence', 'oof_nbsvm_char_2_7_presence.npy', 'test_nbsvm_char_2_7_presence.npy'),\n",
        "    ('lr_wc_fixed',             'oof_lr_wordchar_fixed.npy',       'test_lr_wordchar_fixed.npy'),\n",
        "    ('lr_word13_charwb36',      'oof_lr_word13_charwb36.npy',      'test_lr_word13_charwb36.npy'),\n",
        "    ('lr_char_1_8_hero',        'oof_lr_char_1_8_hero.npy',        'test_lr_char_1_8_hero.npy'),\n",
        "    ('svc_charwb_1_6_sig',      'oof_svc_charwb_1_6_sig.npy',      'test_svc_charwb_1_6_sig.npy'),\n",
        "    ('lr_wordpunct_1_3',        'oof_lr_wordpunct_1_3.npy',        'test_lr_wordpunct_1_3.npy'),\n",
        "    ('mnb_char_2_6',            'oof_mnb_char_2_6.npy',            'test_mnb_char_2_6.npy'),\n",
        "]\n",
        "loaded = []\n",
        "for name, oofp, tsp in cands:\n",
        "    o = load(oofp); t = load(tsp)\n",
        "    assert o is not None and t is not None, f'Missing preds for {name}'\n",
        "    o = np.clip(o, 1e-12, 1-1e-12); o = o / o.sum(axis=1, keepdims=True)\n",
        "    t = np.clip(t, 1e-12, 1-1e-12); t = t / t.sum(axis=1, keepdims=True)\n",
        "    loaded.append((name, o.astype(np.float64), t.astype(np.float64)))\n",
        "names = [n for n,_,_ in loaded]\n",
        "K = len(names)\n",
        "print('Length-gated LOP with models:', names, flush=True)\n",
        "\n",
        "# Scalar temperature helper\n",
        "def scale_probs_scalar(P, T):\n",
        "    S = np.clip(P, 1e-12, 1-1e-12) ** (1.0/float(T))\n",
        "    return S / S.sum(axis=1, keepdims=True)\n",
        "\n",
        "OOFs_raw = [o for _,o,_ in loaded]\n",
        "TESTs_raw = [t for _,_,t in loaded]\n",
        "\n",
        "# Global per-model temps (for diagnostics and shrink target)\n",
        "per_model_T_global = []\n",
        "OOFs_global = []\n",
        "TESTs_global = []\n",
        "for i in range(K):\n",
        "    Pi = OOFs_raw[i]\n",
        "    # fit global T with broad bounds first\n",
        "    resTi = minimize_scalar(lambda T: log_loss(y, scale_probs_scalar(Pi, T), labels=classes),\n",
        "                            bounds=(0.5, 5.0), method='bounded')\n",
        "    Ti = float(resTi.x)\n",
        "    per_model_T_global.append(Ti)\n",
        "    OOFs_global.append(scale_probs_scalar(OOFs_raw[i], Ti))\n",
        "    TESTs_global.append(scale_probs_scalar(TESTs_raw[i], Ti))\n",
        "per_oof = {names[i]: log_loss(y, OOFs_global[i], labels=classes) for i in range(K)}\n",
        "print('Per-model T (global):', {names[i]: round(per_model_T_global[i],3) for i in range(K)})\n",
        "print('Per-model OOF (post scalar cal):', {k: round(v,5) for k,v in per_oof.items()})\n",
        "\n",
        "# Helpers\n",
        "def geo_pool_log_classwise(stacks, W):\n",
        "    n = stacks[0].shape[0]\n",
        "    A = np.zeros((n, C), dtype=np.float64)\n",
        "    for k in range(K):\n",
        "        A += np.log(stacks[k]) * W[k][None, :]\n",
        "    A -= A.max(axis=1, keepdims=True)\n",
        "    P = np.exp(A); P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "def softmax_cols(Z):\n",
        "    W = np.zeros_like(Z)\n",
        "    for j in range(C):\n",
        "        z = Z[:, j]\n",
        "        z = z - z.max()\n",
        "        e = np.exp(z); s = e.sum()\n",
        "        W[:, j] = e / (s if s>0 else 1.0)\n",
        "    return W\n",
        "\n",
        "# Regularization and caps (Run A settings)\n",
        "lambda_ent = 0.0025\n",
        "global_cap = 0.55\n",
        "weak_cap = 0.09\n",
        "tiny_prune_thresh = 0.00\n",
        "# explicit caps will be passed per-bin to include per-bin cap for mnb\n",
        "nb_mask = np.array([n.startswith('nbsvm_') for n in names], dtype=bool)\n",
        "name_to_idx = {n:i for i,n in enumerate(names)}\n",
        "\n",
        "def apply_caps_with_nbcap(W, nb_cap_local, explicit_caps_local):\n",
        "    Wc = W.copy()\n",
        "    # Global entry cap\n",
        "    Wc = np.minimum(Wc, global_cap)\n",
        "    # Explicit per-model caps\n",
        "    for n, cap in explicit_caps_local.items():\n",
        "        if n in name_to_idx:\n",
        "            i = name_to_idx[n]\n",
        "            Wc[i, :] = np.minimum(Wc[i, :], cap)\n",
        "    # Weak cap for very weak bases\n",
        "    for i, n in enumerate(names):\n",
        "        if per_oof.get(n, 1.0) > 0.40:\n",
        "            Wc[i, :] = np.minimum(Wc[i, :], weak_cap)\n",
        "    # NB-family total cap per class\n",
        "    for j in range(C):\n",
        "        s_nb = Wc[nb_mask, j].sum()\n",
        "        if s_nb > nb_cap_local and s_nb > 0:\n",
        "            Wc[nb_mask, j] *= (nb_cap_local / s_nb)\n",
        "    # Per-class renormalize\n",
        "    for j in range(C):\n",
        "        col = Wc[:, j]\n",
        "        if tiny_prune_thresh > 0:\n",
        "            col[col < tiny_prune_thresh] = 0.0\n",
        "        s = col.sum()\n",
        "        if s == 0:\n",
        "            col[:] = 1.0 / K\n",
        "        else:\n",
        "            col[:] = col / s\n",
        "        Wc[:, j] = col\n",
        "    return Wc\n",
        "\n",
        "def make_objective(OOFs_subset, nb_cap_local, explicit_caps_local, y_bin):\n",
        "    def objective(Z):\n",
        "        W0 = softmax_cols(Z)\n",
        "        Wc = apply_caps_with_nbcap(W0, nb_cap_local, explicit_caps_local)\n",
        "        P = geo_pool_log_classwise(OOFs_subset, Wc)\n",
        "        ent = 0.0\n",
        "        for j in range(C):\n",
        "            wj = np.clip(Wc[:, j], 1e-12, 1.0)\n",
        "            ent += float(np.sum(wj * np.log(wj)))\n",
        "        reg = lambda_ent * ent\n",
        "        return log_loss(y_bin, P, labels=classes) + reg\n",
        "    return objective\n",
        "\n",
        "# Length bins (fixed thresholds)\n",
        "train_len = train['text'].astype(str).str.len().values\n",
        "test_len  = test['text'].astype(str).str.len().values\n",
        "short_thr, mid_lo, mid_hi = 100, 101, 180\n",
        "mask_short = (train_len <= short_thr)\n",
        "mask_mid   = (train_len >= mid_lo) & (train_len <= mid_hi)\n",
        "mask_long  = (train_len > mid_hi)\n",
        "test_mask_short = (test_len <= short_thr)\n",
        "test_mask_mid   = (test_len >= mid_lo) & (test_len <= mid_hi)\n",
        "test_mask_long  = (test_len > mid_hi)\n",
        "print('Bin sizes:', {'short': int(mask_short.sum()), 'mid': int(mask_mid.sum()), 'long': int(mask_long.sum())}, flush=True)\n",
        "\n",
        "# Warm start from global Z_star if available\n",
        "Z_global = None\n",
        "try:\n",
        "    Z_global = Z_star.copy()  # from Cell 32 if present\n",
        "except NameError:\n",
        "    Z_global = None\n",
        "Z_warm = np.zeros((K, C), dtype=np.float64)\n",
        "if Z_global is not None and getattr(Z_global, 'shape', None) == (K, C):\n",
        "    Z_warm = Z_global\n",
        "\n",
        "final_oof = np.zeros((len(train), C), dtype=np.float64)\n",
        "final_test = np.zeros((len(test), C), dtype=np.float64)\n",
        "\n",
        "def run_bin(name, tr_mask, te_mask, nb_cap_local, mnb_cap_local):\n",
        "    idx_tr = np.where(tr_mask)[0]\n",
        "    idx_te = np.where(te_mask)[0]\n",
        "    if len(idx_tr) == 0:\n",
        "        return\n",
        "    y_bin = y[idx_tr]\n",
        "    # Per-bin per-model scalar temperatures with bounds (0.75,1.5) and shrink toward global\n",
        "    OOFs_bin = []\n",
        "    TESTs_bin = []\n",
        "    for i in range(K):\n",
        "        Pi_tr = OOFs_raw[i][idx_tr]\n",
        "        # bound temps for stability\n",
        "        resTi = minimize_scalar(lambda T: log_loss(y_bin, scale_probs_scalar(Pi_tr, T), labels=classes),\n",
        "                                bounds=(0.75, 1.5), method='bounded')\n",
        "        Ti_bin = float(resTi.x)\n",
        "        # shrink: 70% global + 30% fitted-in-bin\n",
        "        Ti_shrunk = 0.7 * float(per_model_T_global[i]) + 0.3 * Ti_bin\n",
        "        OOFs_bin.append(scale_probs_scalar(OOFs_raw[i][idx_tr], Ti_shrunk))\n",
        "        TESTs_bin.append(scale_probs_scalar(TESTs_raw[i][idx_te], Ti_shrunk))\n",
        "    # Optimize classwise weights\n",
        "    starts = 64  # reduced per expert\n",
        "    rng = np.random.RandomState(42)\n",
        "    inits = [Z_warm.copy()] + [rng.normal(0, 0.5, size=(K, C)) for _ in range(starts-1)]\n",
        "    best = (1e9, None)\n",
        "    # per-bin explicit caps (svc/lr_wordpunct fixed; mnb per-bin tiny caps)\n",
        "    explicit_caps_local = {\n",
        "        'svc_charwb_1_6_sig': 0.06,\n",
        "        'lr_wordpunct_1_3': 0.05,\n",
        "        'mnb_char_2_6': mnb_cap_local,\n",
        "    }\n",
        "    obj = make_objective(OOFs_bin, nb_cap_local, explicit_caps_local, y_bin)\n",
        "    for si, Z0 in enumerate(inits, 1):\n",
        "        res = minimize(lambda z: obj(z.reshape(K, C)), Z0.ravel(), method='L-BFGS-B')\n",
        "        val = float(res.fun)\n",
        "        if val < best[0]:\n",
        "            best = (val, res.x.reshape(K, C).copy())\n",
        "        if si % 16 == 0:\n",
        "            print(f'  [{name}] start {si}/{starts} best_obj={best[0]:.5f}', flush=True)\n",
        "    Z_bin = best[1]\n",
        "    W0 = softmax_cols(Z_bin)\n",
        "    Wc = apply_caps_with_nbcap(W0, nb_cap_local, explicit_caps_local)\n",
        "    # Blend\n",
        "    P_oof_bin = geo_pool_log_classwise(OOFs_bin, Wc)\n",
        "    P_test_bin = geo_pool_log_classwise(TESTs_bin, Wc)\n",
        "    oof_pre = log_loss(y_bin, P_oof_bin, labels=classes)\n",
        "    # Per-bin classwise temperature scaling with tighter bounds\n",
        "    def scale_classwise(P, Tvec):\n",
        "        T = np.asarray(Tvec, dtype=np.float64)\n",
        "        S = np.clip(P, 1e-12, 1-1e-12) ** (1.0 / T[None, :])\n",
        "        return S / S.sum(axis=1, keepdims=True)\n",
        "    bounds = [(0.75, 1.5)] * C\n",
        "    resTc = minimize(lambda t: log_loss(y_bin, scale_classwise(P_oof_bin, t), labels=classes),\n",
        "                     x0=np.ones(C, dtype=np.float64), method='L-BFGS-B', bounds=bounds)\n",
        "    T_class = resTc.x\n",
        "    P_oof_scaled = scale_classwise(P_oof_bin, T_class)\n",
        "    P_test_scaled = scale_classwise(P_test_bin, T_class)\n",
        "    oof_final_bin = log_loss(y_bin, P_oof_scaled, labels=classes)\n",
        "    print(f'  [{name}] OOF pre-temp={oof_pre:.5f} final={oof_final_bin:.5f} T={np.round(T_class,4)}', flush=True)\n",
        "    # Stitch\n",
        "    final_oof[idx_tr] = P_oof_scaled\n",
        "    final_test[idx_te] = P_test_scaled\n",
        "\n",
        "# Run bins with per-bin nb_cap and per-bin mnb tiny caps\n",
        "run_bin('short', mask_short, test_mask_short, nb_cap_local=0.66, mnb_cap_local=0.04)\n",
        "run_bin('mid',   mask_mid,   test_mask_mid,   nb_cap_local=0.62, mnb_cap_local=0.035)\n",
        "run_bin('long',  mask_long,  test_mask_long,  nb_cap_local=0.58, mnb_cap_local=0.03)\n",
        "\n",
        "# Overall OOF\n",
        "oof_loss = log_loss(y, final_oof, labels=classes)\n",
        "print('Length-gated overall OOF:', round(oof_loss,5), 'elapsed=%.2fs' % (time.time()-t0), flush=True)\n",
        "\n",
        "# Confidence overlay post-processing\n",
        "P_lop_oof = final_oof.copy(); P_lop_test = final_test.copy()\n",
        "P_hero_oof = load('oof_lr_char_1_8_hero.npy'); P_hero_test = load('test_lr_char_1_8_hero.npy')\n",
        "assert P_hero_oof is not None and P_hero_test is not None, 'Missing hero model preds for overlay'\n",
        "P_hero_oof = np.clip(P_hero_oof, 1e-12, 1-1e-12); P_hero_oof /= P_hero_oof.sum(axis=1, keepdims=True)\n",
        "P_hero_test = np.clip(P_hero_test, 1e-12, 1-1e-12); P_hero_test /= P_hero_test.sum(axis=1, keepdims=True)\n",
        "\n",
        "def apply_overlay(P_base, P_aux, mask_conf, alpha=0.8):\n",
        "    P = P_base.copy()\n",
        "    mix = (~mask_conf).astype(np.float64)  # rows to mix if max<0.46 -> False; invert\n",
        "    # Actually build per-row mixture:\n",
        "    # where low confidence (max<0.46): P_final = 0.8*P_base + 0.2*P_aux; else keep P_base\n",
        "    low_conf = mask_conf  # True where max<0.46\n",
        "    P[low_conf] = (alpha * P_base[low_conf] + (1.0 - alpha) * P_aux[low_conf])\n",
        "    P = np.clip(P, 1e-12, 1-1e-12); P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "# Define low-confidence masks\n",
        "mask_low_oof = (P_lop_oof.max(axis=1) < 0.46)\n",
        "mask_low_test = (P_lop_test.max(axis=1) < 0.46)\n",
        "P_final_oof = apply_overlay(P_lop_oof, P_hero_oof, mask_low_oof, alpha=0.8)\n",
        "P_final_test = apply_overlay(P_lop_test, P_hero_test, mask_low_test, alpha=0.8)\n",
        "\n",
        "# Optional micro prior: boost MWS on very short texts (<=60 chars) by 1.02\n",
        "short_mask_60_tr = (train_len <= 60)\n",
        "short_mask_60_te = (test_len <= 60)\n",
        "def boost_mws(P, mask):\n",
        "    P2 = P.copy()\n",
        "    j = classes.index('MWS')\n",
        "    if mask.any():\n",
        "        P2[mask, j] *= 1.02\n",
        "        P2[mask] /= P2[mask].sum(axis=1, keepdims=True)\n",
        "    return P2\n",
        "P_final_oof = boost_mws(P_final_oof, short_mask_60_tr)\n",
        "P_final_test = boost_mws(P_final_test, short_mask_60_te)\n",
        "\n",
        "# Report OOF after overlay\n",
        "oof_loss_final = log_loss(y, P_final_oof, labels=classes)\n",
        "print('Post-overlay OOF:', round(oof_loss_final,5))\n",
        "\n",
        "# Safety checks and save submission\n",
        "probs = P_final_test[:, [classes.index(c) for c in submit_cols]]\n",
        "probs = np.clip(probs, 1e-12, 1-1e-12)\n",
        "probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "assert probs.shape[0] == len(test) and probs.shape[1] == 3, f'Bad submission shape {probs.shape}'\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "assert sub.shape[0] == 1958, f'Expected 1958 rows, got {sub.shape[0]}'\n",
        "assert np.all(np.isfinite(sub[submit_cols].values)), 'NaNs/Infs in submission'\n",
        "row_sums = sub[submit_cols].sum(axis=1).values\n",
        "assert np.allclose(row_sums, 1.0, atol=1e-6), 'Row sums not ~1'\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Run A: gated LOP + overlay). Elapsed=%.2fs' % (time.time()-t0), flush=True)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length-gated LOP with models: ['nbsvm_wc_tweaked', 'nbsvm_char_2_6_counts', 'nbsvm_wc_fixed', 'nbsvm_char_2_7_presence', 'lr_wc_fixed', 'lr_word13_charwb36', 'lr_char_1_8_hero', 'svc_charwb_1_6_sig', 'lr_wordpunct_1_3', 'mnb_char_2_6']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-model T (global): {'nbsvm_wc_tweaked': 1.55, 'nbsvm_char_2_6_counts': 1.579, 'nbsvm_wc_fixed': 1.567, 'nbsvm_char_2_7_presence': 1.477, 'lr_wc_fixed': 0.877, 'lr_word13_charwb36': 1.006, 'lr_char_1_8_hero': 0.871, 'svc_charwb_1_6_sig': 0.803, 'lr_wordpunct_1_3': 0.771, 'mnb_char_2_6': 1.17}\nPer-model OOF (post scalar cal): {'nbsvm_wc_tweaked': 0.32156, 'nbsvm_char_2_6_counts': 0.33019, 'nbsvm_wc_fixed': 0.33633, 'nbsvm_char_2_7_presence': 0.338, 'lr_wc_fixed': 0.36343, 'lr_word13_charwb36': 0.36825, 'lr_char_1_8_hero': 0.36987, 'svc_charwb_1_6_sig': 0.39657, 'lr_wordpunct_1_3': 0.40368, 'mnb_char_2_6': 0.39492}\nBin sizes: {'short': 6232, 'mid': 6419, 'long': 4970}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 16/64 best_obj=0.44223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 32/64 best_obj=0.44223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 48/64 best_obj=0.44223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 64/64 best_obj=0.44223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] OOF pre-temp=0.45688 final=0.45585 T=[1.0988 0.9619 1.0017]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 16/64 best_obj=0.27634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 32/64 best_obj=0.27624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 48/64 best_obj=0.27619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 64/64 best_obj=0.27619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] OOF pre-temp=0.29132 final=0.28391 T=[0.75   0.9423 0.9038]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 16/64 best_obj=0.13268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 32/64 best_obj=0.13268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 48/64 best_obj=0.13268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 64/64 best_obj=0.13263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] OOF pre-temp=0.14728 final=0.13967 T=[0.7819 0.7883 0.75  ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length-gated overall OOF: 0.30404 elapsed=631.55s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Post-overlay OOF: 0.30419\nSaved submission.csv (Run A: gated LOP + overlay). Elapsed=631.58s\n"
          ]
        }
      ]
    },
    {
      "id": "1c44115a-4576-4861-8cd9-82b2f65aba6f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4-bin length-gated Classwise LOP (Run B): refine bins, per-bin NB caps, tiny-cap MNB, stabilized temps, confidence overlay\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from pathlib import Path\n",
        "from scipy.optimize import minimize, minimize_scalar\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "y = train['author'].values\n",
        "classes = np.unique(y).tolist()\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "C = len(classes)\n",
        "\n",
        "def load(p):\n",
        "    return np.load(p) if Path(p).exists() else None\n",
        "\n",
        "# Same portfolio as Run A (10-core including MNB)\n",
        "cands = [\n",
        "    ('nbsvm_wc_tweaked',        'oof_nbsvm_wc_tweaked.npy',        'test_nbsvm_wc_tweaked.npy'),\n",
        "    ('nbsvm_char_2_6_counts',   'oof_nbsvm_char_2_6_counts.npy',   'test_nbsvm_char_2_6_counts.npy'),\n",
        "    ('nbsvm_wc_fixed',          'oof_nbsvm_wc_fixed.npy',          'test_nbsvm_wc_fixed.npy'),\n",
        "    ('nbsvm_char_2_7_presence', 'oof_nbsvm_char_2_7_presence.npy', 'test_nbsvm_char_2_7_presence.npy'),\n",
        "    ('lr_wc_fixed',             'oof_lr_wordchar_fixed.npy',       'test_lr_wordchar_fixed.npy'),\n",
        "    ('lr_word13_charwb36',      'oof_lr_word13_charwb36.npy',      'test_lr_word13_charwb36.npy'),\n",
        "    ('lr_char_1_8_hero',        'oof_lr_char_1_8_hero.npy',        'test_lr_char_1_8_hero.npy'),\n",
        "    ('svc_charwb_1_6_sig',      'oof_svc_charwb_1_6_sig.npy',      'test_svc_charwb_1_6_sig.npy'),\n",
        "    ('lr_wordpunct_1_3',        'oof_lr_wordpunct_1_3.npy',        'test_lr_wordpunct_1_3.npy'),\n",
        "    ('mnb_char_2_6',            'oof_mnb_char_2_6.npy',            'test_mnb_char_2_6.npy'),\n",
        "]\n",
        "loaded = []\n",
        "for name, oofp, tsp in cands:\n",
        "    o = load(oofp); t = load(tsp)\n",
        "    assert o is not None and t is not None, f'Missing preds for {name}'\n",
        "    o = np.clip(o, 1e-12, 1-1e-12); o = o / o.sum(axis=1, keepdims=True)\n",
        "    t = np.clip(t, 1e-12, 1-1e-12); t = t / t.sum(axis=1, keepdims=True)\n",
        "    loaded.append((name, o.astype(np.float64), t.astype(np.float64)))\n",
        "names = [n for n,_,_ in loaded]\n",
        "K = len(names)\n",
        "print('Length-gated LOP (4-bin) with models:', names, flush=True)\n",
        "\n",
        "def scale_probs_scalar(P, T):\n",
        "    S = np.clip(P, 1e-12, 1-1e-12) ** (1.0/float(T))\n",
        "    return S / S.sum(axis=1, keepdims=True)\n",
        "\n",
        "OOFs_raw = [o for _,o,_ in loaded]\n",
        "TESTs_raw = [t for _,_,t in loaded]\n",
        "\n",
        "# Global per-model temps (shrink target)\n",
        "per_model_T_global = []; OOFs_global = []; TESTs_global = []\n",
        "for i in range(K):\n",
        "    Pi = OOFs_raw[i]\n",
        "    resTi = minimize_scalar(lambda T: log_loss(y, scale_probs_scalar(Pi, T), labels=classes),\n",
        "                            bounds=(0.5, 5.0), method='bounded')\n",
        "    Ti = float(resTi.x)\n",
        "    per_model_T_global.append(Ti)\n",
        "    OOFs_global.append(scale_probs_scalar(OOFs_raw[i], Ti))\n",
        "    TESTs_global.append(scale_probs_scalar(TESTs_raw[i], Ti))\n",
        "per_oof = {names[i]: log_loss(y, OOFs_global[i], labels=classes) for i in range(K)}\n",
        "print('Per-model T (global):', {names[i]: round(per_model_T_global[i],3) for i in range(K)})\n",
        "print('Per-model OOF (post scalar cal):', {k: round(v,5) for k,v in per_oof.items()})\n",
        "\n",
        "def geo_pool_log_classwise(stacks, W):\n",
        "    n = stacks[0].shape[0]\n",
        "    A = np.zeros((n, C), dtype=np.float64)\n",
        "    for k in range(K):\n",
        "        A += np.log(stacks[k]) * W[k][None, :]\n",
        "    A -= A.max(axis=1, keepdims=True)\n",
        "    P = np.exp(A); P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "def softmax_cols(Z):\n",
        "    W = np.zeros_like(Z)\n",
        "    for j in range(C):\n",
        "        z = Z[:, j]\n",
        "        z = z - z.max()\n",
        "        e = np.exp(z); s = e.sum()\n",
        "        W[:, j] = e / (s if s>0 else 1.0)\n",
        "    return W\n",
        "\n",
        "# Regularization and caps\n",
        "lambda_ent = 0.0025\n",
        "global_cap = 0.55\n",
        "weak_cap = 0.09\n",
        "tiny_prune_thresh = 0.00\n",
        "nb_mask = np.array([n.startswith('nbsvm_') for n in names], dtype=bool)\n",
        "name_to_idx = {n:i for i,n in enumerate(names)}\n",
        "\n",
        "def apply_caps_with_nbcap(W, nb_cap_local, explicit_caps_local):\n",
        "    Wc = W.copy()\n",
        "    Wc = np.minimum(Wc, global_cap)\n",
        "    for n, cap in explicit_caps_local.items():\n",
        "        if n in name_to_idx:\n",
        "            i = name_to_idx[n]\n",
        "            Wc[i, :] = np.minimum(Wc[i, :], cap)\n",
        "    for i, n in enumerate(names):\n",
        "        if per_oof.get(n, 1.0) > 0.40:\n",
        "            Wc[i, :] = np.minimum(Wc[i, :], weak_cap)\n",
        "    for j in range(C):\n",
        "        s_nb = Wc[nb_mask, j].sum()\n",
        "        if s_nb > nb_cap_local and s_nb > 0:\n",
        "            Wc[nb_mask, j] *= (nb_cap_local / s_nb)\n",
        "    for j in range(C):\n",
        "        col = Wc[:, j]\n",
        "        if tiny_prune_thresh > 0:\n",
        "            col[col < tiny_prune_thresh] = 0.0\n",
        "        s = col.sum()\n",
        "        if s == 0:\n",
        "            col[:] = 1.0 / K\n",
        "        else:\n",
        "            col[:] = col / s\n",
        "        Wc[:, j] = col\n",
        "    return Wc\n",
        "\n",
        "def make_objective(OOFs_subset, nb_cap_local, explicit_caps_local, y_bin):\n",
        "    def objective(Z):\n",
        "        W0 = softmax_cols(Z)\n",
        "        Wc = apply_caps_with_nbcap(W0, nb_cap_local, explicit_caps_local)\n",
        "        P = geo_pool_log_classwise(OOFs_subset, Wc)\n",
        "        ent = 0.0\n",
        "        for j in range(C):\n",
        "            wj = np.clip(Wc[:, j], 1e-12, 1.0)\n",
        "            ent += float(np.sum(wj * np.log(wj)))\n",
        "        reg = lambda_ent * ent\n",
        "        return log_loss(y_bin, P, labels=classes) + reg\n",
        "    return objective\n",
        "\n",
        "# 4-bin thresholds: <=80, 81-130, 131-200, >200\n",
        "train_len = train['text'].astype(str).str.len().values\n",
        "test_len  = test['text'].astype(str).str.len().values\n",
        "b1, b2, b3 = 80, 130, 200\n",
        "mask_vshort = (train_len <= b1)\n",
        "mask_short  = (train_len > b1) & (train_len <= b2)\n",
        "mask_mid    = (train_len > b2) & (train_len <= b3)\n",
        "mask_long   = (train_len > b3)\n",
        "test_vshort = (test_len <= b1)\n",
        "test_short  = (test_len > b1) & (test_len <= b2)\n",
        "test_mid    = (test_len > b2) & (test_len <= b3)\n",
        "test_long   = (test_len > b3)\n",
        "print('4-bin sizes:', {'vshort': int(mask_vshort.sum()), 'short': int(mask_short.sum()), 'mid': int(mask_mid.sum()), 'long': int(mask_long.sum())}, flush=True)\n",
        "\n",
        "Z_global = None\n",
        "try:\n",
        "    Z_global = Z_star.copy()\n",
        "except NameError:\n",
        "    Z_global = None\n",
        "Z_warm = np.zeros((K, C), dtype=np.float64)\n",
        "if Z_global is not None and getattr(Z_global, 'shape', None) == (K, C):\n",
        "    Z_warm = Z_global\n",
        "\n",
        "final_oof = np.zeros((len(train), C), dtype=np.float64)\n",
        "final_test = np.zeros((len(test), C), dtype=np.float64)\n",
        "\n",
        "def run_bin(name, tr_mask, te_mask, nb_cap_local, mnb_cap_local):\n",
        "    idx_tr = np.where(tr_mask)[0]\n",
        "    idx_te = np.where(te_mask)[0]\n",
        "    if len(idx_tr) == 0:\n",
        "        return\n",
        "    y_bin = y[idx_tr]\n",
        "    OOFs_bin = []; TESTs_bin = []\n",
        "    for i in range(K):\n",
        "        Pi_tr = OOFs_raw[i][idx_tr]\n",
        "        resTi = minimize_scalar(lambda T: log_loss(y_bin, scale_probs_scalar(Pi_tr, T), labels=classes),\n",
        "                                bounds=(0.75, 1.5), method='bounded')\n",
        "        Ti_bin = float(resTi.x)\n",
        "        Ti_shrunk = 0.7 * float(per_model_T_global[i]) + 0.3 * Ti_bin\n",
        "        OOFs_bin.append(scale_probs_scalar(OOFs_raw[i][idx_tr], Ti_shrunk))\n",
        "        TESTs_bin.append(scale_probs_scalar(TESTs_raw[i][idx_te], Ti_shrunk))\n",
        "    starts = 64\n",
        "    rng = np.random.RandomState(42)\n",
        "    inits = [Z_warm.copy()] + [rng.normal(0, 0.5, size=(K, C)) for _ in range(starts-1)]\n",
        "    best = (1e9, None)\n",
        "    explicit_caps_local = {\n",
        "        'svc_charwb_1_6_sig': 0.06,\n",
        "        'lr_wordpunct_1_3': 0.05,\n",
        "        'mnb_char_2_6': mnb_cap_local,\n",
        "    }\n",
        "    obj = make_objective(OOFs_bin, nb_cap_local, explicit_caps_local, y_bin)\n",
        "    for si, Z0 in enumerate(inits, 1):\n",
        "        res = minimize(lambda z: obj(z.reshape(K, C)), Z0.ravel(), method='L-BFGS-B')\n",
        "        val = float(res.fun)\n",
        "        if val < best[0]:\n",
        "            best = (val, res.x.reshape(K, C).copy())\n",
        "        if si % 16 == 0:\n",
        "            print(f'  [{name}] start {si}/{starts} best_obj={best[0]:.5f}', flush=True)\n",
        "    Z_bin = best[1]\n",
        "    W0 = softmax_cols(Z_bin)\n",
        "    Wc = apply_caps_with_nbcap(W0, nb_cap_local, explicit_caps_local)\n",
        "    P_oof_bin = geo_pool_log_classwise(OOFs_bin, Wc)\n",
        "    P_test_bin = geo_pool_log_classwise(TESTs_bin, Wc)\n",
        "    oof_pre = log_loss(y_bin, P_oof_bin, labels=classes)\n",
        "    def scale_classwise(P, Tvec):\n",
        "        T = np.asarray(Tvec, dtype=np.float64)\n",
        "        S = np.clip(P, 1e-12, 1-1e-12) ** (1.0 / T[None, :])\n",
        "        return S / S.sum(axis=1, keepdims=True)\n",
        "    bounds = [(0.75, 1.5)] * C\n",
        "    resTc = minimize(lambda t: log_loss(y_bin, scale_classwise(P_oof_bin, t), labels=classes),\n",
        "                     x0=np.ones(C, dtype=np.float64), method='L-BFGS-B', bounds=bounds)\n",
        "    T_class = resTc.x\n",
        "    P_oof_scaled = scale_classwise(P_oof_bin, T_class)\n",
        "    P_test_scaled = scale_classwise(P_test_bin, T_class)\n",
        "    oof_final_bin = log_loss(y_bin, P_oof_scaled, labels=classes)\n",
        "    print(f'  [{name}] OOF pre-temp={oof_pre:.5f} final={oof_final_bin:.5f} T={np.round(T_class,4)}', flush=True)\n",
        "    final_oof[idx_tr] = P_oof_scaled\n",
        "    final_test[idx_te] = P_test_scaled\n",
        "\n",
        "# Per-bin NB caps per expert for 4-bin: [0.68, 0.65, 0.62, 0.58]\n",
        "# MNB tiny caps per-bin (extend slightly for 4th bin): [0.042, 0.038, 0.032, 0.028]\n",
        "run_bin('vshort', mask_vshort, test_vshort, nb_cap_local=0.68, mnb_cap_local=0.042)\n",
        "run_bin('short',  mask_short,  test_short,  nb_cap_local=0.65, mnb_cap_local=0.038)\n",
        "run_bin('mid',    mask_mid,    test_mid,    nb_cap_local=0.62, mnb_cap_local=0.032)\n",
        "run_bin('long',   mask_long,   test_long,   nb_cap_local=0.58, mnb_cap_local=0.028)\n",
        "\n",
        "oof_loss = log_loss(y, final_oof, labels=classes)\n",
        "print('4-bin Length-gated overall OOF:', round(oof_loss,5), 'elapsed=%.2fs' % (time.time()-t0), flush=True)\n",
        "\n",
        "# Confidence overlay with hero model on low-confidence rows (max<0.46):\n",
        "def apply_overlay(P_base, P_aux, mask_conf, alpha=0.8):\n",
        "    P = P_base.copy()\n",
        "    low_conf = mask_conf\n",
        "    P[low_conf] = (alpha * P_base[low_conf] + (1.0 - alpha) * P_aux[low_conf])\n",
        "    P = np.clip(P, 1e-12, 1-1e-12); P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "P_lop_oof = final_oof.copy(); P_lop_test = final_test.copy()\n",
        "P_hero_oof = load('oof_lr_char_1_8_hero.npy'); P_hero_test = load('test_lr_char_1_8_hero.npy')\n",
        "assert P_hero_oof is not None and P_hero_test is not None, 'Missing hero model preds for overlay'\n",
        "P_hero_oof = np.clip(P_hero_oof, 1e-12, 1-1e-12); P_hero_oof /= P_hero_oof.sum(axis=1, keepdims=True)\n",
        "P_hero_test = np.clip(P_hero_test, 1e-12, 1-1e-12); P_hero_test /= P_hero_test.sum(axis=1, keepdims=True)\n",
        "mask_low_oof = (P_lop_oof.max(axis=1) < 0.46)\n",
        "mask_low_test = (P_lop_test.max(axis=1) < 0.46)\n",
        "P_final_oof = apply_overlay(P_lop_oof, P_hero_oof, mask_low_oof, alpha=0.8)\n",
        "P_final_test = apply_overlay(P_lop_test, P_hero_test, mask_low_test, alpha=0.8)\n",
        "\n",
        "# Optional micro prior on very short (<=60) MWS x1.02\n",
        "short_mask_60_tr = (train_len <= 60)\n",
        "short_mask_60_te = (test_len <= 60)\n",
        "def boost_mws(P, mask):\n",
        "    P2 = P.copy()\n",
        "    j = classes.index('MWS')\n",
        "    if mask.any():\n",
        "        P2[mask, j] *= 1.02\n",
        "        P2[mask] /= P2[mask].sum(axis=1, keepdims=True)\n",
        "    return P2\n",
        "P_final_oof = boost_mws(P_final_oof, short_mask_60_tr)\n",
        "P_final_test = boost_mws(P_final_test, short_mask_60_te)\n",
        "\n",
        "oof_loss_final = log_loss(y, P_final_oof, labels=classes)\n",
        "print('4-bin Post-overlay OOF:', round(oof_loss_final,5))\n",
        "\n",
        "probs = P_final_test[:, [classes.index(c) for c in submit_cols]]\n",
        "probs = np.clip(probs, 1e-12, 1-1e-12); probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "assert sub.shape[0] == 1958\n",
        "assert np.all(np.isfinite(sub[submit_cols].values))\n",
        "assert np.allclose(sub[submit_cols].sum(axis=1).values, 1.0, atol=1e-6)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Run B: 4-bin gated LOP + overlay). Elapsed=%.2fs' % (time.time()-t0), flush=True)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length-gated LOP (4-bin) with models: ['nbsvm_wc_tweaked', 'nbsvm_char_2_6_counts', 'nbsvm_wc_fixed', 'nbsvm_char_2_7_presence', 'lr_wc_fixed', 'lr_word13_charwb36', 'lr_char_1_8_hero', 'svc_charwb_1_6_sig', 'lr_wordpunct_1_3', 'mnb_char_2_6']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-model T (global): {'nbsvm_wc_tweaked': 1.55, 'nbsvm_char_2_6_counts': 1.579, 'nbsvm_wc_fixed': 1.567, 'nbsvm_char_2_7_presence': 1.477, 'lr_wc_fixed': 0.877, 'lr_word13_charwb36': 1.006, 'lr_char_1_8_hero': 0.871, 'svc_charwb_1_6_sig': 0.803, 'lr_wordpunct_1_3': 0.771, 'mnb_char_2_6': 1.17}\nPer-model OOF (post scalar cal): {'nbsvm_wc_tweaked': 0.32156, 'nbsvm_char_2_6_counts': 0.33019, 'nbsvm_wc_fixed': 0.33633, 'nbsvm_char_2_7_presence': 0.338, 'lr_wc_fixed': 0.36343, 'lr_word13_charwb36': 0.36825, 'lr_char_1_8_hero': 0.36987, 'svc_charwb_1_6_sig': 0.39657, 'lr_wordpunct_1_3': 0.40368, 'mnb_char_2_6': 0.39492}\n4-bin sizes: {'vshort': 4330, 'short': 4674, 'mid': 4698, 'long': 3919}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [vshort] start 16/64 best_obj=0.47375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [vshort] start 32/64 best_obj=0.47375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [vshort] start 48/64 best_obj=0.47371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [vshort] start 64/64 best_obj=0.47345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [vshort] OOF pre-temp=0.48815 final=0.48532 T=[1.19   0.9634 1.0137]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 16/64 best_obj=0.34177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 32/64 best_obj=0.34177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 48/64 best_obj=0.34177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 64/64 best_obj=0.34177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] OOF pre-temp=0.35646 final=0.35279 T=[0.8029 0.9461 0.9456]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 16/64 best_obj=0.22552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 32/64 best_obj=0.22512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 48/64 best_obj=0.22512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 64/64 best_obj=0.22510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] OOF pre-temp=0.24028 final=0.23172 T=[0.75   0.9341 0.8437]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 16/64 best_obj=0.12151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 32/64 best_obj=0.12148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 48/64 best_obj=0.12141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 64/64 best_obj=0.12141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] OOF pre-temp=0.13632 final=0.12852 T=[0.8209 0.75   0.75  ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4-bin Length-gated overall OOF: 0.3032 elapsed=672.57s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4-bin Post-overlay OOF: 0.30335\nSaved submission.csv (Run B: 4-bin gated LOP + overlay). Elapsed=672.60s\n"
          ]
        }
      ]
    },
    {
      "id": "15e12f19-9a20-4225-99b5-5191354ca22a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3-bin length-gated LOP (Run C): swap lr_wordpunct_1_3 -> lr_char_1_7; keep stabilizers, mnb tiny-cap, overlay\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from pathlib import Path\n",
        "from scipy.optimize import minimize, minimize_scalar\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "y = train['author'].values\n",
        "classes = np.unique(y).tolist()\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "C = len(classes)\n",
        "\n",
        "def load(p):\n",
        "    return np.load(p) if Path(p).exists() else None\n",
        "\n",
        "# Portfolio swap: replace lr_wordpunct_1_3 with lr_char_1_7 (keep 10-core including MNB)\n",
        "cands = [\n",
        "    ('nbsvm_wc_tweaked',        'oof_nbsvm_wc_tweaked.npy',        'test_nbsvm_wc_tweaked.npy'),\n",
        "    ('nbsvm_char_2_6_counts',   'oof_nbsvm_char_2_6_counts.npy',   'test_nbsvm_char_2_6_counts.npy'),\n",
        "    ('nbsvm_wc_fixed',          'oof_nbsvm_wc_fixed.npy',          'test_nbsvm_wc_fixed.npy'),\n",
        "    ('nbsvm_char_2_7_presence', 'oof_nbsvm_char_2_7_presence.npy', 'test_nbsvm_char_2_7_presence.npy'),\n",
        "    ('lr_wc_fixed',             'oof_lr_wordchar_fixed.npy',       'test_lr_wordchar_fixed.npy'),\n",
        "    ('lr_word13_charwb36',      'oof_lr_word13_charwb36.npy',      'test_lr_word13_charwb36.npy'),\n",
        "    ('lr_char_1_8_hero',        'oof_lr_char_1_8_hero.npy',        'test_lr_char_1_8_hero.npy'),\n",
        "    ('svc_charwb_1_6_sig',      'oof_svc_charwb_1_6_sig.npy',      'test_svc_charwb_1_6_sig.npy'),\n",
        "    ('lr_char_1_7',             'oof_lr_char_1_7.npy',             'test_lr_char_1_7.npy'),\n",
        "    ('mnb_char_2_6',            'oof_mnb_char_2_6.npy',            'test_mnb_char_2_6.npy'),\n",
        "]\n",
        "loaded = []\n",
        "for name, oofp, tsp in cands:\n",
        "    o = load(oofp); t = load(tsp)\n",
        "    assert o is not None and t is not None, f'Missing preds for {name}'\n",
        "    o = np.clip(o, 1e-12, 1-1e-12); o = o / o.sum(axis=1, keepdims=True)\n",
        "    t = np.clip(t, 1e-12, 1-1e-12); t = t / t.sum(axis=1, keepdims=True)\n",
        "    loaded.append((name, o.astype(np.float64), t.astype(np.float64)))\n",
        "names = [n for n,_,_ in loaded]\n",
        "K = len(names)\n",
        "print('Length-gated LOP (Run C) with models:', names, flush=True)\n",
        "\n",
        "def scale_probs_scalar(P, T):\n",
        "    S = np.clip(P, 1e-12, 1-1e-12) ** (1.0/float(T))\n",
        "    return S / S.sum(axis=1, keepdims=True)\n",
        "\n",
        "OOFs_raw = [o for _,o,_ in loaded]\n",
        "TESTs_raw = [t for _,_,t in loaded]\n",
        "\n",
        "# Global per-model temps (shrink target)\n",
        "per_model_T_global = []; OOFs_global = []; TESTs_global = []\n",
        "for i in range(K):\n",
        "    Pi = OOFs_raw[i]\n",
        "    resTi = minimize_scalar(lambda T: log_loss(y, scale_probs_scalar(Pi, T), labels=classes),\n",
        "                            bounds=(0.5, 5.0), method='bounded')\n",
        "    Ti = float(resTi.x)\n",
        "    per_model_T_global.append(Ti)\n",
        "    OOFs_global.append(scale_probs_scalar(OOFs_raw[i], Ti))\n",
        "    TESTs_global.append(scale_probs_scalar(TESTs_raw[i], Ti))\n",
        "per_oof = {names[i]: log_loss(y, OOFs_global[i], labels=classes) for i in range(K)}\n",
        "print('Per-model T (global):', {names[i]: round(per_model_T_global[i],3) for i in range(K)})\n",
        "print('Per-model OOF (post scalar cal):', {k: round(v,5) for k,v in per_oof.items()})\n",
        "\n",
        "def geo_pool_log_classwise(stacks, W):\n",
        "    n = stacks[0].shape[0]\n",
        "    A = np.zeros((n, C), dtype=np.float64)\n",
        "    for k in range(K):\n",
        "        A += np.log(stacks[k]) * W[k][None, :]\n",
        "    A -= A.max(axis=1, keepdims=True)\n",
        "    P = np.exp(A); P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "def softmax_cols(Z):\n",
        "    W = np.zeros_like(Z)\n",
        "    for j in range(C):\n",
        "        z = Z[:, j]\n",
        "        z = z - z.max()\n",
        "        e = np.exp(z); s = e.sum()\n",
        "        W[:, j] = e / (s if s>0 else 1.0)\n",
        "    return W\n",
        "\n",
        "# Regularization and caps (Run A/B settings)\n",
        "lambda_ent = 0.0025\n",
        "global_cap = 0.55\n",
        "weak_cap = 0.09\n",
        "tiny_prune_thresh = 0.00\n",
        "nb_mask = np.array([n.startswith('nbsvm_') for n in names], dtype=bool)\n",
        "name_to_idx = {n:i for i,n in enumerate(names)}\n",
        "\n",
        "def apply_caps_with_nbcap(W, nb_cap_local, explicit_caps_local):\n",
        "    Wc = W.copy()\n",
        "    Wc = np.minimum(Wc, global_cap)\n",
        "    for n, cap in explicit_caps_local.items():\n",
        "        if n in name_to_idx:\n",
        "            i = name_to_idx[n]\n",
        "            Wc[i, :] = np.minimum(Wc[i, :], cap)\n",
        "    for i, n in enumerate(names):\n",
        "        if per_oof.get(n, 1.0) > 0.40:\n",
        "            Wc[i, :] = np.minimum(Wc[i, :], weak_cap)\n",
        "    for j in range(C):\n",
        "        s_nb = Wc[nb_mask, j].sum()\n",
        "        if s_nb > nb_cap_local and s_nb > 0:\n",
        "            Wc[nb_mask, j] *= (nb_cap_local / s_nb)\n",
        "    for j in range(C):\n",
        "        col = Wc[:, j]\n",
        "        if tiny_prune_thresh > 0:\n",
        "            col[col < tiny_prune_thresh] = 0.0\n",
        "        s = col.sum()\n",
        "        if s == 0:\n",
        "            col[:] = 1.0 / K\n",
        "        else:\n",
        "            col[:] = col / s\n",
        "        Wc[:, j] = col\n",
        "    return Wc\n",
        "\n",
        "def make_objective(OOFs_subset, nb_cap_local, explicit_caps_local, y_bin):\n",
        "    def objective(Z):\n",
        "        W0 = softmax_cols(Z)\n",
        "        Wc = apply_caps_with_nbcap(W0, nb_cap_local, explicit_caps_local)\n",
        "        P = geo_pool_log_classwise(OOFs_subset, Wc)\n",
        "        ent = 0.0\n",
        "        for j in range(C):\n",
        "            wj = np.clip(Wc[:, j], 1e-12, 1.0)\n",
        "            ent += float(np.sum(wj * np.log(wj)))\n",
        "        reg = lambda_ent * ent\n",
        "        return log_loss(y_bin, P, labels=classes) + reg\n",
        "    return objective\n",
        "\n",
        "# 3-bin thresholds: <=100, 101-180, >180\n",
        "train_len = train['text'].astype(str).str.len().values\n",
        "test_len  = test['text'].astype(str).str.len().values\n",
        "short_thr, mid_lo, mid_hi = 100, 101, 180\n",
        "mask_short = (train_len <= short_thr)\n",
        "mask_mid   = (train_len >= mid_lo) & (train_len <= mid_hi)\n",
        "mask_long  = (train_len > mid_hi)\n",
        "test_mask_short = (test_len <= short_thr)\n",
        "test_mask_mid   = (test_len >= mid_lo) & (test_len <= mid_hi)\n",
        "test_mask_long  = (test_len > mid_hi)\n",
        "print('3-bin sizes:', {'short': int(mask_short.sum()), 'mid': int(mask_mid.sum()), 'long': int(mask_long.sum())}, flush=True)\n",
        "\n",
        "Z_global = None\n",
        "try:\n",
        "    Z_global = Z_star.copy()\n",
        "except NameError:\n",
        "    Z_global = None\n",
        "Z_warm = np.zeros((K, C), dtype=np.float64)\n",
        "if Z_global is not None and getattr(Z_global, 'shape', None) == (K, C):\n",
        "    Z_warm = Z_global\n",
        "\n",
        "final_oof = np.zeros((len(train), C), dtype=np.float64)\n",
        "final_test = np.zeros((len(test), C), dtype=np.float64)\n",
        "\n",
        "def run_bin(name, tr_mask, te_mask, nb_cap_local, mnb_cap_local):\n",
        "    idx_tr = np.where(tr_mask)[0]\n",
        "    idx_te = np.where(te_mask)[0]\n",
        "    if len(idx_tr) == 0:\n",
        "        return\n",
        "    y_bin = y[idx_tr]\n",
        "    OOFs_bin = []; TESTs_bin = []\n",
        "    # Per-bin per-model temps with bounds and shrink toward global\n",
        "    for i in range(K):\n",
        "        Pi_tr = OOFs_raw[i][idx_tr]\n",
        "        resTi = minimize_scalar(lambda T: log_loss(y_bin, scale_probs_scalar(Pi_tr, T), labels=classes),\n",
        "                                bounds=(0.75, 1.5), method='bounded')\n",
        "        Ti_bin = float(resTi.x)\n",
        "        Ti_shrunk = 0.7 * float(per_model_T_global[i]) + 0.3 * Ti_bin\n",
        "        OOFs_bin.append(scale_probs_scalar(OOFs_raw[i][idx_tr], Ti_shrunk))\n",
        "        TESTs_bin.append(scale_probs_scalar(TESTs_raw[i][idx_te], Ti_shrunk))\n",
        "    starts = 64\n",
        "    rng = np.random.RandomState(42)\n",
        "    inits = [Z_warm.copy()] + [rng.normal(0, 0.5, size=(K, C)) for _ in range(starts-1)]\n",
        "    best = (1e9, None)\n",
        "    explicit_caps_local = {\n",
        "        'svc_charwb_1_6_sig': 0.06,\n",
        "        'lr_char_1_7': 0.50,  # no special tiny cap\n",
        "        'mnb_char_2_6': mnb_cap_local,\n",
        "    }\n",
        "    obj = make_objective(OOFs_bin, nb_cap_local, explicit_caps_local, y_bin)\n",
        "    for si, Z0 in enumerate(inits, 1):\n",
        "        res = minimize(lambda z: obj(z.reshape(K, C)), Z0.ravel(), method='L-BFGS-B')\n",
        "        val = float(res.fun)\n",
        "        if val < best[0]:\n",
        "            best = (val, res.x.reshape(K, C).copy())\n",
        "        if si % 16 == 0:\n",
        "            print(f'  [{name}] start {si}/{starts} best_obj={best[0]:.5f}', flush=True)\n",
        "    Z_bin = best[1]\n",
        "    W0 = softmax_cols(Z_bin)\n",
        "    Wc = apply_caps_with_nbcap(W0, nb_cap_local, explicit_caps_local)\n",
        "    P_oof_bin = geo_pool_log_classwise(OOFs_bin, Wc)\n",
        "    P_test_bin = geo_pool_log_classwise(TESTs_bin, Wc)\n",
        "    oof_pre = log_loss(y_bin, P_oof_bin, labels=classes)\n",
        "    def scale_classwise(P, Tvec):\n",
        "        T = np.asarray(Tvec, dtype=np.float64)\n",
        "        S = np.clip(P, 1e-12, 1-1e-12) ** (1.0 / T[None, :])\n",
        "        return S / S.sum(axis=1, keepdims=True)\n",
        "    bounds = [(0.75, 1.5)] * C\n",
        "    resTc = minimize(lambda t: log_loss(y_bin, scale_classwise(P_oof_bin, t), labels=classes),\n",
        "                     x0=np.ones(C, dtype=np.float64), method='L-BFGS-B', bounds=bounds)\n",
        "    T_class = resTc.x\n",
        "    P_oof_scaled = scale_classwise(P_oof_bin, T_class)\n",
        "    P_test_scaled = scale_classwise(P_test_bin, T_class)\n",
        "    oof_final_bin = log_loss(y_bin, P_oof_scaled, labels=classes)\n",
        "    print(f'  [{name}] OOF pre-temp={oof_pre:.5f} final={oof_final_bin:.5f} T={np.round(T_class,4)}', flush=True)\n",
        "    final_oof[idx_tr] = P_oof_scaled\n",
        "    final_test[idx_te] = P_test_scaled\n",
        "\n",
        "# Per-bin NB caps per expert: short=0.66, mid=0.62, long=0.58; MNB tiny caps per-bin\n",
        "run_bin('short', mask_short, test_mask_short, nb_cap_local=0.66, mnb_cap_local=0.04)\n",
        "run_bin('mid',   mask_mid,   test_mask_mid,   nb_cap_local=0.62, mnb_cap_local=0.035)\n",
        "run_bin('long',  mask_long,  test_mask_long,  nb_cap_local=0.58, mnb_cap_local=0.03)\n",
        "\n",
        "# Overall OOF\n",
        "oof_loss = log_loss(y, final_oof, labels=classes)\n",
        "print('Run C 3-bin overall OOF:', round(oof_loss,5), 'elapsed=%.2fs' % (time.time()-t0), flush=True)\n",
        "\n",
        "# Confidence overlay with hero model (max<0.46 => mix 0.8/0.2)\n",
        "P_lop_oof = final_oof.copy(); P_lop_test = final_test.copy()\n",
        "P_hero_oof = load('oof_lr_char_1_8_hero.npy'); P_hero_test = load('test_lr_char_1_8_hero.npy')\n",
        "assert P_hero_oof is not None and P_hero_test is not None, 'Missing hero model preds for overlay'\n",
        "P_hero_oof = np.clip(P_hero_oof, 1e-12, 1-1e-12); P_hero_oof /= P_hero_oof.sum(axis=1, keepdims=True)\n",
        "P_hero_test = np.clip(P_hero_test, 1e-12, 1-1e-12); P_hero_test /= P_hero_test.sum(axis=1, keepdims=True)\n",
        "def apply_overlay(P_base, P_aux, mask_conf, alpha=0.8):\n",
        "    P = P_base.copy()\n",
        "    low_conf = mask_conf\n",
        "    P[low_conf] = (alpha * P_base[low_conf] + (1.0 - alpha) * P_aux[low_conf])\n",
        "    P = np.clip(P, 1e-12, 1-1e-12); P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "mask_low_oof = (P_lop_oof.max(axis=1) < 0.46)\n",
        "mask_low_test = (P_lop_test.max(axis=1) < 0.46)\n",
        "P_final_oof = apply_overlay(P_lop_oof, P_hero_oof, mask_low_oof, alpha=0.8)\n",
        "P_final_test = apply_overlay(P_lop_test, P_hero_test, mask_low_test, alpha=0.8)\n",
        "\n",
        "# Optional micro prior: boost MWS for very short (<=60 chars)\n",
        "short_mask_60_tr = (train_len <= 60)\n",
        "short_mask_60_te = (test_len <= 60)\n",
        "def boost_mws(P, mask):\n",
        "    P2 = P.copy()\n",
        "    j = classes.index('MWS')\n",
        "    if mask.any():\n",
        "        P2[mask, j] *= 1.02\n",
        "        P2[mask] /= P2[mask].sum(axis=1, keepdims=True)\n",
        "    return P2\n",
        "P_final_oof = boost_mws(P_final_oof, short_mask_60_tr)\n",
        "P_final_test = boost_mws(P_final_test, short_mask_60_te)\n",
        "\n",
        "oof_loss_final = log_loss(y, P_final_oof, labels=classes)\n",
        "print('Run C Post-overlay OOF:', round(oof_loss_final,5))\n",
        "\n",
        "# Save submission\n",
        "probs = P_final_test[:, [classes.index(c) for c in submit_cols]]\n",
        "probs = np.clip(probs, 1e-12, 1-1e-12); probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "assert sub.shape[0] == 1958\n",
        "assert np.all(np.isfinite(sub[submit_cols].values))\n",
        "assert np.allclose(sub[submit_cols].sum(axis=1).values, 1.0, atol=1e-6)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Run C: 3-bin swap + overlay). Elapsed=%.2fs' % (time.time()-t0), flush=True)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length-gated LOP (Run C) with models: ['nbsvm_wc_tweaked', 'nbsvm_char_2_6_counts', 'nbsvm_wc_fixed', 'nbsvm_char_2_7_presence', 'lr_wc_fixed', 'lr_word13_charwb36', 'lr_char_1_8_hero', 'svc_charwb_1_6_sig', 'lr_char_1_7', 'mnb_char_2_6']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-model T (global): {'nbsvm_wc_tweaked': 1.55, 'nbsvm_char_2_6_counts': 1.579, 'nbsvm_wc_fixed': 1.567, 'nbsvm_char_2_7_presence': 1.477, 'lr_wc_fixed': 0.877, 'lr_word13_charwb36': 1.006, 'lr_char_1_8_hero': 0.871, 'svc_charwb_1_6_sig': 0.803, 'lr_char_1_7': 0.88, 'mnb_char_2_6': 1.17}\nPer-model OOF (post scalar cal): {'nbsvm_wc_tweaked': 0.32156, 'nbsvm_char_2_6_counts': 0.33019, 'nbsvm_wc_fixed': 0.33633, 'nbsvm_char_2_7_presence': 0.338, 'lr_wc_fixed': 0.36343, 'lr_word13_charwb36': 0.36825, 'lr_char_1_8_hero': 0.36987, 'svc_charwb_1_6_sig': 0.39657, 'lr_char_1_7': 0.37154, 'mnb_char_2_6': 0.39492}\n3-bin sizes: {'short': 6232, 'mid': 6419, 'long': 4970}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 16/64 best_obj=0.44541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 32/64 best_obj=0.44541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 48/64 best_obj=0.44541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 64/64 best_obj=0.44541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] OOF pre-temp=0.45984 final=0.45836 T=[1.1331 0.9752 1.0227]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 16/64 best_obj=0.27953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 32/64 best_obj=0.27948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 48/64 best_obj=0.27948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 64/64 best_obj=0.27948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] OOF pre-temp=0.29433 final=0.28808 T=[0.75   0.9669 0.9296]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 16/64 best_obj=0.13378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 32/64 best_obj=0.13378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 48/64 best_obj=0.13378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 64/64 best_obj=0.13377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] OOF pre-temp=0.14797 final=0.14139 T=[0.7961 0.797  0.75  ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run C 3-bin overall OOF: 0.30693 elapsed=707.53s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run C Post-overlay OOF: 0.30707\nSaved submission.csv (Run C: 3-bin swap + overlay). Elapsed=707.55s\n"
          ]
        }
      ]
    },
    {
      "id": "0d3a5aa0-21f5-49de-9286-d88c6ca4d161",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# New bases: Char 5-gram LM-like generative probs + Stylometric LR features (OOF/Test saved)\n",
        "import numpy as np, pandas as pd, time, gc, re\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "X_text = train['text'].astype(str).values\n",
        "y = train['author'].values\n",
        "X_test_text = test['text'].astype(str).values\n",
        "classes = np.unique(y).tolist()\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "C = len(classes)\n",
        "SEED = 42; N_FOLDS = 5\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "# --- Base 1: Char 5-gram LM-like (bag-of-ngrams generative with Laplace smoothing) ---\n",
        "def char5_lm_oof(alpha: float = 0.5, ngram_range=(5,5), lowercase=False):\n",
        "    oof = np.zeros((len(train), C), dtype=np.float64)\n",
        "    tpred = np.zeros((len(test), C), dtype=np.float64)\n",
        "    for fold, (tr, va) in enumerate(skf.split(X_text, y), 1):\n",
        "        x_tr, x_va = X_text[tr], X_text[va]\n",
        "        y_tr, y_va = y[tr], y[va]\n",
        "        vec = CountVectorizer(analyzer='char', ngram_range=ngram_range, min_df=1, lowercase=lowercase, strip_accents=None, dtype=np.int32)\n",
        "        X_tr = vec.fit_transform(x_tr); X_va = vec.transform(x_va); X_te = vec.transform(X_test_text)\n",
        "        V = X_tr.shape[1]\n",
        "        # compute per-class smoothed log probs over ngrams\n",
        "        class_sum = np.zeros(C, dtype=np.float64)\n",
        "        class_counts = np.zeros((C, V), dtype=np.float64)\n",
        "        for ci, c in enumerate(classes):\n",
        "            mask = (y_tr == c)\n",
        "            if mask.any():\n",
        "                cnts = np.asarray(X_tr[mask].sum(axis=0)).ravel().astype(np.float64)\n",
        "            else:\n",
        "                cnts = np.zeros(V, dtype=np.float64)\n",
        "            class_counts[ci] = cnts + alpha\n",
        "            class_sum[ci] = class_counts[ci].sum()\n",
        "        logp = np.log(class_counts) - np.log(class_sum[:, None])  # shape (C,V)\n",
        "        # scores: doc_counts dot logp^T per class\n",
        "        def scores(Xm):\n",
        "            # result shape (n_samples, C)\n",
        "            S = np.zeros((Xm.shape[0], C), dtype=np.float64)\n",
        "            # compute per class efficiently via matrix mult in blocks to save memory\n",
        "            # S[:,ci] = Xm @ logp[ci].T\n",
        "            for ci in range(C):\n",
        "                S[:, ci] = Xm.dot(logp[ci].astype(np.float64))\n",
        "            # softmax across classes to get probs\n",
        "            S = S - S.max(axis=1, keepdims=True)\n",
        "            P = np.exp(S); P /= P.sum(axis=1, keepdims=True)\n",
        "            return P\n",
        "        P_va = scores(X_va); P_te = scores(X_te)\n",
        "        oof[va] = P_va\n",
        "        tpred += P_te / N_FOLDS\n",
        "        loss = log_loss(y_va, P_va, labels=classes)\n",
        "        print(f'[char5lm] Fold {fold} loss={loss:.5f} V={V}', flush=True)\n",
        "        del X_tr, X_va, X_te, class_counts, logp; gc.collect()\n",
        "    oof_loss = log_loss(y, oof, labels=classes)\n",
        "    print(f'[char5lm] OOF={oof_loss:.5f}', flush=True)\n",
        "    return oof.astype(np.float32), tpred.astype(np.float32), float(oof_loss)\n",
        "\n",
        "oof_char5lm, test_char5lm, loss_char5lm = char5_lm_oof(alpha=0.5, ngram_range=(5,5), lowercase=False)\n",
        "np.save('oof_char5lm.npy', oof_char5lm); np.save('test_char5lm.npy', test_char5lm)\n",
        "print('[char5lm] Saved oof_char5lm.npy and test_char5lm.npy')\n",
        "\n",
        "# --- Base 2: Stylometric features + Logistic Regression (multinomial) ---\n",
        "def compute_stylo(texts: np.ndarray) -> np.ndarray:\n",
        "    feats = []\n",
        "    for t in texts:\n",
        "        s = t\n",
        "        L = len(s)\n",
        "        wc = len(s.split()) if L>0 else 0\n",
        "        avg_wlen = (sum(len(w) for w in s.split())/wc) if wc>0 else 0.0\n",
        "        p_excl = s.count('!')/max(L,1)\n",
        "        p_q    = s.count('?')/max(L,1)\n",
        "        p_sem  = s.count(';')/max(L,1)\n",
        "        p_col  = s.count(':')/max(L,1)\n",
        "        p_dash = s.count('\u2014')/max(L,1) + s.count('-')/max(L,1)\n",
        "        p_ell  = s.count('\u2026')/max(L,1)\n",
        "        p_com  = s.count(',')/max(L,1)\n",
        "        p_dot  = s.count('.')/max(L,1)\n",
        "        caps = sum(1 for ch in s if ch.isupper())/max(L,1)\n",
        "        digits = sum(1 for ch in s if ch.isdigit())/max(L,1)\n",
        "        # type-token ratio (rough)\n",
        "        toks = re.findall(r\"[A-Za-z']+\", s)\n",
        "        ttr = (len(set(toks))/max(len(toks),1)) if toks else 0.0\n",
        "        feats.append([L, wc, avg_wlen, p_excl, p_q, p_sem, p_col, p_dash, p_ell, p_com, p_dot, caps, digits, ttr])\n",
        "    return np.asarray(feats, dtype=np.float32)\n",
        "\n",
        "X_tr_sty = compute_stylo(X_text)\n",
        "X_te_sty = compute_stylo(X_test_text)\n",
        "oof_sty = np.zeros((len(train), C), dtype=np.float32)\n",
        "test_sty = np.zeros((len(test), C), dtype=np.float32)\n",
        "for fold, (tr, va) in enumerate(skf.split(X_tr_sty, y), 1):\n",
        "    Xtr, Xva = X_tr_sty[tr], X_tr_sty[va]\n",
        "    ytr, yva = y[tr], y[va]\n",
        "    clf = LogisticRegression(solver='lbfgs', multi_class='multinomial', C=2.0, max_iter=2000, n_jobs=-1, random_state=SEED)\n",
        "    clf.fit(Xtr, ytr)\n",
        "    P_va = clf.predict_proba(Xva).astype(np.float32)\n",
        "    P_te = clf.predict_proba(X_te_sty).astype(np.float32)\n",
        "    # reorder\n",
        "    order = [list(clf.classes_).index(c) for c in classes]\n",
        "    P_va = P_va[:, order]; P_te = P_te[:, order]\n",
        "    # clip+renorm\n",
        "    P_va = np.clip(P_va, 1e-12, 1-1e-12); P_va /= P_va.sum(axis=1, keepdims=True)\n",
        "    P_te = np.clip(P_te, 1e-12, 1-1e-12); P_te /= P_te.sum(axis=1, keepdims=True)\n",
        "    oof_sty[va] = P_va\n",
        "    test_sty += P_te / N_FOLDS\n",
        "    loss = log_loss(yva, P_va, labels=classes)\n",
        "    print(f'[styloLR] Fold {fold} loss={loss:.5f}', flush=True)\n",
        "oof_sty_loss = log_loss(y, oof_sty, labels=classes)\n",
        "print(f'[styloLR] OOF={oof_sty_loss:.5f}', flush=True)\n",
        "np.save('oof_stylo_lr.npy', oof_sty); np.save('test_stylo_lr.npy', test_sty)\n",
        "print('[styloLR] Saved oof_stylo_lr.npy and test_stylo_lr.npy')\n",
        "\n",
        "print('Finished new bases. Elapsed=%.2fs' % (time.time()-t0), flush=True)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[char5lm] Fold 1 loss=1.53375 V=141747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[char5lm] Fold 2 loss=1.51921 V=142475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[char5lm] Fold 3 loss=1.58201 V=142659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[char5lm] Fold 4 loss=1.51984 V=141991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[styloLR] Fold 1 loss=1.06426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[styloLR] Fold 2 loss=1.06370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[styloLR] Fold 3 loss=1.07204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[styloLR] Fold 4 loss=1.06293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[styloLR] Fold 5 loss=1.06496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[styloLR] OOF=1.06558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[styloLR] Saved oof_stylo_lr.npy and test_stylo_lr.npy\nFinished new bases. Elapsed=23.59s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "id": "ac3132af-24cd-46f9-ae5f-bea7f67890ed",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4-bin length-gated LOP (Run D): add char5lm + stylo bases with ultra-tight caps; stabilize temps; overlay\n",
        "import numpy as np, pandas as pd, time, gc\n",
        "from pathlib import Path\n",
        "from scipy.optimize import minimize, minimize_scalar\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "y = train['author'].values\n",
        "classes = np.unique(y).tolist()\n",
        "submit_cols = ['EAP','HPL','MWS']\n",
        "assert set(classes) == set(submit_cols)\n",
        "C = len(classes)\n",
        "\n",
        "def load(p):\n",
        "    return np.load(p) if Path(p).exists() else None\n",
        "\n",
        "# Portfolio: Run B 10-core + new tiny-capped diversity bases (char5lm, stylo)\n",
        "cands = [\n",
        "    ('nbsvm_wc_tweaked',        'oof_nbsvm_wc_tweaked.npy',        'test_nbsvm_wc_tweaked.npy'),\n",
        "    ('nbsvm_char_2_6_counts',   'oof_nbsvm_char_2_6_counts.npy',   'test_nbsvm_char_2_6_counts.npy'),\n",
        "    ('nbsvm_wc_fixed',          'oof_nbsvm_wc_fixed.npy',          'test_nbsvm_wc_fixed.npy'),\n",
        "    ('nbsvm_char_2_7_presence', 'oof_nbsvm_char_2_7_presence.npy', 'test_nbsvm_char_2_7_presence.npy'),\n",
        "    ('lr_wc_fixed',             'oof_lr_wordchar_fixed.npy',       'test_lr_wordchar_fixed.npy'),\n",
        "    ('lr_word13_charwb36',      'oof_lr_word13_charwb36.npy',      'test_lr_word13_charwb36.npy'),\n",
        "    ('lr_char_1_8_hero',        'oof_lr_char_1_8_hero.npy',        'test_lr_char_1_8_hero.npy'),\n",
        "    ('svc_charwb_1_6_sig',      'oof_svc_charwb_1_6_sig.npy',      'test_svc_charwb_1_6_sig.npy'),\n",
        "    ('lr_wordpunct_1_3',        'oof_lr_wordpunct_1_3.npy',        'test_lr_wordpunct_1_3.npy'),\n",
        "    ('mnb_char_2_6',            'oof_mnb_char_2_6.npy',            'test_mnb_char_2_6.npy'),\n",
        "    ('char5lm',                 'oof_char5lm.npy',                 'test_char5lm.npy'),\n",
        "    ('stylo_lr',                'oof_stylo_lr.npy',                'test_stylo_lr.npy'),\n",
        "]\n",
        "loaded = []\n",
        "for name, oofp, tsp in cands:\n",
        "    o = load(oofp); t = load(tsp)\n",
        "    assert o is not None and t is not None, f'Missing preds for {name}'\n",
        "    o = np.clip(o, 1e-12, 1-1e-12); o = o / o.sum(axis=1, keepdims=True)\n",
        "    t = np.clip(t, 1e-12, 1-1e-12); t = t / t.sum(axis=1, keepdims=True)\n",
        "    loaded.append((name, o.astype(np.float64), t.astype(np.float64)))\n",
        "names = [n for n,_,_ in loaded]\n",
        "K = len(names)\n",
        "print('Length-gated LOP (4-bin + extras) with models:', names, flush=True)\n",
        "\n",
        "def scale_probs_scalar(P, T):\n",
        "    S = np.clip(P, 1e-12, 1-1e-12) ** (1.0/float(T))\n",
        "    return S / S.sum(axis=1, keepdims=True)\n",
        "\n",
        "OOFs_raw = [o for _,o,_ in loaded]\n",
        "TESTs_raw = [t for _,_,t in loaded]\n",
        "\n",
        "# Global per-model temps (shrink target)\n",
        "per_model_T_global = []; OOFs_global = []; TESTs_global = []\n",
        "for i in range(K):\n",
        "    Pi = OOFs_raw[i]\n",
        "    resTi = minimize_scalar(lambda T: log_loss(y, scale_probs_scalar(Pi, T), labels=classes),\n",
        "                            bounds=(0.5, 5.0), method='bounded')\n",
        "    Ti = float(resTi.x)\n",
        "    per_model_T_global.append(Ti)\n",
        "    OOFs_global.append(scale_probs_scalar(OOFs_raw[i], Ti))\n",
        "    TESTs_global.append(scale_probs_scalar(TESTs_raw[i], Ti))\n",
        "per_oof = {names[i]: log_loss(y, OOFs_global[i], labels=classes) for i in range(K)}\n",
        "print('Per-model T (global):', {names[i]: round(per_model_T_global[i],3) for i in range(K)})\n",
        "print('Per-model OOF (post scalar cal):', {k: round(v,5) for k,v in per_oof.items()})\n",
        "\n",
        "def geo_pool_log_classwise(stacks, W):\n",
        "    n = stacks[0].shape[0]\n",
        "    A = np.zeros((n, C), dtype=np.float64)\n",
        "    for k in range(K):\n",
        "        A += np.log(stacks[k]) * W[k][None, :]\n",
        "    A -= A.max(axis=1, keepdims=True)\n",
        "    P = np.exp(A); P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "def softmax_cols(Z):\n",
        "    W = np.zeros_like(Z)\n",
        "    for j in range(C):\n",
        "        z = Z[:, j]\n",
        "        z = z - z.max()\n",
        "        e = np.exp(z); s = e.sum()\n",
        "        W[:, j] = e / (s if s>0 else 1.0)\n",
        "    return W\n",
        "\n",
        "# Regularization and caps\n",
        "lambda_ent = 0.0025\n",
        "global_cap = 0.55\n",
        "weak_cap = 0.09\n",
        "tiny_prune_thresh = 0.00\n",
        "nb_mask = np.array([n.startswith('nbsvm_') for n in names], dtype=bool)\n",
        "name_to_idx = {n:i for i,n in enumerate(names)}\n",
        "\n",
        "def apply_caps_with_nbcap(W, nb_cap_local, explicit_caps_local):\n",
        "    Wc = W.copy()\n",
        "    Wc = np.minimum(Wc, global_cap)\n",
        "    # explicit model caps\n",
        "    for n, cap in explicit_caps_local.items():\n",
        "        if n in name_to_idx:\n",
        "            i = name_to_idx[n]\n",
        "            Wc[i, :] = np.minimum(Wc[i, :], cap)\n",
        "    # weak cap for very weak bases\n",
        "    for i, n in enumerate(names):\n",
        "        if per_oof.get(n, 1.0) > 0.40:\n",
        "            Wc[i, :] = np.minimum(Wc[i, :], weak_cap)\n",
        "    # NB-family total cap per class\n",
        "    for j in range(C):\n",
        "        s_nb = Wc[nb_mask, j].sum()\n",
        "        if s_nb > nb_cap_local and s_nb > 0:\n",
        "            Wc[nb_mask, j] *= (nb_cap_local / s_nb)\n",
        "    # Per-class renormalize\n",
        "    for j in range(C):\n",
        "        col = Wc[:, j]\n",
        "        if tiny_prune_thresh > 0:\n",
        "            col[col < tiny_prune_thresh] = 0.0\n",
        "        s = col.sum()\n",
        "        if s == 0:\n",
        "            col[:] = 1.0 / K\n",
        "        else:\n",
        "            col[:] = col / s\n",
        "        Wc[:, j] = col\n",
        "    return Wc\n",
        "\n",
        "def make_objective(OOFs_subset, nb_cap_local, explicit_caps_local, y_bin):\n",
        "    def objective(Z):\n",
        "        W0 = softmax_cols(Z)\n",
        "        Wc = apply_caps_with_nbcap(W0, nb_cap_local, explicit_caps_local)\n",
        "        P = geo_pool_log_classwise(OOFs_subset, Wc)\n",
        "        ent = 0.0\n",
        "        for j in range(C):\n",
        "            wj = np.clip(Wc[:, j], 1e-12, 1.0)\n",
        "            ent += float(np.sum(wj * np.log(wj)))\n",
        "        reg = lambda_ent * ent\n",
        "        return log_loss(y_bin, P, labels=classes) + reg\n",
        "    return objective\n",
        "\n",
        "# 4-bin thresholds: <=80, 81-130, 131-200, >200\n",
        "train_len = train['text'].astype(str).str.len().values\n",
        "test_len  = test['text'].astype(str).str.len().values\n",
        "b1, b2, b3 = 80, 130, 200\n",
        "mask_vshort = (train_len <= b1)\n",
        "mask_short  = (train_len > b1) & (train_len <= b2)\n",
        "mask_mid    = (train_len > b2) & (train_len <= b3)\n",
        "mask_long   = (train_len > b3)\n",
        "test_vshort = (test_len <= b1)\n",
        "test_short  = (test_len > b1) & (test_len <= b2)\n",
        "test_mid    = (test_len > b2) & (test_len <= b3)\n",
        "test_long   = (test_len > b3)\n",
        "print('4-bin sizes:', {'vshort': int(mask_vshort.sum()), 'short': int(mask_short.sum()), 'mid': int(mask_mid.sum()), 'long': int(mask_long.sum())}, flush=True)\n",
        "\n",
        "Z_global = None\n",
        "try:\n",
        "    Z_global = Z_star.copy()\n",
        "except NameError:\n",
        "    Z_global = None\n",
        "Z_warm = np.zeros((K, C), dtype=np.float64)\n",
        "if Z_global is not None and getattr(Z_global, 'shape', None) == (K, C):\n",
        "    Z_warm = Z_global\n",
        "\n",
        "final_oof = np.zeros((len(train), C), dtype=np.float64)\n",
        "final_test = np.zeros((len(test), C), dtype=np.float64)\n",
        "\n",
        "def run_bin(name, tr_mask, te_mask, nb_cap_local, mnb_cap_local, char5_cap_local, stylo_cap_local):\n",
        "    idx_tr = np.where(tr_mask)[0]\n",
        "    idx_te = np.where(te_mask)[0]\n",
        "    if len(idx_tr) == 0:\n",
        "        return\n",
        "    y_bin = y[idx_tr]\n",
        "    OOFs_bin = []; TESTs_bin = []\n",
        "    for i in range(K):\n",
        "        Pi_tr = OOFs_raw[i][idx_tr]\n",
        "        resTi = minimize_scalar(lambda T: log_loss(y_bin, scale_probs_scalar(Pi_tr, T), labels=classes),\n",
        "                                bounds=(0.75, 1.5), method='bounded')\n",
        "        Ti_bin = float(resTi.x)\n",
        "        Ti_shrunk = 0.7 * float(per_model_T_global[i]) + 0.3 * Ti_bin\n",
        "        OOFs_bin.append(scale_probs_scalar(OOFs_raw[i][idx_tr], Ti_shrunk))\n",
        "        TESTs_bin.append(scale_probs_scalar(TESTs_raw[i][idx_te], Ti_shrunk))\n",
        "    starts = 64\n",
        "    rng = np.random.RandomState(42)\n",
        "    inits = [Z_warm.copy()] + [rng.normal(0, 0.5, size=(K, C)) for _ in range(starts-1)]\n",
        "    best = (1e9, None)\n",
        "    explicit_caps_local = {\n",
        "        'svc_charwb_1_6_sig': 0.06,\n",
        "        'lr_wordpunct_1_3': 0.05,\n",
        "        'mnb_char_2_6': mnb_cap_local,\n",
        "        'char5lm': char5_cap_local,\n",
        "        'stylo_lr': stylo_cap_local,\n",
        "    }\n",
        "    obj = make_objective(OOFs_bin, nb_cap_local, explicit_caps_local, y_bin)\n",
        "    for si, Z0 in enumerate(inits, 1):\n",
        "        res = minimize(lambda z: obj(z.reshape(K, C)), Z0.ravel(), method='L-BFGS-B')\n",
        "        val = float(res.fun)\n",
        "        if val < best[0]:\n",
        "            best = (val, res.x.reshape(K, C).copy())\n",
        "        if si % 16 == 0:\n",
        "            print(f'  [{name}] start {si}/{starts} best_obj={best[0]:.5f}', flush=True)\n",
        "    Z_bin = best[1]\n",
        "    W0 = softmax_cols(Z_bin)\n",
        "    Wc = apply_caps_with_nbcap(W0, nb_cap_local, explicit_caps_local)\n",
        "    P_oof_bin = geo_pool_log_classwise(OOFs_bin, Wc)\n",
        "    P_test_bin = geo_pool_log_classwise(TESTs_bin, Wc)\n",
        "    oof_pre = log_loss(y_bin, P_oof_bin, labels=classes)\n",
        "    def scale_classwise(P, Tvec):\n",
        "        T = np.asarray(Tvec, dtype=np.float64)\n",
        "        S = np.clip(P, 1e-12, 1-1e-12) ** (1.0 / T[None, :])\n",
        "        return S / S.sum(axis=1, keepdims=True)\n",
        "    bounds = [(0.75, 1.5)] * C\n",
        "    resTc = minimize(lambda t: log_loss(y_bin, scale_classwise(P_oof_bin, t), labels=classes),\n",
        "                     x0=np.ones(C, dtype=np.float64), method='L-BFGS-B', bounds=bounds)\n",
        "    T_class = resTc.x\n",
        "    P_oof_scaled = scale_classwise(P_oof_bin, T_class)\n",
        "    P_test_scaled = scale_classwise(P_test_bin, T_class)\n",
        "    oof_final_bin = log_loss(y_bin, P_oof_scaled, labels=classes)\n",
        "    print(f'  [{name}] OOF pre-temp={oof_pre:.5f} final={oof_final_bin:.5f} T={np.round(T_class,4)}', flush=True)\n",
        "    final_oof[idx_tr] = P_oof_scaled\n",
        "    final_test[idx_te] = P_test_scaled\n",
        "\n",
        "# Per-bin caps: NB-family [0.68,0.65,0.62,0.58]; MNB tiny [0.042,0.038,0.032,0.028];\n",
        "# char5lm super tiny [0.02,0.018,0.015,0.012]; stylo super tiny [0.02,0.018,0.015,0.012]\n",
        "run_bin('vshort', mask_vshort, test_vshort, nb_cap_local=0.68, mnb_cap_local=0.042, char5_cap_local=0.02,  stylo_cap_local=0.02)\n",
        "run_bin('short',  mask_short,  test_short,  nb_cap_local=0.65, mnb_cap_local=0.038, char5_cap_local=0.018, stylo_cap_local=0.018)\n",
        "run_bin('mid',    mask_mid,    test_mid,    nb_cap_local=0.62, mnb_cap_local=0.032, char5_cap_local=0.015, stylo_cap_local=0.015)\n",
        "run_bin('long',   mask_long,   test_long,   nb_cap_local=0.58, mnb_cap_local=0.028, char5_cap_local=0.012, stylo_cap_local=0.012)\n",
        "\n",
        "oof_loss = log_loss(y, final_oof, labels=classes)\n",
        "print('Run D 4-bin overall OOF:', round(oof_loss,5), 'elapsed=%.2fs' % (time.time()-t0), flush=True)\n",
        "\n",
        "# Confidence overlay with hero model on low-confidence rows (max<0.46):\n",
        "def apply_overlay(P_base, P_aux, mask_conf, alpha=0.8):\n",
        "    P = P_base.copy()\n",
        "    low_conf = mask_conf\n",
        "    P[low_conf] = (alpha * P_base[low_conf] + (1.0 - alpha) * P_aux[low_conf])\n",
        "    P = np.clip(P, 1e-12, 1-1e-12); P /= P.sum(axis=1, keepdims=True)\n",
        "    return P\n",
        "\n",
        "P_lop_oof = final_oof.copy(); P_lop_test = final_test.copy()\n",
        "P_hero_oof = load('oof_lr_char_1_8_hero.npy'); P_hero_test = load('test_lr_char_1_8_hero.npy')\n",
        "assert P_hero_oof is not None and P_hero_test is not None, 'Missing hero model preds for overlay'\n",
        "P_hero_oof = np.clip(P_hero_oof, 1e-12, 1-1e-12); P_hero_oof /= P_hero_oof.sum(axis=1, keepdims=True)\n",
        "P_hero_test = np.clip(P_hero_test, 1e-12, 1-1e-12); P_hero_test /= P_hero_test.sum(axis=1, keepdims=True)\n",
        "mask_low_oof = (P_lop_oof.max(axis=1) < 0.46)\n",
        "mask_low_test = (P_lop_test.max(axis=1) < 0.46)\n",
        "P_final_oof = apply_overlay(P_lop_oof, P_hero_oof, mask_low_oof, alpha=0.8)\n",
        "P_final_test = apply_overlay(P_lop_test, P_hero_test, mask_low_test, alpha=0.8)\n",
        "\n",
        "# Optional micro prior on very short (<=60) MWS x1.02\n",
        "short_mask_60_tr = (train_len <= 60)\n",
        "short_mask_60_te = (test_len <= 60)\n",
        "def boost_mws(P, mask):\n",
        "    P2 = P.copy()\n",
        "    j = classes.index('MWS')\n",
        "    if mask.any():\n",
        "        P2[mask, j] *= 1.02\n",
        "        P2[mask] /= P2[mask].sum(axis=1, keepdims=True)\n",
        "    return P2\n",
        "P_final_oof = boost_mws(P_final_oof, short_mask_60_tr)\n",
        "P_final_test = boost_mws(P_final_test, short_mask_60_te)\n",
        "\n",
        "oof_loss_final = log_loss(y, P_final_oof, labels=classes)\n",
        "print('Run D 4-bin Post-overlay OOF:', round(oof_loss_final,5))\n",
        "\n",
        "probs = P_final_test[:, [classes.index(c) for c in submit_cols]]\n",
        "probs = np.clip(probs, 1e-12, 1-1e-12); probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "sub = pd.DataFrame(probs, columns=submit_cols)\n",
        "sub.insert(0, 'id', test['id'].values)\n",
        "assert sub.shape[0] == 1958\n",
        "assert np.all(np.isfinite(sub[submit_cols].values))\n",
        "assert np.allclose(sub[submit_cols].sum(axis=1).values, 1.0, atol=1e-6)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv (Run D: 4-bin + char5lm + stylo + overlay). Elapsed=%.2fs' % (time.time()-t0), flush=True)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length-gated LOP (4-bin + extras) with models: ['nbsvm_wc_tweaked', 'nbsvm_char_2_6_counts', 'nbsvm_wc_fixed', 'nbsvm_char_2_7_presence', 'lr_wc_fixed', 'lr_word13_charwb36', 'lr_char_1_8_hero', 'svc_charwb_1_6_sig', 'lr_wordpunct_1_3', 'mnb_char_2_6', 'char5lm', 'stylo_lr']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-model T (global): {'nbsvm_wc_tweaked': 1.55, 'nbsvm_char_2_6_counts': 1.579, 'nbsvm_wc_fixed': 1.567, 'nbsvm_char_2_7_presence': 1.477, 'lr_wc_fixed': 0.877, 'lr_word13_charwb36': 1.006, 'lr_char_1_8_hero': 0.871, 'svc_charwb_1_6_sig': 0.803, 'lr_wordpunct_1_3': 0.771, 'mnb_char_2_6': 1.17, 'char5lm': 5.0, 'stylo_lr': 0.784}\nPer-model OOF (post scalar cal): {'nbsvm_wc_tweaked': 0.32156, 'nbsvm_char_2_6_counts': 0.33019, 'nbsvm_wc_fixed': 0.33633, 'nbsvm_char_2_7_presence': 0.338, 'lr_wc_fixed': 0.36343, 'lr_word13_charwb36': 0.36825, 'lr_char_1_8_hero': 0.36987, 'svc_charwb_1_6_sig': 0.39657, 'lr_wordpunct_1_3': 0.40368, 'mnb_char_2_6': 0.39492, 'char5lm': 0.42481, 'stylo_lr': 1.06406}\n4-bin sizes: {'vshort': 4330, 'short': 4674, 'mid': 4698, 'long': 3919}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [vshort] start 16/64 best_obj=0.46941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [vshort] start 32/64 best_obj=0.46941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [vshort] start 48/64 best_obj=0.46941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [vshort] start 64/64 best_obj=0.46937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [vshort] OOF pre-temp=0.48591 final=0.48287 T=[1.1475 0.9265 0.9561]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 16/64 best_obj=0.33630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 32/64 best_obj=0.33613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 48/64 best_obj=0.33613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] start 64/64 best_obj=0.33613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [short] OOF pre-temp=0.35159 final=0.35091 T=[0.9413 0.945  0.9399]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 16/64 best_obj=0.22400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 32/64 best_obj=0.22245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 48/64 best_obj=0.22245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] start 64/64 best_obj=0.22245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [mid] OOF pre-temp=0.23822 final=0.22958 T=[0.75   0.9113 0.8251]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 16/64 best_obj=0.12054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 32/64 best_obj=0.12039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 48/64 best_obj=0.12039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] start 64/64 best_obj=0.12039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [long] OOF pre-temp=0.13598 final=0.12792 T=[0.8107 0.75   0.75  ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run D 4-bin overall OOF: 0.30139 elapsed=831.02s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run D 4-bin Post-overlay OOF: 0.3015\nSaved submission.csv (Run D: 4-bin + char5lm + stylo + overlay). Elapsed=831.04s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}