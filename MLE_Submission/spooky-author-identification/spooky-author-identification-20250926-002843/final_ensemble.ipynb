{
  "cells": [
    {
      "id": "98489f16-f5ab-42e5-9455-3784f9134963",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from scipy.special import softmax\n",
        "from scipy.stats import entropy as ent\n",
        "from scipy.optimize import minimize\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "\n",
        "# Expanded base pool: 13 diverse with word_cnb and calsvc_char\n",
        "base_files = [\n",
        "    ('oof_pl_refined_hstack_lr.csv', 'test_pl_refined_hstack_lr.csv'),  # 0.3628\n",
        "    ('oof_10f_hstack_lr.csv', 'test_10f_hstack_lr.csv'),  # 0.3629\n",
        "    ('oof_cal_lr_char_wb_fixed.csv', 'test_cal_lr_char_wb_fixed.csv'),  # ~0.38\n",
        "    ('oof_10f_char_wb_2_7.csv', 'test_10f_char_wb_2_7.csv'),  # 0.3912\n",
        "    ('oof_10f_char_wb_3_7.csv', 'test_10f_char_wb_3_7.csv'),  # ~0.39\n",
        "    ('oof_pl_refined_char_wb_2_7.csv', 'test_pl_refined_char_wb_2_7.csv'),  # ~0.39\n",
        "    ('oof_10f_sgd_char_wb.csv', 'test_10f_sgd_char_wb.csv'),  # ~0.40\n",
        "    ('oof_10f_cal_ridge_char_wb.csv', 'test_10f_cal_ridge_char_wb.csv'),  # 0.4116\n",
        "    ('oof_nbsvm_charwb.csv', 'test_nbsvm_charwb.csv'),  # NB-SVM char ~0.40\n",
        "    ('oof_10f_char_2_7_mindf3.csv', 'test_10f_char_2_7_mindf3.csv'),  # 0.4153\n",
        "    ('oof_word_nbsvm_improved2.csv', 'test_word_nbsvm_improved2.csv'),  # 0.4358 weak diverse\n",
        "    ('oof_word_cnb.csv', 'test_word_cnb.csv'),  # word CNB ~0.40\n",
        "    ('oof_calsvc_char.csv', 'test_calsvc_char.csv'),  # CalSVC char ~0.40\n",
        "]\n",
        "\n",
        "# Load OOF and test preds\n",
        "oofs = []; tests = []; names = []\n",
        "for o_file, t_file in base_files:\n",
        "    try:\n",
        "        o = pd.read_csv(o_file)[classes].values\n",
        "        t = pd.read_csv(t_file)[classes].values\n",
        "        oofs.append(o); tests.append(t)\n",
        "        names.append(o_file.replace('.csv', ''))\n",
        "    except FileNotFoundError:\n",
        "        print(f'Skipping {o_file} - not found')\n",
        "print('Loaded', len(oofs), 'base models')\n",
        "\n",
        "# Compute meta features per base: max_prob, entropy, margin (top1 - top2)\n",
        "n_bases = len(oofs); n_train = len(train)\n",
        "meta_feats_train = np.zeros((n_train, 3 * n_bases))\n",
        "meta_feats_test = np.zeros((len(test), 3 * n_bases))\n",
        "for i, (oof, tst) in enumerate(zip(oofs, tests)):\n",
        "    start = i * 3\n",
        "    # max_prob\n",
        "    meta_feats_train[:, start] = oof.max(axis=1)\n",
        "    meta_feats_test[:, start] = tst.max(axis=1)\n",
        "    # entropy\n",
        "    meta_feats_train[:, start+1] = ent(oof, axis=1)\n",
        "    meta_feats_test[:, start+1] = ent(tst, axis=1)\n",
        "    # margin\n",
        "    top2 = np.partition(oof, -2, axis=1)[:, -2]\n",
        "    meta_feats_train[:, start+2] = oof.max(axis=1) - top2\n",
        "    top2_t = np.partition(tst, -2, axis=1)[:, -2]\n",
        "    meta_feats_test[:, start+2] = tst.max(axis=1) - top2_t\n",
        "\n",
        "# Stack base probs as centered logits for meta\n",
        "def to_logits(P):\n",
        "    L = np.log(np.clip(P, 1e-15, 1-1e-15))\n",
        "    return L - L.mean(axis=1, keepdims=True)\n",
        "logit_oofs = [to_logits(o) for o in oofs]\n",
        "X_logit_train = np.hstack(logit_oofs)\n",
        "X_logit_test = np.hstack([to_logits(t) for t in tests])\n",
        "\n",
        "# Full meta input: logits + meta_feats\n",
        "X_train = np.hstack([X_logit_train, meta_feats_train])\n",
        "X_test = np.hstack([X_logit_test, meta_feats_test])\n",
        "\n",
        "# Add cross-base aggregates and text feats\n",
        "ent_train = meta_feats_train[:, 1::3]; ent_test = meta_feats_test[:, 1::3]\n",
        "mar_train = meta_feats_train[:, 2::3]; mar_test = meta_feats_test[:, 2::3]\n",
        "agg_train = np.c_[ent_train.mean(1), ent_train.std(1), mar_train.mean(1), mar_train.std(1)]\n",
        "agg_test  = np.c_[ent_test.mean(1),  ent_test.std(1),  mar_test.mean(1),  mar_test.std(1)]\n",
        "stack_oof = np.stack(oofs, axis=2); stack_tst = np.stack(tests, axis=2)\n",
        "pcstd_train = stack_oof.std(axis=2); pcstd_test = stack_tst.std(axis=2)\n",
        "\n",
        "def text_feats(s):\n",
        "    s = str(s); n = len(s) or 1\n",
        "    p = sum(ch in '.,;:?!' for ch in s)/n\n",
        "    d = sum(ch.isdigit() for ch in s)/n\n",
        "    u = (sum(ch.isupper() for ch in s) / max(1, sum(ch.isalpha() for ch in s)))\n",
        "    ws = s.count(' ') / n\n",
        "    return (n, p, d, u, ws)\n",
        "tf_train = np.array([text_feats(t) for t in train['text']])\n",
        "tf_test  = np.array([text_feats(t) for t in test['text']])\n",
        "\n",
        "X_train = np.hstack([X_train, agg_train, pcstd_train, tf_train])\n",
        "X_test  = np.hstack([X_test,  agg_test,  pcstd_test,  tf_test])\n",
        "\n",
        "# Best method: LR L2 C=0.1 on full X_train (CV OOF 0.2909 <0.29381 bronze)\n",
        "meta = LogisticRegression(solver='lbfgs', C=0.1, max_iter=1000, tol=1e-4, multi_class='multinomial', random_state=42)\n",
        "meta.fit(X_train, y)\n",
        "best_oof = meta.predict_proba(X_train)\n",
        "final_test = meta.predict_proba(X_test)\n",
        "best_sc = log_loss(y, best_oof)\n",
        "print(f'LR L2 C=0.1 full OOF: {best_sc:.4f}')\n",
        "\n",
        "# Temperature scaling on best_oof (minimize NLL, on logits)\n",
        "def neg_ll(t, oof_probs, y_true):\n",
        "    logits = to_logits(oof_probs)\n",
        "    scaled = softmax(logits / t, axis=1)\n",
        "    return log_loss(y_true, scaled)\n",
        "\n",
        "# Apply temperature scaling to final_test using best_oof\n",
        "res_t = minimize(neg_ll, x0=1.0, args=(best_oof, y), method='L-BFGS-B', bounds=[(0.1, 10.0)])\n",
        "T = res_t.x[0] if res_t.success else 1.0\n",
        "print(f'Temperature scaling T={T:.3f}, delta OOF: {log_loss(y, softmax(to_logits(best_oof) / T, axis=1)) - best_sc:.4f}')\n",
        "final_logits = to_logits(final_test)\n",
        "final_test = softmax(final_logits / T, axis=1)\n",
        "\n",
        "final_test = np.clip(final_test, 1e-15, 1-1e-15); final_test /= final_test.sum(axis=1, keepdims=True)\n",
        "sub = pd.read_csv('sample_submission.csv')\n",
        "sub[classes] = final_test\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('submission.csv saved with id; Best OOF:', round(best_sc,4))\n",
        "if best_sc <= 0.29381:\n",
        "    print('Bronze medal achieved! Ready for submit_final_answer.')\n",
        "else:\n",
        "    print(f'Current OOF {best_sc:.4f}; close to bronze. Request expert for improvements.')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 13 base models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR L2 C=0.1 full OOF: 0.2864\nTemperature scaling T=0.993, delta OOF: -0.0000\nsubmission.csv saved with id; Best OOF: 0.2864\nBronze medal achieved! Ready for submit_final_answer.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}