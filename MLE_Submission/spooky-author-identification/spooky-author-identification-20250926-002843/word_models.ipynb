{
  "cells": [
    {
      "id": "cc8a526a-dc3d-459a-ad86-766cda1dfbcf",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from scipy import sparse\n",
        "from unicodedata import normalize as uni_norm\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "def simple_clean(s):\n",
        "    if pd.isna(s): return \"\"\n",
        "    s = str(s)\n",
        "    s = s.replace(\"\u2019\",\"'\") .replace(\"\u2018\",\"'\") .replace(\"\u201c\",\"\\\"\") .replace(\"\u201d\",\"\\\"\") .replace(\"\u2014\",\"-\") .replace(\"\u2013\",\"-\")\n",
        "    s = uni_norm(\"NFKC\", s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "train['text'] = train['text'].map(simple_clean)\n",
        "test['text']  = test['text'].map(simple_clean)\n",
        "\n",
        "print('Data loaded and cleaned. Train shape:', train.shape, 'Classes:', classes)\n",
        "\n",
        "# 1. Word TF-IDF + LR (strong baseline)\n",
        "def run_word_tfidf_lr(ngram=(1,2), min_df=2, C=2.0):\n",
        "    vec = TfidfVectorizer(\n",
        "        analyzer='word',\n",
        "        ngram_range=ngram,\n",
        "        lowercase=True,\n",
        "        sublinear_tf=True,\n",
        "        min_df=min_df,\n",
        "        max_df=1.0,\n",
        "        strip_accents='unicode',\n",
        "        stop_words=None,\n",
        "        token_pattern=r'(?u)\\b\\w+\\b',   # keep 1-char tokens\n",
        "        preprocessor=simple_clean,\n",
        "        dtype=np.float32\n",
        "    )\n",
        "    oof = np.zeros((len(train), 3), dtype=np.float32)\n",
        "    test_preds = np.zeros((len(test), 3), dtype=np.float32)\n",
        "    scores = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(train, y), 1):\n",
        "        X_tr = vec.fit_transform(train['text'].iloc[tr_idx])\n",
        "        X_va = vec.transform(train['text'].iloc[va_idx])\n",
        "        X_te = vec.transform(test['text'])\n",
        "        clf = LogisticRegression(\n",
        "            solver='saga', C=C, penalty='l2',\n",
        "            max_iter=5000, tol=1e-4, multi_class='multinomial',\n",
        "            random_state=42+fold\n",
        "        )\n",
        "        clf.fit(X_tr, y[tr_idx])\n",
        "        oof[va_idx] = clf.predict_proba(X_va)\n",
        "        test_preds += clf.predict_proba(X_te)\n",
        "        sc = log_loss(y[va_idx], oof[va_idx]); scores.append(sc)\n",
        "        print(f'Word TF-IDF LR Fold {fold}: {sc:.4f}')\n",
        "    test_preds /= skf.n_splits\n",
        "    oof_score = float(np.mean(scores))\n",
        "    print(f'Word TF-IDF LR OOF: {oof_score:.4f}')\n",
        "    return oof_score, oof, test_preds\n",
        "\n",
        "for C in [1.0, 2.0, 4.0]:\n",
        "    sc, oof_lr, te_lr = run_word_tfidf_lr(ngram=(1,2), min_df=2, C=C)\n",
        "    if sc < 0.40:\n",
        "        pd.DataFrame(oof_lr, columns=classes).to_csv(f'oof_word_tfidf_lr_C{C}.csv', index=False)\n",
        "        pd.DataFrame(te_lr, columns=classes).to_csv(f'test_word_tfidf_lr_C{C}.csv', index=False)\n",
        "\n",
        "# 2. Word NB-SVM (count binary, class-normalized log-ratio, try SVC+calibration; try with/without L2 row-norm)\n",
        "def run_word_nbsvm(ngram=(1,2), min_df=2, alpha=0.5, C=2.0, use_svc=True, l2norm=True):\n",
        "    vec = CountVectorizer(\n",
        "        analyzer='word',\n",
        "        ngram_range=ngram,\n",
        "        lowercase=True,\n",
        "        min_df=min_df,\n",
        "        max_df=1.0,\n",
        "        binary=True,\n",
        "        token_pattern=r'(?u)\\b\\w+\\b',\n",
        "        strip_accents='unicode',\n",
        "        preprocessor=simple_clean,\n",
        "        dtype=np.float32\n",
        "    )\n",
        "    oof = np.zeros((len(train), 3), dtype=np.float32)\n",
        "    test_preds = np.zeros((len(test), 3), dtype=np.float32)\n",
        "    scores = []\n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(train, y), 1):\n",
        "        X_tr = vec.fit_transform(train['text'].iloc[tr_idx])\n",
        "        X_va = vec.transform(train['text'].iloc[va_idx])\n",
        "        X_te = vec.transform(test['text'])\n",
        "\n",
        "        R_cols = []\n",
        "        for c in range(3):\n",
        "            y_bin = (y[tr_idx] == c).astype(int)\n",
        "            pos = np.array(X_tr[y_bin==1].sum(axis=0)).ravel() + alpha\n",
        "            neg = np.array(X_tr[y_bin==0].sum(axis=0)).ravel() + alpha\n",
        "            pos /= pos.sum()\n",
        "            neg /= neg.sum()\n",
        "            r = np.log(pos / neg)\n",
        "            R_cols.append(r.astype(np.float32))\n",
        "        R = np.vstack(R_cols).T  # (vocab, classes)\n",
        "\n",
        "        def apply_r(X):\n",
        "            blocks = [X.multiply(R[:, c]) for c in range(3)]\n",
        "            Xr = sparse.hstack(blocks).tocsr()\n",
        "            if l2norm:\n",
        "                norms = np.sqrt(Xr.power(2).sum(axis=1).A.ravel() + 1e-8)\n",
        "                Xr = Xr.multiply(1.0 / norms[:, None])\n",
        "            return Xr\n",
        "\n",
        "        X_tr_nb = apply_r(X_tr)\n",
        "        X_va_nb = apply_r(X_va)\n",
        "        X_te_nb = apply_r(X_te)\n",
        "\n",
        "        if use_svc:\n",
        "            base = LinearSVC(C=C, max_iter=5000, random_state=42+fold)\n",
        "            clf = CalibratedClassifierCV(base, method='sigmoid', cv=3)\n",
        "        else:\n",
        "            clf = LogisticRegression(solver='saga', C=C, max_iter=5000, tol=1e-4,\n",
        "                                     multi_class='ovr', random_state=42+fold)\n",
        "        clf.fit(X_tr_nb, y[tr_idx])\n",
        "        oof[va_idx] = clf.predict_proba(X_va_nb)\n",
        "        test_preds += clf.predict_proba(X_te_nb)\n",
        "        sc = log_loss(y[va_idx], oof[va_idx]); scores.append(sc)\n",
        "        print(f'Word NB-SVM Fold {fold}: {sc:.4f}')\n",
        "    test_preds /= skf.n_splits\n",
        "    oof_score = float(np.mean(scores))\n",
        "    print(f'Word NB-SVM OOF: {oof_score:.4f}')\n",
        "    return oof_score, oof, test_preds\n",
        "\n",
        "# Small sweep: alpha and normalization/backend\n",
        "configs = [\n",
        "    dict(alpha=0.5, C=2.0, use_svc=True,  l2norm=True),\n",
        "    dict(alpha=0.75,C=2.0, use_svc=True,  l2norm=True),\n",
        "    dict(alpha=0.5, C=2.0, use_svc=True,  l2norm=False),  # A/B the norm\n",
        "    dict(alpha=0.75,C=2.0, use_svc=False, l2norm=False),  # LR backend, no norm\n",
        "]\n",
        "for cfg in configs:\n",
        "    sc, oof_nb, te_nb = run_word_nbsvm(ngram=(1,2), min_df=2, **cfg)\n",
        "    tag = f\"a{cfg['alpha']}_C{cfg['C']}_svc{cfg['use_svc']}_l2{cfg['l2norm']}\"\n",
        "    if sc < 0.41:\n",
        "        pd.DataFrame(oof_nb, columns=classes).to_csv(f'oof_word_nbsvm_{tag}.csv', index=False)\n",
        "        pd.DataFrame(te_nb, columns=classes).to_csv(f'test_word_nbsvm_{tag}.csv', index=False)\n",
        "\n",
        "print('Word models sweep complete. Check for OOF <0.40 files to integrate into final_ensemble.ipynb. If none, drop words and focus on char/ensemble improvements for bronze.')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded and cleaned. Train shape: (17621, 3) Classes: ['EAP', 'HPL', 'MWS']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 1: 0.5604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 2: 0.5474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 3: 0.5521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 4: 0.5681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 5: 0.5656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 6: 0.5678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 7: 0.5560\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 8: 0.5450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 9: 0.5530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 10: 0.5469\nWord TF-IDF LR OOF: 0.5562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 1: 0.4974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 2: 0.4831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 3: 0.4889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 4: 0.5075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 5: 0.5047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 6: 0.5059\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m oof_score, oof, test_preds\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m C \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m1.0\u001b[39m, \u001b[32m2.0\u001b[39m, \u001b[32m4.0\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     sc, oof_lr, te_lr = \u001b[43mrun_word_tfidf_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngram\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_df\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m=\u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sc < \u001b[32m0.40\u001b[39m:\n\u001b[32m     77\u001b[39m         pd.DataFrame(oof_lr, columns=classes).to_csv(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33moof_word_tfidf_lr_C\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mC\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mrun_word_tfidf_lr\u001b[39m\u001b[34m(ngram, min_df, C)\u001b[39m\n\u001b[32m     54\u001b[39m scores = []\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fold, (tr_idx, va_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(skf.split(train, y), \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     X_tr = \u001b[43mvec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     X_va = vec.transform(train[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].iloc[va_idx])\n\u001b[32m     58\u001b[39m     X_te = vec.transform(test[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/feature_extraction/text.py:2126\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2119\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2120\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2121\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2122\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2123\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2124\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2125\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2126\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2127\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2128\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2129\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/base.py:1151\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1144\u001b[39m     estimator._validate_params()\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1147\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1148\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1149\u001b[39m     )\n\u001b[32m   1150\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}