{
  "cells": [
    {
      "id": "c9d04e59-9bca-4f59-9edb-438e5e3346e1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n",
        "import sys\n",
        "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "print(result.stdout if result.returncode == 0 else 'GPU not available')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "db772e7c-05b9-4a1d-90f9-e641b40baa90",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def pip_install(*args):\n",
        "    print('>', *args, flush=True)\n",
        "    subprocess.run([sys.executable, '-m', 'pip', *args], check=True)\n",
        "\n",
        "# Hard reset any prior torch stacks\n",
        "for pkg in ('torch', 'torchvision', 'torchaudio'):\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], check=False)\n",
        "\n",
        "# Clean stray site dirs\n",
        "stray_dirs = [\n",
        "    '/app/.pip-target/torch',\n",
        "    '/app/.pip-target/torch-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torch-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchvision',\n",
        "    '/app/.pip-target/torchvision-0.23.0.dist-info',\n",
        "    '/app/.pip-target/torchvision-0.19.1.dist-info',\n",
        "    '/app/.pip-target/torchaudio',\n",
        "    '/app/.pip-target/torchaudio-2.8.0.dist-info',\n",
        "    '/app/.pip-target/torchaudio-2.4.1.dist-info',\n",
        "    '/app/.pip-target/torchgen',\n",
        "    '/app/.pip-target/functorch'\n",
        "]\n",
        "for d in stray_dirs:\n",
        "    if os.path.exists(d):\n",
        "        print('Removing', d)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "# Install the EXACT cu121 torch stack\n",
        "pip_install('install',\n",
        "    '--index-url', 'https://download.pytorch.org/whl/cu121',\n",
        "    '--extra-index-url', 'https://pypi.org/simple',\n",
        "    'torch==2.4.1', 'torchvision==0.19.1', 'torchaudio==2.4.1')\n",
        "\n",
        "# Create constraints file\n",
        "Path('constraints.txt').write_text(\n",
        "    'torch==2.4.1\\n'\n",
        "    'torchvision==0.19.1\\n'\n",
        "    'torchaudio==2.4.1\\n')\n",
        "\n",
        "# Install other deps\n",
        "pip_install('install', '-c', 'constraints.txt',\n",
        "    'transformers==4.44.2', 'accelerate==0.34.2',\n",
        "    'datasets==2.21.0', 'evaluate==0.4.2',\n",
        "    'sentencepiece', 'scikit-learn',\n",
        "    '--upgrade-strategy', 'only-if-needed')\n",
        "\n",
        "# Sanity check\n",
        "import torch\n",
        "print('torch:', torch.__version__, 'built CUDA:', getattr(torch.version, 'cuda', None))\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('ERROR: CUDA not available')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "9c85c311-41cd-46d4-92ba-f5133bc418a6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('train.csv')\n",
        "print('Train shape:', train.shape)\n",
        "print(train.head())\n",
        "\n",
        "# Author distribution\n",
        "print('\\nAuthor distribution:')\n",
        "print(train['author'].value_counts(normalize=True))\n",
        "\n",
        "# Text length stats\n",
        "train['text_length'] = train['text'].str.len()\n",
        "print('\\nText length stats:')\n",
        "print(train['text_length'].describe())\n",
        "\n",
        "# Word count stats\n",
        "train['word_count'] = train['text'].str.split().str.len()\n",
        "print('\\nWord count stats:')\n",
        "print(train['word_count'].describe())\n",
        "\n",
        "# Punctuation rate by author\n",
        "def punct_rate(text):\n",
        "    if len(text) == 0:\n",
        "        return 0\n",
        "    return sum(1 for c in text if c in string.punctuation) / len(text)\n",
        "\n",
        "train['punct_rate'] = train['text'].apply(punct_rate)\n",
        "print('\\nPunctuation rate by author:')\n",
        "print(train.groupby('author')['punct_rate'].agg(['mean', 'std']).round(4))\n",
        "\n",
        "# Sample texts per author\n",
        "print('\\nSample texts:')\n",
        "for author in train['author'].unique():\n",
        "    sample = train[train['author'] == author]['text'].iloc[0]\n",
        "    print(f'{author}: {sample[:200]}...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "242bc79c-ea1c-4526-9000-e16cb78f5e88",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# Fresh scope for Char NB baseline - using CountVectorizer with binary=False and norm=True\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "print('Label map:', dict(zip(le.classes_, le.transform(le.classes_))))\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def run_cv(pipe, X, y, name):\n",
        "    oof = np.zeros((len(X), 3)); scores=[]\n",
        "    for f,(tr,va) in enumerate(skf.split(X, y)):\n",
        "        pipe.fit(X.iloc[tr], y[tr])\n",
        "        if f==0:\n",
        "            step = next((k for k in pipe.named_steps if hasattr(pipe.named_steps[k],'get_feature_names_out')), None)\n",
        "            if step:\n",
        "                print(name, 'features:', len(pipe.named_steps[step].get_feature_names_out()))\n",
        "        p = pipe.predict_proba(X.iloc[va]); oof[va]=p\n",
        "        s = log_loss(y[va], p); scores.append(s); print(f'{name} Fold {f+1}: {s:.4f}')\n",
        "        if f==0:\n",
        "            print('Sample probs Fold 1:', np.round(p[:3],3))\n",
        "    score = float(np.mean(scores))\n",
        "    print(f'{name} OOF: {score:.4f} | prob sum sanity:', np.allclose(oof.sum(1).mean(), 1.0, atol=1e-6))\n",
        "    return score, oof, pipe\n",
        "\n",
        "# Char Count NB baseline (ComplementNB alpha=0.5, norm=True, binary=False, expect 0.33-0.38 OOF)\n",
        "char_nb = Pipeline([\n",
        "    ('cv', CountVectorizer(analyzer='char', ngram_range=(3,5), lowercase=False, min_df=3, max_features=200000, binary=False)),\n",
        "    ('nb', ComplementNB(alpha=0.5, norm=True))\n",
        "])\n",
        "sc, oof, pipe = run_cv(char_nb, train['text'], y, 'Char Count NB')\n",
        "\n",
        "print('Char NB OOF:', round(sc,4))\n",
        "oof_preds_char = oof\n",
        "char_pipe = pipe\n",
        "pd.DataFrame(oof_preds_char, columns=le.classes_).to_csv('oof_char.csv', index=False)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label map: {'EAP': 0, 'HPL': 1, 'MWS': 2}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char Count NB features: 105935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char Count NB Fold 1: 1.0986\nSample probs Fold 1: [[0.333 0.333 0.333]\n [0.333 0.333 0.333]\n [0.333 0.333 0.333]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char Count NB Fold 2: 1.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char Count NB Fold 3: 1.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char Count NB Fold 4: 1.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char Count NB Fold 5: 1.0986\nChar Count NB OOF: 1.0986 | prob sum sanity: True\nChar NB OOF: 1.0986\n"
          ]
        }
      ]
    },
    {
      "id": "f795f714-9c90-4d14-b3be-1a121f593670",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def run_cv(pipe, X, y, name):\n",
        "    oof = np.zeros((len(X), 3)); scores=[]\n",
        "    for f,(tr,va) in enumerate(skf.split(X, y)):\n",
        "        pipe.fit(X.iloc[tr], y[tr])\n",
        "        p = pipe.predict_proba(X.iloc[va]); oof[va]=p\n",
        "        s = log_loss(y[va], p); scores.append(s); print(f'{name} Fold {f+1}: {s:.4f}')\n",
        "    score = float(np.mean(scores)); print(f'{name} OOF: {score:.4f}')\n",
        "    return score, oof, pipe\n",
        "\n",
        "word_lr = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(analyzer='word', ngram_range=(1,2),\n",
        "                              lowercase=True, sublinear_tf=True,\n",
        "                              min_df=2, max_df=0.95)),  # float64 default\n",
        "    ('lr', LogisticRegression(solver='lbfgs', C=1.5,\n",
        "                              max_iter=5000, tol=1e-4,\n",
        "                              random_state=42, n_jobs=1))\n",
        "])\n",
        "sc_word, oof_word, word_pipe = run_cv(word_lr, train['text'], y, 'Tweaked Word LR')\n",
        "pd.DataFrame(oof_word, columns=le.classes_).to_csv('oof_word_tweaked.csv', index=False)\n",
        "\n",
        "# Fit full and save test preds for blending\n",
        "word_pipe.fit(train['text'], y)\n",
        "test_word = word_pipe.predict_proba(test['text'])\n",
        "pd.DataFrame(test_word, columns=le.classes_).to_csv('test_word_tweaked.csv', index=False)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweaked Word LR Fold 1: 0.5316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweaked Word LR Fold 2: 0.5416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweaked Word LR Fold 3: 0.5424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweaked Word LR Fold 4: 0.5308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweaked Word LR Fold 5: 0.5312\nTweaked Word LR OOF: 0.5355\n"
          ]
        }
      ]
    },
    {
      "id": "41b4995b-f6b5-4976-8ce8-d68adc51f49a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "import itertools\n",
        "\n",
        "# Load OOF predictions for the three best models\n",
        "oof_char_lr_df = pd.read_csv('oof_char_lr.csv')\n",
        "oof_char_lr_df = oof_char_lr_df.reindex(columns=['EAP', 'HPL', 'MWS'])\n",
        "oof_char_lr = oof_char_lr_df.values\n",
        "\n",
        "oof_char_wb_lr_df = pd.read_csv('oof_char_wb_lr.csv')\n",
        "oof_char_wb_lr_df = oof_char_wb_lr_df.reindex(columns=['EAP', 'HPL', 'MWS'])\n",
        "oof_char_wb_lr = oof_char_wb_lr_df.values\n",
        "\n",
        "oof_word_df = pd.read_csv('oof_word.csv')\n",
        "oof_word_df = oof_word_df.reindex(columns=['EAP', 'HPL', 'MWS'])\n",
        "oof_word = oof_word_df.values\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "\n",
        "# Grid search for optimal weights (3 models, sum to 1)\n",
        "best_score = float('inf')\n",
        "best_weights = None\n",
        "oof_list = [oof_char_lr, oof_char_wb_lr, oof_word]\n",
        "labels = ['char_lr', 'char_wb_lr', 'word']\n",
        "\n",
        "for weights in itertools.product(np.linspace(0, 1, 11), repeat=3):\n",
        "    if abs(sum(weights) - 1.0) > 1e-6: continue\n",
        "    blend_oof = sum(w * oof for w, oof in zip(weights, oof_list))\n",
        "    score = log_loss(y, blend_oof)\n",
        "    if score < best_score:\n",
        "        best_score = score\n",
        "        best_weights = weights\n",
        "\n",
        "print(f'Best blend weights: {dict(zip(labels, best_weights))}', f'Blended OOF: {best_score:.4f}')\n",
        "\n",
        "# Refit original word_pipe (from early Cell 4 config)\n",
        "word_pipe = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(analyzer='word', ngram_range=(1,2),\n",
        "                              lowercase=True, sublinear_tf=True,\n",
        "                              min_df=3, max_df=0.95, dtype=np.float32)),\n",
        "    ('lr', LogisticRegression(solver='lbfgs',\n",
        "                              C=4.0, max_iter=3000, tol=1e-3,\n",
        "                              random_state=42, n_jobs=1))\n",
        "])\n",
        "word_pipe.fit(train['text'], y)\n",
        "\n",
        "# Refit original char_wb_lr pipe (from early Cell 9 config)\n",
        "char_wb_pipe = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(\n",
        "        analyzer='char_wb',\n",
        "        ngram_range=(3,5),\n",
        "        lowercase=False,\n",
        "        sublinear_tf=True,\n",
        "        min_df=2,\n",
        "        max_df=0.98,\n",
        "        max_features=200000\n",
        "    )),\n",
        "    ('lr', LogisticRegression(\n",
        "        solver='lbfgs',\n",
        "        C=4.0,\n",
        "        max_iter=3000,\n",
        "        tol=1e-3,\n",
        "        random_state=42,\n",
        "        n_jobs=1\n",
        "    ))\n",
        "])\n",
        "char_wb_pipe.fit(train['text'], y)\n",
        "\n",
        "# Use char_pipe from Cell 12 (already fitted)\n",
        "\n",
        "# Generate test predictions\n",
        "test_char_lr_df = pd.DataFrame(char_pipe.predict_proba(test['text']), columns=le.classes_)\n",
        "test_char_lr_df = test_char_lr_df.reindex(columns=['EAP', 'HPL', 'MWS'])\n",
        "test_char_lr = test_char_lr_df.values\n",
        "\n",
        "test_char_wb_lr_df = pd.DataFrame(char_wb_pipe.predict_proba(test['text']), columns=le.classes_)\n",
        "test_char_wb_lr_df = test_char_wb_lr_df.reindex(columns=['EAP', 'HPL', 'MWS'])\n",
        "test_char_wb_lr = test_char_wb_lr_df.values\n",
        "\n",
        "test_word_df = pd.DataFrame(word_pipe.predict_proba(test['text']), columns=le.classes_)\n",
        "test_word_df = test_word_df.reindex(columns=['EAP', 'HPL', 'MWS'])\n",
        "test_word = test_word_df.values\n",
        "\n",
        "test_list = [test_char_lr, test_char_wb_lr, test_word]\n",
        "blend_test = sum(w * test_preds for w, test_preds in zip(best_weights, test_list))\n",
        "\n",
        "# Ensure probs sum to 1 and clip extremes\n",
        "blend_test = np.clip(blend_test, 1e-9, 1 - 1e-9)\n",
        "blend_test /= blend_test.sum(axis=1, keepdims=True)\n",
        "\n",
        "# Submission\n",
        "sub = pd.read_csv('sample_submission.csv')\n",
        "sub[['EAP', 'HPL', 'MWS']] = blend_test\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Blended submission saved with OOF:', round(best_score, 4))\n",
        "print('Test probs shape:', blend_test.shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best blend weights: {'char_lr': 0.2, 'char_wb_lr': 0.5, 'word': 0.30000000000000004} Blended OOF: 0.4219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blended submission saved with OOF: 0.4219\nTest probs shape: (1958, 3)\n"
          ]
        }
      ]
    },
    {
      "id": "5d8a1457-fb59-4813-b829-e363428185ae",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Uninstall current scikit-learn\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', 'scikit-learn'], check=False)\n",
        "print('Uninstalled scikit-learn')\n",
        "\n",
        "# Install stable version 1.3.0 with force-reinstall to overwrite existing\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'install', '--force-reinstall', 'scikit-learn==1.3.0'], check=True)\n",
        "print('Installed scikit-learn==1.3.0 with force-reinstall')\n",
        "\n",
        "# Verify\n",
        "import sklearn\n",
        "print('Sklearn version after install:', sklearn.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: scikit-learn 1.3.0\nUninstalling scikit-learn-1.3.0:\n  Successfully uninstalled scikit-learn-1.3.0\nUninstalled scikit-learn\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.3.0\n  Downloading scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10.9/10.9 MB 120.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy>=1.5.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 173.4 MB/s eta 0:00:00\nCollecting threadpoolctl>=2.0.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.17.3\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 270.4 MB/s eta 0:00:00\nCollecting joblib>=1.1.1\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 513.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 2.21.0 requires fsspec[http]<=2024.6.1,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\nWARNING: Target directory /app/.pip-target/scipy-1.16.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib-1.5.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl-3.6.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed joblib-1.5.2 numpy-1.26.4 scikit-learn-1.3.0 scipy-1.16.2 threadpoolctl-3.6.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed scikit-learn==1.3.0 with force-reinstall\nSklearn version after install: 1.7.2\n"
          ]
        }
      ]
    },
    {
      "id": "2410bbee-2beb-4a39-93a9-517f712602c0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np, pandas as pd, os\n",
        "\n",
        "os.environ['OPENBLAS_NUM_THREADS']='1'; os.environ['MKL_NUM_THREADS']='1'; os.environ['NUMEXPR_NUM_THREADS']='1'\n",
        "train=pd.read_csv('train.csv'); test=pd.read_csv('test.csv')\n",
        "le=LabelEncoder(); y=le.fit_transform(train['author'])\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def run_cv(pipe,X,y,name):\n",
        "    oof=np.zeros((len(X),3)); scores=[]\n",
        "    for f,(tr,va) in enumerate(skf.split(X,y)):\n",
        "        pipe.fit(X.iloc[tr], y[tr])\n",
        "        p=pipe.predict_proba(X.iloc[va]); oof[va]=p\n",
        "        s=log_loss(y[va], p); scores.append(s); print(f'{name} Fold {f+1}: {s:.4f}')\n",
        "    sc=float(np.mean(scores)); print(f'{name} OOF: {sc:.4f}'); return sc,oof,pipe\n",
        "\n",
        "char_wb_lr=Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(analyzer='char_wb', ngram_range=(3,6),\n",
        "                              lowercase=False, sublinear_tf=True,\n",
        "                              min_df=7, max_df=0.95, max_features=120_000)),\n",
        "    ('lr', LogisticRegression(solver='saga', multi_class='multinomial',\n",
        "                              C=1.2, max_iter=2500, tol=1e-3,\n",
        "                              random_state=42, n_jobs=1))\n",
        "])\n",
        "sc,oof,pipe=run_cv(char_wb_lr, train['text'], y, 'Char_wb LR')\n",
        "pd.DataFrame(oof, columns=le.classes_).to_csv('oof_char_wb.csv', index=False)\n",
        "char_wb_pipe=pipe"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char_wb LR Fold 1: 0.4860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char_wb LR Fold 2: 0.4936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char_wb LR Fold 3: 0.4960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "id": "3c46f21b-9a6c-4d5d-9b7f-e7736eb8e46c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np, pandas as pd, os\n",
        "\n",
        "os.environ['OPENBLAS_NUM_THREADS']='1'; os.environ['MKL_NUM_THREADS']='1'; os.environ['NUMEXPR_NUM_THREADS']='1'\n",
        "train=pd.read_csv('train.csv'); test=pd.read_csv('test.csv')\n",
        "le=LabelEncoder(); y=le.fit_transform(train['author'])\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def run_cv(pipe,X,y,name):\n",
        "    oof=np.zeros((len(X),3)); scores=[]\n",
        "    for f,(tr,va) in enumerate(skf.split(X,y)):\n",
        "        pipe.fit(X.iloc[tr], y[tr])\n",
        "        p=pipe.predict_proba(X.iloc[va]); oof[va]=p\n",
        "        s=log_loss(y[va], p); scores.append(s); print(f'{name} Fold {f+1}: {s:.4f}')\n",
        "    sc=float(np.mean(scores)); print(f'{name} OOF: {sc:.4f}'); return sc,oof,pipe\n",
        "\n",
        "char_sgd=Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(analyzer='char', ngram_range=(3,5),\n",
        "                              lowercase=False, sublinear_tf=True,\n",
        "                              min_df=7, max_df=0.95, max_features=150_000)),\n",
        "    ('sgd', SGDClassifier(loss='log_loss', penalty='l2',\n",
        "                          alpha=1.2e-4, max_iter=2000, tol=1e-3,\n",
        "                          early_stopping=True, validation_fraction=0.1,\n",
        "                          n_iter_no_change=5, random_state=42))\n",
        "])\n",
        "sc,oof,pipe=run_cv(char_sgd, train['text'], y, 'Char SGD')\n",
        "pd.DataFrame(oof, columns=le.classes_).to_csv('oof_char.csv', index=False)\n",
        "char_pipe=pipe"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char SGD Fold 1: 0.6422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char SGD Fold 2: 0.6453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char SGD Fold 3: 0.6455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char SGD Fold 4: 0.6416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char SGD Fold 5: 0.6333\nChar SGD OOF: 0.6416\n"
          ]
        }
      ]
    },
    {
      "id": "abb1c344-a3fe-4527-9bbc-e7e2a57f0861",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd, numpy as np, os\n",
        "\n",
        "os.environ['OPENBLAS_NUM_THREADS']='1'; os.environ['MKL_NUM_THREADS']='1'; os.environ['NUMEXPR_NUM_THREADS']='1'\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def run_cv(pipe, X, y, name):\n",
        "    oof = np.zeros((len(X), 3)); scores=[]\n",
        "    for f,(tr,va) in enumerate(skf.split(X, y)):\n",
        "        pipe.fit(X.iloc[tr], y[tr])\n",
        "        p = pipe.predict_proba(X.iloc[va]); oof[va]=p\n",
        "        s = log_loss(y[va], p); scores.append(s); print(f'{name} Fold {f+1}: {s:.4f}')\n",
        "    score = float(np.mean(scores)); print(f'{name} OOF: {score:.4f}')\n",
        "    return score, oof, pipe\n",
        "\n",
        "char_wb_lr = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(analyzer='char_wb', ngram_range=(3,6),\n",
        "                              lowercase=False, sublinear_tf=True,\n",
        "                              min_df=6, max_df=0.95)),  # prune more\n",
        "    ('lr', LogisticRegression(solver='lbfgs', C=1.5,\n",
        "                              max_iter=5000, tol=1e-4,\n",
        "                              random_state=42, n_jobs=1))\n",
        "])\n",
        "sc_char, oof_char, fitted_pipe = run_cv(char_wb_lr, train['text'], y, 'Tweaked Char_wb LR')\n",
        "pd.DataFrame(oof_char, columns=le.classes_).to_csv('oof_char_wb_tweaked.csv', index=False)\n",
        "\n",
        "# Fit full and save test preds for blending\n",
        "fitted_pipe.fit(train['text'], y)\n",
        "test_char = fitted_pipe.predict_proba(test['text'])\n",
        "pd.DataFrame(test_char, columns=le.classes_).to_csv('test_char_wb_tweaked.csv', index=False)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweaked Char_wb LR Fold 1: 0.4684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweaked Char_wb LR Fold 2: 0.4775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweaked Char_wb LR Fold 3: 0.4787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweaked Char_wb LR Fold 4: 0.4747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweaked Char_wb LR Fold 5: 0.4664\nTweaked Char_wb LR OOF: 0.4731\n"
          ]
        }
      ]
    },
    {
      "id": "1e39aec2-b16f-4f9f-ab97-5b629c2d97a2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, os\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "os.environ['OPENBLAS_NUM_THREADS']='1'; os.environ['MKL_NUM_THREADS']='1'; os.environ['NUMEXPR_NUM_THREADS']='1'\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "vec = CountVectorizer(analyzer='char_wb', ngram_range=(3,5),\n",
        "                      lowercase=False, min_df=3, binary=True)\n",
        "\n",
        "def nb_ratio(X, y_bin, alpha=0.1):\n",
        "    pos = np.asarray(X[y_bin==1].sum(axis=0)).ravel() + alpha\n",
        "    neg = np.asarray(X[y_bin==0].sum(axis=0)).ravel() + alpha\n",
        "    return np.log(pos/neg)\n",
        "\n",
        "oof = np.zeros((len(train), 3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y)):\n",
        "    Xtr = vec.fit_transform(train['text'].iloc[tr])\n",
        "    Xva = vec.transform(train['text'].iloc[va])\n",
        "\n",
        "    R = []\n",
        "    for c in range(3):\n",
        "        yb = (y[tr]==c).astype(int)\n",
        "        R.append(nb_ratio(Xtr, yb, alpha=0.1))\n",
        "    R = np.vstack(R).T  # shape: n_features x 3\n",
        "\n",
        "    # Transform features per class by elementwise multiply, then stack\n",
        "    Xtr_nb = []\n",
        "    Xva_nb = []\n",
        "    for c in range(3):\n",
        "        Xtr_nb.append(Xtr.multiply(R[:,c]))\n",
        "        Xva_nb.append(Xva.multiply(R[:,c]))\n",
        "    Xtr_nb = hstack(Xtr_nb)\n",
        "    Xva_nb = hstack(Xva_nb)\n",
        "\n",
        "    clf = LogisticRegression(solver='lbfgs', C=1.0, max_iter=4000, tol=1e-4, random_state=42, n_jobs=1)\n",
        "    clf.fit(Xtr_nb, y[tr])\n",
        "\n",
        "    p = clf.predict_proba(Xva_nb)\n",
        "    oof[va] = p\n",
        "    s = log_loss(y[va], p); scores.append(s); print(f'NB-SVM Fold {f+1}: {s:.4f}')\n",
        "\n",
        "nbsvm_oof = float(np.mean(scores))\n",
        "print(f'NB-SVM OOF: {nbsvm_oof:.4f}')\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_nbsvm.csv', index=False)\n",
        "\n",
        "# Fit full for test\n",
        "Xfull = vec.fit_transform(train['text'])\n",
        "R = []\n",
        "for c in range(3):\n",
        "    yb = (y==c).astype(int)\n",
        "    R.append(nb_ratio(Xfull, yb, alpha=0.1))\n",
        "R = np.vstack(R).T\n",
        "Xfull_nb = hstack([Xfull.multiply(R[:,c]) for c in range(3)])\n",
        "Xtest_nb = hstack([vec.transform(test['text']).multiply(R[:,c]) for c in range(3)])\n",
        "\n",
        "clf_full = LogisticRegression(solver='lbfgs', C=1.0, max_iter=4000, tol=1e-4, random_state=42, n_jobs=1)\n",
        "clf_full.fit(Xfull_nb, y)\n",
        "test_nbsvm = clf_full.predict_proba(Xtest_nb)\n",
        "pd.DataFrame(test_nbsvm, columns=classes).to_csv('test_nbsvm.csv', index=False)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Fold 1: 0.6337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Fold 2: 0.6080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Fold 3: 0.6855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Fold 4: 0.6477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Fold 5: 0.6429\nNB-SVM OOF: 0.6436\n"
          ]
        }
      ]
    },
    {
      "id": "c5518dc4-9bc5-47d4-bc02-a801dfdb5e6e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, os\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.sparse import issparse\n",
        "\n",
        "os.environ['OPENBLAS_NUM_THREADS']='1'; os.environ['MKL_NUM_THREADS']='1'; os.environ['NUMEXPR_NUM_THREADS']='1'\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "vec = CountVectorizer(analyzer='word', ngram_range=(1,2),\n",
        "                      lowercase=True, min_df=2, max_df=0.98,\n",
        "                      max_features=200_000, binary=True)\n",
        "\n",
        "def log_count_ratio(X, y_bin, alpha=0.5):\n",
        "    pos = np.asarray(X[y_bin==1].sum(axis=0)).ravel() + alpha\n",
        "    neg = np.asarray(X[y_bin==0].sum(axis=0)).ravel() + alpha\n",
        "    return np.log(pos/neg)\n",
        "\n",
        "def normalize_ovr_probs(P, eps=1e-9):\n",
        "    P = np.clip(P, eps, 1 - eps)\n",
        "    odds = P / (1.0 - P)\n",
        "    return odds / (odds.sum(axis=1, keepdims=True) + eps)\n",
        "\n",
        "oof = np.zeros((len(train), 3)); scores = []\n",
        "X = vec.fit_transform(train['text']); Xtest = vec.transform(test['text'])\n",
        "\n",
        "for f,(tr,va) in enumerate(skf.split(X, y)):\n",
        "    Xtr, Xva = X[tr], X[va]\n",
        "    Pva = np.zeros((len(va), 3))\n",
        "    for c in range(3):\n",
        "        yb = (y[tr]==c).astype(int)\n",
        "        r = log_count_ratio(Xtr, yb, alpha=0.5)\n",
        "        clf = LogisticRegression(solver='liblinear', C=2.0, penalty='l2',\n",
        "                                 max_iter=2000, tol=1e-4, random_state=42+c)\n",
        "        clf.fit(Xtr.multiply(r), yb)\n",
        "        Pva[:, c] = clf.predict_proba(Xva.multiply(r))[:,1]\n",
        "    Pva = normalize_ovr_probs(Pva)\n",
        "    oof[va] = Pva\n",
        "    s = log_loss(y[va], Pva); scores.append(s); print(f'NB-SVM-LR Fold {f+1}: {s:.4f}')\n",
        "\n",
        "sc_nb = float(np.mean(scores)); print(f'NB-SVM-LR OOF: {sc_nb:.4f}')\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_nbsvm_lr.csv', index=False)\n",
        "\n",
        "# Fit full and predict test\n",
        "Ptest = np.zeros((len(test), 3))\n",
        "for c in range(3):\n",
        "    yb = (y==c).astype(int)\n",
        "    r = log_count_ratio(X, yb, alpha=0.5)\n",
        "    clf = LogisticRegression(solver='liblinear', C=2.0, penalty='l2',\n",
        "                             max_iter=2000, tol=1e-4, random_state=42+c)\n",
        "    clf.fit(X.multiply(r), yb)\n",
        "    Ptest[:, c] = clf.predict_proba(Xtest.multiply(r))[:,1]\n",
        "Ptest = normalize_ovr_probs(Ptest)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_nbsvm_lr.csv', index=False)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/metrics/_classification.py:259: UserWarning: The y_prob values do not sum to one. Make sure to pass probabilities.\n  Ground truth (correct) target values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM-LR Fold 1: 0.7337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/metrics/_classification.py:259: UserWarning: The y_prob values do not sum to one. Make sure to pass probabilities.\n  Ground truth (correct) target values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM-LR Fold 2: 0.6839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/metrics/_classification.py:259: UserWarning: The y_prob values do not sum to one. Make sure to pass probabilities.\n  Ground truth (correct) target values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM-LR Fold 3: 0.7686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/metrics/_classification.py:259: UserWarning: The y_prob values do not sum to one. Make sure to pass probabilities.\n  Ground truth (correct) target values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM-LR Fold 4: 0.7149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/metrics/_classification.py:259: UserWarning: The y_prob values do not sum to one. Make sure to pass probabilities.\n  Ground truth (correct) target values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM-LR Fold 5: 0.7448\nNB-SVM-LR OOF: 0.7292\n"
          ]
        }
      ]
    },
    {
      "id": "dc37aa47-04ba-4943-9a8d-75429dfbc72c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd, numpy as np, os\n",
        "\n",
        "os.environ['OPENBLAS_NUM_THREADS']='1'; os.environ['MKL_NUM_THREADS']='1'; os.environ['NUMEXPR_NUM_THREADS']='1'\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def run_cv(pipe, X, y, name):\n",
        "    oof = np.zeros((len(X), 3)); scores=[]\n",
        "    for f,(tr,va) in enumerate(skf.split(X, y)):\n",
        "        pipe.fit(X.iloc[tr], y[tr])\n",
        "        p = pipe.predict_proba(X.iloc[va]); oof[va]=p\n",
        "        s = log_loss(y[va], p); scores.append(s); print(f'{name} Fold {f+1}: {s:.4f}')\n",
        "    score = float(np.mean(scores)); print(f'{name} OOF: {score:.4f}')\n",
        "    return score, oof, pipe\n",
        "\n",
        "char_lr = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(analyzer='char', ngram_range=(3,5),\n",
        "                              lowercase=False, sublinear_tf=True,\n",
        "                              min_df=1, max_df=1.0)),  # no max_features\n",
        "    ('lr', LogisticRegression(solver='lbfgs', C=4.0,\n",
        "                              max_iter=3000, tol=1e-3,\n",
        "                              random_state=42, n_jobs=1))\n",
        "])\n",
        "sc_char, oof_char, char_pipe = run_cv(char_lr, train['text'], y, 'Char LR')\n",
        "pd.DataFrame(oof_char, columns=le.classes_).to_csv('oof_char_lr.csv', index=False)\n",
        "\n",
        "char_pipe.fit(train['text'], y)\n",
        "test_char = char_pipe.predict_proba(test['text'])\n",
        "pd.DataFrame(test_char, columns=le.classes_).to_csv('test_char_lr.csv', index=False)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char LR Fold 1: 0.4327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char LR Fold 2: 0.4519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char LR Fold 3: 0.4496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char LR Fold 4: 0.4313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char LR Fold 5: 0.4414\nChar LR OOF: 0.4414\n"
          ]
        }
      ]
    },
    {
      "id": "163e597e-b798-40fd-a191-377a6a9fcdfd",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd, numpy as np, os\n",
        "\n",
        "os.environ['OPENBLAS_NUM_THREADS']='1'; os.environ['MKL_NUM_THREADS']='1'; os.environ['NUMEXPR_NUM_THREADS']='1'\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def run_cv(pipe, X, y, name):\n",
        "    oof = np.zeros((len(X), 3)); scores=[]\n",
        "    print(f'--- {name} ---')\n",
        "    for f,(tr,va) in enumerate(skf.split(X, y)):\n",
        "        pipe.fit(X.iloc[tr], y[tr])\n",
        "        p = pipe.predict_proba(X.iloc[va]); oof[va]=p\n",
        "        s = log_loss(y[va], p); scores.append(s); print(f'Fold {f+1}: {s:.4f}')\n",
        "    sc = float(np.mean(scores)); print(f'OOF: {sc:.4f}')\n",
        "    return sc, oof, pipe\n",
        "\n",
        "lr_pipe = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(analyzer='word', ngram_range=(1,3),\n",
        "                              lowercase=True, sublinear_tf=True,\n",
        "                              min_df=3, max_df=0.9, max_features=150_000)),\n",
        "    ('lr', LogisticRegression(solver='lbfgs', C=2.0,\n",
        "                              max_iter=3000, tol=1e-4, random_state=42, n_jobs=1))\n",
        "])\n",
        "\n",
        "sc_lr, oof_lr, fitted_lr_pipe = run_cv(lr_pipe, train['text'], y, 'LR_word')\n",
        "pd.DataFrame(oof_lr, columns=le.classes_).to_csv('oof_lr_word.csv', index=False)\n",
        "\n",
        "fitted_lr_pipe.fit(train['text'], y)\n",
        "test_lr = fitted_lr_pipe.predict_proba(test['text'])\n",
        "pd.DataFrame(test_lr, columns=le.classes_).to_csv('test_lr_word.csv', index=False)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- LR_word ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: 0.5062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: 0.5160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: 0.5146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: 0.5049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5: 0.5050\nOOF: 0.5094\n"
          ]
        }
      ]
    },
    {
      "id": "35aebed8-3679-4e3f-86fc-154869c1e4a3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "os.environ['OPENBLAS_NUM_THREADS']='1'\n",
        "os.environ['MKL_NUM_THREADS']='1'\n",
        "os.environ['NUMEXPR_NUM_THREADS']='1'\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Manual Platt-calibrated LinearSVC\n",
        "class ManualPlattSVC:\n",
        "    def __init__(self, C=0.5, max_iter=4000, tol=1e-4, random_state=42):\n",
        "        self.svc = LinearSVC(C=C, dual='auto', max_iter=max_iter, tol=tol, random_state=random_state)\n",
        "        self.calibs = []\n",
        "    def fit(self, X, y):\n",
        "        self.svc.fit(X, y)\n",
        "        F = self.svc.decision_function(X)\n",
        "        if F.ndim == 1: F = F[:, None]\n",
        "        self.n_classes_ = F.shape[1]\n",
        "        self.calibs = []\n",
        "        for c in range(self.n_classes_):\n",
        "            lr = LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000)\n",
        "            lr.fit(F[:, [c]], (y==c).astype(int))\n",
        "            self.calibs.append(lr)\n",
        "        return self\n",
        "    def predict_proba(self, X):\n",
        "        F = self.svc.decision_function(X)\n",
        "        if F.ndim == 1: F = F[:, None]\n",
        "        P = np.zeros((F.shape[0], self.n_classes_), dtype=float)\n",
        "        for c, lr in enumerate(self.calibs):\n",
        "            P[:, c] = lr.predict_proba(F[:, [c]])[:, 1]\n",
        "        P = np.clip(P, 1e-9, 1-1e-9)\n",
        "        odds = P/(1-P)\n",
        "        return odds/(odds.sum(axis=1, keepdims=True)+1e-12)\n",
        "\n",
        "def odds_normalize(P, eps=1e-9):\n",
        "    P = np.clip(P, eps, 1-eps)\n",
        "    odds = P/(1-P)\n",
        "    return odds/(odds.sum(axis=1, keepdims=True)+eps)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "id": "6389df4b-b33d-43ce-a58a-d66af2a71360",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Threads\n",
        "os.environ['OPENBLAS_NUM_THREADS']='1'\n",
        "os.environ['MKL_NUM_THREADS']='1'\n",
        "os.environ['NUMEXPR_NUM_THREADS']='1'\n",
        "\n",
        "# Data\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "skf_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def odds_normalize(P, eps=1e-9):\n",
        "    P = np.clip(P, eps, 1-eps)\n",
        "    odds = P/(1-P)\n",
        "    return odds/(odds.sum(axis=1, keepdims=True)+eps)\n",
        "\n",
        "# Strong, safe vectorizer and SVC settings\n",
        "vec_params = dict(\n",
        "    analyzer='char_wb', ngram_range=(2,5),\n",
        "    lowercase=False, sublinear_tf=True,\n",
        "    min_df=3, max_df=0.98, max_features=250_000\n",
        ")\n",
        "svc_params = dict(C=0.5, loss='squared_hinge', dual='auto', max_iter=3000, tol=1e-4, random_state=42)\n",
        "inner_cv_splits = 3  # cross-fit Platt calibrators\n",
        "\n",
        "def run_calsvc_ovr_platt(X_text, y):\n",
        "    oof = np.zeros((len(X_text), 3)); scores=[]\n",
        "    for f,(tr,va) in enumerate(skf_outer.split(X_text, y), 1):\n",
        "        vec = TfidfVectorizer(**vec_params)\n",
        "        Xtr = vec.fit_transform(X_text.iloc[tr]); Xva = vec.transform(X_text.iloc[va])\n",
        "\n",
        "        Pva = np.zeros((len(va), 3))\n",
        "        for c in range(3):\n",
        "            yb_tr = (y[tr]==c).astype(int)\n",
        "\n",
        "            # Inner CV: build Platt on out-of-fold decision scores\n",
        "            skf_inner = StratifiedKFold(n_splits=inner_cv_splits, shuffle=True, random_state=42+c)\n",
        "            F_cal = []; z_cal = []\n",
        "            for itr, iva in skf_inner.split(Xtr, yb_tr):\n",
        "                svc = LinearSVC(**svc_params)\n",
        "                svc.fit(Xtr[itr], yb_tr[itr])\n",
        "                s = svc.decision_function(Xtr[iva])\n",
        "                if s.ndim > 1: s = s[:,0]\n",
        "                F_cal.append(s.reshape(-1,1)); z_cal.append(yb_tr[iva])\n",
        "            F_cal = np.vstack(F_cal); z_cal = np.concatenate(z_cal)\n",
        "\n",
        "            platt = LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000, random_state=1337)\n",
        "            platt.fit(F_cal, z_cal)\n",
        "\n",
        "            # Final binary SVC on full tr -> score outer va -> calibrate\n",
        "            svc_full = LinearSVC(**svc_params)\n",
        "            svc_full.fit(Xtr, yb_tr)\n",
        "            s_va = svc_full.decision_function(Xva)\n",
        "            if s_va.ndim > 1: s_va = s_va[:,0]\n",
        "            Pva[:, c] = platt.predict_proba(s_va.reshape(-1,1))[:,1]\n",
        "\n",
        "        Pva = odds_normalize(Pva)\n",
        "        oof[va] = Pva\n",
        "        fold_ll = log_loss(y[va], Pva); scores.append(fold_ll)\n",
        "        print(f'CalSVC(OvR+Platt) Fold {f}: {fold_ll:.4f}')\n",
        "\n",
        "    sc = float(np.mean(scores))\n",
        "    print(f'CalSVC(OvR+Platt) OOF: {sc:.4f}')\n",
        "    return sc, oof\n",
        "\n",
        "# CV + save OOF\n",
        "sc, oof = run_calsvc_ovr_platt(train['text'], y)\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_calsvc_char.csv', index=False)\n",
        "\n",
        "# Fit full for test: cross-fit Platt on full train, then final SVCs and test preds\n",
        "vec_full = TfidfVectorizer(**vec_params)\n",
        "Xfull = vec_full.fit_transform(train['text']); Xtest = vec_full.transform(test['text'])\n",
        "\n",
        "Ptest = np.zeros((len(test), 3))\n",
        "for c in range(3):\n",
        "    yb = (y==c).astype(int)\n",
        "    skf_inner = StratifiedKFold(n_splits=inner_cv_splits, shuffle=True, random_state=4242+c)\n",
        "    F_cal = []; z_cal = []\n",
        "    for tr_in, va_in in skf_inner.split(Xfull, yb):\n",
        "        svc = LinearSVC(**svc_params)\n",
        "        svc.fit(Xfull[tr_in], yb[tr_in])\n",
        "        s = svc.decision_function(Xfull[va_in])\n",
        "        if s.ndim > 1: s = s[:,0]\n",
        "        F_cal.append(s.reshape(-1,1)); z_cal.append(yb[va_in])\n",
        "    F_cal = np.vstack(F_cal); z_cal = np.concatenate(z_cal)\n",
        "    platt = LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000, random_state=2025+c)\n",
        "    platt.fit(F_cal, z_cal)\n",
        "\n",
        "    svc_final = LinearSVC(**svc_params)\n",
        "    svc_final.fit(Xfull, yb)\n",
        "    s_test = svc_final.decision_function(Xtest)\n",
        "    if s_test.ndim > 1: s_test = s_test[:,0]\n",
        "    Ptest[:, c] = platt.predict_proba(s_test.reshape(-1,1))[:,1]\n",
        "\n",
        "Ptest = odds_normalize(Ptest)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_calsvc_char.csv', index=False)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CalSVC(OvR+Platt) Fold 1: 0.4303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CalSVC(OvR+Platt) Fold 2: 0.4316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CalSVC(OvR+Platt) Fold 3: 0.4623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CalSVC(OvR+Platt) Fold 4: 0.4453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CalSVC(OvR+Platt) Fold 5: 0.4318\nCalSVC(OvR+Platt) OOF: 0.4403\n"
          ]
        }
      ]
    },
    {
      "id": "f96167f3-cf15-4a39-9948-a3ecc552c9ee",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def log_count_ratio(X, y_bin, alpha=0.5):\n",
        "    pos = np.asarray(X[y_bin==1].sum(0)).ravel() + alpha\n",
        "    neg = np.asarray(X[y_bin==0].sum(0)).ravel() + alpha\n",
        "    return np.log(pos / neg)\n",
        "\n",
        "def odds_norm(P, eps=1e-9):\n",
        "    P = np.clip(P, eps, 1-eps)\n",
        "    odds = P/(1-P)\n",
        "    return odds/(odds.sum(axis=1, keepdims=True)+eps)\n",
        "\n",
        "def run_nbsvm(X_text, y, analyzer='char_wb', ngram_range=(3,5), min_df=3, max_features=200000, C=3.0, alpha=0.5):\n",
        "    oof = np.zeros((len(X_text), 3)); scores=[]\n",
        "    for f,(tr,va) in enumerate(skf.split(X_text, y), 1):\n",
        "        vec = CountVectorizer(analyzer=analyzer, ngram_range=ngram_range, lowercase=False,\n",
        "                              min_df=min_df, max_features=max_features, binary=True)\n",
        "        Xtr = vec.fit_transform(X_text.iloc[tr])\n",
        "        Xva = vec.transform(X_text.iloc[va])\n",
        "        y_tr, y_va = y[tr], y[va]\n",
        "        Pva = np.zeros((len(va), 3))\n",
        "        for c in range(3):\n",
        "            yb = (y_tr==c).astype(int)\n",
        "            r = log_count_ratio(Xtr, yb, alpha=alpha)\n",
        "            clf = LogisticRegression(solver='liblinear', C=C, max_iter=2000, tol=1e-4, random_state=42+c)\n",
        "            Xtr_r = Xtr.multiply(csr_matrix(r))\n",
        "            Xva_r = Xva.multiply(csr_matrix(r))\n",
        "            clf.fit(Xtr_r, yb)\n",
        "            Pva[:, c] = clf.predict_proba(Xva_r)[:, 1]\n",
        "        Pva = odds_norm(Pva)\n",
        "        Pva = Pva / Pva.sum(axis=1, keepdims=True)  # Force exact sum to 1\n",
        "        oof[va] = Pva\n",
        "        s = log_loss(y_va, Pva); scores.append(s); print(f'NB-SVM Fold {f}: {s:.4f}')\n",
        "    sc = float(np.mean(scores)); print(f'NB-SVM OOF: {sc:.4f}')\n",
        "    # Full fit\n",
        "    vec_full = CountVectorizer(analyzer=analyzer, ngram_range=ngram_range, lowercase=False,\n",
        "                               min_df=min_df, max_features=max_features, binary=True)\n",
        "    Xfull = vec_full.fit_transform(X_text)\n",
        "    Xtest = vec_full.transform(test['text'])\n",
        "    Ptest = np.zeros((len(test), 3))\n",
        "    for c in range(3):\n",
        "        yb = (y==c).astype(int)\n",
        "        r = log_count_ratio(Xfull, yb, alpha=alpha)\n",
        "        clf = LogisticRegression(solver='liblinear', C=C, max_iter=2000, tol=1e-4, random_state=42+c)\n",
        "        clf.fit(Xfull.multiply(csr_matrix(r)), yb)\n",
        "        Ptest[:, c] = clf.predict_proba(Xtest.multiply(csr_matrix(r)))[:, 1]\n",
        "    Ptest = odds_norm(Ptest)\n",
        "    Ptest = Ptest / Ptest.sum(axis=1, keepdims=True)  # Force exact sum to 1\n",
        "    pd.DataFrame(oof, columns=classes).to_csv('oof_nbsvm_charwb.csv', index=False)\n",
        "    pd.DataFrame(Ptest, columns=classes).to_csv('test_nbsvm_charwb.csv', index=False)\n",
        "    return sc\n",
        "\n",
        "# Run NB-SVM\n",
        "sc_nbsvm = run_nbsvm(train['text'], y, analyzer='char_wb', ngram_range=(3,5), min_df=3, max_features=200000, C=3.0, alpha=0.5)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Fold 1: 0.8142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Fold 2: 0.7629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Fold 3: 0.8488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Fold 4: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Fold 5: 0.8269\nNB-SVM OOF: 0.8106\n"
          ]
        }
      ]
    },
    {
      "id": "a2513562-fc76-4b1d-a0d3-d70ce6914581",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "os.environ['OPENBLAS_NUM_THREADS']='1'; os.environ['MKL_NUM_THREADS']='1'; os.environ['NUMEXPR_NUM_THREADS']='1'\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def log_count_ratio(X, y_bin, alpha=1.0):\n",
        "    pos = np.asarray(X[y_bin==1].sum(0)).ravel() + alpha\n",
        "    neg = np.asarray(X[y_bin==0].sum(0)).ravel() + alpha\n",
        "    r = np.log(pos/neg)\n",
        "    r[~np.isfinite(r)] = 0.0\n",
        "    return r\n",
        "\n",
        "def odds_norm(P, eps=1e-9):\n",
        "    P = np.clip(P, eps, 1-eps)\n",
        "    odds = P/(1-P)\n",
        "    return odds/(odds.sum(axis=1, keepdims=True)+eps)\n",
        "\n",
        "# Robust TF-IDF setup for NB-SVM\n",
        "vec = TfidfVectorizer(analyzer='char_wb', ngram_range=(2,5), lowercase=False,\n",
        "                      min_df=1, max_df=0.99, max_features=200_000, sublinear_tf=True)\n",
        "X_all = vec.fit_transform(train['text']); X_test = vec.transform(test['text'])\n",
        "\n",
        "oof = np.zeros((len(train), 3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(X_all, y), 1):\n",
        "    Xtr, Xva = X_all[tr], X_all[va]\n",
        "    Pva = np.zeros((len(va), 3))\n",
        "    for c in range(3):\n",
        "        yb = (y[tr]==c).astype(int)\n",
        "        r = log_count_ratio(Xtr, yb, alpha=1.0)\n",
        "        clf = LogisticRegression(solver='liblinear', C=1.5, penalty='l2',\n",
        "                                 max_iter=2000, tol=1e-4, random_state=42+c)\n",
        "        clf.fit(Xtr.multiply(csr_matrix(r)), yb)\n",
        "        Pva[:, c] = clf.predict_proba(Xva.multiply(csr_matrix(r)))[:, 1]\n",
        "    Pva = odds_norm(Pva)  # single, correct normalization\n",
        "    oof[va] = Pva\n",
        "    s = log_loss(y[va], Pva); scores.append(s); print(f'NB-SVM Fold {f}: {s:.4f}')\n",
        "sc = float(np.mean(scores)); print(f'NB-SVM OOF: {sc:.4f}')\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_nbsvm_fixed.csv', index=False)\n",
        "\n",
        "# Full fit for test\n",
        "Ptest = np.zeros((len(test), 3))\n",
        "for c in range(3):\n",
        "    yb = (y==c).astype(int)\n",
        "    r = log_count_ratio(X_all, yb, alpha=1.0)\n",
        "    clf = LogisticRegression(solver='liblinear', C=1.5, penalty='l2',\n",
        "                             max_iter=2000, tol=1e-4, random_state=99+c)\n",
        "    clf.fit(X_all.multiply(csr_matrix(r)), yb)\n",
        "    Ptest[:, c] = clf.predict_proba(X_test.multiply(csr_matrix(r)))[:, 1]\n",
        "Ptest = odds_norm(Ptest)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_nbsvm_fixed.csv', index=False)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Fold 1: 0.4757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Fold 2: 0.4817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Fold 3: 0.5009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Fold 4: 0.4813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM Fold 5: 0.4701\nNB-SVM OOF: 0.4819\n"
          ]
        }
      ]
    },
    {
      "id": "f873f6be-bbea-415e-8b54-7f85ae4fac5c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, numpy as np, pandas as pd, itertools\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "\n",
        "# Ensure test_word.csv exists (refit if needed)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "vec_w = TfidfVectorizer(analyzer='word', ngram_range=(1,2), lowercase=True, sublinear_tf=True, min_df=3, max_df=0.95, dtype=np.float32)\n",
        "Xw = vec_w.fit_transform(train['text']); Xw_te = vec_w.transform(test['text'])\n",
        "lr_w = LogisticRegression(solver='lbfgs', C=4.0, max_iter=3000, tol=1e-3, n_jobs=1, random_state=42).fit(Xw, y)\n",
        "test_word = lr_w.predict_proba(Xw_te)\n",
        "pd.DataFrame(test_word, columns=classes).to_csv('test_word.csv', index=False)\n",
        "\n",
        "# Load best OOFs and tests\n",
        "oof1 = pd.read_csv('oof_10fold_uncal_char_wb_lr.csv')[classes].values  # 10-fold char_wb 0.4082\n",
        "test1 = pd.read_csv('test_10fold_uncal_char_wb_lr.csv')[classes].values\n",
        "oof2 = pd.read_csv('oof_char_wb_lr.csv')[classes].values  # 0.4361\n",
        "test2 = pd.read_csv('test_char_wb_lr.csv')[classes].values if os.path.exists('test_char_wb_lr.csv') else pd.read_csv('test_char_wb_tweaked.csv')[classes].values\n",
        "oof3 = pd.read_csv('oof_calsvc_char.csv')[classes].values  # 0.4403\n",
        "test3 = pd.read_csv('test_calsvc_char.csv')[classes].values\n",
        "oof4 = pd.read_csv('oof_word.csv')[classes].values  # 0.4602\n",
        "test4 = pd.read_csv('test_word.csv')[classes].values\n",
        "oof5 = pd.read_csv('oof_10fold_char_lr.csv')[classes].values  # 10-fold char 0.4284\n",
        "test5 = pd.read_csv('test_10fold_char_lr.csv')[classes].values\n",
        "\n",
        "oof_list = [oof1, oof2, oof3, oof4, oof5]\n",
        "test_list = [test1, test2, test3, test4, test5]\n",
        "labels = ['10fold_Uncal_Char_wb', 'Char_wb_LR', 'CalSVC', 'Word_LR', '10fold_Char_LR']\n",
        "\n",
        "# 5-way grid search weights summing to 1\n",
        "best_score = 1e9; best_w = None\n",
        "for w in itertools.product(np.arange(0, 1.05, 0.05), repeat=5):\n",
        "    if abs(sum(w) - 1.0) > 1e-9: continue\n",
        "    blend_oof = sum(ww * oof for ww, oof in zip(w, oof_list))\n",
        "    s = log_loss(y, blend_oof)\n",
        "    if s < best_score:\n",
        "        best_score, best_w = s, w\n",
        "\n",
        "print(f'Best 5-way weights: {{dict(zip(labels, best_w))}}  Blended OOF: {best_score:.4f}')\n",
        "\n",
        "if best_score <= 0.34:\n",
        "    blend_test = sum(ww * tst for ww, tst in zip(best_w, test_list))\n",
        "    blend_test = np.clip(blend_test, 1e-15, 1-1e-15)\n",
        "    blend_test /= blend_test.sum(axis=1, keepdims=True)\n",
        "    sub = pd.read_csv('sample_submission.csv')\n",
        "    sub[classes] = blend_test\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    print('Medal-ready submission.csv saved! Ready to submit_final_answer.')\n",
        "else:\n",
        "    print('Blended OOF >0.34; need further improvement (e.g., stylometrics or 10-fold).')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best 4-way weights: {dict(zip(labels, best_w))}  Blended OOF: 0.3783\nBlended OOF >0.34; need further improvement (e.g., stylometrics or 10-fold).\n"
          ]
        }
      ]
    },
    {
      "id": "cb950b97-5256-4762-8402-771be7e719b2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 19 \u2014 Fixed Calibrated Char_wb LR (true OvR binary base + leak-free inner-CV sigmoid calibration)\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def odds_norm(P, eps=1e-9):\n",
        "    P = np.clip(P, eps, 1-eps)\n",
        "    odds = P/(1-P)\n",
        "    return odds/(odds.sum(axis=1, keepdims=True)+eps)\n",
        "\n",
        "vec_params = dict(analyzer='char_wb', ngram_range=(2,5), lowercase=False,\n",
        "                  sublinear_tf=True, min_df=2, max_df=0.98, max_features=250_000)\n",
        "\n",
        "# Calibration: 'sigmoid' (Platt)\n",
        "calibration = 'sigmoid'\n",
        "inner_cv_splits = 3\n",
        "\n",
        "oof = np.zeros((len(train), 3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y), 1):\n",
        "    vec = TfidfVectorizer(**vec_params)\n",
        "    Xtr = vec.fit_transform(train['text'].iloc[tr])\n",
        "    Xva = vec.transform(train['text'].iloc[va])\n",
        "\n",
        "    Pva = np.zeros((len(va), 3))\n",
        "    for c in range(3):\n",
        "        yb_tr = (y[tr]==c).astype(int)\n",
        "\n",
        "        # Inner CV for leak-free calibration data\n",
        "        skf_inner = StratifiedKFold(n_splits=inner_cv_splits, shuffle=True, random_state=42 + c*10)\n",
        "        F_cal = []\n",
        "        z_cal = []\n",
        "        for i_tr, i_va in skf_inner.split(Xtr, yb_tr):\n",
        "            base_inner = LogisticRegression(solver='liblinear', penalty='l2', C=2.0, max_iter=2000, tol=1e-4, random_state=42 + c)\n",
        "            base_inner.fit(Xtr[i_tr], yb_tr[i_tr])\n",
        "            s_inner = base_inner.decision_function(Xtr[i_va])\n",
        "            F_cal.append(s_inner.reshape(-1, 1))\n",
        "            z_cal.append(yb_tr[i_va])\n",
        "        F_cal = np.vstack(F_cal)\n",
        "        z_cal = np.concatenate(z_cal)\n",
        "\n",
        "        # Fit calibrator on inner OOF margins\n",
        "        calib = LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000, random_state=2025 + c).fit(F_cal, z_cal)\n",
        "\n",
        "        # Final base on full outer tr\n",
        "        base = LogisticRegression(solver='liblinear', penalty='l2', C=2.0, max_iter=2000, tol=1e-4, random_state=42 + c)\n",
        "        base.fit(Xtr, yb_tr)\n",
        "        s_va = base.decision_function(Xva).reshape(-1, 1)\n",
        "\n",
        "        Pva[:, c] = calib.predict_proba(s_va)[:, 1]\n",
        "\n",
        "    Pva = odds_norm(Pva)\n",
        "    oof[va] = Pva\n",
        "    s = log_loss(y[va], Pva); scores.append(s)\n",
        "    print(f'Fixed Cal-Char_wb-LR Fold {f}: {s:.4f}')\n",
        "\n",
        "sc = float(np.mean(scores)); print(f'Fixed Cal-Char_wb-LR OOF: {sc:.4f}')\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_cal_lr_char_wb_fixed.csv', index=False)\n",
        "\n",
        "# Full fit for test\n",
        "vec_full = TfidfVectorizer(**vec_params)\n",
        "Xfull = vec_full.fit_transform(train['text'])\n",
        "Xtest = vec_full.transform(test['text'])\n",
        "Ptest = np.zeros((len(test), 3))\n",
        "for c in range(3):\n",
        "    yb = (y==c).astype(int)\n",
        "    # Inner CV on full train\n",
        "    skf_inner = StratifiedKFold(n_splits=inner_cv_splits, shuffle=True, random_state=42 + c*10)\n",
        "    F_cal = []\n",
        "    z_cal = []\n",
        "    indices = np.arange(len(train))\n",
        "    for i_tr, i_va in skf_inner.split(indices, yb):\n",
        "        base_inner = LogisticRegression(solver='liblinear', penalty='l2', C=2.0, max_iter=2000, tol=1e-4, random_state=42 + c)\n",
        "        base_inner.fit(Xfull[i_tr], yb[i_tr])\n",
        "        s_inner = base_inner.decision_function(Xfull[i_va])\n",
        "        F_cal.append(s_inner.reshape(-1, 1))\n",
        "        z_cal.append(yb[i_va])\n",
        "    F_cal = np.vstack(F_cal)\n",
        "    z_cal = np.concatenate(z_cal)\n",
        "\n",
        "    # Fit calibrator\n",
        "    calib = LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000, random_state=2025 + c).fit(F_cal, z_cal)\n",
        "\n",
        "    # Final base on full\n",
        "    base = LogisticRegression(solver='liblinear', penalty='l2', C=2.0, max_iter=2000, tol=1e-4, random_state=42 + c)\n",
        "    base.fit(Xfull, yb)\n",
        "    s_te = base.decision_function(Xtest).reshape(-1, 1)\n",
        "\n",
        "    Ptest[:, c] = calib.predict_proba(s_te)[:, 1]\n",
        "\n",
        "Ptest = odds_norm(Ptest)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_cal_lr_char_wb_fixed.csv', index=False)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed Cal-Char_wb-LR Fold 1: 0.4673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed Cal-Char_wb-LR Fold 2: 0.4650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed Cal-Char_wb-LR Fold 3: 0.4981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed Cal-Char_wb-LR Fold 4: 0.4785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed Cal-Char_wb-LR Fold 5: 0.4625\nFixed Cal-Char_wb-LR OOF: 0.4743\n"
          ]
        }
      ]
    },
    {
      "id": "138db29d-cfde-4b9a-ba85-d56b2483bf4c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 20 \u2014 Word NB-SVM (strong settings, enforced normalization)\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def log_count_ratio(X, y_bin, alpha=1.0):\n",
        "    pos = np.asarray(X[y_bin==1].sum(axis=0)).ravel() + alpha\n",
        "    neg = np.asarray(X[y_bin==0].sum(axis=0)).ravel() + alpha\n",
        "    r = np.log(pos / neg)\n",
        "    r[~np.isfinite(r)] = 0.0\n",
        "    return r\n",
        "\n",
        "def odds_norm(P, eps=1e-12):\n",
        "    P = np.clip(P, eps, 1-eps)\n",
        "    odds = P / (1 - P)\n",
        "    Pn = odds / (odds.sum(axis=1, keepdims=True) + eps)\n",
        "    return Pn / Pn.sum(axis=1, keepdims=True)  # force exact sum=1\n",
        "\n",
        "vec_params = dict(analyzer='word', ngram_range=(1,3), lowercase=True,\n",
        "                  min_df=1, max_df=0.99, binary=False, max_features=None)\n",
        "\n",
        "oof = np.zeros((len(train), 3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y), 1):\n",
        "    vec = CountVectorizer(**vec_params)\n",
        "    Xtr = vec.fit_transform(train['text'].iloc[tr])\n",
        "    Xva = vec.transform(train['text'].iloc[va])\n",
        "\n",
        "    Pva = np.zeros((len(va), 3))\n",
        "    for c in range(3):\n",
        "        yb_tr = (y[tr]==c).astype(int)\n",
        "        r = log_count_ratio(Xtr, yb_tr, alpha=1.0)\n",
        "        Xtr_r = Xtr.multiply(csr_matrix(r)); Xva_r = Xva.multiply(csr_matrix(r))\n",
        "        clf = LogisticRegression(solver='liblinear', penalty='l2',\n",
        "                                 C=8.0, max_iter=3000, tol=1e-4, random_state=4242+c)\n",
        "        clf.fit(Xtr_r, yb_tr)\n",
        "        Pva[:, c] = clf.predict_proba(Xva_r)[:, 1]\n",
        "\n",
        "    Pva = odds_norm(Pva)\n",
        "    oof[va] = Pva\n",
        "    s = log_loss(y[va], Pva); scores.append(s)\n",
        "    print(f'Word NB-SVM Fold {f}: {s:.4f}')\n",
        "\n",
        "sc = float(np.mean(scores)); print(f'Word NB-SVM OOF: {sc:.4f}')\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_word_nbsvm_fixed.csv', index=False)\n",
        "\n",
        "# Full fit for test\n",
        "vec_full = CountVectorizer(**vec_params)\n",
        "Xfull = vec_full.fit_transform(train['text'])\n",
        "Xtest = vec_full.transform(test['text'])\n",
        "Ptest = np.zeros((len(test), 3))\n",
        "for c in range(3):\n",
        "    yb = (y==c).astype(int)\n",
        "    r = log_count_ratio(Xfull, yb, alpha=1.0)\n",
        "    clf = LogisticRegression(solver='liblinear', penalty='l2',\n",
        "                             C=8.0, max_iter=3000, tol=1e-4, random_state=999+c)\n",
        "    clf.fit(Xfull.multiply(csr_matrix(r)), yb)\n",
        "    Ptest[:, c] = clf.predict_proba(Xtest.multiply(csr_matrix(r)))[:, 1]\n",
        "\n",
        "Ptest = odds_norm(Ptest)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_word_nbsvm_fixed.csv', index=False)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 1: 0.5732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 2: 0.5448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 3: 0.5853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 4: 0.5419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 5: 0.5700\nWord NB-SVM OOF: 0.5630\n"
          ]
        }
      ]
    },
    {
      "id": "8ede2893-ab8a-4261-92d8-8d8f16856155",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 21 \u2014 Uncalibrated Char_wb LR (wide ngrams, low min_df, large vocab, no calibration)\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "vec_params = dict(analyzer='char_wb', ngram_range=(2,6), lowercase=False,\n",
        "                  sublinear_tf=True, min_df=1, max_df=0.98, max_features=400_000)\n",
        "\n",
        "oof = np.zeros((len(train), 3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y), 1):\n",
        "    vec = TfidfVectorizer(**vec_params)\n",
        "    Xtr = vec.fit_transform(train['text'].iloc[tr])\n",
        "    Xva = vec.transform(train['text'].iloc[va])\n",
        "\n",
        "    clf = LogisticRegression(solver='lbfgs', C=4.0, max_iter=3000, tol=1e-4,\n",
        "                             random_state=42, n_jobs=1)\n",
        "    clf.fit(Xtr, y[tr])\n",
        "    p = clf.predict_proba(Xva)\n",
        "    oof[va] = p\n",
        "    s = log_loss(y[va], p); scores.append(s)\n",
        "    print(f'Uncal Char_wb LR Fold {f}: {s:.4f}')\n",
        "\n",
        "sc = float(np.mean(scores)); print(f'Uncal Char_wb LR OOF: {sc:.4f}')\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_uncal_char_wb_lr.csv', index=False)\n",
        "\n",
        "# Full fit for test\n",
        "vec_full = TfidfVectorizer(**vec_params)\n",
        "Xfull = vec_full.fit_transform(train['text'])\n",
        "Xtest = vec_full.transform(test['text'])\n",
        "clf_full = LogisticRegression(solver='lbfgs', C=4.0, max_iter=3000, tol=1e-4,\n",
        "                              random_state=42, n_jobs=1)\n",
        "clf_full.fit(Xfull, y)\n",
        "ptest = clf_full.predict_proba(Xtest)\n",
        "pd.DataFrame(ptest, columns=classes).to_csv('test_uncal_char_wb_lr.csv', index=False)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uncal Char_wb LR Fold 1: 0.4125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uncal Char_wb LR Fold 2: 0.4132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uncal Char_wb LR Fold 3: 0.4296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uncal Char_wb LR Fold 4: 0.4183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uncal Char_wb LR Fold 5: 0.4132\nUncal Char_wb LR OOF: 0.4173\n"
          ]
        }
      ]
    },
    {
      "id": "7a00d41c-0d7c-4077-9ebc-5085f8c4206e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 22 \u2014 Word TF-IDF + Stylometrics LR (manual stacking to avoid sklearn version issues)\n",
        "import numpy as np, pandas as pd, re, string\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, MaxAbsScaler\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "\n",
        "def compute_stylo(series):\n",
        "    def feats(t):\n",
        "        L = len(t); \n",
        "        if L == 0: return [0,0,0,0,0,0,0,0]\n",
        "        punct = sum(c in string.punctuation for c in t)\n",
        "        exclam = t.count('!'); semi = t.count(';')\n",
        "        digits = sum(c.isdigit() for c in t)\n",
        "        letters = sum(c.isalpha() for c in t)\n",
        "        words = t.split(); wc = len(words)\n",
        "        avg_wlen = (sum(len(w) for w in words)/wc) if wc else 0.0\n",
        "        sents = [s for s in re.split(r'[.!?]+', t) if s.strip()]\n",
        "        sc = len(sents); avg_sent_wc = (wc/sc) if sc else wc\n",
        "        cap_ratio = (sum(c.isupper() for c in t)/letters) if letters else 0.0\n",
        "        return [punct/L, exclam/L, semi/L, digits/L, cap_ratio, avg_wlen, avg_sent_wc, wc]\n",
        "    X = [feats(t) for t in series]\n",
        "    return np.array(X)\n",
        "\n",
        "train_sty = compute_stylo(train['text']); test_sty = compute_stylo(test['text'])\n",
        "scaler = MaxAbsScaler().fit(train_sty)\n",
        "train_sty_scaled = scaler.transform(train_sty)\n",
        "test_sty_scaled = scaler.transform(test_sty)\n",
        "\n",
        "tfidf_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True,\n",
        "                    sublinear_tf=True, min_df=2, max_df=0.95, max_features=200_000)\n",
        "clf = LogisticRegression(solver='lbfgs', C=4.0, max_iter=3000, tol=1e-4, random_state=42)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof = np.zeros((len(train), 3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y), 1):\n",
        "    vec = TfidfVectorizer(**tfidf_params)\n",
        "    Xtr_tfidf = vec.fit_transform(train['text'].iloc[tr])\n",
        "    Xva_tfidf = vec.transform(train['text'].iloc[va])\n",
        "    Xtr_sty = csr_matrix(train_sty_scaled[tr])\n",
        "    Xva_sty = csr_matrix(train_sty_scaled[va])\n",
        "    Xtr = hstack([Xtr_tfidf, Xtr_sty])\n",
        "    Xva = hstack([Xva_tfidf, Xva_sty])\n",
        "    clf.fit(Xtr, y[tr])\n",
        "    p = clf.predict_proba(Xva)\n",
        "    dfp = pd.DataFrame(p, columns=clf.classes_).reindex(columns=classes).fillna(0.0).values\n",
        "    dfp = np.clip(dfp, 1e-15, 1-1e-15); dfp /= dfp.sum(axis=1, keepdims=True)\n",
        "    oof[va] = dfp\n",
        "    s = log_loss(y[va], dfp); scores.append(s); print(f'Word+Stylo LR Fold {f}: {s:.4f}')\n",
        "print('Word+Stylo LR OOF:', float(np.mean(scores)))\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_stylo_word_lr.csv', index=False)\n",
        "\n",
        "# Full fit\n",
        "vec_full = TfidfVectorizer(**tfidf_params)\n",
        "Xfull_tfidf = vec_full.fit_transform(train['text'])\n",
        "Xtest_tfidf = vec_full.transform(test['text'])\n",
        "Xfull_sty = csr_matrix(train_sty_scaled)\n",
        "Xtest_sty = csr_matrix(test_sty_scaled)\n",
        "Xfull = hstack([Xfull_tfidf, Xfull_sty])\n",
        "Xtest = hstack([Xtest_tfidf, Xtest_sty])\n",
        "clf.fit(Xfull, y)\n",
        "ptest = clf.predict_proba(Xtest)\n",
        "dfp = pd.DataFrame(ptest, columns=clf.classes_).reindex(columns=classes).fillna(0.0).values\n",
        "dfp = np.clip(dfp, 1e-15, 1-1e-15); dfp /= dfp.sum(axis=1, keepdims=True)\n",
        "pd.DataFrame(dfp, columns=classes).to_csv('test_stylo_word_lr.csv', index=False)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word+Stylo LR Fold 1: 1.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word+Stylo LR Fold 2: 1.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word+Stylo LR Fold 3: 1.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word+Stylo LR Fold 4: 1.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word+Stylo LR Fold 5: 1.0986\nWord+Stylo LR OOF: 1.0986122886681096\n"
          ]
        }
      ]
    },
    {
      "id": "10c2448b-24eb-4126-b811-c000e0ec1046",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 23 \u2014 Strong Word NB-SVM (fix the broken one)\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def log_count_ratio(X, yb, alpha=1.0):\n",
        "    pos = np.asarray(X[yb==1].sum(axis=0)).ravel() + alpha\n",
        "    neg = np.asarray(X[yb==0].sum(axis=0)).ravel() + alpha\n",
        "    r = np.log(pos/neg); r[~np.isfinite(r)] = 0.0\n",
        "    return r\n",
        "\n",
        "def odds_norm(P, eps=1e-12):\n",
        "    P = np.clip(P, eps, 1-eps); odds = P/(1-P)\n",
        "    Pn = odds/(odds.sum(axis=1, keepdims=True)+eps)\n",
        "    return Pn / Pn.sum(axis=1, keepdims=True)\n",
        "\n",
        "vec_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_df=0.9, binary=True)\n",
        "clf_params = dict(solver='liblinear', penalty='l2', C=4.0, max_iter=3000, tol=1e-4)\n",
        "\n",
        "oof = np.zeros((len(train), 3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y), 1):\n",
        "    vec = CountVectorizer(**vec_params)\n",
        "    Xtr = vec.fit_transform(train['text'].iloc[tr]); Xva = vec.transform(train['text'].iloc[va])\n",
        "    Pva = np.zeros((len(va), 3))\n",
        "    for c in range(3):\n",
        "        yb = (y[tr]==c).astype(int)\n",
        "        r = log_count_ratio(Xtr, yb, alpha=1.0)\n",
        "        clf = LogisticRegression(**clf_params, random_state=42+c)\n",
        "        clf.fit(Xtr.multiply(csr_matrix(r)), yb)\n",
        "        Pva[:, c] = clf.predict_proba(Xva.multiply(csr_matrix(r)))[:, 1]\n",
        "    Pva = odds_norm(Pva); oof[va] = Pva\n",
        "    s = log_loss(y[va], Pva); scores.append(s); print(f'Word NB-SVM Fold {f}: {s:.4f}')\n",
        "print('Word NB-SVM OOF:', float(np.mean(scores)))\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_word_nbsvm.csv', index=False)\n",
        "\n",
        "vec_full = CountVectorizer(**vec_params)\n",
        "Xfull = vec_full.fit_transform(train['text']); Xtest = vec_full.transform(test['text'])\n",
        "Ptest = np.zeros((len(test), 3))\n",
        "for c in range(3):\n",
        "    yb = (y==c).astype(int)\n",
        "    r = log_count_ratio(Xfull, yb, alpha=1.0)\n",
        "    clf = LogisticRegression(**clf_params, random_state=999+c)\n",
        "    clf.fit(Xfull.multiply(csr_matrix(r)), yb)\n",
        "    Ptest[:, c] = clf.predict_proba(Xtest.multiply(csr_matrix(r)))[:, 1]\n",
        "Ptest = odds_norm(Ptest)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_word_nbsvm.csv', index=False)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 1: 0.5832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 2: 0.5586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 3: 0.5976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 4: 0.5428\n"
          ]
        }
      ]
    },
    {
      "id": "bb34be51-af7d-426b-896f-582335d627a6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 24 \u2014 10-fold Uncalibrated Char_wb LR (for stability, wide ngrams, low min_df)\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "vec_params = dict(analyzer='char_wb', ngram_range=(2,6), lowercase=False,\n",
        "                  sublinear_tf=True, min_df=1, max_df=0.98, max_features=400_000)\n",
        "\n",
        "oof = np.zeros((len(train), 3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y), 1):\n",
        "    vec = TfidfVectorizer(**vec_params)\n",
        "    Xtr = vec.fit_transform(train['text'].iloc[tr])\n",
        "    Xva = vec.transform(train['text'].iloc[va])\n",
        "\n",
        "    clf = LogisticRegression(solver='lbfgs', C=4.0, max_iter=3000, tol=1e-4,\n",
        "                             random_state=42, n_jobs=1)\n",
        "    clf.fit(Xtr, y[tr])\n",
        "    p = clf.predict_proba(Xva)\n",
        "    oof[va] = p\n",
        "    s = log_loss(y[va], p); scores.append(s)\n",
        "    print(f'10-fold Uncal Char_wb LR Fold {f}: {s:.4f}')\n",
        "\n",
        "sc = float(np.mean(scores)); print(f'10-fold Uncal Char_wb LR OOF: {sc:.4f}')\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_10fold_uncal_char_wb_lr.csv', index=False)\n",
        "\n",
        "# Full fit for test (average 10-fold predictions for bagging)\n",
        "skf_test = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "Ptest = np.zeros((len(test), 3))\n",
        "for f,(tr,va) in enumerate(skf_test.split(train['text'], y), 1):\n",
        "    vec = TfidfVectorizer(**vec_params)\n",
        "    Xtr = vec.fit_transform(train['text'].iloc[tr])\n",
        "    Xtest_f = vec.transform(test['text'])\n",
        "    clf = LogisticRegression(solver='lbfgs', C=4.0, max_iter=3000, tol=1e-4,\n",
        "                             random_state=42 + f, n_jobs=1)\n",
        "    clf.fit(Xtr, y[tr])\n",
        "    Ptest += clf.predict_proba(Xtest_f)\n",
        "Ptest /= 10.0  # average bagged predictions\n",
        "Ptest = np.clip(Ptest, 1e-15, 1-1e-15)\n",
        "Ptest /= Ptest.sum(axis=1, keepdims=True)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_10fold_uncal_char_wb_lr.csv', index=False)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold Uncal Char_wb LR Fold 1: 0.4062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold Uncal Char_wb LR Fold 2: 0.4029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold Uncal Char_wb LR Fold 3: 0.3967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold Uncal Char_wb LR Fold 4: 0.4097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold Uncal Char_wb LR Fold 5: 0.4305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold Uncal Char_wb LR Fold 6: 0.4137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold Uncal Char_wb LR Fold 7: 0.4188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold Uncal Char_wb LR Fold 8: 0.4012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold Uncal Char_wb LR Fold 9: 0.4058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold Uncal Char_wb LR Fold 10: 0.3968\n10-fold Uncal Char_wb LR OOF: 0.4082\n"
          ]
        }
      ]
    },
    {
      "id": "cf501c3f-c171-47ef-b41c-40be32fc83ac",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 25 \u2014 Generate test predictions for 10-fold Char LR (full fit only, CV already done)\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "\n",
        "vec_params = dict(analyzer='char', ngram_range=(3,7), lowercase=False,\n",
        "                  sublinear_tf=True, min_df=1, max_df=0.98, max_features=400_000)\n",
        "\n",
        "# Full fit for test (single fit to save time)\n",
        "vec_full = TfidfVectorizer(**vec_params)\n",
        "Xfull = vec_full.fit_transform(train['text'])\n",
        "Xtest = vec_full.transform(test['text'])\n",
        "clf_full = LogisticRegression(solver='lbfgs', C=4.0, max_iter=3000, tol=1e-4,\n",
        "                              random_state=42, n_jobs=1)\n",
        "clf_full.fit(Xfull, y)\n",
        "ptest = clf_full.predict_proba(Xtest)\n",
        "pd.DataFrame(ptest, columns=classes).to_csv('test_10fold_char_lr.csv', index=False)\n",
        "print('Test predictions saved to test_10fold_char_lr.csv')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold Char LR Fold 1: 0.4327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold Char LR Fold 2: 0.4260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold Char LR Fold 3: 0.4240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold Char LR Fold 4: 0.4299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-fold Char LR Fold 5: 0.4460\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     22\u001b[39m Xva = vec.transform(train[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].iloc[va])\n\u001b[32m     24\u001b[39m clf = LogisticRegression(solver=\u001b[33m'\u001b[39m\u001b[33mlbfgs\u001b[39m\u001b[33m'\u001b[39m, C=\u001b[32m4.0\u001b[39m, max_iter=\u001b[32m3000\u001b[39m, tol=\u001b[32m1e-4\u001b[39m,\n\u001b[32m     25\u001b[39m                          random_state=\u001b[32m42\u001b[39m, n_jobs=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m p = clf.predict_proba(Xva)\n\u001b[32m     28\u001b[39m oof[va] = p\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/linear_model/_logistic.py:1384\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1382\u001b[39m     n_threads = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1409\u001b[39m fold_coefs_, _, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n\u001b[32m   1410\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, \u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/joblib/parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/joblib/parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/utils/parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/linear_model/_logistic.py:459\u001b[39m, in \u001b[36m_logistic_regression_path\u001b[39m\u001b[34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[39m\n\u001b[32m    455\u001b[39m l2_reg_strength = \u001b[32m1.0\u001b[39m / (C * sw_sum)\n\u001b[32m    456\u001b[39m iprint = [-\u001b[32m1\u001b[39m, \u001b[32m50\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m101\u001b[39m][\n\u001b[32m    457\u001b[39m     np.searchsorted(np.array([\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m]), verbose)\n\u001b[32m    458\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m opt_res = \u001b[43moptimize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mL-BFGS-B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxiter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default is 20\u001b[39;49;00m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgtol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mftol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_get_additional_lbfgs_options_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43miprint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m n_iter_i = _check_optimize_result(\n\u001b[32m    474\u001b[39m     solver,\n\u001b[32m    475\u001b[39m     opt_res,\n\u001b[32m    476\u001b[39m     max_iter,\n\u001b[32m    477\u001b[39m     extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[32m    478\u001b[39m )\n\u001b[32m    479\u001b[39m w0, loss = opt_res.x, opt_res.fun\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/scipy/optimize/_minimize.py:784\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[39m\n\u001b[32m    781\u001b[39m     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[32m    782\u001b[39m                              **options)\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33ml-bfgs-b\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     res = \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33mtnc\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    787\u001b[39m     res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[32m    788\u001b[39m                         **options)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/scipy/optimize/_lbfgsb_py.py:461\u001b[39m, in \u001b[36m_minimize_lbfgsb\u001b[39m\u001b[34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, workers, **unknown_options)\u001b[39m\n\u001b[32m    459\u001b[39m g = g.astype(np.float64)\n\u001b[32m    460\u001b[39m \u001b[38;5;66;03m# x, f, g, wa, iwa, task, csave, lsave, isave, dsave = \\\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m \u001b[43m_lbfgsb\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetulb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_bnd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper_bnd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m               \u001b[49m\u001b[43miwa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlsave\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misave\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdsave\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mln_task\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m3\u001b[39m:\n\u001b[32m    465\u001b[39m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[32m    466\u001b[39m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[32m    467\u001b[39m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[32m    468\u001b[39m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[32m    469\u001b[39m     f, g = func_and_grad(x)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "40a35691-c8d6-4457-a38e-68bae8cf5d3f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train=pd.read_csv('train.csv'); test=pd.read_csv('test.csv')\n",
        "train['text']=train['text'].fillna(''); test['text']=test['text'].fillna('')\n",
        "le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def log_count_ratio(X, yb, alpha=0.5):\n",
        "    pos=np.asarray(X[yb==1].sum(0)).ravel()+alpha\n",
        "    neg=np.asarray(X[yb==0].sum(0)).ravel()+alpha\n",
        "    r=np.log(pos/neg); r[~np.isfinite(r)]=0.0\n",
        "    return r\n",
        "\n",
        "def odds_norm(P, eps=1e-12):\n",
        "    P=np.clip(P,eps,1-eps); odds=P/(1-P)\n",
        "    Pn=odds/(odds.sum(axis=1, keepdims=True)+eps)\n",
        "    return Pn / Pn.sum(axis=1, keepdims=True)\n",
        "\n",
        "vec_params=dict(analyzer='word', ngram_range=(1,3), lowercase=True, min_df=3, max_df=0.95, binary=True, max_features=200000)\n",
        "clf_params=dict(solver='liblinear', penalty='l2', C=6.0, max_iter=3000, tol=1e-4)\n",
        "\n",
        "oof=np.zeros((len(train),3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "    vec=CountVectorizer(**vec_params)\n",
        "    Xtr=vec.fit_transform(train['text'].iloc[tr]); Xva=vec.transform(train['text'].iloc[va])\n",
        "    Pva=np.zeros((len(va),3))\n",
        "    for c in range(3):\n",
        "        yb=(y[tr]==c).astype(int)\n",
        "        r=log_count_ratio(Xtr, yb, alpha=0.5)\n",
        "        clf=LogisticRegression(**clf_params, random_state=42+c)\n",
        "        clf.fit(Xtr.multiply(csr_matrix(r)), yb)\n",
        "        Pva[:,c]=clf.predict_proba(Xva.multiply(csr_matrix(r)))[:,1]\n",
        "    Pva=odds_norm(Pva); oof[va]=Pva\n",
        "    s=log_loss(y[va], Pva); scores.append(s); print(f'Word NB-SVM Fold {f}: {s:.4f}')\n",
        "print('Word NB-SVM OOF:', float(np.mean(scores)))\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_word_nbsvm.csv', index=False)\n",
        "\n",
        "vec_full=CountVectorizer(**vec_params)\n",
        "Xfull=vec_full.fit_transform(train['text']); Xtest=vec_full.transform(test['text'])\n",
        "Ptest=np.zeros((len(test),3))\n",
        "for c in range(3):\n",
        "    yb=(y==c).astype(int)\n",
        "    r=log_count_ratio(Xfull, yb, alpha=0.5)\n",
        "    clf=LogisticRegression(**clf_params, random_state=999+c)\n",
        "    clf.fit(Xfull.multiply(csr_matrix(r)), yb)\n",
        "    Ptest[:,c]=clf.predict_proba(Xtest.multiply(csr_matrix(r)))[:,1]\n",
        "Ptest=odds_norm(Ptest)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_word_nbsvm.csv', index=False)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 1: 0.6271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 2: 0.5958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 3: 0.6400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 4: 0.5812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 5: 0.6242\nWord NB-SVM OOF: 0.613671142726013\n"
          ]
        }
      ]
    },
    {
      "id": "e09db632-9778-4612-8d9c-61f673504498",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, re, string\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, MaxAbsScaler\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "train=pd.read_csv('train.csv'); test=pd.read_csv('test.csv')\n",
        "train['text']=train['text'].fillna(''); test['text']=test['text'].fillna('')\n",
        "le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def stylo(series):\n",
        "    rows=[]\n",
        "    for t in series:\n",
        "        L=len(t); words=t.split(); wc=len(words)\n",
        "        letters=sum(c.isalpha() for c in t) or 1\n",
        "        sents=[s for s in re.split(r'[.!?]+', t) if s.strip()]; sc=len(sents) or 1\n",
        "        rows.append([\n",
        "            (sum(c in string.punctuation for c in t)/L) if L else 0.0,\n",
        "            t.count('!')/L if L else 0.0,\n",
        "            t.count(';')/L if L else 0.0,\n",
        "            (sum(c.isdigit() for c in t)/L) if L else 0.0,\n",
        "            (sum(c.isupper() for c in t)/letters),\n",
        "            (sum(len(w) for w in words)/wc) if wc else 0.0,\n",
        "            (wc/sc) if sc else 0.0,\n",
        "            wc\n",
        "        ])\n",
        "    return np.array(rows, dtype=float)\n",
        "\n",
        "train_sty=stylo(train['text']); test_sty=stylo(test['text'])\n",
        "\n",
        "tfidf_params=dict(analyzer='word', ngram_range=(1,2), lowercase=True, sublinear_tf=True,\n",
        "                  min_df=2, max_df=0.95, max_features=200_000)\n",
        "clf=LogisticRegression(solver='lbfgs', C=4.0, max_iter=3000, tol=1e-4, random_state=42)\n",
        "\n",
        "oof=np.zeros((len(train),3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "    vec=TfidfVectorizer(**tfidf_params)\n",
        "    Xtr_tfidf=vec.fit_transform(train['text'].iloc[tr]); Xva_tfidf=vec.transform(train['text'].iloc[va])\n",
        "    scaler=MaxAbsScaler().fit(train_sty[tr])\n",
        "    Xtr_sty=csr_matrix(scaler.transform(train_sty[tr])); Xva_sty=csr_matrix(scaler.transform(train_sty[va]))\n",
        "    Xtr=hstack([Xtr_tfidf, Xtr_sty]); Xva=hstack([Xva_tfidf, Xva_sty])\n",
        "    clf.fit(Xtr, y[tr])\n",
        "    p=clf.predict_proba(Xva)             # already aligned with y\u2019s label order\n",
        "    oof[va]=p\n",
        "    s=log_loss(y[va], p); scores.append(s); print(f'Stylo+Word LR Fold {f}: {s:.4f}')\n",
        "print('Stylo+Word LR OOF:', float(np.mean(scores)))\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_stylo_word_lr.csv', index=False)\n",
        "\n",
        "vec_full=TfidfVectorizer(**tfidf_params)\n",
        "Xfull_tfidf=vec_full.fit_transform(train['text']); Xtest_tfidf=vec_full.transform(test['text'])\n",
        "scaler_full=MaxAbsScaler().fit(train_sty)\n",
        "Xfull_sty=csr_matrix(scaler_full.transform(train_sty)); Xtest_sty=csr_matrix(scaler_full.transform(test_sty))\n",
        "Xfull=hstack([Xfull_tfidf, Xfull_sty]); Xtest=hstack([Xtest_tfidf, Xtest_sty])\n",
        "clf.fit(Xfull, y); ptest=clf.predict_proba(Xtest)\n",
        "pd.DataFrame(ptest, columns=classes).to_csv('test_stylo_word_lr.csv', index=False)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stylo+Word LR Fold 1: 0.4600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stylo+Word LR Fold 2: 0.4684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stylo+Word LR Fold 3: 0.4766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stylo+Word LR Fold 4: 0.4528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stylo+Word LR Fold 5: 0.4541\nStylo+Word LR OOF: 0.46236992048108905\n"
          ]
        }
      ]
    },
    {
      "id": "b093760b-d28a-4590-95ca-10a1a431f429",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "train=pd.read_csv('train.csv'); le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "model_files=[\n",
        "    ('oof_10fold_uncal_char_wb_lr.csv','test_10fold_uncal_char_wb_lr.csv'),\n",
        "    ('oof_char_lr.csv','test_char_lr.csv'),\n",
        "    ('oof_calsvc_char.csv','test_calsvc_char.csv'),\n",
        "    ('oof_word_nbsvm.csv','test_word_nbsvm.csv'),\n",
        "    ('oof_stylo_word_lr.csv','test_stylo_word_lr.csv')\n",
        "]\n",
        "\n",
        "meta_train=np.hstack([pd.read_csv(o)[classes].values for o,_ in model_files])\n",
        "meta_test=np.hstack([pd.read_csv(t)[classes].values for _,t in model_files])\n",
        "\n",
        "oof=np.zeros((len(train),3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(meta_train,y),1):\n",
        "    meta=LogisticRegression(solver='lbfgs', C=1.5, max_iter=2000, random_state=2025)\n",
        "    meta.fit(meta_train[tr], y[tr])\n",
        "    p=meta.predict_proba(meta_train[va]); oof[va]=p\n",
        "    s=log_loss(y[va], p); scores.append(s); print(f'Meta-LR Fold {f}: {s:.4f}')\n",
        "meta_sc=float(np.mean(scores)); print('Meta-LR OOF:', meta_sc)\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_stacked_meta_lr.csv', index=False)\n",
        "\n",
        "meta.fit(meta_train, y)\n",
        "ptest=meta.predict_proba(meta_test)\n",
        "ptest=np.clip(ptest,1e-15,1-1e-15); ptest/=ptest.sum(axis=1, keepdims=True)\n",
        "pd.DataFrame(ptest, columns=classes).to_csv('test_stacked_meta_lr.csv', index=False)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR Fold 1: 0.3551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR Fold 2: 0.3549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR Fold 3: 0.3712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR Fold 4: 0.3511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR Fold 5: 0.3561\nMeta-LR OOF: 0.35768948685097157\n"
          ]
        }
      ]
    },
    {
      "id": "2fd934a7-d487-4188-a4f2-0c2735dd7231",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "train=pd.read_csv('train.csv'); test=pd.read_csv('test.csv')\n",
        "train['text']=train['text'].fillna(''); test['text']=test['text'].fillna('')\n",
        "le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Fallback: Strong Word TF-IDF LR (since NB-SVM configs failing, use this for diversity ~0.45 OOF)\n",
        "word_lr_pipe = TfidfVectorizer(analyzer='word', ngram_range=(1,3), lowercase=True, sublinear_tf=True, min_df=2, max_df=0.95, max_features=200000)\n",
        "X_word = word_lr_pipe.fit_transform(train['text']); X_word_test = word_lr_pipe.transform(test['text'])\n",
        "lr = LogisticRegression(solver='lbfgs', C=4.0, max_iter=3000, tol=1e-4, random_state=42, n_jobs=1)\n",
        "oof_word_lr = np.zeros((len(train),3)); scores_word=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "    lr.fit(X_word[tr], y[tr])\n",
        "    p = lr.predict_proba(X_word[va])\n",
        "    oof_word_lr[va] = p\n",
        "    s = log_loss(y[va], p); scores_word.append(s); print(f'Word TF-IDF LR Fold {f}: {s:.4f}')\n",
        "sc_word = float(np.mean(scores_word)); print('Word TF-IDF LR OOF:', sc_word)\n",
        "pd.DataFrame(oof_word_lr, columns=classes).to_csv('oof_word_lr_strong.csv', index=False)\n",
        "lr.fit(X_word, y)\n",
        "ptest_word = lr.predict_proba(X_word_test)\n",
        "pd.DataFrame(ptest_word, columns=classes).to_csv('test_word_lr_strong.csv', index=False)\n",
        "\n",
        "print('Fallback Word LR OOF:', sc_word)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 1: 0.4859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 2: 0.4912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 3: 0.4947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 4: 0.4811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word TF-IDF LR Fold 5: 0.4850\nWord TF-IDF LR OOF: 0.48757614282367695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fallback Word LR OOF: 0.48757614282367695\n"
          ]
        }
      ]
    },
    {
      "id": "18fb35a5-a356-424a-b2e3-338399ea3d04",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "train=pd.read_csv('train.csv'); le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "model_files=[\n",
        "    ('oof_10fold_uncal_char_wb_lr.csv','test_10fold_uncal_char_wb_lr.csv'),\n",
        "    ('oof_char_lr.csv','test_char_lr.csv'),\n",
        "    ('oof_calsvc_char.csv','test_calsvc_char.csv'),\n",
        "    ('oof_stylo_word_lr.csv','test_stylo_word_lr.csv'),\n",
        "    ('oof_word_lr_strong.csv','test_word_lr_strong.csv')\n",
        "]\n",
        "\n",
        "meta_train=np.hstack([pd.read_csv(o)[classes].values for o,_ in model_files])\n",
        "meta_test=np.hstack([pd.read_csv(t)[classes].values for _,t in model_files])\n",
        "\n",
        "oof=np.zeros((len(train),3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(meta_train,y),1):\n",
        "    meta=LogisticRegression(solver='lbfgs', C=1.5, max_iter=2000, random_state=2025)\n",
        "    meta.fit(meta_train[tr], y[tr])\n",
        "    p=meta.predict_proba(meta_train[va]); oof[va]=p\n",
        "    s=log_loss(y[va], p); scores.append(s); print(f'Meta-LR Fold {f}: {s:.4f}')\n",
        "meta_sc=float(np.mean(scores)); print('Meta-LR OOF:', meta_sc)\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_stacked_meta_lr.csv', index=False)\n",
        "\n",
        "meta.fit(meta_train, y)\n",
        "ptest=meta.predict_proba(meta_test)\n",
        "ptest=np.clip(ptest,1e-15,1-1e-15); ptest/=ptest.sum(axis=1, keepdims=True)\n",
        "pd.DataFrame(ptest, columns=classes).to_csv('test_stacked_meta_lr.csv', index=False)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR Fold 1: 0.3583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR Fold 2: 0.3639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR Fold 3: 0.3754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR Fold 4: 0.3564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR Fold 5: 0.3613\nMeta-LR OOF: 0.3630690794929753\n"
          ]
        }
      ]
    },
    {
      "id": "62f1de3b-18b5-447b-b040-54f9990e8462",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "train=pd.read_csv('train.csv'); test=pd.read_csv('test.csv')\n",
        "train['text']=train['text'].fillna(''); test['text']=test['text'].fillna('')\n",
        "le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "vec_params=dict(analyzer='char', ngram_range=(3,7), lowercase=False,\n",
        "                sublinear_tf=True, min_df=2, max_df=0.98, max_features=300_000)\n",
        "clf=LogisticRegression(solver='lbfgs', C=4.0, max_iter=3000, tol=1e-4, random_state=42, n_jobs=1)\n",
        "\n",
        "oof=np.zeros((len(train),3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "    vec=TfidfVectorizer(**vec_params)\n",
        "    Xtr=vec.fit_transform(train['text'].iloc[tr]); Xva=vec.transform(train['text'].iloc[va])\n",
        "    clf.fit(Xtr, y[tr])\n",
        "    p=clf.predict_proba(Xva)\n",
        "    oof[va]=p\n",
        "    s=log_loss(y[va], p); scores.append(s); print(f'Char Variant LR Fold {f}: {s:.4f}')\n",
        "sc=float(np.mean(scores)); print('Char Variant LR OOF:', sc)\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_char_variant.csv', index=False)\n",
        "\n",
        "# Full fit for test\n",
        "vec_full=TfidfVectorizer(**vec_params)\n",
        "Xfull=vec_full.fit_transform(train['text']); Xtest=vec_full.transform(test['text'])\n",
        "clf.fit(Xfull, y)\n",
        "ptest=clf.predict_proba(Xtest)\n",
        "pd.DataFrame(ptest, columns=classes).to_csv('test_char_variant.csv', index=False)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char Variant LR Fold 1: 0.4389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char Variant LR Fold 2: 0.4409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char Variant LR Fold 3: 0.4489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char Variant LR Fold 4: 0.4400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char Variant LR Fold 5: 0.4278\nChar Variant LR OOF: 0.43930143225025275\n"
          ]
        }
      ]
    },
    {
      "id": "1ef78c0b-ecda-4d75-93a0-8e6d3ac7402c",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "train=pd.read_csv('train.csv'); le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "model_files=[\n",
        "    ('oof_10fold_uncal_char_wb_lr.csv','test_10fold_uncal_char_wb_lr.csv'),\n",
        "    ('oof_char_lr.csv','test_char_lr.csv'),\n",
        "    ('oof_calsvc_char.csv','test_calsvc_char.csv'),\n",
        "    ('oof_stylo_word_lr.csv','test_stylo_word_lr.csv'),\n",
        "    ('oof_word_lr_strong.csv','test_word_lr_strong.csv'),\n",
        "    ('oof_char_variant.csv','test_char_variant.csv')\n",
        "]\n",
        "\n",
        "meta_train=np.hstack([pd.read_csv(o)[classes].values for o,_ in model_files])\n",
        "meta_test=np.hstack([pd.read_csv(t)[classes].values for _,t in model_files])\n",
        "\n",
        "oof=np.zeros((len(train),3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(meta_train,y),1):\n",
        "    meta=LogisticRegression(solver='lbfgs', C=1.5, max_iter=2000, random_state=2025)\n",
        "    meta.fit(meta_train[tr], y[tr])\n",
        "    p=meta.predict_proba(meta_train[va]); oof[va]=p\n",
        "    s=log_loss(y[va], p); scores.append(s); print(f'Meta-LR Fold {f}: {s:.4f}')\n",
        "meta_sc=float(np.mean(scores)); print('Meta-LR OOF:', meta_sc)\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_stacked_meta_lr.csv', index=False)\n",
        "\n",
        "meta.fit(meta_train, y)\n",
        "ptest=meta.predict_proba(meta_test)\n",
        "ptest=np.clip(ptest,1e-15,1-1e-15); ptest/=ptest.sum(axis=1, keepdims=True)\n",
        "pd.DataFrame(ptest, columns=classes).to_csv('test_stacked_meta_lr.csv', index=False)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR Fold 1: 0.3565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR Fold 2: 0.3595\nMeta-LR Fold 3: 0.3732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR Fold 4: 0.3561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR Fold 5: 0.3574\nMeta-LR OOF: 0.36055009858050163\n"
          ]
        }
      ]
    },
    {
      "id": "ffc0c5bc-c5cf-4c51-92de-cef9e5759294",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train=pd.read_csv('train.csv'); test=pd.read_csv('test.csv')\n",
        "train['text']=train['text'].fillna(''); test['text']=test['text'].fillna('')\n",
        "le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def log_count_ratio(X, yb, alpha=1.0):\n",
        "    pos=np.asarray(X[yb==1].sum(0)).ravel()+alpha\n",
        "    neg=np.asarray(X[yb==0].sum(0)).ravel()+alpha\n",
        "    r=np.log(pos/neg); r[~np.isfinite(r)]=0.0\n",
        "    return r\n",
        "\n",
        "def odds_norm(P, eps=1e-12):\n",
        "    P=np.clip(P,eps,1-eps); odds=P/(1-P)\n",
        "    Pn=odds/(odds.sum(axis=1, keepdims=True)+eps)\n",
        "    return Pn / Pn.sum(axis=1, keepdims=True)\n",
        "\n",
        "vec_params=dict(analyzer='char_wb', ngram_range=(2,6), lowercase=False, min_df=1, max_df=0.99, binary=False, max_features=None)\n",
        "clf_params=dict(solver='liblinear', penalty='l2', C=6.0, max_iter=3000, tol=1e-4)\n",
        "\n",
        "oof=np.zeros((len(train),3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "    vec=CountVectorizer(**vec_params)\n",
        "    Xtr=vec.fit_transform(train['text'].iloc[tr]); Xva=vec.transform(train['text'].iloc[va])\n",
        "    Pva=np.zeros((len(va),3))\n",
        "    for c in range(3):\n",
        "        yb=(y[tr]==c).astype(int)\n",
        "        r=log_count_ratio(Xtr, yb, alpha=1.0)\n",
        "        clf=LogisticRegression(**clf_params, random_state=42+c)\n",
        "        clf.fit(Xtr.multiply(csr_matrix(r)), yb)\n",
        "        Pva[:,c]=clf.predict_proba(Xva.multiply(csr_matrix(r)))[:,1]\n",
        "    Pva=odds_norm(Pva); oof[va]=Pva\n",
        "    s=log_loss(y[va], Pva); scores.append(s); print(f'Char_wb NB-SVM Fold {f}: {s:.4f}')\n",
        "print('Char_wb NB-SVM OOF:', float(np.mean(scores)))\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_char_wb_nbsvm.csv', index=False)\n",
        "\n",
        "# Full fit -> test\n",
        "vec_full=CountVectorizer(**vec_params)\n",
        "Xfull=vec_full.fit_transform(train['text']); Xtest=vec_full.transform(test['text'])\n",
        "Ptest=np.zeros((len(test),3))\n",
        "for c in range(3):\n",
        "    yb=(y==c).astype(int)\n",
        "    r=log_count_ratio(Xfull, yb, alpha=1.0)\n",
        "    clf=LogisticRegression(**clf_params, random_state=999+c)\n",
        "    clf.fit(Xfull.multiply(csr_matrix(r)), yb)\n",
        "    Ptest[:,c]=clf.predict_proba(Xtest.multiply(csr_matrix(r)))[:,1]\n",
        "Ptest=odds_norm(Ptest)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_char_wb_nbsvm.csv', index=False)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char_wb NB-SVM Fold 1: 0.9719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char_wb NB-SVM Fold 2: 0.9026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char_wb NB-SVM Fold 3: 1.0630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char_wb NB-SVM Fold 4: 0.9351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char_wb NB-SVM Fold 5: 0.9420\nChar_wb NB-SVM OOF: 0.9629195888044508\n"
          ]
        }
      ]
    },
    {
      "id": "c8d0a677-aaa1-41d2-907f-8d2528ac72f6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "\n",
        "# Seed bagging for top models: 3 seeds on full data, average test preds\n",
        "n_seeds = 3\n",
        "seeds = [42, 123, 2025]\n",
        "\n",
        "# 1. Bag 10fold char_wb LR (your best: analyzer='char_wb' (2,6), C=4.0)\n",
        "vec_params_char_wb = dict(analyzer='char_wb', ngram_range=(2,6), lowercase=False,\n",
        "                          sublinear_tf=True, min_df=1, max_df=0.98, max_features=400_000)\n",
        "Ptest_char_wb_bagged = np.zeros((len(test), 3))\n",
        "for seed in seeds:\n",
        "    clf = LogisticRegression(solver='lbfgs', C=4.0, max_iter=3000, tol=1e-4, random_state=seed, n_jobs=1)\n",
        "    vec = TfidfVectorizer(**vec_params_char_wb)\n",
        "    Xfull = vec.fit_transform(train['text']); Xtest_f = vec.transform(test['text'])\n",
        "    clf.fit(Xfull, y)\n",
        "    Ptest_char_wb_bagged += clf.predict_proba(Xtest_f)\n",
        "Ptest_char_wb_bagged /= n_seeds\n",
        "Ptest_char_wb_bagged = np.clip(Ptest_char_wb_bagged, 1e-15, 1-1e-15)\n",
        "Ptest_char_wb_bagged /= Ptest_char_wb_bagged.sum(axis=1, keepdims=True)\n",
        "pd.DataFrame(Ptest_char_wb_bagged, columns=classes).to_csv('test_10fold_uncal_char_wb_lr_bagged.csv', index=False)\n",
        "\n",
        "# 2. Bag char_variant LR (char (3,7), C=4.0)\n",
        "vec_params_char = dict(analyzer='char', ngram_range=(3,7), lowercase=False,\n",
        "                       sublinear_tf=True, min_df=2, max_df=0.98, max_features=300_000)\n",
        "Ptest_char_bagged = np.zeros((len(test), 3))\n",
        "for seed in seeds:\n",
        "    clf = LogisticRegression(solver='lbfgs', C=4.0, max_iter=3000, tol=1e-4, random_state=seed, n_jobs=1)\n",
        "    vec = TfidfVectorizer(**vec_params_char)\n",
        "    Xfull = vec.fit_transform(train['text']); Xtest_f = vec.transform(test['text'])\n",
        "    clf.fit(Xfull, y)\n",
        "    Ptest_char_bagged += clf.predict_proba(Xtest_f)\n",
        "Ptest_char_bagged /= n_seeds\n",
        "Ptest_char_bagged = np.clip(Ptest_char_bagged, 1e-15, 1-1e-15)\n",
        "Ptest_char_bagged /= Ptest_char_bagged.sum(axis=1, keepdims=True)\n",
        "pd.DataFrame(Ptest_char_bagged, columns=classes).to_csv('test_char_variant_bagged.csv', index=False)\n",
        "\n",
        "# 3. Bag Calibrated LinearSVC (reuse your vec_params from cell 15, light 3-fold inner CV per seed)\n",
        "vec_params_svc = dict(analyzer='char_wb', ngram_range=(2,5), lowercase=False,\n",
        "                      sublinear_tf=True, min_df=3, max_df=0.98, max_features=250_000)\n",
        "svc_params = dict(C=0.5, loss='squared_hinge', dual='auto', max_iter=3000, tol=1e-4)\n",
        "inner_cv_splits = 3\n",
        "Ptest_svc_bagged = np.zeros((len(test), 3))\n",
        "for seed in seeds:\n",
        "    vec = TfidfVectorizer(**vec_params_svc)\n",
        "    Xfull = vec.fit_transform(train['text']); Xtest_f = vec.transform(test['text'])\n",
        "    Ptest_seed = np.zeros((len(test), 3))\n",
        "    for c in range(3):\n",
        "        yb = (y == c).astype(int)\n",
        "        skf_inner = StratifiedKFold(n_splits=inner_cv_splits, shuffle=True, random_state=seed + c)\n",
        "        F_cal = []; z_cal = []\n",
        "        indices = np.arange(len(train))\n",
        "        for i_tr, i_va in skf_inner.split(indices, yb):\n",
        "            svc = LinearSVC(**svc_params, random_state=seed + c)\n",
        "            svc.fit(Xfull[i_tr], yb[i_tr])\n",
        "            s = svc.decision_function(Xfull[i_va])\n",
        "            if s.ndim > 1: s = s[:, 0]\n",
        "            F_cal.append(s.reshape(-1, 1)); z_cal.append(yb[i_va])\n",
        "        F_cal = np.vstack(F_cal); z_cal = np.concatenate(z_cal)\n",
        "        platt = LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000, random_state=seed + c*10)\n",
        "        platt.fit(F_cal, z_cal)\n",
        "        svc_final = LinearSVC(**svc_params, random_state=seed + c)\n",
        "        svc_final.fit(Xfull, yb)\n",
        "        s_test = svc_final.decision_function(Xtest_f)\n",
        "        if s_test.ndim > 1: s_test = s_test[:, 0]\n",
        "        Ptest_seed[:, c] = platt.predict_proba(s_test.reshape(-1, 1))[:, 1]\n",
        "    Ptest_seed = odds_norm(Ptest_seed)  # define odds_norm if not already\n",
        "    Ptest_svc_bagged += Ptest_seed\n",
        "Ptest_svc_bagged /= n_seeds\n",
        "Ptest_svc_bagged = np.clip(Ptest_svc_bagged, 1e-15, 1-1e-15)\n",
        "Ptest_svc_bagged /= Ptest_svc_bagged.sum(axis=1, keepdims=True)\n",
        "pd.DataFrame(Ptest_svc_bagged, columns=classes).to_csv('test_calsvc_bagged.csv', index=False)\n",
        "\n",
        "print('Seed bagging complete for top models. Use bagged test files in re-stack.')\n",
        "\n",
        "def odds_norm(P, eps=1e-9):\n",
        "    P = np.clip(P, eps, 1-eps)\n",
        "    odds = P / (1 - P)\n",
        "    return odds / (odds.sum(axis=1, keepdims=True) + eps)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed bagging complete for top models. Use bagged test files in re-stack.\n"
          ]
        }
      ]
    },
    {
      "id": "4a7829ea-a3e4-46ab-8407-c0545701ea34",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "train=pd.read_csv('train.csv'); le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Strong models only (OOF <0.46): exclude word_lr_strong 0.4876 and bad NB-SVM 0.96\n",
        "model_files=[\n",
        "    ('oof_10fold_uncal_char_wb_lr.csv','test_10fold_uncal_char_wb_lr_bagged.csv'),  # bagged test\n",
        "    ('oof_char_lr.csv','test_char_lr.csv'),\n",
        "    ('oof_calsvc_char.csv','test_calsvc_bagged.csv'),  # bagged test\n",
        "    ('oof_char_variant.csv','test_char_variant_bagged.csv'),  # bagged test\n",
        "    ('oof_stylo_word_lr.csv','test_stylo_word_lr.csv')\n",
        "]\n",
        "\n",
        "meta_train=np.hstack([pd.read_csv(o)[classes].values for o,_ in model_files])\n",
        "meta_test=np.hstack([pd.read_csv(t)[classes].values for _,t in model_files])\n",
        "\n",
        "# Tune meta-LR C over grid with 5-fold CV\n",
        "c_grid = [0.5, 0.75, 1.0, 1.5, 2.0]\n",
        "best_c = None; best_sc = float('inf')\n",
        "for c in c_grid:\n",
        "    oof_cv=np.zeros((len(train),3)); scores_cv=[]\n",
        "    for f,(tr,va) in enumerate(skf.split(meta_train,y),1):\n",
        "        meta=LogisticRegression(solver='lbfgs', C=c, max_iter=2000, random_state=2025)\n",
        "        meta.fit(meta_train[tr], y[tr])\n",
        "        p=meta.predict_proba(meta_train[va]); oof_cv[va]=p\n",
        "        s=log_loss(y[va], p); scores_cv.append(s)\n",
        "    sc_cv=float(np.mean(scores_cv))\n",
        "    print(f'Meta-LR C={c} CV OOF: {sc_cv:.4f}')\n",
        "    if sc_cv < best_sc:\n",
        "        best_sc = sc_cv; best_c = c\n",
        "\n",
        "print(f'Best meta-LR C: {best_c} with CV OOF: {best_sc:.4f}')\n",
        "\n",
        "# Fit final meta on full with best C\n",
        "meta=LogisticRegression(solver='lbfgs', C=best_c, max_iter=2000, random_state=2025)\n",
        "meta.fit(meta_train, y)\n",
        "ptest=meta.predict_proba(meta_test)\n",
        "ptest=np.clip(ptest,1e-15,1-1e-15); ptest/=ptest.sum(axis=1, keepdims=True)\n",
        "pd.DataFrame(ptest, columns=classes).to_csv('test_stacked_meta_lr.csv', index=False)\n",
        "\n",
        "# Compute final OOF with best C (for validation)\n",
        "oof_final=np.zeros((len(train),3)); scores_final=[]\n",
        "for f,(tr,va) in enumerate(skf.split(meta_train,y),1):\n",
        "    meta_cv=LogisticRegression(solver='lbfgs', C=best_c, max_iter=2000, random_state=2025)\n",
        "    meta_cv.fit(meta_train[tr], y[tr])\n",
        "    p=meta_cv.predict_proba(meta_train[va]); oof_final[va]=p\n",
        "    s=log_loss(y[va], p); scores_final.append(s); print(f'Final Meta-LR Fold {f}: {s:.4f}')\n",
        "final_sc=float(np.mean(scores_final)); print('Final Stacked Meta-LR OOF:', final_sc)\n",
        "pd.DataFrame(oof_final, columns=classes).to_csv('oof_stacked_meta_lr_final.csv', index=False)\n",
        "\n",
        "if final_sc <= 0.34:\n",
        "    print('Medal-ready! Copy test_stacked_meta_lr.csv to submission.csv and submit.')\n",
        "else:\n",
        "    print('Still >0.34; need more diversity or tuning.')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR C=0.5 CV OOF: 0.3648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR C=0.75 CV OOF: 0.3648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR C=1.0 CV OOF: 0.3647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR C=1.5 CV OOF: 0.3648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR C=2.0 CV OOF: 0.3647\nBest meta-LR C: 2.0 with CV OOF: 0.3647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Meta-LR Fold 1: 0.3630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Meta-LR Fold 2: 0.3631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Meta-LR Fold 3: 0.3794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Meta-LR Fold 4: 0.3607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Meta-LR Fold 5: 0.3574\nFinal Stacked Meta-LR OOF: 0.36472125242237086\nStill >0.34; need more diversity or tuning.\n"
          ]
        }
      ]
    },
    {
      "id": "31bd9fc7-9ce6-4eb3-835e-d38fceee8b45",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Word+Char hstack LR\n",
        "word_params = dict(analyzer='word', ngram_range=(1,3), lowercase=True,\n",
        "                   sublinear_tf=True, min_df=2, max_df=0.95, max_features=200_000)\n",
        "char_params = dict(analyzer='char_wb', ngram_range=(2,6), lowercase=False,\n",
        "                   sublinear_tf=True, min_df=2, max_df=0.98, max_features=400_000)\n",
        "c_grid = [3.0, 4.0, 5.0, 6.0]\n",
        "best_c = None; best_sc = float('inf'); best_oof = None\n",
        "for c in c_grid:\n",
        "    oof_c = np.zeros((len(train), 3)); scores_c = []\n",
        "    for f, (tr, va) in enumerate(skf.split(train['text'], y), 1):\n",
        "        vec_word = TfidfVectorizer(**word_params)\n",
        "        Xtr_word = vec_word.fit_transform(train['text'].iloc[tr])\n",
        "        Xva_word = vec_word.transform(train['text'].iloc[va])\n",
        "        vec_char = TfidfVectorizer(**char_params)\n",
        "        Xtr_char = vec_char.fit_transform(train['text'].iloc[tr])\n",
        "        Xva_char = vec_char.transform(train['text'].iloc[va])\n",
        "        Xtr = hstack([Xtr_word, Xtr_char])\n",
        "        Xva = hstack([Xva_word, Xva_char])\n",
        "        clf = LogisticRegression(solver='lbfgs', C=c, max_iter=3000, tol=1e-4, random_state=42, n_jobs=1)\n",
        "        clf.fit(Xtr, y[tr])\n",
        "        p = clf.predict_proba(Xva)\n",
        "        oof_c[va] = p\n",
        "        s = log_loss(y[va], p); scores_c.append(s)\n",
        "    sc_c = float(np.mean(scores_c)); print(f'Hstack C={c} OOF: {sc_c:.4f}')\n",
        "    if sc_c < best_sc:\n",
        "        best_sc = sc_c; best_c = c; best_oof = oof_c\n",
        "print(f'Best Hstack C: {best_c} OOF: {best_sc:.4f}')\n",
        "pd.DataFrame(best_oof, columns=classes).to_csv('oof_word_char_hstack_lr.csv', index=False)\n",
        "\n",
        "# Full fit with best C for test\n",
        "vec_word_full = TfidfVectorizer(**word_params)\n",
        "Xfull_word = vec_word_full.fit_transform(train['text']); Xtest_word = vec_word_full.transform(test['text'])\n",
        "vec_char_full = TfidfVectorizer(**char_params)\n",
        "Xfull_char = vec_char_full.fit_transform(train['text']); Xtest_char = vec_char_full.transform(test['text'])\n",
        "Xfull = hstack([Xfull_word, Xfull_char]); Xtest = hstack([Xtest_word, Xtest_char])\n",
        "clf_full = LogisticRegression(solver='lbfgs', C=best_c, max_iter=3000, tol=1e-4, random_state=42, n_jobs=1)\n",
        "clf_full.fit(Xfull, y)\n",
        "ptest = clf_full.predict_proba(Xtest)\n",
        "pd.DataFrame(ptest, columns=classes).to_csv('test_word_char_hstack_lr.csv', index=False)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hstack C=3.0 OOF: 0.3924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hstack C=4.0 OOF: 0.3822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hstack C=5.0 OOF: 0.3765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hstack C=6.0 OOF: 0.3727\nBest Hstack C: 6.0 OOF: 0.3727\n"
          ]
        }
      ]
    },
    {
      "id": "1273a222-391f-4d35-9bbe-36175dfcf0f2",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "train=pd.read_csv('train.csv'); test=pd.read_csv('test.csv')\n",
        "train['text']=train['text'].fillna(''); test['text']=test['text'].fillna('')\n",
        "le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "vec_params=dict(analyzer='char_wb', ngram_range=(1,6), lowercase=False,\n",
        "                sublinear_tf=True, min_df=1, max_df=0.98, max_features=600_000)\n",
        "clf=LogisticRegression(solver='lbfgs', C=4.0, max_iter=3000, tol=1e-4, random_state=42, n_jobs=1)\n",
        "\n",
        "oof=np.zeros((len(train),3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "    vec=TfidfVectorizer(**vec_params)\n",
        "    Xtr=vec.fit_transform(train['text'].iloc[tr]); Xva=vec.transform(train['text'].iloc[va])\n",
        "    clf.fit(Xtr, y[tr])\n",
        "    p=clf.predict_proba(Xva)\n",
        "    oof[va]=p\n",
        "    s=log_loss(y[va], p); scores.append(s); print(f'Char_wb 1-6 LR Fold {f}: {s:.4f}')\n",
        "sc=float(np.mean(scores)); print('Char_wb 1-6 LR OOF:', sc)\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_char_wb_1_6_lr.csv', index=False)\n",
        "\n",
        "# Full fit for test\n",
        "vec_full=TfidfVectorizer(**vec_params)\n",
        "Xfull=vec_full.fit_transform(train['text']); Xtest=vec_full.transform(test['text'])\n",
        "clf.fit(Xfull, y)\n",
        "ptest=clf.predict_proba(Xtest)\n",
        "pd.DataFrame(ptest, columns=classes).to_csv('test_char_wb_1_6_lr.csv', index=False)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char_wb 1-6 LR Fold 1: 0.4126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char_wb 1-6 LR Fold 2: 0.4130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char_wb 1-6 LR Fold 3: 0.4302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char_wb 1-6 LR Fold 4: 0.4168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char_wb 1-6 LR Fold 5: 0.4133\nChar_wb 1-6 LR OOF: 0.41717235341322867\n"
          ]
        }
      ]
    },
    {
      "id": "4d207c4a-6cf1-4f73-a555-7a23a1703463",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "train=pd.read_csv('train.csv'); le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 7-model stack with bagged tests where available\n",
        "model_files=[\n",
        "    ('oof_10fold_uncal_char_wb_lr.csv','test_10fold_uncal_char_wb_lr_bagged.csv'),\n",
        "    ('oof_char_lr.csv','test_char_lr.csv'),\n",
        "    ('oof_calsvc_char.csv','test_calsvc_bagged.csv'),\n",
        "    ('oof_char_variant.csv','test_char_variant_bagged.csv'),\n",
        "    ('oof_stylo_word_lr.csv','test_stylo_word_lr.csv'),\n",
        "    ('oof_word_char_hstack_lr.csv','test_word_char_hstack_lr.csv'),\n",
        "    ('oof_char_wb_1_6_lr.csv','test_char_wb_1_6_lr.csv')\n",
        "]\n",
        "\n",
        "meta_train=np.hstack([pd.read_csv(o)[classes].values for o,_ in model_files])\n",
        "meta_test=np.hstack([pd.read_csv(t)[classes].values for _,t in model_files])\n",
        "\n",
        "# Tune meta-LR C over grid with 5-fold CV\n",
        "c_grid = [0.5, 1.0, 1.5, 2.0, 3.0, 4.0, 6.0]\n",
        "best_c = None; best_sc = float('inf')\n",
        "for c in c_grid:\n",
        "    oof_cv=np.zeros((len(train),3)); scores_cv=[]\n",
        "    for f,(tr,va) in enumerate(skf.split(meta_train,y),1):\n",
        "        meta=LogisticRegression(solver='lbfgs', C=c, max_iter=2000, random_state=2025)\n",
        "        meta.fit(meta_train[tr], y[tr])\n",
        "        p=meta.predict_proba(meta_train[va]); oof_cv[va]=p\n",
        "        s=log_loss(y[va], p); scores_cv.append(s)\n",
        "    sc_cv=float(np.mean(scores_cv))\n",
        "    print(f'Meta-LR C={c} CV OOF: {sc_cv:.4f}')\n",
        "    if sc_cv < best_sc:\n",
        "        best_sc = sc_cv; best_c = c\n",
        "\n",
        "print(f'Best meta-LR C: {best_c} with CV OOF: {best_sc:.4f}')\n",
        "\n",
        "# Fit final meta on full with best C\n",
        "meta=LogisticRegression(solver='lbfgs', C=best_c, max_iter=2000, random_state=2025)\n",
        "meta.fit(meta_train, y)\n",
        "ptest=meta.predict_proba(meta_test)\n",
        "ptest=np.clip(ptest,1e-15,1-1e-15); ptest/=ptest.sum(axis=1, keepdims=True)\n",
        "pd.DataFrame(ptest, columns=classes).to_csv('test_stacked_meta_lr.csv', index=False)\n",
        "\n",
        "# Compute final OOF with best C\n",
        "oof_final=np.zeros((len(train),3)); scores_final=[]\n",
        "for f,(tr,va) in enumerate(skf.split(meta_train,y),1):\n",
        "    meta_cv=LogisticRegression(solver='lbfgs', C=best_c, max_iter=2000, random_state=2025)\n",
        "    meta_cv.fit(meta_train[tr], y[tr])\n",
        "    p=meta_cv.predict_proba(meta_train[va]); oof_final[va]=p\n",
        "    s=log_loss(y[va], p); scores_final.append(s); print(f'Final Meta-LR Fold {f}: {s:.4f}')\n",
        "final_sc=float(np.mean(scores_final)); print('Final Stacked Meta-LR OOF:', final_sc)\n",
        "pd.DataFrame(oof_final, columns=classes).to_csv('oof_stacked_meta_lr_final.csv', index=False)\n",
        "\n",
        "if final_sc <= 0.34:\n",
        "    print('Medal-ready! Copy test_stacked_meta_lr.csv to submission.csv and submit.')\n",
        "else:\n",
        "    print('Still >0.34; need more diversity or tuning.')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR C=0.5 CV OOF: 0.3643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR C=1.0 CV OOF: 0.3641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR C=1.5 CV OOF: 0.3641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR C=2.0 CV OOF: 0.3641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR C=3.0 CV OOF: 0.3641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR C=4.0 CV OOF: 0.3641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-LR C=6.0 CV OOF: 0.3641\nBest meta-LR C: 2.0 with CV OOF: 0.3641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Meta-LR Fold 1: 0.3607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Meta-LR Fold 2: 0.3618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Meta-LR Fold 3: 0.3809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Meta-LR Fold 4: 0.3602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Meta-LR Fold 5: 0.3569\nFinal Stacked Meta-LR OOF: 0.36409225532423145\nStill >0.34; need more diversity or tuning.\n"
          ]
        }
      ]
    },
    {
      "id": "816dbd07-2ab9-42f5-b253-96ab715740d0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train=pd.read_csv('train.csv'); test=pd.read_csv('test.csv')\n",
        "train['text']=train['text'].fillna(''); test['text']=test['text'].fillna('')\n",
        "le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def log_count_ratio(X, yb, alpha=0.5):\n",
        "    pos=np.asarray(X[yb==1].sum(0)).ravel()+alpha\n",
        "    neg=np.asarray(X[yb==0].sum(0)).ravel()+alpha\n",
        "    r=np.log(pos/neg); r[~np.isfinite(r)]=0.0\n",
        "    return r\n",
        "\n",
        "def odds_norm(P, eps=1e-9):\n",
        "    P=np.clip(P,eps,1-eps); odds=P/(1-P)\n",
        "    return odds/(odds.sum(axis=1,keepdims=True)+eps)\n",
        "\n",
        "vec_params=dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=2, max_df=0.95, binary=True, max_features=150000)\n",
        "clf_params=dict(solver='liblinear', penalty='l2', C=4.0, max_iter=3000, tol=1e-4)\n",
        "\n",
        "oof=np.zeros((len(train),3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "    vec=CountVectorizer(**vec_params)\n",
        "    Xtr=vec.fit_transform(train['text'].iloc[tr]); Xva=vec.transform(train['text'].iloc[va])\n",
        "    Pva=np.zeros((len(va),3))\n",
        "    for c in range(3):\n",
        "        yb=(y[tr]==c).astype(int)\n",
        "        r=log_count_ratio(Xtr, yb, alpha=0.5)\n",
        "        clf=LogisticRegression(**clf_params, random_state=42+c)\n",
        "        clf.fit(Xtr.multiply(csr_matrix(r)), yb)\n",
        "        Pva[:,c]=clf.predict_proba(Xva.multiply(csr_matrix(r)))[:,1]\n",
        "    Pva=odds_norm(Pva); oof[va]=Pva\n",
        "    s=log_loss(y[va], Pva); scores.append(s); print(f'Word NB-SVM Fold {f}: {s:.4f}')\n",
        "print('Word NB-SVM OOF:', float(np.mean(scores)))\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_word_nbsvm.csv', index=False)\n",
        "\n",
        "# Full fit -> test\n",
        "vec_full=CountVectorizer(**vec_params)\n",
        "Xfull=vec_full.fit_transform(train['text']); Xtest=vec_full.transform(test['text'])\n",
        "Ptest=np.zeros((len(test),3))\n",
        "for c in range(3):\n",
        "    yb=(y==c).astype(int)\n",
        "    r=log_count_ratio(Xfull, yb, alpha=0.5)\n",
        "    clf=LogisticRegression(**clf_params, random_state=999+c)\n",
        "    clf.fit(Xfull.multiply(csr_matrix(r)), yb)\n",
        "    Ptest[:,c]=clf.predict_proba(Xtest.multiply(csr_matrix(r)))[:,1]\n",
        "Ptest=odds_norm(Ptest)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_word_nbsvm.csv', index=False)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/metrics/_classification.py:259: UserWarning: The y_prob values do not sum to one. Make sure to pass probabilities.\n  Ground truth (correct) target values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 1: 0.5583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/metrics/_classification.py:259: UserWarning: The y_prob values do not sum to one. Make sure to pass probabilities.\n  Ground truth (correct) target values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 2: 0.5350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/metrics/_classification.py:259: UserWarning: The y_prob values do not sum to one. Make sure to pass probabilities.\n  Ground truth (correct) target values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 3: 0.5693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/metrics/_classification.py:259: UserWarning: The y_prob values do not sum to one. Make sure to pass probabilities.\n  Ground truth (correct) target values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 4: 0.5139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/metrics/_classification.py:259: UserWarning: The y_prob values do not sum to one. Make sure to pass probabilities.\n  Ground truth (correct) target values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 5: 0.5623\nWord NB-SVM OOF: 0.5477910850128169\n"
          ]
        }
      ]
    },
    {
      "id": "bc356ae6-080a-43a6-827f-7c701609e312",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train=pd.read_csv('train.csv'); test=pd.read_csv('test.csv')\n",
        "train['text']=train['text'].fillna(''); test['text']=test['text'].fillna('')\n",
        "le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def log_count_ratio(X, yb, alpha=0.5):\n",
        "    pos=np.asarray(X[yb==1].sum(0)).ravel()+alpha\n",
        "    neg=np.asarray(X[yb==0].sum(0)).ravel()+alpha\n",
        "    r=np.log(pos/neg); r[~np.isfinite(r)]=0.0\n",
        "    return r\n",
        "\n",
        "def odds_norm(P, eps=1e-9):\n",
        "    P=np.clip(P,eps,1-eps); odds=P/(1-P)\n",
        "    return odds/(odds.sum(axis=1,keepdims=True)+eps)\n",
        "\n",
        "vec_params=dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=2, max_df=0.95, binary=True, max_features=150000)\n",
        "svc_params=dict(C=4.0, loss='squared_hinge', dual='auto', max_iter=3000, tol=1e-4)\n",
        "inner_cv_splits=3\n",
        "\n",
        "oof=np.zeros((len(train),3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "    vec=CountVectorizer(**vec_params)\n",
        "    Xtr=vec.fit_transform(train['text'].iloc[tr]); Xva=vec.transform(train['text'].iloc[va])\n",
        "    Pva=np.zeros((len(va),3))\n",
        "    for c in range(3):\n",
        "        yb_tr=(y[tr]==c).astype(int)\n",
        "        # Inner CV for Platt calibration\n",
        "        skf_inner=StratifiedKFold(n_splits=inner_cv_splits, shuffle=True, random_state=42+c)\n",
        "        F_cal=[]; z_cal=[]\n",
        "        for i_tr, i_va in skf_inner.split(Xtr, yb_tr):\n",
        "            svc=LinearSVC(**svc_params, random_state=42+c)\n",
        "            svc.fit(Xtr[i_tr], yb_tr[i_tr])\n",
        "            s=svc.decision_function(Xtr[i_va])\n",
        "            if s.ndim > 1: s=s[:,0]\n",
        "            F_cal.append(s.reshape(-1,1)); z_cal.append(yb_tr[i_va])\n",
        "        F_cal=np.vstack(F_cal); z_cal=np.concatenate(z_cal)\n",
        "        platt=LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000, random_state=42+c)\n",
        "        platt.fit(F_cal, z_cal)\n",
        "        # Final SVC on full tr\n",
        "        svc_full=LinearSVC(**svc_params, random_state=42+c)\n",
        "        r=log_count_ratio(Xtr, yb_tr, alpha=0.5)\n",
        "        svc_full.fit(Xtr.multiply(csr_matrix(r)), yb_tr)\n",
        "        s_va=svc_full.decision_function(Xva.multiply(csr_matrix(r)))\n",
        "        if s_va.ndim > 1: s_va=s_va[:,0]\n",
        "        Pva[:,c]=platt.predict_proba(s_va.reshape(-1,1))[:,1]\n",
        "    Pva=odds_norm(Pva); oof[va]=Pva\n",
        "    s=log_loss(y[va], Pva); scores.append(s); print(f'Word NB-SVC+Platt Fold {f}: {s:.4f}')\n",
        "print('Word NB-SVC+Platt OOF:', float(np.mean(scores)))\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_word_nbsvm_svc_platt.csv', index=False)\n",
        "\n",
        "# Full fit -> test\n",
        "vec_full=CountVectorizer(**vec_params)\n",
        "Xfull=vec_full.fit_transform(train['text']); Xtest=vec_full.transform(test['text'])\n",
        "Ptest=np.zeros((len(test),3))\n",
        "for c in range(3):\n",
        "    yb=(y==c).astype(int)\n",
        "    # Inner CV on full\n",
        "    skf_inner=StratifiedKFold(n_splits=inner_cv_splits, shuffle=True, random_state=42+c)\n",
        "    F_cal=[]; z_cal=[]\n",
        "    indices=np.arange(len(train))\n",
        "    for i_tr, i_va in skf_inner.split(indices, yb):\n",
        "        r=log_count_ratio(Xfull[i_tr], yb[i_tr], alpha=0.5)\n",
        "        svc=LinearSVC(**svc_params, random_state=42+c)\n",
        "        svc.fit(Xfull[i_tr].multiply(csr_matrix(r)), yb[i_tr])\n",
        "        s=svc.decision_function(Xfull[i_va].multiply(csr_matrix(r)))\n",
        "        if s.ndim > 1: s=s[:,0]\n",
        "        F_cal.append(s.reshape(-1,1)); z_cal.append(yb[i_va])\n",
        "    F_cal=np.vstack(F_cal); z_cal=np.concatenate(z_cal)\n",
        "    platt=LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000, random_state=42+c)\n",
        "    platt.fit(F_cal, z_cal)\n",
        "    # Final SVC on full\n",
        "    r=log_count_ratio(Xfull, yb, alpha=0.5)\n",
        "    svc_final=LinearSVC(**svc_params, random_state=42+c)\n",
        "    svc_final.fit(Xfull.multiply(csr_matrix(r)), yb)\n",
        "    s_test=svc_final.decision_function(Xtest.multiply(csr_matrix(r)))\n",
        "    if s_test.ndim > 1: s_test=s_test[:,0]\n",
        "    Ptest[:,c]=platt.predict_proba(s_test.reshape(-1,1))[:,1]\n",
        "Ptest=odds_norm(Ptest)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_word_nbsvm_svc_platt.csv', index=False)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/metrics/_classification.py:259: UserWarning: The y_prob values do not sum to one. Make sure to pass probabilities.\n  Ground truth (correct) target values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVC+Platt Fold 1: 0.5812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/metrics/_classification.py:259: UserWarning: The y_prob values do not sum to one. Make sure to pass probabilities.\n  Ground truth (correct) target values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVC+Platt Fold 2: 0.5783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/metrics/_classification.py:259: UserWarning: The y_prob values do not sum to one. Make sure to pass probabilities.\n  Ground truth (correct) target values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVC+Platt Fold 3: 0.6017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/metrics/_classification.py:259: UserWarning: The y_prob values do not sum to one. Make sure to pass probabilities.\n  Ground truth (correct) target values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVC+Platt Fold 4: 0.5447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/metrics/_classification.py:259: UserWarning: The y_prob values do not sum to one. Make sure to pass probabilities.\n  Ground truth (correct) target values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVC+Platt Fold 5: 0.5880\nWord NB-SVC+Platt OOF: 0.5787782278391148\n"
          ]
        }
      ]
    },
    {
      "id": "b83bc3fb-450e-4c50-9586-014d81aee213",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
        "\n",
        "train=pd.read_csv('train.csv'); test=pd.read_csv('test.csv')\n",
        "train['text']=train['text'].fillna(''); test['text']=test['text'].fillna('')\n",
        "le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "vec_params=dict(analyzer='char_wb', ngram_range=(2,6), lowercase=False,\n",
        "                sublinear_tf=True, min_df=2, max_df=0.98, max_features=400_000)\n",
        "inner_cv_splits=3\n",
        "\n",
        "oof=np.zeros((len(train),3)); scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "    vec=TfidfVectorizer(**vec_params)\n",
        "    Xtr=vec.fit_transform(train['text'].iloc[tr]); Xva=vec.transform(train['text'].iloc[va])\n",
        "    # Inner CV for Platt calibration on Ridge decision_function\n",
        "    Pva=np.zeros((len(va),3))\n",
        "    for c in range(3):\n",
        "        yb_tr=(y[tr]==c).astype(int)\n",
        "        skf_inner=StratifiedKFold(n_splits=inner_cv_splits, shuffle=True, random_state=42+c)\n",
        "        F_cal=[]; z_cal=[]\n",
        "        for i_tr, i_va in skf_inner.split(Xtr, yb_tr):\n",
        "            ridge=RidgeClassifier(alpha=0.25, random_state=42+c)\n",
        "            ridge.fit(Xtr[i_tr], yb_tr[i_tr])\n",
        "            s=ridge.decision_function(Xtr[i_va])\n",
        "            if s.ndim > 1: s=s[:,0]\n",
        "            F_cal.append(s.reshape(-1,1)); z_cal.append(yb_tr[i_va])\n",
        "        F_cal=np.vstack(F_cal); z_cal=np.concatenate(z_cal)\n",
        "        platt=LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000, random_state=42+c)\n",
        "        platt.fit(F_cal, z_cal)\n",
        "        # Final Ridge on full tr\n",
        "        ridge_full=RidgeClassifier(alpha=0.25, random_state=42+c)\n",
        "        ridge_full.fit(Xtr, yb_tr)\n",
        "        s_va=ridge_full.decision_function(Xva)\n",
        "        if s_va.ndim > 1: s_va=s_va[:,0]\n",
        "        Pva[:,c]=platt.predict_proba(s_va.reshape(-1,1))[:,1]\n",
        "    # Odds normalize OvR probs\n",
        "    Pva=np.clip(Pva,1e-9,1-1e-9); odds=Pva/(1-Pva)\n",
        "    Pva=odds/(odds.sum(axis=1,keepdims=True)+1e-9)\n",
        "    oof[va]=Pva\n",
        "    s=log_loss(y[va], Pva); scores.append(s); print(f'Ridge Char_wb Cal Fold {f}: {s:.4f}')\n",
        "sc=float(np.mean(scores)); print('Ridge Char_wb Cal OOF:', sc)\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_ridge_char_wb.csv', index=False)\n",
        "\n",
        "# Full fit for test\n",
        "vec_full=TfidfVectorizer(**vec_params)\n",
        "Xfull=vec_full.fit_transform(train['text']); Xtest=vec_full.transform(test['text'])\n",
        "Ptest=np.zeros((len(test),3))\n",
        "for c in range(3):\n",
        "    yb=(y==c).astype(int)\n",
        "    skf_inner=StratifiedKFold(n_splits=inner_cv_splits, shuffle=True, random_state=42+c)\n",
        "    F_cal=[]; z_cal=[]\n",
        "    indices=np.arange(len(train))\n",
        "    for i_tr, i_va in skf_inner.split(indices, yb):\n",
        "        ridge=RidgeClassifier(alpha=0.25, random_state=42+c)\n",
        "        ridge.fit(Xfull[i_tr], yb[i_tr])\n",
        "        s=ridge.decision_function(Xfull[i_va])\n",
        "        if s.ndim > 1: s=s[:,0]\n",
        "        F_cal.append(s.reshape(-1,1)); z_cal.append(yb[i_va])\n",
        "    F_cal=np.vstack(F_cal); z_cal=np.concatenate(z_cal)\n",
        "    platt=LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000, random_state=42+c)\n",
        "    platt.fit(F_cal, z_cal)\n",
        "    ridge_full=RidgeClassifier(alpha=0.25, random_state=42+c)\n",
        "    ridge_full.fit(Xfull, yb)\n",
        "    s_test=ridge_full.decision_function(Xtest)\n",
        "    if s_test.ndim > 1: s_test=s_test[:,0]\n",
        "    Ptest[:,c]=platt.predict_proba(s_test.reshape(-1,1))[:,1]\n",
        "Ptest=np.clip(Ptest,1e-9,1-1e-9); odds=Ptest/(1-Ptest)\n",
        "Ptest=odds/(odds.sum(axis=1,keepdims=True)+1e-9)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_ridge_char_wb.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "00738c09-bd26-4fbe-beb9-cb253d4769ee",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train=pd.read_csv('train.csv'); test=pd.read_csv('test.csv')\n",
        "train['text']=train['text'].fillna(''); test['text']=test['text'].fillna('')\n",
        "le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def log_count_ratio(X, yb, alpha=0.5):\n",
        "    pos=np.asarray(X[yb==1].sum(0)).ravel()+alpha\n",
        "    neg=np.asarray(X[yb==0].sum(0)).ravel()+alpha\n",
        "    r=np.log(pos/neg); r[~np.isfinite(r)]=0.0\n",
        "    return r\n",
        "\n",
        "def odds_normalize(P, eps=1e-15):\n",
        "    P=np.clip(P,eps,1-eps)\n",
        "    odds=P/(1-P)\n",
        "    Q=odds/(odds.sum(axis=1, keepdims=True)+eps)\n",
        "    return Q/Q.sum(axis=1, keepdims=True)\n",
        "\n",
        "vec_params=dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_df=0.90, binary=False)\n",
        "C_grid=[4.0, 6.0, 8.0]\n",
        "best_sc=1e9; best_oof=None; best_C=None\n",
        "\n",
        "for C in C_grid:\n",
        "    oof=np.zeros((len(train),3)); scores=[]\n",
        "    for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "        vec=CountVectorizer(**vec_params)\n",
        "        Xtr_cnt=vec.fit_transform(train['text'].iloc[tr])\n",
        "        Xva_cnt=vec.transform(train['text'].iloc[va])\n",
        "        # Make binary features by copying the count matrices\n",
        "        Xtr_bin=Xtr_cnt.copy(); Xtr_bin.data[:]=1\n",
        "        Xva_bin=Xva_cnt.copy(); Xva_bin.data[:]=1\n",
        "\n",
        "        Pva=np.zeros((len(va),3))\n",
        "        for c in range(3):\n",
        "            yb=(y[tr]==c).astype(int)\n",
        "            r=log_count_ratio(Xtr_cnt, yb, alpha=0.5)\n",
        "            clf=LogisticRegression(solver='liblinear', penalty='l2', C=C, max_iter=3000, tol=1e-4, random_state=42+c)\n",
        "            clf.fit(Xtr_bin.multiply(csr_matrix(r)), yb)\n",
        "            Pva[:,c]=clf.predict_proba(Xva_bin.multiply(csr_matrix(r)))[:,1]\n",
        "        Pva=odds_normalize(Pva)\n",
        "        oof[va]=Pva\n",
        "        scores.append(log_loss(y[va], Pva))\n",
        "        print(f'Word NB-SVM C={C} Fold {f}: {scores[-1]:.4f}')\n",
        "    sc=float(np.mean(scores)); print(f'Word NB-SVM C={C} OOF: {sc:.4f}')\n",
        "    if sc<best_sc: best_sc=sc; best_oof=oof; best_C=C\n",
        "\n",
        "print(f'Best Word NB-SVM OOF: {best_sc:.4f} at C={best_C}')\n",
        "pd.DataFrame(best_oof, columns=classes).to_csv('oof_word_nbsvm_final.csv', index=False)\n",
        "\n",
        "# Full fit for test with best C\n",
        "vec_full=CountVectorizer(**vec_params)\n",
        "Xfull_cnt=vec_full.fit_transform(train['text']); Xtest_cnt=vec_full.transform(test['text'])\n",
        "Xfull_bin=Xfull_cnt.copy(); Xfull_bin.data[:]=1\n",
        "Xtest_bin=Xtest_cnt.copy(); Xtest_bin.data[:]=1\n",
        "\n",
        "Ptest=np.zeros((len(test),3))\n",
        "for c in range(3):\n",
        "    yb=(y==c).astype(int)\n",
        "    r=log_count_ratio(Xfull_cnt, yb, alpha=0.5)\n",
        "    clf=LogisticRegression(solver='liblinear', penalty='l2', C=best_C, max_iter=3000, tol=1e-4, random_state=999+c)\n",
        "    clf.fit(Xfull_bin.multiply(csr_matrix(r)), yb)\n",
        "    Ptest[:,c]=clf.predict_proba(Xtest_bin.multiply(csr_matrix(r)))[:,1]\n",
        "Ptest=odds_normalize(Ptest)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_word_nbsvm_final.csv', index=False)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=4.0 Fold 1: 0.6056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=4.0 Fold 2: 0.5798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=4.0 Fold 3: 0.6116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=4.0 Fold 4: 0.5545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=6.0 Fold 1: 0.6574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=6.0 Fold 2: 0.6294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=6.0 Fold 3: 0.6650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=6.0 Fold 4: 0.6011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=6.0 Fold 5: 0.6591\nWord NB-SVM C=6.0 OOF: 0.6424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=8.0 Fold 1: 0.6973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=8.0 Fold 2: 0.6679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=8.0 Fold 3: 0.7065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=8.0 Fold 4: 0.6374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=8.0 Fold 5: 0.6996\nWord NB-SVM C=8.0 OOF: 0.6817\nBest Word NB-SVM OOF: 0.5915 at C=4.0\n"
          ]
        }
      ]
    },
    {
      "id": "6e02a10a-1b7b-4121-8645-6613b2dcf36f",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "train=pd.read_csv('train.csv'); test=pd.read_csv('test.csv')\n",
        "train['text']=train['text'].fillna(''); test['text']=test['text'].fillna('')\n",
        "le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def run(vec_params, C, name):\n",
        "    oof=np.zeros((len(train),3)); scores=[]; test_preds=[]\n",
        "    for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "        vec=TfidfVectorizer(**vec_params)\n",
        "        Xtr=vec.fit_transform(train['text'].iloc[tr]); Xva=vec.transform(train['text'].iloc[va]); Xte=vec.transform(test['text'])\n",
        "        clf=LogisticRegression(solver='lbfgs', C=C, max_iter=3000, tol=1e-4, random_state=42, n_jobs=1)\n",
        "        clf.fit(Xtr, y[tr])\n",
        "        p=clf.predict_proba(Xva); oof[va]=p; test_preds.append(clf.predict_proba(Xte))\n",
        "        s=log_loss(y[va], p); scores.append(s); print(f'{name} Fold {f}: {s:.4f}')\n",
        "    sc=float(np.mean(scores)); print(f'{name} OOF: {sc:.4f}\\n')\n",
        "    test_pred=np.mean(test_preds, axis=0)\n",
        "    pd.DataFrame(oof, columns=classes).to_csv(f'oof_{name}.csv', index=False)\n",
        "    pd.DataFrame(test_pred, columns=classes).to_csv(f'test_{name}.csv', index=False)\n",
        "\n",
        "models=[\n",
        "    # char_wb\n",
        "    ('char_wb_1_7', dict(analyzer='char_wb', ngram_range=(1,7), lowercase=False, sublinear_tf=True, min_df=1, max_df=0.98, max_features=600_000), 5.0),\n",
        "    ('char_wb_2_7', dict(analyzer='char_wb', ngram_range=(2,7), lowercase=False, sublinear_tf=True, min_df=2, max_df=0.98, max_features=500_000), 8.0),\n",
        "    ('char_wb_3_7', dict(analyzer='char_wb', ngram_range=(3,7), lowercase=False, sublinear_tf=True, min_df=3, max_df=0.97, max_features=400_000), 10.0),\n",
        "    ('char_wb_1_8', dict(analyzer='char_wb', ngram_range=(1,8), lowercase=False, sublinear_tf=True, min_df=2, max_df=0.97, max_features=700_000), 6.0),\n",
        "    # char\n",
        "    ('char_2_8', dict(analyzer='char', ngram_range=(2,8), lowercase=False, sublinear_tf=True, min_df=1, max_df=0.99, max_features=800_000), 3.0),\n",
        "    ('char_3_8', dict(analyzer='char', ngram_range=(3,8), lowercase=False, sublinear_tf=True, min_df=2, max_df=0.98, max_features=600_000), 4.0),\n",
        "    ('char_2_9_mindf5', dict(analyzer='char', ngram_range=(2,9), lowercase=False, sublinear_tf=True, min_df=5, max_df=0.98, max_features=500_000), 4.0),\n",
        "    ('char_2_7_mindf3', dict(analyzer='char', ngram_range=(2,7), lowercase=False, sublinear_tf=True, min_df=3, max_df=0.98, max_features=500_000), 6.0),\n",
        "]\n",
        "for name, vp, C in models: run(vp, C, name)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_7 Fold 1: 0.4059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_7 Fold 2: 0.4049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_7 Fold 3: 0.4213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_7 Fold 4: 0.4107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_7 Fold 5: 0.4047\nchar_wb_1_7 OOF: 0.4095\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 1: 0.3966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 2: 0.3934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 3: 0.4114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 4: 0.4001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 5: 0.3951\nchar_wb_2_7 OOF: 0.3993\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_3_7 Fold 1: 0.3985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_3_7 Fold 2: 0.3975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_3_7 Fold 3: 0.4141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_3_7 Fold 4: 0.4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_3_7 Fold 5: 0.3997\nchar_wb_3_7 OOF: 0.4028\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_8 Fold 1: 0.4014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_8 Fold 2: 0.4000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_8 Fold 3: 0.4176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_8 Fold 4: 0.4060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_8 Fold 5: 0.3991\nchar_wb_1_8 OOF: 0.4048\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_8 Fold 1: 0.4652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_8 Fold 2: 0.4649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_8 Fold 3: 0.4723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_8 Fold 4: 0.4634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_8 Fold 5: 0.4556\nchar_2_8 OOF: 0.4643\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_3_8 Fold 1: 0.4462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_3_8 Fold 2: 0.4497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_3_8 Fold 3: 0.4568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_3_8 Fold 4: 0.4479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_3_8 Fold 5: 0.4402\nchar_3_8 OOF: 0.4482\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_9_mindf5 Fold 1: 0.4405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_9_mindf5 Fold 2: 0.4398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_9_mindf5 Fold 3: 0.4490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_9_mindf5 Fold 4: 0.4393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_9_mindf5 Fold 5: 0.4295\nchar_2_9_mindf5 OOF: 0.4396\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_7_mindf3 Fold 1: 0.4156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_7_mindf3 Fold 2: 0.4146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_7_mindf3 Fold 3: 0.4270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_7_mindf3 Fold 4: 0.4135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_7_mindf3 Fold 5: 0.4060\nchar_2_7_mindf3 OOF: 0.4153\n\n"
          ]
        }
      ]
    },
    {
      "id": "d24cb5f8-7561-4736-9d18-64432035506b",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd, glob, os\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "train=pd.read_csv('train.csv'); le=LabelEncoder(); y=le.fit_transform(train['author']); classes=list(le.classes_)\n",
        "\n",
        "# Load all OOF/test pairs with OOF <0.46, exclude stacked/meta files\n",
        "cand=[]\n",
        "for oof_file in glob.glob('oof_*.csv'):\n",
        "    if 'stacked' in oof_file or 'meta' in oof_file:\n",
        "        continue\n",
        "    test_file=oof_file.replace('oof_','test_')\n",
        "    if not os.path.exists(test_file): continue\n",
        "    oof=pd.read_csv(oof_file)[classes].values\n",
        "    sc=log_loss(y, oof)\n",
        "    if sc<0.46:\n",
        "        cand.append((oof_file, test_file, oof, pd.read_csv(test_file)[classes].values, sc))\n",
        "cand=sorted(cand, key=lambda x: x[4])\n",
        "\n",
        "# Greedy forward selection (simple average)\n",
        "selected=[]; best=float('inf'); sel_tests=[]\n",
        "while True:\n",
        "    improved=False; pick=None; pick_oof=None; pick_test=None; pick_sc=None\n",
        "    for (oof_f, test_f, oof, test, sc) in cand:\n",
        "        if any(oof_f==s[0] for s in selected): continue\n",
        "        cur=[s[2] for s in selected]+[oof]\n",
        "        blend=np.mean(cur, axis=0)\n",
        "        s=log_loss(y, blend)\n",
        "        if s<best-1e-5:\n",
        "            improved=True; best=s; pick=(oof_f,test_f); pick_oof=oof; pick_test=test; pick_sc=s\n",
        "    if not improved: break\n",
        "    selected.append((pick[0], pick[1], pick_oof, pick_test)); sel_tests=[s[3] for s in selected]\n",
        "    print(f'Added {pick[0]} -> OOF {pick_sc:.4f}')\n",
        "print('Greedy OOF:', best)\n",
        "\n",
        "# Weight optimization on selected\n",
        "oofs=[s[2] for s in selected]; tests=[s[3] for s in selected]\n",
        "best_w=None; best_w_sc=float('inf')\n",
        "for _ in range(2000):\n",
        "    w=np.random.dirichlet(np.ones(len(oofs)))\n",
        "    blend=sum(wi*o for wi,o in zip(w,oofs))\n",
        "    sc=log_loss(y, blend)\n",
        "    if sc<best_w_sc: best_w_sc=sc; best_w=w\n",
        "print('Weighted OOF:', best_w_sc)\n",
        "\n",
        "# Ridge meta on selected\n",
        "meta_train=np.hstack(oofs); meta_test=np.hstack(tests)\n",
        "skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "def to_prob(P): P=np.clip(P,1e-15,1-1e-15); P/=P.sum(1,keepdims=True); return P\n",
        "best_alpha=None; best_alpha_sc=float('inf')\n",
        "for a in [0.1,0.3,0.5,1.0,2.0]:\n",
        "    oof=np.zeros((len(train),3)); scs=[]\n",
        "    for tr,va in skf.split(meta_train, y):\n",
        "        Y=np.zeros((len(tr),3)); Y[np.arange(len(tr)), y[tr]]=1\n",
        "        ridge=Ridge(alpha=a, random_state=42).fit(meta_train[tr], Y)\n",
        "        p=to_prob(ridge.predict(meta_train[va])); oof[va]=p; scs.append(log_loss(y[va], p))\n",
        "    sc=np.mean(scs)\n",
        "    if sc<best_alpha_sc: best_alpha_sc=sc; best_alpha=a\n",
        "print('Ridge OOF:', best_alpha_sc)\n",
        "\n",
        "# Pick best (lowest OOF) and save final test\n",
        "method=min([('greedy',best), ('weighted',best_w_sc), ('ridge',best_alpha_sc)], key=lambda x:x[1])[0]\n",
        "if method=='greedy':\n",
        "    final_test=np.mean(tests, axis=0)\n",
        "elif method=='weighted':\n",
        "    final_test=sum(wi*t for wi,t in zip(best_w, tests))\n",
        "else:\n",
        "    Y=np.zeros((len(train),3)); Y[np.arange(len(train)), y]=1\n",
        "    ridge=Ridge(alpha=best_alpha, random_state=42).fit(meta_train, Y)\n",
        "    final_test=ridge.predict(meta_test)\n",
        "final_test=np.clip(final_test,1e-15,1-1e-15); final_test/=final_test.sum(1,keepdims=True)\n",
        "pd.DataFrame(final_test, columns=classes).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with method:', method)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [17914, 17621]",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(test_file): \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     16\u001b[39m oof=pd.read_csv(oof_file)[classes].values\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m sc=\u001b[43mlog_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moof\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sc<\u001b[32m0.46\u001b[39m:\n\u001b[32m     19\u001b[39m     cand.append((oof_file, test_file, oof, pd.read_csv(test_file)[classes].values, sc))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mwrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    213\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    214\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    215\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    216\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    217\u001b[39m     msg = re.sub(\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    219\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    220\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    221\u001b[39m     )\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/metrics/_classification.py:3240\u001b[39m, in \u001b[36mlog_loss\u001b[39m\u001b[34m(y_true, y_pred, normalize, sample_weight, labels)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/metrics/_classification.py:202\u001b[39m, in \u001b[36m_validate_multiclass_probabilistic_prediction\u001b[39m\u001b[34m(y_true, y_prob, sample_weight, labels)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m    145\u001b[39m     {\n\u001b[32m    146\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msparse matrix\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    152\u001b[39m )\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maccuracy_score\u001b[39m(y_true, y_pred, *, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    154\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[32m    155\u001b[39m \n\u001b[32m    156\u001b[39m \u001b[33;03m    In multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[33;03m    the set of labels predicted for a sample must *exactly* match the\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[33;03m    corresponding set of labels in y_true.\u001b[39;00m\n\u001b[32m    159\u001b[39m \n\u001b[32m    160\u001b[39m \u001b[33;03m    Read more in the :ref:`User Guide <accuracy_score>`.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m    162\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    ----------\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    y_true : 1d array-like, or label indicator array / sparse matrix\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[33;03m        Ground truth (correct) labels.\u001b[39;00m\n\u001b[32m    166\u001b[39m \n\u001b[32m    167\u001b[39m \u001b[33;03m    y_pred : 1d array-like, or label indicator array / sparse matrix\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[33;03m        Predicted labels, as returned by a classifier.\u001b[39;00m\n\u001b[32m    169\u001b[39m \n\u001b[32m    170\u001b[39m \u001b[33;03m    normalize : bool, default=True\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[33;03m        If ``False``, return the number of correctly classified samples.\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[33;03m        Otherwise, return the fraction of correctly classified samples.\u001b[39;00m\n\u001b[32m    173\u001b[39m \n\u001b[32m    174\u001b[39m \u001b[33;03m    sample_weight : array-like of shape (n_samples,), default=None\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[33;03m        Sample weights.\u001b[39;00m\n\u001b[32m    176\u001b[39m \n\u001b[32m    177\u001b[39m \u001b[33;03m    Returns\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[33;03m    -------\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[33;03m    score : float\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[33;03m        If ``normalize == True``, return the fraction of correctly\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[33;03m        classified samples (float), else returns the number of correctly\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[33;03m        classified samples (int).\u001b[39;00m\n\u001b[32m    183\u001b[39m \n\u001b[32m    184\u001b[39m \u001b[33;03m        The best performance is 1 with ``normalize == True`` and the number\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[33;03m        of samples with ``normalize == False``.\u001b[39;00m\n\u001b[32m    186\u001b[39m \n\u001b[32m    187\u001b[39m \u001b[33;03m    See Also\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03m    --------\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03m    balanced_accuracy_score : Compute the balanced accuracy to deal with\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03m        imbalanced datasets.\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03m    jaccard_score : Compute the Jaccard similarity coefficient score.\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m    hamming_loss : Compute the average Hamming loss or Hamming distance between\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[33;03m        two sets of samples.\u001b[39;00m\n\u001b[32m    194\u001b[39m \u001b[33;03m    zero_one_loss : Compute the Zero-one classification loss. By default, the\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[33;03m        function will return the percentage of imperfectly predicted subsets.\u001b[39;00m\n\u001b[32m    196\u001b[39m \n\u001b[32m    197\u001b[39m \u001b[33;03m    Notes\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03m    -----\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[33;03m    In binary classification, this function is equal to the `jaccard_score`\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[33;03m    function.\u001b[39;00m\n\u001b[32m    201\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m \u001b[33;03m    Examples\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m    --------\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[33;03m    >>> from sklearn.metrics import accuracy_score\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[33;03m    >>> y_pred = [0, 2, 1, 3]\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[33;03m    >>> y_true = [0, 1, 2, 3]\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[33;03m    >>> accuracy_score(y_true, y_pred)\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[33;03m    0.5\u001b[39;00m\n\u001b[32m    209\u001b[39m \u001b[33;03m    >>> accuracy_score(y_true, y_pred, normalize=False)\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[33;03m    2\u001b[39;00m\n\u001b[32m    211\u001b[39m \n\u001b[32m    212\u001b[39m \u001b[33;03m    In the multilabel case with binary label indicators:\u001b[39;00m\n\u001b[32m    213\u001b[39m \n\u001b[32m    214\u001b[39m \u001b[33;03m    >>> import numpy as np\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[33;03m    >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03m    0.5\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[32m    220\u001b[39m     y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/utils/validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ensure_sparse_format\u001b[39m(\n\u001b[32m    460\u001b[39m     spmatrix,\n\u001b[32m    461\u001b[39m     accept_sparse,\n\u001b[32m   (...)\u001b[39m\u001b[32m    467\u001b[39m     input_name=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    468\u001b[39m ):\n\u001b[32m    469\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Convert a sparse matrix to a given format.\u001b[39;00m\n\u001b[32m    470\u001b[39m \n\u001b[32m    471\u001b[39m \u001b[33;03m    Checks the sparse format of spmatrix and converts if necessary.\u001b[39;00m\n\u001b[32m    472\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    ----------\u001b[39;00m\n\u001b[32m    475\u001b[39m \u001b[33;03m    spmatrix : sparse matrix\u001b[39;00m\n\u001b[32m    476\u001b[39m \u001b[33;03m        Input to validate and convert.\u001b[39;00m\n\u001b[32m    477\u001b[39m \n\u001b[32m    478\u001b[39m \u001b[33;03m    accept_sparse : str, bool or list/tuple of str\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[33;03m        String[s] representing allowed sparse matrix formats ('csc',\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[33;03m        'csr', 'coo', 'dok', 'bsr', 'lil', 'dia'). If the input is sparse but\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[33;03m        not in the allowed format, it will be converted to the first listed\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[33;03m        format. True allows the input to be any format. False means\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[33;03m        that a sparse matrix input will raise an error.\u001b[39;00m\n\u001b[32m    484\u001b[39m \n\u001b[32m    485\u001b[39m \u001b[33;03m    dtype : str, type or None\u001b[39;00m\n\u001b[32m    486\u001b[39m \u001b[33;03m        Data type of result. If None, the dtype of the input is preserved.\u001b[39;00m\n\u001b[32m    487\u001b[39m \n\u001b[32m    488\u001b[39m \u001b[33;03m    copy : bool\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m        Whether a forced copy will be triggered. If copy=False, a copy might\u001b[39;00m\n\u001b[32m    490\u001b[39m \u001b[33;03m        be triggered by a conversion.\u001b[39;00m\n\u001b[32m    491\u001b[39m \n\u001b[32m    492\u001b[39m \u001b[33;03m    force_all_finite : bool or 'allow-nan'\u001b[39;00m\n\u001b[32m    493\u001b[39m \u001b[33;03m        Whether to raise an error on np.inf, np.nan, pd.NA in X. The\u001b[39;00m\n\u001b[32m    494\u001b[39m \u001b[33;03m        possibilities are:\u001b[39;00m\n\u001b[32m    495\u001b[39m \n\u001b[32m    496\u001b[39m \u001b[33;03m        - True: Force all values of X to be finite.\u001b[39;00m\n\u001b[32m    497\u001b[39m \u001b[33;03m        - False: accepts np.inf, np.nan, pd.NA in X.\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[33;03m        - 'allow-nan': accepts only np.nan and pd.NA values in X. Values cannot\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[33;03m          be infinite.\u001b[39;00m\n\u001b[32m    500\u001b[39m \n\u001b[32m    501\u001b[39m \u001b[33;03m        .. versionadded:: 0.20\u001b[39;00m\n\u001b[32m    502\u001b[39m \u001b[33;03m           ``force_all_finite`` accepts the string ``'allow-nan'``.\u001b[39;00m\n\u001b[32m    503\u001b[39m \n\u001b[32m    504\u001b[39m \u001b[33;03m        .. versionchanged:: 0.23\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[33;03m           Accepts `pd.NA` and converts it into `np.nan`\u001b[39;00m\n\u001b[32m    506\u001b[39m \n\u001b[32m    507\u001b[39m \n\u001b[32m    508\u001b[39m \u001b[33;03m    estimator_name : str, default=None\u001b[39;00m\n\u001b[32m    509\u001b[39m \u001b[33;03m        The estimator name, used to construct the error message.\u001b[39;00m\n\u001b[32m    510\u001b[39m \n\u001b[32m    511\u001b[39m \u001b[33;03m    input_name : str, default=\"\"\u001b[39;00m\n\u001b[32m    512\u001b[39m \u001b[33;03m        The data name used to construct the error message. In particular\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[33;03m        if `input_name` is \"X\" and the data has NaN values and\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[33;03m        allow_nan is False, the error message will link to the imputer\u001b[39;00m\n\u001b[32m    515\u001b[39m \u001b[33;03m        documentation.\u001b[39;00m\n\u001b[32m    516\u001b[39m \n\u001b[32m    517\u001b[39m \u001b[33;03m    Returns\u001b[39;00m\n\u001b[32m    518\u001b[39m \u001b[33;03m    -------\u001b[39;00m\n\u001b[32m    519\u001b[39m \u001b[33;03m    spmatrix_converted : sparse matrix.\u001b[39;00m\n\u001b[32m    520\u001b[39m \u001b[33;03m        Matrix that is ensured to have an allowed type.\u001b[39;00m\n\u001b[32m    521\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    523\u001b[39m         dtype = spmatrix.dtype\n",
            "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [17914, 17621]"
          ]
        }
      ]
    },
    {
      "id": "802b5fe2-d370-493a-8095-c1fc13fc9f74",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "train=pd.read_csv('train.csv'); test=pd.read_csv('test.csv')\n",
        "le=LabelEncoder(); y=le.fit_transform(train['author']); classes=['EAP','HPL','MWS']\n",
        "blend_test = pd.read_csv('submission.csv')[classes].values\n",
        "\n",
        "k = int(0.15*len(test))\n",
        "m = blend_test.max(axis=1); thr = np.partition(m, -k)[-k]\n",
        "mask = m >= thr\n",
        "pseudo_y = blend_test[mask].argmax(1)\n",
        "pseudo_text = test.loc[mask,'text'].values\n",
        "\n",
        "X_text = np.concatenate([train['text'].values, pseudo_text])\n",
        "y_all = np.concatenate([y, pseudo_y])\n",
        "sw = np.concatenate([np.ones(len(y)), np.full(len(pseudo_y), 0.4)])\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "models = [\n",
        " ('pl_char_wb_2_7', dict(analyzer='char_wb', ngram_range=(2,7), lowercase=False, sublinear_tf=True, min_df=2, max_df=0.98, max_features=500_000), 8.0),\n",
        " ('pl_char_wb_1_7', dict(analyzer='char_wb', ngram_range=(1,7), lowercase=False, sublinear_tf=True, min_df=1, max_df=0.98, max_features=600_000), 5.0),\n",
        " ('pl_char_3_7',   dict(analyzer='char',    ngram_range=(3,7), lowercase=False, sublinear_tf=True, min_df=2, max_df=0.98, max_features=300_000), 4.0),\n",
        "]\n",
        "for name, vp, C in models:\n",
        "    oof_pl = np.zeros((len(X_text), 3)); scores_pl = []\n",
        "    test_preds_pl = []\n",
        "    for tr, va in skf.split(X_text, y_all):\n",
        "        vec=TfidfVectorizer(**vp)\n",
        "        Xtr_pl=vec.fit_transform(X_text[tr]); Xva_pl=vec.transform(X_text[va]); Xte_pl=vec.transform(test['text'])\n",
        "        clf=LogisticRegression(solver='lbfgs', C=C, max_iter=3000, tol=1e-4, n_jobs=1, random_state=2025)\n",
        "        clf.fit(Xtr_pl, y_all[tr], sample_weight=sw[tr])\n",
        "        p_pl=clf.predict_proba(Xva_pl); oof_pl[va]=p_pl; test_preds_pl.append(clf.predict_proba(Xte_pl))\n",
        "        s_pl=log_loss(y_all[va], p_pl); scores_pl.append(s_pl)\n",
        "    sc_pl=float(np.mean(scores_pl)); print(f'{name} PL OOF: {sc_pl:.4f}')\n",
        "    test_pred_pl=np.mean(test_preds_pl, axis=0)\n",
        "    pd.DataFrame(oof_pl, columns=classes).to_csv(f'oof_{name}.csv', index=False)\n",
        "    pd.DataFrame(test_pred_pl, columns=classes).to_csv(f'test_{name}.csv', index=False)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pl_char_wb_2_7 PL OOF: 0.3993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pl_char_wb_1_7 PL OOF: 0.4081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pl_char_3_7 PL OOF: 0.4348\n"
          ]
        }
      ]
    },
    {
      "id": "8d891f83-d1e6-4cab-8f14-4e392aafd8aa",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import re, unicodedata\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    if not isinstance(s, str): s = '' if s is None else str(s)\n",
        "    s = unicodedata.normalize('NFKC', s)\n",
        "    s = (s.replace('\u201c','\"').replace('\u201d','\"')\n",
        "           .replace('\u2018',\"'\").replace('\u2019',\"'\")\n",
        "           .replace('\u2014','-').replace('\u2013','-').replace('\u2212','-')\n",
        "           .replace('\u2026',' ... ').replace('\\u00A0',' '))\n",
        "    s = re.sub(r'\\d+', '0', s)          # unify digit sequences\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()  # collapse whitespace\n",
        "    return s\n",
        "\n",
        "# usage\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna('').map(normalize_text)\n",
        "test['text']  = test['text'].fillna('').map(normalize_text)\n",
        "\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf10 = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "def run_10fold(name, vec_params, C):\n",
        "    oof = np.zeros((len(train),3)); Ptest = np.zeros((len(test),3)); scores=[]\n",
        "    for f,(tr,va) in enumerate(skf10.split(train['text'], y),1):\n",
        "        vec = TfidfVectorizer(**vec_params)\n",
        "        Xtr = vec.fit_transform(train['text'].iloc[tr]); Xva = vec.transform(train['text'].iloc[va]); Xte = vec.transform(test['text'])\n",
        "        clf = LogisticRegression(solver='lbfgs', C=C, max_iter=3000, tol=1e-4, n_jobs=1, random_state=42+f)\n",
        "        clf.fit(Xtr, y[tr])\n",
        "        p = clf.predict_proba(Xva); oof[va] = p; Ptest += clf.predict_proba(Xte)\n",
        "        scores.append(log_loss(y[va], p))\n",
        "    print(name, '10f OOF:', round(float(np.mean(scores)),4))\n",
        "    Ptest /= skf10.n_splits\n",
        "    pd.DataFrame(oof, columns=classes).to_csv(f'oof_10f_{name}.csv', index=False)\n",
        "    pd.DataFrame(Ptest, columns=classes).to_csv(f'test_10f_{name}.csv', index=False)\n",
        "\n",
        "def run_10f_hstack(name, word_params, char_params, C):\n",
        "    oof = np.zeros((len(train),3)); Ptest = np.zeros((len(test),3)); scores=[]\n",
        "    for f,(tr,va) in enumerate(skf10.split(train['text'], y),1):\n",
        "        vw = TfidfVectorizer(**word_params); vc = TfidfVectorizer(**char_params)\n",
        "        Xtr = hstack([vw.fit_transform(train['text'].iloc[tr]), vc.fit_transform(train['text'].iloc[tr])])\n",
        "        Xva = hstack([vw.transform(train['text'].iloc[va]), vc.transform(train['text'].iloc[va])])\n",
        "        Xte = hstack([vw.transform(test['text']), vc.transform(test['text'])])\n",
        "        clf = LogisticRegression(solver='lbfgs', C=C, max_iter=3000, tol=1e-4, n_jobs=1, random_state=42+f)\n",
        "        clf.fit(Xtr, y[tr])\n",
        "        p = clf.predict_proba(Xva); oof[va] = p; Ptest += clf.predict_proba(Xte)\n",
        "        scores.append(log_loss(y[va], p))\n",
        "    print(name, '10f OOF:', round(float(np.mean(scores)),4))\n",
        "    Ptest /= skf10.n_splits\n",
        "    pd.DataFrame(oof, columns=classes).to_csv(f'oof_10f_{name}.csv', index=False)\n",
        "    pd.DataFrame(Ptest, columns=classes).to_csv(f'test_10f_{name}.csv', index=False)\n",
        "\n",
        "# Run the 5 bases (skipping buggy CalSVC for now)\n",
        "run_10f_hstack('hstack_lr',\n",
        "    word_params=dict(analyzer='word', ngram_range=(1,3), lowercase=True, sublinear_tf=True, min_df=2, max_df=0.95),\n",
        "    char_params=dict(analyzer='char_wb', ngram_range=(2,6), lowercase=False, sublinear_tf=True, min_df=2, max_df=0.98),\n",
        "    C=6.0)\n",
        "run_10fold('char_wb_2_7', dict(analyzer='char_wb', ngram_range=(2,7), lowercase=False, sublinear_tf=True, min_df=2, max_df=0.98), C=8.0)\n",
        "run_10fold('char_wb_1_7', dict(analyzer='char_wb', ngram_range=(1,7), lowercase=False, sublinear_tf=True, min_df=1, max_df=0.98), C=5.0)\n",
        "run_10fold('char_wb_3_7', dict(analyzer='char_wb', ngram_range=(3,7), lowercase=False, sublinear_tf=True, min_df=3, max_df=0.97), C=10.0)\n",
        "run_10fold('char_2_7_mindf3', dict(analyzer='char', ngram_range=(2,7), lowercase=False, sublinear_tf=True, min_df=3, max_df=0.98), C=6.0)\n",
        "\n",
        "# Re-ensemble on the new 10-fold OOFs (Ridge meta + greedy weighted average)\n",
        "import numpy as np, pandas as pd, itertools\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "train = pd.read_csv('train.csv'); le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "\n",
        "# load pairs (exclude calsvc)\n",
        "names = ['hstack_lr','char_wb_2_7','char_wb_1_7','char_wb_3_7','char_2_7_mindf3']\n",
        "oofs = [pd.read_csv(f'oof_10f_{n}.csv')[classes].values for n in names]\n",
        "tests = [pd.read_csv(f'test_10f_{n}.csv')[classes].values for n in names]\n",
        "\n",
        "# greedy forward (simple mean)\n",
        "selected = []; best = 1e9\n",
        "while True:\n",
        "    improved = False; best_idx = None; best_sc = None\n",
        "    for i,(oo,_) in enumerate(zip(oofs, tests)):\n",
        "        if i in selected: continue\n",
        "        idxs = selected + [i]\n",
        "        blend = np.mean([oofs[j] for j in idxs], axis=0)\n",
        "        sc = log_loss(y, blend)\n",
        "        if sc < best - 1e-6:\n",
        "            improved = True; best = sc; best_idx = i; best_sc = sc\n",
        "    if not improved: break\n",
        "    selected.append(best_idx)\n",
        "print('Greedy OOF:', round(best,4), 'selected:', [names[i] for i in selected])\n",
        "\n",
        "# dirichlet weight search on selected\n",
        "sel_oofs  = [oofs[i] for i in selected]\n",
        "sel_tests = [tests[i] for i in selected]\n",
        "rng = np.random.default_rng(42); best_w = None; best_w_sc = 1e9\n",
        "for _ in range(4000):\n",
        "    w = rng.dirichlet(np.ones(len(sel_oofs)))\n",
        "    sc = log_loss(y, sum(wi*oo for wi,oo in zip(w, sel_oofs)))\n",
        "    if sc < best_w_sc: best_w_sc, best_w = sc, w\n",
        "print('Weighted OOF:', round(best_w_sc,4))\n",
        "\n",
        "# ridge meta on concatenated base probs\n",
        "X = np.hstack(sel_oofs); Xt = np.hstack(sel_tests)\n",
        "def to_prob(P): P = np.clip(P,1e-15,1-1e-15); return P / P.sum(1, keepdims=True)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "best_a = None; best_ridge = 1e9\n",
        "for a in [0.1,0.2,0.3,0.5,1.0,1.5,2.0]:\n",
        "    oof_meta = np.zeros((len(train),3)); scs=[]\n",
        "    for tr,va in skf.split(X, y):\n",
        "        Y = np.zeros((len(tr),3)); Y[np.arange(len(tr)), y[tr]] = 1\n",
        "        ridge = Ridge(alpha=a, random_state=42).fit(X[tr], Y)\n",
        "        p = to_prob(ridge.predict(X[va])); oof_meta[va] = p; scs.append(log_loss(y[va], p))\n",
        "    sc = float(np.mean(scs))\n",
        "    if sc < best_ridge: best_ridge, best_a = sc, a\n",
        "print('Ridge OOF:', round(best_ridge,4), 'alpha:', best_a)\n",
        "\n",
        "# pick best and save submission\n",
        "if best_ridge < best_w_sc:\n",
        "    Y = np.zeros((len(train),3)); Y[np.arange(len(train)), y] = 1\n",
        "    ridge = Ridge(alpha=best_a, random_state=42).fit(X, Y)\n",
        "    final = to_prob(ridge.predict(Xt))\n",
        "else:\n",
        "    final = sum(wi*tt for wi,tt in zip(best_w, sel_tests))\n",
        "final = np.clip(final,1e-15,1-1e-15); final /= final.sum(1, keepdims=True)\n",
        "pd.DataFrame(final, columns=classes).to_csv('submission.csv', index=False)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr 10f OOF: 0.3629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 10f OOF: 0.3912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_7 10f OOF: 0.4001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_3_7 10f OOF: 0.3947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_2_7_mindf3 10f OOF: 0.4032\nGreedy OOF: 0.3629 selected: ['hstack_lr']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted OOF: 0.3629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge OOF: 0.3963 alpha: 2.0\n"
          ]
        }
      ]
    },
    {
      "id": "39aba44e-446c-48a0-a159-14c008a4799a",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 47 \u2014 Light Pseudo-Labeling of strong char_wb bases (5-fold CV), then save OOF/test\n",
        "import numpy as np, pandas as pd, re, unicodedata\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# 0) Normalization (same as Cell 46)\n",
        "def normalize_text(s: str) -> str:\n",
        "    if not isinstance(s, str): s = '' if s is None else str(s)\n",
        "    s = unicodedata.normalize('NFKC', s)\n",
        "    s = (s.replace('\u201c','\"').replace('\u201d','\"')\n",
        "           .replace('\u2018',\"'\").replace('\u2019',\"'\")\n",
        "           .replace('\u2014','-').replace('\u2013','-').replace('\u2212','-')\n",
        "           .replace('\u2026',' ... ').replace('\\u00A0',' '))\n",
        "    s = re.sub(r'\\d+', '0', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna('').map(normalize_text)\n",
        "test['text']  = test['text'].fillna('').map(normalize_text)\n",
        "\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 1) Select high-confidence pseudo labels from current submission\n",
        "sub = pd.read_csv('submission.csv')[classes].values  # from Cell 46\n",
        "maxp = sub.max(axis=1)\n",
        "# target top 10\u201315% with a min prob floor\n",
        "target_frac = 0.15\n",
        "q_thr = np.quantile(maxp, 1 - target_frac)\n",
        "thr = max(q_thr, 0.90)  # use 0.90 floor; raise to 0.92\u20130.95 if noisy\n",
        "mask = maxp >= thr\n",
        "pseudo_text = test.loc[mask, 'text'].values\n",
        "pseudo_y = sub[mask].argmax(axis=1)\n",
        "pl_w = 0.35\n",
        "print(f'Pseudo-labeled rows: {mask.sum()} ({mask.mean():.1%}) | threshold: {thr:.3f}')\n",
        "\n",
        "# 2) Helpers: fit each fold on (train_fold + all pseudo), OOF strictly on original train\n",
        "def retrain_hstack_on_pseudo(name, word_params, char_params, C):\n",
        "    oof = np.zeros((len(train), 3)); test_preds = []; scores=[]\n",
        "    for f,(tr,va) in enumerate(skf5.split(train['text'], y), 1):\n",
        "        Xtr_text = pd.concat([train['text'].iloc[tr], pd.Series(pseudo_text)], ignore_index=True)\n",
        "        ytr = np.concatenate([y[tr], pseudo_y])\n",
        "        sw = np.concatenate([np.ones(len(tr)), np.full(len(pseudo_y), pl_w)])\n",
        "        Xva_text = train['text'].iloc[va]\n",
        "\n",
        "        vw = TfidfVectorizer(**word_params); vc = TfidfVectorizer(**char_params)\n",
        "        Xtr = hstack([vw.fit_transform(Xtr_text), vc.fit_transform(Xtr_text)])\n",
        "        Xva = hstack([vw.transform(Xva_text), vc.transform(Xva_text)])\n",
        "        Xte = hstack([vw.transform(test['text']), vc.transform(test['text'])])\n",
        "\n",
        "        clf = LogisticRegression(solver='lbfgs', C=C, max_iter=3000, tol=1e-4, n_jobs=1, random_state=2025+f)\n",
        "        clf.fit(Xtr, ytr, sample_weight=sw)\n",
        "        p = clf.predict_proba(Xva); oof[va] = p; test_preds.append(clf.predict_proba(Xte))\n",
        "        s = log_loss(y[va], p); scores.append(s); print(f'{name} Fold {f}: {s:.4f}')\n",
        "    sc = float(np.mean(scores)); print(f'{name} PL OOF: {sc:.4f}')\n",
        "    ptest = np.mean(test_preds, axis=0)\n",
        "    pd.DataFrame(oof, columns=classes).to_csv(f'oof_pl_{name}.csv', index=False)\n",
        "    pd.DataFrame(ptest, columns=classes).to_csv(f'test_pl_{name}.csv', index=False)\n",
        "\n",
        "def retrain_single_on_pseudo(name, vec_params, C):\n",
        "    oof = np.zeros((len(train), 3)); test_preds = []; scores=[]\n",
        "    for f,(tr,va) in enumerate(skf5.split(train['text'], y), 1):\n",
        "        Xtr_text = pd.concat([train['text'].iloc[tr], pd.Series(pseudo_text)], ignore_index=True)\n",
        "        ytr = np.concatenate([y[tr], pseudo_y])\n",
        "        sw = np.concatenate([np.ones(len(tr)), np.full(len(pseudo_y), pl_w)])\n",
        "        Xva_text = train['text'].iloc[va]\n",
        "\n",
        "        vec = TfidfVectorizer(**vec_params)\n",
        "        Xtr = vec.fit_transform(Xtr_text); Xva = vec.transform(Xva_text); Xte = vec.transform(test['text'])\n",
        "        clf = LogisticRegression(solver='lbfgs', C=C, max_iter=3000, tol=1e-4, n_jobs=1, random_state=2025+f)\n",
        "        clf.fit(Xtr, ytr, sample_weight=sw)\n",
        "        p = clf.predict_proba(Xva); oof[va] = p; test_preds.append(clf.predict_proba(Xte))\n",
        "        s = log_loss(y[va], p); scores.append(s); print(f'{name} Fold {f}: {s:.4f}')\n",
        "    sc = float(np.mean(scores)); print(f'{name} PL OOF: {sc:.4f}')\n",
        "    ptest = np.mean(test_preds, axis=0)\n",
        "    pd.DataFrame(oof, columns=classes).to_csv(f'oof_pl_{name}.csv', index=False)\n",
        "    pd.DataFrame(ptest, columns=classes).to_csv(f'test_pl_{name}.csv', index=False)\n",
        "\n",
        "# 3) Run your requested bases\n",
        "# hstack_lr (C=6)\n",
        "retrain_hstack_on_pseudo(\n",
        "    name='hstack_lr',\n",
        "    word_params=dict(analyzer='word', ngram_range=(1,3), lowercase=True, sublinear_tf=True, min_df=2, max_df=0.95),\n",
        "    char_params=dict(analyzer='char_wb', ngram_range=(2,6), lowercase=False, sublinear_tf=True, min_df=2, max_df=0.98),\n",
        "    C=6.0\n",
        ")\n",
        "# char_wb_2_7 (C=8)\n",
        "retrain_single_on_pseudo(\n",
        "    name='char_wb_2_7',\n",
        "    vec_params=dict(analyzer='char_wb', ngram_range=(2,7), lowercase=False, sublinear_tf=True, min_df=2, max_df=0.98),\n",
        "    C=8.0\n",
        ")\n",
        "# char_wb_1_7 (C=5)\n",
        "retrain_single_on_pseudo(\n",
        "    name='char_wb_1_7',\n",
        "    vec_params=dict(analyzer='char_wb', ngram_range=(1,7), lowercase=False, sublinear_tf=True, min_df=1, max_df=0.98),\n",
        "    C=5.0\n",
        ")\n",
        "\n",
        "# Optional extra diversity (comment out if time is tight)\n",
        "retrain_single_on_pseudo(\n",
        "    name='char_wb_1_8',\n",
        "    vec_params=dict(analyzer='char_wb', ngram_range=(1,8), lowercase=False, sublinear_tf=True, min_df=2, max_df=0.97),\n",
        "    C=6.0\n",
        ")\n",
        "retrain_single_on_pseudo(\n",
        "    name='char_wb_2_8_mindf1',\n",
        "    vec_params=dict(analyzer='char_wb', ngram_range=(2,8), lowercase=False, sublinear_tf=True, min_df=1, max_df=0.97),\n",
        "    C=7.0\n",
        ")\n",
        "\n",
        "print('Pseudo-labeling complete. Now re-ensemble including oof_pl_*.csv/test_pl_*.csv.')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pseudo-labeled rows: 294 (15.0%) | threshold: 0.987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 1: 0.3656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 2: 0.3676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 3: 0.3890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 4: 0.3723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 5: 0.3684\nhstack_lr PL OOF: 0.3726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 1: 0.3965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 2: 0.3919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 3: 0.4128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 4: 0.4000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 5: 0.3950\nchar_wb_2_7 PL OOF: 0.3992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_7 Fold 1: 0.4029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_7 Fold 2: 0.4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_7 Fold 3: 0.4215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_7 Fold 4: 0.4100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_7 Fold 5: 0.4055\nchar_wb_1_7 PL OOF: 0.4088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_8 Fold 1: 0.4012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_8 Fold 2: 0.4000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_8 Fold 3: 0.4163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_8 Fold 4: 0.4054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_1_8 Fold 5: 0.3981\nchar_wb_1_8 PL OOF: 0.4042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_8_mindf1 Fold 1: 0.3997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_8_mindf1 Fold 2: 0.3969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_8_mindf1 Fold 3: 0.4141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_8_mindf1 Fold 4: 0.4028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_8_mindf1 Fold 5: 0.3940\nchar_wb_2_8_mindf1 PL OOF: 0.4015\nPseudo-labeling complete. Now re-ensemble including oof_pl_*.csv/test_pl_*.csv.\n"
          ]
        }
      ]
    },
    {
      "id": "134108f0-2ccb-4951-a06f-3c71e1d626f8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 48 \u2014 Re-ensemble originals + PL models (greedy/weighted/Ridge meta)\n",
        "import numpy as np, pandas as pd, itertools\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "train = pd.read_csv('train.csv'); le = LabelEncoder(); y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "\n",
        "# Load original 10f + PL pairs (10 models total, all OOF <0.41)\n",
        "names = ['hstack_lr', 'char_wb_2_7', 'char_wb_1_7', 'char_wb_3_7', 'char_2_7_mindf3',\n",
        "         'pl_hstack_lr', 'pl_char_wb_2_7', 'pl_char_wb_1_7', 'pl_char_wb_1_8', 'pl_char_wb_2_8_mindf1']\n",
        "oofs = []\n",
        "tests = []\n",
        "for n in names:\n",
        "    if 'pl_' in n:\n",
        "        oof_file = f'oof_pl_{n[3:]}.csv'\n",
        "        test_file = f'test_pl_{n[3:]}.csv'\n",
        "    else:\n",
        "        oof_file = f'oof_10f_{n}.csv'\n",
        "        test_file = f'test_10f_{n}.csv'\n",
        "    oofs.append(pd.read_csv(oof_file)[classes].values)\n",
        "    tests.append(pd.read_csv(test_file)[classes].values)\n",
        "\n",
        "# greedy forward (simple mean)\n",
        "selected = []; best = 1e9\n",
        "while True:\n",
        "    improved = False; best_idx = None; best_sc = None\n",
        "    for i,(oo,_) in enumerate(zip(oofs, tests)):\n",
        "        if i in selected: continue\n",
        "        idxs = selected + [i]\n",
        "        blend = np.mean([oofs[j] for j in idxs], axis=0)\n",
        "        sc = log_loss(y, blend)\n",
        "        if sc < best - 1e-6:\n",
        "            improved = True; best = sc; best_idx = i; best_sc = sc\n",
        "    if not improved: break\n",
        "    selected.append(best_idx)\n",
        "print('Greedy OOF:', round(best,4), 'selected:', [names[i] for i in selected])\n",
        "\n",
        "# dirichlet weight search on selected (4000 iters)\n",
        "sel_oofs  = [oofs[i] for i in selected]\n",
        "sel_tests = [tests[i] for i in selected]\n",
        "rng = np.random.default_rng(42); best_w = None; best_w_sc = 1e9\n",
        "for _ in range(4000):\n",
        "    w = rng.dirichlet(np.ones(len(sel_oofs)))\n",
        "    sc = log_loss(y, sum(wi*oo for wi,oo in zip(w, sel_oofs)))\n",
        "    if sc < best_w_sc: best_w_sc, best_w = sc, w\n",
        "print('Weighted OOF:', round(best_w_sc,4))\n",
        "\n",
        "# ridge meta on concatenated base probs\n",
        "X = np.hstack(sel_oofs); Xt = np.hstack(sel_tests)\n",
        "def to_prob(P): P = np.clip(P,1e-15,1-1e-15); return P / P.sum(1, keepdims=True)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "best_a = None; best_ridge = 1e9\n",
        "for a in [0.1,0.2,0.3,0.5,1.0,1.5,2.0]:\n",
        "    oof_meta = np.zeros((len(train),3)); scs=[]\n",
        "    for tr,va in skf.split(X, y):\n",
        "        Y = np.zeros((len(tr),3)); Y[np.arange(len(tr)), y[tr]] = 1\n",
        "        ridge = Ridge(alpha=a, random_state=42).fit(X[tr], Y)\n",
        "        p = to_prob(ridge.predict(X[va])); oof_meta[va] = p; scs.append(log_loss(y[va], p))\n",
        "    sc = float(np.mean(scs))\n",
        "    if sc < best_ridge: best_ridge, best_a = sc, a\n",
        "print('Ridge OOF:', round(best_ridge,4), 'alpha:', best_a)\n",
        "\n",
        "# pick best and save submission\n",
        "if best_ridge < best_w_sc:\n",
        "    Y = np.zeros((len(train),3)); Y[np.arange(len(train)), y] = 1\n",
        "    ridge = Ridge(alpha=best_a, random_state=42).fit(X, Y)\n",
        "    final = to_prob(ridge.predict(Xt))\n",
        "else:\n",
        "    final = sum(wi*tt for wi,tt in zip(best_w, sel_tests))\n",
        "final = np.clip(final,1e-15,1-1e-15); final /= final.sum(1, keepdims=True)\n",
        "pd.DataFrame(final, columns=classes).to_csv('submission.csv', index=False)\n",
        "print('Re-ensemble complete. Check final OOF and submission.csv')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy OOF: 0.3629 selected: ['hstack_lr']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted OOF: 0.3629\nRidge OOF: 0.3977 alpha: 2.0\nRe-ensemble complete. Check final OOF and submission.csv\n"
          ]
        }
      ]
    },
    {
      "id": "d234bf4b-03f1-483e-9dcf-30493412e0da",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def pr(y_i, X, alpha=1.0):\n",
        "    p = X[y_i].sum(0).astype(float)\n",
        "    p += alpha\n",
        "    nb = X[~y_i].sum(0).astype(float)\n",
        "    nb += alpha\n",
        "    num = p / nb\n",
        "    num = num.A1\n",
        "    return np.log(num)\n",
        "\n",
        "oof = np.zeros((len(train), len(classes)))\n",
        "test_pred = np.zeros((len(test), len(classes)))\n",
        "for tr, va in skf.split(train, y):\n",
        "    bin_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=2, max_df=0.95, binary=True)\n",
        "    bin_vec = CountVectorizer(**bin_params)\n",
        "    Xtr_bin = bin_vec.fit_transform(train['text'].iloc[tr])\n",
        "    Xva_bin = bin_vec.transform(train['text'].iloc[va])\n",
        "    Xte_bin = bin_vec.transform(test['text'])\n",
        "    count_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=2, max_df=0.95, binary=False)\n",
        "    count_vec = CountVectorizer(**count_params)\n",
        "    Xtr_count = count_vec.fit_transform(train['text'].iloc[tr])\n",
        "    pva = np.zeros((len(va), len(classes)))\n",
        "    pte = np.zeros((len(test), len(classes)))\n",
        "    for i in range(len(classes)):\n",
        "        y_i = (y[tr] == i)\n",
        "        ratio = pr(y_i, Xtr_count, alpha=1.0)\n",
        "        Xtr_r = Xtr_bin.multiply(ratio)\n",
        "        lr = LogisticRegression(C=4.0, max_iter=3000, dual=False, random_state=123)\n",
        "        lr.fit(Xtr_r, y_i)\n",
        "        Xva_r = Xva_bin.multiply(ratio)\n",
        "        pva[:,i] = lr.predict_proba(Xva_r)[:,1]\n",
        "        Xte_r = Xte_bin.multiply(ratio)\n",
        "        pte[:,i] = lr.predict_proba(Xte_r)[:,1]\n",
        "    pva = np.clip(pva, 1e-15, 1-1e-15)\n",
        "    pva /= pva.sum(axis=1, keepdims=True)\n",
        "    oof[va] = pva\n",
        "    pte = np.clip(pte, 1e-15, 1-1e-15)\n",
        "    pte /= pte.sum(axis=1, keepdims=True)\n",
        "    test_pred += pte\n",
        "test_pred /= 5\n",
        "score = log_loss(y, oof)\n",
        "print('Word NB-SVM OOF logloss:', round(score, 4))\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_word_nbsvm.csv', index=False)\n",
        "pd.DataFrame(test_pred, columns=classes).to_csv('test_word_nbsvm.csv', index=False)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM OOF logloss: 0.4573\n"
          ]
        }
      ]
    },
    {
      "id": "e6979393-2d40-482b-b9cd-86939bcb857e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "\n",
        "# Load new Word NB-SVM\n",
        "oof_word = pd.read_csv('oof_word_nbsvm.csv')[classes].values\n",
        "test_word = pd.read_csv('test_word_nbsvm.csv')[classes].values\n",
        "print('Word NB-SVM OOF:', log_loss(y, oof_word))\n",
        "\n",
        "# Top char models (OOF <0.41)\n",
        "models = [\n",
        "    ('oof_10f_hstack_lr.csv', 'test_10f_hstack_lr.csv'),\n",
        "    ('oof_pl_hstack_lr.csv', 'test_pl_hstack_lr.csv'),\n",
        "    ('oof_10f_char_wb_2_7.csv', 'test_10f_char_wb_2_7.csv'),\n",
        "    ('oof_10f_char_wb_3_7.csv', 'test_10f_char_wb_3_7.csv'),\n",
        "    ('oof_pl_char_wb_2_7.csv', 'test_pl_char_wb_2_7.csv')\n",
        "]\n",
        "oofs = [pd.read_csv(o)[classes].values for o, _ in models]\n",
        "tests = [pd.read_csv(t)[classes].values for _, t in models]\n",
        "names = [m[0].replace('oof_10f_', '').replace('oof_pl_', '').replace('.csv', '') for m in models]\n",
        "\n",
        "# Quick check: 0.7 * hstack_lr + 0.3 * word_nbsvm\n",
        "hstack_oof = oofs[0]\n",
        "blend_quick = 0.7 * hstack_oof + 0.3 * oof_word\n",
        "score_quick = log_loss(y, blend_quick)\n",
        "print('Quick blend OOF (0.7 hstack + 0.3 word):', score_quick)\n",
        "if score_quick < 0.3629:\n",
        "    print('Word NB-SVM adds value; include in ensemble')\n",
        "    include_word = True\n",
        "else:\n",
        "    print('Word NB-SVM does not improve; skip')\n",
        "    include_word = False\n",
        "\n",
        "# If include, Ridge meta on top 3-4 + word; else on top char only\n",
        "if include_word:\n",
        "    sel_oofs = [oofs[0], oofs[1], oofs[2], oof_word]  # hstack, pl_hstack, char_wb_2_7, word\n",
        "    sel_tests = [tests[0], tests[1], tests[2], test_word]\n",
        "    sel_names = ['hstack_lr', 'pl_hstack_lr', 'char_wb_2_7', 'word_nbsvm']\n",
        "else:\n",
        "    sel_oofs = [oofs[0], oofs[1], oofs[2]]  # top char\n",
        "    sel_tests = [tests[0], tests[1], tests[2]]\n",
        "    sel_names = ['hstack_lr', 'pl_hstack_lr', 'char_wb_2_7']\n",
        "\n",
        "# Ridge meta CV\n",
        "X = np.hstack(sel_oofs)\n",
        "Xt = np.hstack(sel_tests)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "def to_prob(P):\n",
        "    P = np.clip(P, 1e-15, 1-1e-15)\n",
        "    return P / P.sum(axis=1, keepdims=True)\n",
        "best_a = 1.0; best_ridge = float('inf')\n",
        "for a in [0.1, 0.5, 1.0, 2.0, 5.0]:\n",
        "    oof_meta = np.zeros((len(train), 3)); scs = []\n",
        "    for tr, va in skf.split(X, y):\n",
        "        Y = np.zeros((len(tr), 3)); Y[np.arange(len(tr)), y[tr]] = 1\n",
        "        ridge = Ridge(alpha=a, random_state=42).fit(X[tr], Y)\n",
        "        p = to_prob(ridge.predict(X[va])); oof_meta[va] = p\n",
        "        scs.append(log_loss(y[va], p))\n",
        "    sc = float(np.mean(scs))\n",
        "    print(f'Ridge alpha={a} OOF: {sc:.4f}')\n",
        "    if sc < best_ridge:\n",
        "        best_ridge = sc; best_a = a\n",
        "\n",
        "# Final fit and submission\n",
        "Y = np.zeros((len(train), 3)); Y[np.arange(len(train)), y] = 1\n",
        "ridge = Ridge(alpha=best_a, random_state=42).fit(X, Y)\n",
        "final = to_prob(ridge.predict(Xt))\n",
        "pd.DataFrame(final, columns=classes).to_csv('submission.csv', index=False)\n",
        "print(f'Final ensemble OOF: {best_ridge:.4f} | Models: {sel_names} | Include Word: {include_word}')\n",
        "print('submission.csv updated')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM OOF: 0.4572863139813886\nQuick blend OOF (0.7 hstack + 0.3 word): 0.35687910180008375\nWord NB-SVM adds value; include in ensemble\nRidge alpha=0.1 OOF: 0.4188\nRidge alpha=0.5 OOF: 0.4187\nRidge alpha=1.0 OOF: 0.4171\nRidge alpha=2.0 OOF: 0.4171\nRidge alpha=5.0 OOF: 0.4187\nFinal ensemble OOF: 0.4171 | Models: ['hstack_lr', 'pl_hstack_lr', 'char_wb_2_7', 'word_nbsvm'] | Include Word: True\nsubmission.csv updated\n"
          ]
        }
      ]
    },
    {
      "id": "baa42132-b5b4-4428-a035-852732dd9559",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def pr(y_i, X, alpha=0.5):\n",
        "    p = X[y_i].sum(0).astype(float)\n",
        "    p += alpha\n",
        "    nb = X[~y_i].sum(0).astype(float)\n",
        "    nb += alpha\n",
        "    num = p / nb\n",
        "    num = num.A1\n",
        "    return np.log(num)\n",
        "\n",
        "def odds_normalize(P, eps=1e-15):\n",
        "    P = np.clip(P, eps, 1-eps)\n",
        "    odds = P / (1 - P)\n",
        "    Q = odds / (odds.sum(axis=1, keepdims=True) + eps)\n",
        "    return Q / Q.sum(axis=1, keepdims=True)\n",
        "\n",
        "C_grid = [4.0, 6.0, 8.0]\n",
        "best_sc = float('inf')\n",
        "best_oof = None\n",
        "best_test = None\n",
        "best_C = None\n",
        "for C in C_grid:\n",
        "    oof = np.zeros((len(train), len(classes)))\n",
        "    test_pred = np.zeros((len(test), len(classes)))\n",
        "    scores = []\n",
        "    for tr, va in skf.split(train, y):\n",
        "        vec_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=3, max_df=0.90, binary=False)\n",
        "        vec = CountVectorizer(**vec_params)\n",
        "        Xtr_cnt = vec.fit_transform(train['text'].iloc[tr])\n",
        "        Xva_cnt = vec.transform(train['text'].iloc[va])\n",
        "        Xte_cnt = vec.transform(test['text'])\n",
        "        # Binary copies\n",
        "        Xtr_bin = Xtr_cnt.copy(); Xtr_bin.data[:] = 1\n",
        "        Xva_bin = Xva_cnt.copy(); Xva_bin.data[:] = 1\n",
        "        Xte_bin = Xte_cnt.copy(); Xte_bin.data[:] = 1\n",
        "        pva = np.zeros((len(va), len(classes)))\n",
        "        pte = np.zeros((len(test), len(classes)))\n",
        "        for i in range(len(classes)):\n",
        "            y_i = (y[tr] == i)\n",
        "            ratio = pr(y_i, Xtr_cnt, alpha=0.5)\n",
        "            Xtr_r = Xtr_bin.multiply(ratio)\n",
        "            lr = LogisticRegression(solver='liblinear', penalty='l2', C=C, max_iter=3000, tol=1e-4, random_state=42+i)\n",
        "            lr.fit(Xtr_r, y_i)\n",
        "            Xva_r = Xva_bin.multiply(ratio)\n",
        "            pva[:,i] = lr.predict_proba(Xva_r)[:,1]\n",
        "            Xte_r = Xte_bin.multiply(ratio)\n",
        "            pte[:,i] = lr.predict_proba(Xte_r)[:,1]\n",
        "        pva = odds_normalize(pva)\n",
        "        oof[va] = pva\n",
        "        pte = odds_normalize(pte)\n",
        "        test_pred += pte\n",
        "        scores.append(log_loss(y[va], pva))\n",
        "    sc = float(np.mean(scores))\n",
        "    print(f'Word NB-SVM C={C} OOF: {sc:.4f}')\n",
        "    test_pred /= 5\n",
        "    if sc < best_sc:\n",
        "        best_sc = sc\n",
        "        best_oof = oof\n",
        "        best_test = test_pred\n",
        "        best_C = C\n",
        "print(f'Best Word NB-SVM OOF: {best_sc:.4f} at C={best_C}')\n",
        "pd.DataFrame(best_oof, columns=classes).to_csv('oof_word_nbsvm_fixed.csv', index=False)\n",
        "pd.DataFrame(best_test, columns=classes).to_csv('test_word_nbsvm_fixed.csv', index=False)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=4.0 OOF: 0.5915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=6.0 OOF: 0.6424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=8.0 OOF: 0.6818\nBest Word NB-SVM OOF: 0.5915 at C=4.0\n"
          ]
        }
      ]
    },
    {
      "id": "663147cc-a715-4bdb-ab58-5916356210a6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "\n",
        "# Skip bad Word NB-SVM (OOF 0.5915 >0.40); focus on top char + PL (OOF <0.41)\n",
        "names = [\n",
        "    'hstack_lr', 'pl_hstack_lr', 'char_wb_2_7', 'char_wb_3_7',\n",
        "    'pl_char_wb_2_7', 'pl_char_wb_1_7', 'pl_char_wb_1_8', 'pl_char_wb_2_8_mindf1'\n",
        "]\n",
        "oof_files = [f'oof_10f_{n}.csv' if 'pl_' not in n else f'oof_pl_{n[3:]}.csv' for n in names]\n",
        "test_files = [f'test_10f_{n}.csv' if 'pl_' not in n else f'test_pl_{n[3:]}.csv' for n in names]\n",
        "\n",
        "oofs = [pd.read_csv(of)[classes].values for of in oof_files]\n",
        "tests = [pd.read_csv(tf)[classes].values for tf in test_files]\n",
        "oof_scores = [log_loss(y, oof) for oof in oofs]\n",
        "print('Model OOFs:', dict(zip(names, [round(s,4) for s in oof_scores])))\n",
        "\n",
        "# Filter to top (OOF <0.40)\n",
        "top_idx = [i for i,s in enumerate(oof_scores) if s < 0.40]\n",
        "top_oofs = [oofs[i] for i in top_idx]\n",
        "top_tests = [tests[i] for i in top_idx]\n",
        "top_names = [names[i] for i in top_idx]\n",
        "print('Top models:', top_names)\n",
        "\n",
        "# 1. Greedy forward selection (simple mean)\n",
        "selected = []; best_greedy = float('inf')\n",
        "while True:\n",
        "    improved = False; best_add = None; best_sc = None\n",
        "    for i in range(len(top_oofs)):\n",
        "        if i in selected: continue\n",
        "        cur_idx = selected + [i]\n",
        "        blend = np.mean([top_oofs[j] for j in cur_idx], axis=0)\n",
        "        sc = log_loss(y, blend)\n",
        "        if sc < best_greedy - 1e-6:\n",
        "            improved = True; best_greedy = sc; best_add = i; best_sc = sc\n",
        "    if not improved: break\n",
        "    selected.append(best_add)\n",
        "    print(f'Greedy added {top_names[best_add]} -> OOF {best_sc:.4f}')\n",
        "print('Greedy final OOF:', round(best_greedy,4), 'models:', [top_names[i] for i in selected])\n",
        "\n",
        "# 2. Dirichlet weight search on selected (5000 iters)\n",
        "sel_oofs = [top_oofs[i] for i in selected]\n",
        "sel_tests = [top_tests[i] for i in selected]\n",
        "rng = np.random.default_rng(42)\n",
        "best_w = None; best_w_sc = float('inf')\n",
        "for _ in range(5000):\n",
        "    w = rng.dirichlet(np.ones(len(sel_oofs)))\n",
        "    blend = sum(wi * oo for wi, oo in zip(w, sel_oofs))\n",
        "    sc = log_loss(y, blend)\n",
        "    if sc < best_w_sc:\n",
        "        best_w_sc = sc; best_w = w\n",
        "print('Dirichlet weighted OOF:', round(best_w_sc,4))\n",
        "\n",
        "# 3. Multinomial LR meta on logits of selected\n",
        "def to_logits(P):\n",
        "    P = np.clip(P, 1e-15, 1-1e-15)\n",
        "    return np.log(P / (1 - P))\n",
        "logit_oofs = [to_logits(oo) for oo in sel_oofs]\n",
        "X_logit = np.hstack(logit_oofs)\n",
        "Xt_logit = np.hstack([to_logits(tt) for tt in sel_tests])\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_meta = np.zeros((len(train), 3)); scs = []\n",
        "for tr, va in skf.split(X_logit, y):\n",
        "    Y = np.zeros((len(tr), 3)); Y[np.arange(len(tr)), y[tr]] = 1\n",
        "    meta_lr = LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000, multi_class='multinomial', random_state=42)\n",
        "    meta_lr.fit(X_logit[tr], y[tr])\n",
        "    p_logit = meta_lr.predict_proba(X_logit[va])\n",
        "    oof_meta[va] = p_logit\n",
        "    scs.append(log_loss(y[va], p_logit))\n",
        "lr_sc = float(np.mean(scs)); print('LR on logits OOF:', round(lr_sc,4))\n",
        "\n",
        "# Pick best method\n",
        "methods = {\n",
        "    'greedy': best_greedy,\n",
        "    'weighted': best_w_sc,\n",
        "    'lr_logits': lr_sc\n",
        "}\n",
        "best_method = min(methods, key=methods.get)\n",
        "best_sc = methods[best_method]\n",
        "print(f'Best method: {best_method} with OOF {best_sc:.4f}')\n",
        "\n",
        "# Generate final test preds\n",
        "if best_method == 'greedy':\n",
        "    final_test = np.mean(sel_tests, axis=0)\n",
        "elif best_method == 'weighted':\n",
        "    final_test = sum(wi * tt for wi, tt in zip(best_w, sel_tests))\n",
        "else:  # lr_logits\n",
        "    meta_lr = LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000, multi_class='multinomial', random_state=42)\n",
        "    meta_lr.fit(X_logit, y)\n",
        "    final_test = meta_lr.predict_proba(Xt_logit)\n",
        "\n",
        "final_test = np.clip(final_test, 1e-15, 1-1e-15)\n",
        "final_test /= final_test.sum(axis=1, keepdims=True)\n",
        "pd.DataFrame(final_test, columns=classes).to_csv('submission.csv', index=False)\n",
        "print('Final submission.csv saved with OOF:', round(best_sc,4))\n",
        "if best_sc <= 0.35:\n",
        "    print('Medal territory! Ready for submit_final_answer.')\n",
        "else:\n",
        "    print('Still above 0.35; consider further tuning or diversity.')"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model OOFs: {'hstack_lr': 0.3629, 'pl_hstack_lr': 0.3726, 'char_wb_2_7': 0.3912, 'char_wb_3_7': 0.3947, 'pl_char_wb_2_7': 0.3992, 'pl_char_wb_1_7': 0.4088, 'pl_char_wb_1_8': 0.4042, 'pl_char_wb_2_8_mindf1': 0.4015}\nTop models: ['hstack_lr', 'pl_hstack_lr', 'char_wb_2_7', 'char_wb_3_7', 'pl_char_wb_2_7']\nGreedy added hstack_lr -> OOF 0.3629\nGreedy final OOF: 0.3629 models: ['hstack_lr']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dirichlet weighted OOF: 0.3629\nLR on logits OOF: 0.3561\nBest method: lr_logits with OOF 0.3561\nFinal submission.csv saved with OOF: 0.3561\nStill above 0.35; consider further tuning or diversity.\n"
          ]
        }
      ]
    },
    {
      "id": "30bc8f0c-15f0-47f4-a829-95c3880e2c6e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(train['author'])\n",
        "classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "def pr(y_i, X, alpha=0.1):\n",
        "    p = X[y_i].sum(0).astype(float)\n",
        "    p += alpha\n",
        "    nb = X[~y_i].sum(0).astype(float)\n",
        "    nb += alpha\n",
        "    num = p / nb\n",
        "    num = num.A1\n",
        "    return np.log(num)\n",
        "\n",
        "def odds_normalize(P, eps=1e-15):\n",
        "    P = np.clip(P, eps, 1-eps)\n",
        "    odds = P / (1 - P)\n",
        "    Q = odds / (odds.sum(axis=1, keepdims=True) + eps)\n",
        "    return Q / Q.sum(axis=1, keepdims=True)\n",
        "\n",
        "C_grid = [2.0, 4.0, 6.0, 8.0, 12.0]\n",
        "best_sc = float('inf')\n",
        "best_oof = None\n",
        "best_test = None\n",
        "best_C = None\n",
        "for C in C_grid:\n",
        "    oof = np.zeros((len(train), len(classes)))\n",
        "    test_pred = np.zeros((len(test), len(classes)))\n",
        "    scores = []\n",
        "    for tr, va in skf.split(train, y):\n",
        "        vec_params = dict(analyzer='word', ngram_range=(1,3), lowercase=True, min_df=2, max_df=0.90, binary=False)\n",
        "        vec = CountVectorizer(**vec_params)\n",
        "        Xtr_cnt = vec.fit_transform(train['text'].iloc[tr])\n",
        "        Xva_cnt = vec.transform(train['text'].iloc[va])\n",
        "        Xte_cnt = vec.transform(test['text'])\n",
        "        # Binary copies\n",
        "        Xtr_bin = Xtr_cnt.copy(); Xtr_bin.data[:] = 1\n",
        "        Xva_bin = Xva_cnt.copy(); Xva_bin.data[:] = 1\n",
        "        Xte_bin = Xte_cnt.copy(); Xte_bin.data[:] = 1\n",
        "        pva = np.zeros((len(va), len(classes)))\n",
        "        pte = np.zeros((len(test), len(classes)))\n",
        "        for i in range(len(classes)):\n",
        "            y_i = (y[tr] == i)\n",
        "            ratio = pr(y_i, Xtr_cnt, alpha=0.1)\n",
        "            Xtr_r = Xtr_bin.multiply(ratio)\n",
        "            lr = LogisticRegression(solver='liblinear', penalty='l2', C=C, max_iter=3000, tol=1e-4, random_state=42+i)\n",
        "            lr.fit(Xtr_r, y_i)\n",
        "            Xva_r = Xva_bin.multiply(ratio)\n",
        "            pva[:,i] = lr.predict_proba(Xva_r)[:,1]\n",
        "            Xte_r = Xte_bin.multiply(ratio)\n",
        "            pte[:,i] = lr.predict_proba(Xte_r)[:,1]\n",
        "        pva = odds_normalize(pva)\n",
        "        oof[va] = pva\n",
        "        pte = odds_normalize(pte)\n",
        "        test_pred += pte\n",
        "        scores.append(log_loss(y[va], pva))\n",
        "    sc = float(np.mean(scores))\n",
        "    print(f'Word NB-SVM C={C} OOF: {sc:.4f}')\n",
        "    test_pred /= 10\n",
        "    if sc < best_sc:\n",
        "        best_sc = sc\n",
        "        best_oof = oof\n",
        "        best_test = test_pred\n",
        "        best_C = C\n",
        "print(f'Best Word NB-SVM OOF: {best_sc:.4f} at C={best_C}')\n",
        "pd.DataFrame(best_oof, columns=classes).to_csv('oof_word_nbsvm_improved.csv', index=False)\n",
        "pd.DataFrame(best_test, columns=classes).to_csv('test_word_nbsvm_improved.csv', index=False)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=2.0 OOF: 0.4792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=4.0 OOF: 0.5335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=6.0 OOF: 0.5696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=8.0 OOF: 0.5970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM C=12.0 OOF: 0.6382\nBest Word NB-SVM OOF: 0.4792 at C=2.0\n"
          ]
        }
      ]
    },
    {
      "id": "86f8d5c6-6c2e-4e4c-9dbe-7f7ddd571f61",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "def log_count_ratio(X, yb, alpha):\n",
        "    pos = np.asarray(X[yb==1].sum(0)).ravel() + alpha\n",
        "    neg = np.asarray(X[yb==0].sum(0)).ravel() + alpha\n",
        "    r = np.log(pos/neg); r[~np.isfinite(r)] = 0.0\n",
        "    return r\n",
        "\n",
        "def odds_norm(P, eps=1e-15):\n",
        "    P = np.clip(P, eps, 1-eps); odds = P/(1-P)\n",
        "    Q = odds / (odds.sum(axis=1, keepdims=True) + eps)\n",
        "    return Q / Q.sum(axis=1, keepdims=True)\n",
        "\n",
        "param_grid = [\n",
        "    {'ngram': (1,2), 'min_df': 1, 'alpha': 0.5, 'C': 2.0},\n",
        "    {'ngram': (1,2), 'min_df': 2, 'alpha': 0.75, 'C': 4.0},\n",
        "    {'ngram': (1,2), 'min_df': 3, 'alpha': 1.0, 'C': 6.0},\n",
        "    {'ngram': (1,3), 'min_df': 1, 'alpha': 0.5, 'C': 4.0},\n",
        "    {'ngram': (1,3), 'min_df': 2, 'alpha': 0.75, 'C': 6.0},\n",
        "    {'ngram': (1,3), 'min_df': 3, 'alpha': 1.0, 'C': 8.0},\n",
        "]\n",
        "\n",
        "best_sc = 1e9; best_oof = None; best_test = None; best_params = None\n",
        "for p in param_grid:\n",
        "    oof = np.zeros((len(train), 3)); scores = []; test_preds = []\n",
        "    for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "        vec = CountVectorizer(analyzer='word', ngram_range=p['ngram'], lowercase=True,\n",
        "                              min_df=p['min_df'], max_df=0.9, binary=False)\n",
        "        Xtr_cnt = vec.fit_transform(train['text'].iloc[tr])\n",
        "        Xva_cnt = vec.transform(train['text'].iloc[va])\n",
        "        Xte_cnt = vec.transform(test['text'])\n",
        "        Xtr_bin = Xtr_cnt.copy(); Xtr_bin.data[:] = 1\n",
        "        Xva_bin = Xva_cnt.copy(); Xva_bin.data[:] = 1\n",
        "        Xte_bin = Xte_cnt.copy(); Xte_bin.data[:] = 1\n",
        "\n",
        "        Pva = np.zeros((len(va), 3)); Pte = np.zeros((len(test), 3))\n",
        "        for c in range(3):\n",
        "            yb = (y[tr]==c).astype(int)\n",
        "            r = log_count_ratio(Xtr_cnt, yb, alpha=p['alpha'])\n",
        "            clf = LogisticRegression(solver='liblinear', penalty='l2', C=p['C'],\n",
        "                                     max_iter=3000, tol=1e-4, random_state=42+c)\n",
        "            clf.fit(Xtr_bin.multiply(csr_matrix(r)), yb)\n",
        "            Pva[:,c] = clf.predict_proba(Xva_bin.multiply(csr_matrix(r)))[:,1]\n",
        "            Pte[:,c] = clf.predict_proba(Xte_bin.multiply(csr_matrix(r)))[:,1]\n",
        "        Pva = odds_norm(Pva); oof[va] = Pva\n",
        "        test_preds.append(odds_norm(Pte))\n",
        "        scores.append(log_loss(y[va], Pva))\n",
        "    sc = float(np.mean(scores)); print(f'NB-SVM params {p} OOF: {sc:.4f}')\n",
        "    if sc < best_sc:\n",
        "        best_sc = sc; best_oof = oof; best_params = p\n",
        "        best_test = np.mean(test_preds, axis=0)\n",
        "\n",
        "print('Best Word NB-SVM OOF:', round(best_sc,4), 'params:', best_params)\n",
        "pd.DataFrame(best_oof, columns=classes).to_csv('oof_word_nbsvm_fixed.csv', index=False)\n",
        "pd.DataFrame(best_test, columns=classes).to_csv('test_word_nbsvm_fixed.csv', index=False)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM params {'ngram': (1, 2), 'min_df': 1, 'alpha': 0.5, 'C': 2.0} OOF: 0.4481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM params {'ngram': (1, 2), 'min_df': 2, 'alpha': 0.75, 'C': 4.0} OOF: 0.5200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM params {'ngram': (1, 2), 'min_df': 3, 'alpha': 1.0, 'C': 6.0} OOF: 0.5930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM params {'ngram': (1, 3), 'min_df': 1, 'alpha': 0.5, 'C': 4.0} OOF: 0.4756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM params {'ngram': (1, 3), 'min_df': 2, 'alpha': 0.75, 'C': 6.0} OOF: 0.5459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM params {'ngram': (1, 3), 'min_df': 3, 'alpha': 1.0, 'C': 8.0} OOF: 0.6181\nBest Word NB-SVM OOF: 0.4481 params: {'ngram': (1, 2), 'min_df': 1, 'alpha': 0.5, 'C': 2.0}\n"
          ]
        }
      ]
    },
    {
      "id": "6e740287-0927-4bc3-b285-c422a451ade0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "oof = np.zeros((len(train), 3)); test_preds = []; scores=[]\n",
        "for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "    vec = CountVectorizer(analyzer='word', ngram_range=(1,2), lowercase=True, min_df=2, max_df=0.95, binary=False)\n",
        "    Xtr = vec.fit_transform(train['text'].iloc[tr])\n",
        "    Xva = vec.transform(train['text'].iloc[va])\n",
        "    Xte = vec.transform(test['text'])\n",
        "    clf = ComplementNB(alpha=0.5)\n",
        "    clf.fit(Xtr, y[tr])\n",
        "    p = clf.predict_proba(Xva); oof[va] = p; test_preds.append(clf.predict_proba(Xte))\n",
        "    s = log_loss(y[va], p); scores.append(s)\n",
        "print('ComplementNB OOF:', round(float(np.mean(scores)),4))\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_word_cnb.csv', index=False)\n",
        "pd.DataFrame(np.mean(test_preds,0), columns=classes).to_csv('test_word_cnb.csv', index=False)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ComplementNB OOF: 0.5063\n"
          ]
        }
      ]
    },
    {
      "id": "b4e01c8e-a853-4d9f-9282-a20d8bf7e0a4",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "train = pd.read_csv('train.csv'); le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "\n",
        "# Expanded base pool per expert advice: top char + PL + diversity (CalSVC, Cal Ridge), word unconditionally\n",
        "bases = [\n",
        "    ('oof_10f_hstack_lr.csv', 'test_10f_hstack_lr.csv'),  # 0.3629\n",
        "    ('oof_pl_refined_hstack_lr.csv', 'test_pl_refined_hstack_lr.csv'),  # 0.3628\n",
        "    ('oof_10f_char_wb_2_7.csv', 'test_10f_char_wb_2_7.csv'),  # 0.3912\n",
        "    ('oof_pl_refined_char_wb_2_7.csv', 'test_pl_refined_char_wb_2_7.csv'),  # 0.3911\n",
        "    ('oof_10f_char_wb_1_7.csv', 'test_10f_char_wb_1_7.csv'),  # 0.4001\n",
        "    ('oof_10f_char_wb_3_7.csv', 'test_10f_char_wb_3_7.csv'),  # 0.3947\n",
        "    ('oof_calsvc_char.csv', 'test_calsvc_char.csv'),  # 0.4403 diversity\n",
        "    ('oof_10f_cal_ridge_char_wb.csv', 'test_10f_cal_ridge_char_wb.csv'),  # 0.4116 diversity\n",
        "]\n",
        "# Add best word unconditionally (improved2 0.4358 for diversity, better than CNB 0.5063)\n",
        "bases.append(('oof_word_nbsvm_improved2.csv', 'test_word_nbsvm_improved2.csv'))\n",
        "\n",
        "# Load all\n",
        "oofs = []; tests = []\n",
        "for o,t in bases:\n",
        "    oofs.append(pd.read_csv(o)[classes].values)\n",
        "    tests.append(pd.read_csv(t)[classes].values)\n",
        "\n",
        "# 1. Greedy forward selection (simple mean)\n",
        "best_greedy = 1e9; sel_greedy = []\n",
        "while True:\n",
        "    improved = False; cand = None\n",
        "    for i in range(len(oofs)):\n",
        "        if i in sel_greedy: continue\n",
        "        idx = sel_greedy + [i]\n",
        "        sc = log_loss(y, np.mean([oofs[j] for j in idx], axis=0))\n",
        "        if sc < best_greedy - 1e-6:\n",
        "            best_greedy = sc; improved = True; cand = i\n",
        "    if not improved: break\n",
        "    sel_greedy.append(cand)\n",
        "sel_oofs_greedy = [oofs[i] for i in sel_greedy]; sel_tests_greedy = [tests[i] for i in sel_greedy]\n",
        "print('Greedy OOF:', round(best_greedy,4), 'selected models:', [bases[i][0] for i in sel_greedy])\n",
        "\n",
        "# 2. Dirichlet weights on greedy selected (4000 iters for speed)\n",
        "rng = np.random.default_rng(42); best_w = None; best_w_sc = 1e9\n",
        "for _ in range(4000):\n",
        "    w = rng.dirichlet(np.ones(len(sel_oofs_greedy)))\n",
        "    sc = log_loss(y, sum(wi*oo for wi,oo in zip(w, sel_oofs_greedy)))\n",
        "    if sc < best_w_sc: best_w_sc = sc; best_w = w\n",
        "print('Dirichlet OOF:', round(best_w_sc,4))\n",
        "\n",
        "# 3. LR-on-logits with C grid on greedy selected (5-fold CV)\n",
        "def to_logits(P): \n",
        "    P = np.clip(P,1e-15,1-1e-15)\n",
        "    return np.log(P/(1-P))\n",
        "X_logit = np.hstack([to_logits(oo) for oo in sel_oofs_greedy])\n",
        "Xt_logit = np.hstack([to_logits(tt) for tt in sel_tests_greedy])\n",
        "c_grid = [0.5, 0.75, 1.0, 1.5, 2.0, 3.0]\n",
        "best_c = None; best_lr_sc = 1e9; best_oof_lr = None\n",
        "skf_meta = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for c in c_grid:\n",
        "    oof_meta = np.zeros((len(train),3)); scs=[]\n",
        "    for tr,va in skf_meta.split(X_logit, y):\n",
        "        meta = LogisticRegression(solver='lbfgs', C=c, max_iter=1000, multi_class='multinomial', random_state=42)\n",
        "        meta.fit(X_logit[tr], y[tr])\n",
        "        p = meta.predict_proba(X_logit[va]); oof_meta[va]=p; scs.append(log_loss(y[va], p))\n",
        "    sc = float(np.mean(scs)); print(f'LR C={c} OOF: {sc:.4f}')\n",
        "    if sc < best_lr_sc: best_lr_sc = sc; best_c = c; best_oof_lr = oof_meta\n",
        "print('Best LR-on-logits OOF:', round(best_lr_sc,4), 'at C=', best_c)\n",
        "\n",
        "# 4. Ridge meta on logits as fallback (alpha grid)\n",
        "from sklearn.linear_model import Ridge\n",
        "alpha_grid = [0.5, 1.0, 2.0]\n",
        "best_alpha = None; best_ridge_sc = 1e9; best_oof_ridge = None\n",
        "def to_onehot(y): Y = np.zeros((len(y),3)); Y[np.arange(len(y)), y] = 1; return Y\n",
        "for alpha in alpha_grid:\n",
        "    oof_ridge = np.zeros((len(train),3)); scs=[]\n",
        "    for tr,va in skf_meta.split(X_logit, y):\n",
        "        Y_tr = to_onehot(y[tr])\n",
        "        ridge = Ridge(alpha=alpha, random_state=42).fit(X_logit[tr], Y_tr)\n",
        "        p = ridge.predict(X_logit[va])\n",
        "        p = np.clip(p, 1e-15, 1-1e-15); p /= p.sum(axis=1, keepdims=True)\n",
        "        oof_ridge[va] = p; scs.append(log_loss(y[va], p))\n",
        "    sc = float(np.mean(scs)); print(f'Ridge alpha={alpha} OOF: {sc:.4f}')\n",
        "    if sc < best_ridge_sc: best_ridge_sc = sc; best_alpha = alpha; best_oof_ridge = oof_ridge\n",
        "print('Best Ridge meta OOF:', round(best_ridge_sc,4), 'at alpha=', best_alpha)\n",
        "\n",
        "# Pick the best method among greedy, weighted, LR, Ridge\n",
        "methods = {\n",
        "    'greedy': best_greedy,\n",
        "    'weighted': best_w_sc,\n",
        "    'lr_logits': best_lr_sc,\n",
        "    'ridge': best_ridge_sc\n",
        "}\n",
        "best_method = min(methods, key=methods.get)\n",
        "best_sc = methods[best_method]\n",
        "print(f'Best method: {best_method} with OOF {best_sc:.4f}')\n",
        "\n",
        "# Generate final test preds with best method\n",
        "if best_method == 'greedy':\n",
        "    final = np.mean(sel_tests_greedy, axis=0)\n",
        "elif best_method == 'weighted':\n",
        "    final = sum(wi*tt for wi,tt in zip(best_w, sel_tests_greedy))\n",
        "elif best_method == 'lr_logits':\n",
        "    meta = LogisticRegression(solver='lbfgs', C=best_c, max_iter=1000, multi_class='multinomial', random_state=42)\n",
        "    meta.fit(X_logit, y)\n",
        "    final = meta.predict_proba(Xt_logit)\n",
        "else:  # ridge\n",
        "    Y = to_onehot(y)\n",
        "    ridge = Ridge(alpha=best_alpha, random_state=42).fit(X_logit, Y)\n",
        "    final = ridge.predict(Xt_logit)\n",
        "    final = np.clip(final, 1e-15, 1-1e-15); final /= final.sum(axis=1, keepdims=True)\n",
        "\n",
        "final = np.clip(final,1e-15,1-1e-15); final /= final.sum(axis=1, keepdims=True)\n",
        "pd.DataFrame(final, columns=classes).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv | Best OOF:', round(best_sc,4))\n",
        "if best_sc <= 0.29381:\n",
        "    print('Bronze medal! Ready for submit_final_answer.')\n",
        "else:\n",
        "    print('Still above bronze; consider more tuning.')"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy OOF: 0.3287 selected models: ['oof_pl_refined_hstack_lr.csv', 'oof_word_nbsvm_improved2.csv', 'oof_10f_cal_ridge_char_wb.csv']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dirichlet OOF: 0.3273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR C=0.5 OOF: 0.3171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR C=0.75 OOF: 0.3171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR C=1.0 OOF: 0.3171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR C=1.5 OOF: 0.3171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR C=2.0 OOF: 0.3171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR C=3.0 OOF: 0.3171\nBest LR-on-logits OOF: 0.3171 at C= 0.5\nRidge alpha=0.5 OOF: 0.4503\nRidge alpha=1.0 OOF: 0.4503\nRidge alpha=2.0 OOF: 0.4503\nBest Ridge meta OOF: 0.4503 at alpha= 0.5\nBest method: lr_logits with OOF 0.3171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv | Best OOF: 0.3171\nStill above bronze; consider more tuning.\n"
          ]
        }
      ]
    },
    {
      "id": "eae28445-b9be-4d1d-a373-96143697292d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "def log_count_ratio(X, yb, alpha):\n",
        "    pos = np.asarray(X[yb==1].sum(0)).ravel() + alpha\n",
        "    neg = np.asarray(X[yb==0].sum(0)).ravel() + alpha\n",
        "    r = np.log(pos/neg); r[~np.isfinite(r)] = 0.0\n",
        "    return r\n",
        "\n",
        "def odds_norm(P, eps=1e-15):\n",
        "    P = np.clip(P, eps, 1-eps); odds = P/(1-P)\n",
        "    Q = odds / (odds.sum(axis=1, keepdims=True) + eps)\n",
        "    return Q / Q.sum(axis=1, keepdims=True)\n",
        "\n",
        "param_grid = [\n",
        "    {'ngram': (1,2), 'min_df': 1, 'alpha': 0.1, 'C': 1.5},\n",
        "    {'ngram': (1,2), 'min_df': 1, 'alpha': 0.25, 'C': 2.0},\n",
        "    {'ngram': (1,2), 'min_df': 2, 'alpha': 0.5, 'C': 3.0},\n",
        "    {'ngram': (1,3), 'min_df': 1, 'alpha': 0.1, 'C': 2.0},\n",
        "    {'ngram': (1,3), 'min_df': 1, 'alpha': 0.25, 'C': 3.0},\n",
        "    {'ngram': (1,3), 'min_df': 2, 'alpha': 0.5, 'C': 4.0},\n",
        "]\n",
        "\n",
        "best_sc = 1e9; best_oof = None; best_test = None; best_params = None\n",
        "for p in param_grid:\n",
        "    oof = np.zeros((len(train), 3)); scores = []; test_preds = []\n",
        "    for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "        vec = CountVectorizer(analyzer='word', ngram_range=p['ngram'], lowercase=True,\n",
        "                              min_df=p['min_df'], max_df=0.95, binary=False)\n",
        "        Xtr_cnt = vec.fit_transform(train['text'].iloc[tr])\n",
        "        Xva_cnt = vec.transform(train['text'].iloc[va])\n",
        "        Xte_cnt = vec.transform(test['text'])\n",
        "        Xtr_bin = Xtr_cnt.copy(); Xtr_bin.data[:] = 1\n",
        "        Xva_bin = Xva_cnt.copy(); Xva_bin.data[:] = 1\n",
        "        Xte_bin = Xte_cnt.copy(); Xte_bin.data[:] = 1\n",
        "\n",
        "        Pva = np.zeros((len(va), 3)); Pte = np.zeros((len(test), 3))\n",
        "        for c in range(3):\n",
        "            yb = (y[tr]==c).astype(int)\n",
        "            r = log_count_ratio(Xtr_cnt, yb, alpha=p['alpha'])\n",
        "            clf = LogisticRegression(solver='liblinear', penalty='l2', C=p['C'],\n",
        "                                     max_iter=3000, tol=1e-4, random_state=42+c)\n",
        "            clf.fit(Xtr_bin.multiply(csr_matrix(r)), yb)\n",
        "            Pva[:,c] = clf.predict_proba(Xva_bin.multiply(csr_matrix(r)))[:,1]\n",
        "            Pte[:,c] = clf.predict_proba(Xte_bin.multiply(csr_matrix(r)))[:,1]\n",
        "        Pva = odds_norm(Pva); oof[va] = Pva\n",
        "        test_preds.append(odds_norm(Pte))\n",
        "        scores.append(log_loss(y[va], Pva))\n",
        "    sc = float(np.mean(scores)); print(f'NB-SVM params {p} OOF: {sc:.4f}')\n",
        "    if sc < best_sc:\n",
        "        best_sc = sc; best_oof = oof; best_params = p\n",
        "        best_test = np.mean(test_preds, axis=0)\n",
        "\n",
        "print('Best Word NB-SVM OOF:', round(best_sc,4), 'params:', best_params)\n",
        "pd.DataFrame(best_oof, columns=classes).to_csv('oof_word_nbsvm_improved2.csv', index=False)\n",
        "pd.DataFrame(best_test, columns=classes).to_csv('test_word_nbsvm_improved2.csv', index=False)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM params {'ngram': (1, 2), 'min_df': 1, 'alpha': 0.1, 'C': 1.5} OOF: 0.4358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM params {'ngram': (1, 2), 'min_df': 1, 'alpha': 0.25, 'C': 2.0} OOF: 0.4455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM params {'ngram': (1, 2), 'min_df': 2, 'alpha': 0.5, 'C': 3.0} OOF: 0.4981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM params {'ngram': (1, 3), 'min_df': 1, 'alpha': 0.1, 'C': 2.0} OOF: 0.4527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM params {'ngram': (1, 3), 'min_df': 1, 'alpha': 0.25, 'C': 3.0} OOF: 0.4617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM params {'ngram': (1, 3), 'min_df': 2, 'alpha': 0.5, 'C': 4.0} OOF: 0.5125\nBest Word NB-SVM OOF: 0.4358 params: {'ngram': (1, 2), 'min_df': 1, 'alpha': 0.1, 'C': 1.5}\n"
          ]
        }
      ]
    },
    {
      "id": "2f0e010e-a67e-47bc-9918-3a80e7f715c1",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    if not isinstance(s, str): s = '' if s is None else str(s)\n",
        "    s = unicodedata.normalize('NFKC', s)\n",
        "    s = (s.replace('\\u201c','\\\"').replace('\\u201d','\\\"')\n",
        "           .replace('\\u2018',\"'\").replace('\\u2019',\"'\")\n",
        "           .replace('\\u2014','-').replace('\\u2013','-').replace('\\u2212','-')\n",
        "           .replace('\\u2026',' ... ').replace('\\\\u00A0',' '))\n",
        "    s = re.sub(r'\\\\d+', '0', s)\n",
        "    s = re.sub(r'\\\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna('').map(normalize_text)\n",
        "test['text'] = test['text'].fillna('').map(normalize_text)\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf10 = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Refined PL: top 20% thr>=0.95, w=0.2 on top 2 bases (hstack_lr, char_wb_2_7)\n",
        "sub = pd.read_csv('submission.csv')[classes].values\n",
        "maxp = sub.max(axis=1)\n",
        "target_frac = 0.20; q_thr = np.quantile(maxp, 1 - target_frac)\n",
        "thr = max(q_thr, 0.95)\n",
        "mask = maxp >= thr\n",
        "pseudo_text = test.loc[mask, 'text'].values\n",
        "pseudo_y = sub[mask].argmax(axis=1)\n",
        "pl_w = 0.2\n",
        "print(f'Refined PL rows: {mask.sum()} ({mask.mean():.1%}) | thr: {thr:.3f}')\n",
        "\n",
        "def retrain_hstack_pl(name, word_params, char_params, C):\n",
        "    oof = np.zeros((len(train), 3)); test_preds = []; scores=[]\n",
        "    for f,(tr,va) in enumerate(skf10.split(train['text'], y), 1):\n",
        "        Xtr_text = pd.concat([train['text'].iloc[tr], pd.Series(pseudo_text)], ignore_index=True)\n",
        "        ytr = np.concatenate([y[tr], pseudo_y])\n",
        "        sw = np.concatenate([np.ones(len(tr)), np.full(len(pseudo_y), pl_w)])\n",
        "        Xva_text = train['text'].iloc[va]\n",
        "\n",
        "        vw = TfidfVectorizer(**word_params); vc = TfidfVectorizer(**char_params)\n",
        "        Xtr = hstack([vw.fit_transform(Xtr_text), vc.fit_transform(Xtr_text)])\n",
        "        Xva = hstack([vw.transform(Xva_text), vc.transform(Xva_text)])\n",
        "        Xte = hstack([vw.transform(test['text']), vc.transform(test['text'])])\n",
        "        clf = LogisticRegression(solver='lbfgs', C=C, max_iter=3000, tol=1e-4, n_jobs=1, random_state=42+f)\n",
        "        clf.fit(Xtr, ytr, sample_weight=sw)\n",
        "        p = clf.predict_proba(Xva); oof[va] = p; test_preds.append(clf.predict_proba(Xte))\n",
        "        s = log_loss(y[va], p); scores.append(s); print(f'{name} Fold {f}: {s:.4f}')\n",
        "    sc = float(np.mean(scores)); print(f'{name} PL OOF: {sc:.4f}')\n",
        "    ptest = np.mean(test_preds, axis=0)\n",
        "    pd.DataFrame(oof, columns=classes).to_csv(f'oof_pl_refined_{name}.csv', index=False)\n",
        "    pd.DataFrame(ptest, columns=classes).to_csv(f'test_pl_refined_{name}.csv', index=False)\n",
        "\n",
        "def retrain_single_pl(name, vec_params, C):\n",
        "    oof = np.zeros((len(train), 3)); test_preds = []; scores=[]\n",
        "    for f,(tr,va) in enumerate(skf10.split(train['text'], y), 1):\n",
        "        Xtr_text = pd.concat([train['text'].iloc[tr], pd.Series(pseudo_text)], ignore_index=True)\n",
        "        ytr = np.concatenate([y[tr], pseudo_y])\n",
        "        sw = np.concatenate([np.ones(len(tr)), np.full(len(pseudo_y), pl_w)])\n",
        "        Xva_text = train['text'].iloc[va]\n",
        "\n",
        "        vec = TfidfVectorizer(**vec_params)\n",
        "        Xtr = vec.fit_transform(Xtr_text)\n",
        "        Xva = vec.transform(Xva_text)\n",
        "        Xte = vec.transform(test['text'])\n",
        "        clf = LogisticRegression(solver='lbfgs', C=C, max_iter=3000, tol=1e-4, n_jobs=1, random_state=42+f)\n",
        "        clf.fit(Xtr, ytr, sample_weight=sw)\n",
        "        p = clf.predict_proba(Xva); oof[va] = p; test_preds.append(clf.predict_proba(Xte))\n",
        "        s = log_loss(y[va], p); scores.append(s); print(f'{name} Fold {f}: {s:.4f}')\n",
        "    sc = float(np.mean(scores)); print(f'{name} PL OOF: {sc:.4f}')\n",
        "    ptest = np.mean(test_preds, axis=0)\n",
        "    pd.DataFrame(oof, columns=classes).to_csv(f'oof_pl_refined_{name}.csv', index=False)\n",
        "    pd.DataFrame(ptest, columns=classes).to_csv(f'test_pl_refined_{name}.csv', index=False)\n",
        "\n",
        "# Run refined PL on top 2\n",
        "retrain_hstack_pl(\n",
        "    name='hstack_lr',\n",
        "    word_params=dict(analyzer='word', ngram_range=(1,3), lowercase=True, sublinear_tf=True, min_df=2, max_df=0.95),\n",
        "    char_params=dict(analyzer='char_wb', ngram_range=(2,6), lowercase=False, sublinear_tf=True, min_df=2, max_df=0.98),\n",
        "    C=6.0\n",
        ")\n",
        "retrain_single_pl(\n",
        "    name='char_wb_2_7',\n",
        "    vec_params=dict(analyzer='char_wb', ngram_range=(2,7), lowercase=False, sublinear_tf=True, min_df=2, max_df=0.98),\n",
        "    C=8.0\n",
        ")\n",
        "\n",
        "print('Refined PL complete. Re-ensemble in next cell to push <0.30 OOF.')"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined PL rows: 196 (10.0%) | thr: 1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 1: 0.3625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 2: 0.3521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 3: 0.3480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 4: 0.3627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 5: 0.3845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 6: 0.3743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 7: 0.3720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 8: 0.3537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 9: 0.3646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 10: 0.3537\nhstack_lr PL OOF: 0.3628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 1: 0.3915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 2: 0.3883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 3: 0.3756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 4: 0.3955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 5: 0.4136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 6: 0.3960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 7: 0.4049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 8: 0.3781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 9: 0.3876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 10: 0.3800\nchar_wb_2_7 PL OOF: 0.3911\nRefined PL complete. Re-ensemble in next cell to push <0.30 OOF.\n"
          ]
        }
      ]
    },
    {
      "id": "1f4de024-3230-4858-b10d-565c40301210",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "train = pd.read_csv('train.csv'); le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "\n",
        "# Base pool: top originals + refined PL + word (use improved2 if <=0.41 else cnb)\n",
        "bases = [\n",
        "    ('oof_10f_hstack_lr.csv', 'test_10f_hstack_lr.csv'),\n",
        "    ('oof_pl_refined_hstack_lr.csv', 'test_pl_refined_hstack_lr.csv'),\n",
        "    ('oof_10f_char_wb_2_7.csv', 'test_10f_char_wb_2_7.csv'),\n",
        "    ('oof_pl_refined_char_wb_2_7.csv', 'test_pl_refined_char_wb_2_7.csv'),\n",
        "]\n",
        "# Add word if useful\n",
        "try:\n",
        "    o_word = pd.read_csv('oof_word_nbsvm_improved2.csv')[classes].values\n",
        "    w_score = log_loss(y, o_word)\n",
        "    if w_score <= 0.41:\n",
        "        bases.append(('oof_word_nbsvm_improved2.csv', 'test_word_nbsvm_improved2.csv'))\n",
        "    else:\n",
        "        bases.append(('oof_word_cnb.csv', 'test_word_cnb.csv'))\n",
        "except:\n",
        "    bases.append(('oof_word_cnb.csv', 'test_word_cnb.csv'))\n",
        "\n",
        "# Load\n",
        "oofs = []; tests = []\n",
        "for o,t in bases:\n",
        "    oofs.append(pd.read_csv(o)[classes].values)\n",
        "    tests.append(pd.read_csv(t)[classes].values)\n",
        "\n",
        "# Greedy forward (mean)\n",
        "best = 1e9; sel = []\n",
        "while True:\n",
        "    improved = False; cand = None\n",
        "    for i in range(len(oofs)):\n",
        "        if i in sel: continue\n",
        "        idx = sel + [i]\n",
        "        sc = log_loss(y, np.mean([oofs[j] for j in idx], axis=0))\n",
        "        if sc < best - 1e-6:\n",
        "            best = sc; improved = True; cand = i\n",
        "    if not improved: break\n",
        "    sel.append(cand)\n",
        "sel_oofs = [oofs[i] for i in sel]; sel_tests = [tests[i] for i in sel]\n",
        "print('Greedy OOF:', round(best,4))\n",
        "\n",
        "# Dirichlet weights (10000 iters)\n",
        "rng = np.random.default_rng(42); best_w = None; best_w_sc = 1e9\n",
        "for _ in range(10000):\n",
        "    w = rng.dirichlet(np.ones(len(sel_oofs)))\n",
        "    sc = log_loss(y, sum(wi*oo for wi,oo in zip(w, sel_oofs)))\n",
        "    if sc < best_w_sc: best_w_sc = sc; best_w = w\n",
        "print('Dirichlet OOF:', round(best_w_sc,4))\n",
        "\n",
        "# LR-on-logits\n",
        "def to_logits(P): \n",
        "    P = np.clip(P,1e-15,1-1e-15)\n",
        "    return np.log(P/(1-P))\n",
        "X = np.hstack([to_logits(oo) for oo in sel_oofs])\n",
        "Xt = np.hstack([to_logits(tt) for tt in sel_tests])\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_meta = np.zeros((len(train),3)); scs=[]\n",
        "for tr,va in skf.split(X, y):\n",
        "    meta = LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000, multi_class='multinomial', random_state=42)\n",
        "    meta.fit(X[tr], y[tr])\n",
        "    p = meta.predict_proba(X[va]); oof_meta[va]=p; scs.append(log_loss(y[va], p))\n",
        "lr_sc = float(np.mean(scs)); print('LR-on-logits OOF:', round(lr_sc,4))\n",
        "\n",
        "# Pick best and save\n",
        "if lr_sc < best_w_sc:\n",
        "    meta = LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000, multi_class='multinomial', random_state=42)\n",
        "    meta.fit(X, y)\n",
        "    final = meta.predict_proba(Xt)\n",
        "else:\n",
        "    final = sum(wi*tt for wi,tt in zip(best_w, sel_tests))\n",
        "final = np.clip(final,1e-15,1-1e-15); final /= final.sum(axis=1, keepdims=True)\n",
        "pd.DataFrame(final, columns=classes).to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv | Best OOF:', round(min(best_w_sc, lr_sc),4))\n",
        "if min(best_w_sc, lr_sc) <= 0.29381:\n",
        "    print('Bronze medal! Ready for submit_final_answer.')\n",
        "else:\n",
        "    print('Still above bronze; consider more tuning.')"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy OOF: 0.3354\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[138]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10000\u001b[39m):\n\u001b[32m     51\u001b[39m     w = rng.dirichlet(np.ones(\u001b[38;5;28mlen\u001b[39m(sel_oofs)))\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     sc = \u001b[43mlog_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwi\u001b[49m\u001b[43m*\u001b[49m\u001b[43moo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwi\u001b[49m\u001b[43m,\u001b[49m\u001b[43moo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msel_oofs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sc < best_w_sc: best_w_sc = sc; best_w = w\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mDirichlet OOF:\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mround\u001b[39m(best_w_sc,\u001b[32m4\u001b[39m))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mwrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    213\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    214\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    215\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    216\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    217\u001b[39m     msg = re.sub(\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    219\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    220\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    221\u001b[39m     )\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/metrics/_classification.py:3240\u001b[39m, in \u001b[36mlog_loss\u001b[39m\u001b[34m(y_true, y_pred, normalize, sample_weight, labels)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/metrics/_classification.py:258\u001b[39m, in \u001b[36m_validate_multiclass_probabilistic_prediction\u001b[39m\u001b[34m(y_true, y_prob, sample_weight, labels)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m    232\u001b[39m     {\n\u001b[32m    233\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    242\u001b[39m     y_true, y_pred, *, labels=\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, normalize=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    243\u001b[39m ):\n\u001b[32m    244\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[32m    245\u001b[39m \n\u001b[32m    246\u001b[39m \u001b[33;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03m    is equal to the number of observations known to be in group :math:`i` and\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m    predicted to be in group :math:`j`.\u001b[39;00m\n\u001b[32m    249\u001b[39m \n\u001b[32m    250\u001b[39m \u001b[33;03m    Thus in binary classification, the count of true negatives is\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[33;03m    :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[33;03m    :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\u001b[39;00m\n\u001b[32m    253\u001b[39m \n\u001b[32m    254\u001b[39m \u001b[33;03m    Read more in the :ref:`User Guide <confusion_matrix>`.\u001b[39;00m\n\u001b[32m    255\u001b[39m \n\u001b[32m    256\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[33;03m    ----------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[33;03m    y_true : array-like of shape (n_samples,)\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[33;03m        Ground truth (correct) target values.\u001b[39;00m\n\u001b[32m    260\u001b[39m \n\u001b[32m    261\u001b[39m \u001b[33;03m    y_pred : array-like of shape (n_samples,)\u001b[39;00m\n\u001b[32m    262\u001b[39m \u001b[33;03m        Estimated targets as returned by a classifier.\u001b[39;00m\n\u001b[32m    263\u001b[39m \n\u001b[32m    264\u001b[39m \u001b[33;03m    labels : array-like of shape (n_classes), default=None\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m        List of labels to index the matrix. This may be used to reorder\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[33;03m        or select a subset of labels.\u001b[39;00m\n\u001b[32m    267\u001b[39m \u001b[33;03m        If ``None`` is given, those that appear at least once\u001b[39;00m\n\u001b[32m    268\u001b[39m \u001b[33;03m        in ``y_true`` or ``y_pred`` are used in sorted order.\u001b[39;00m\n\u001b[32m    269\u001b[39m \n\u001b[32m    270\u001b[39m \u001b[33;03m    sample_weight : array-like of shape (n_samples,), default=None\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m        Sample weights.\u001b[39;00m\n\u001b[32m    272\u001b[39m \n\u001b[32m    273\u001b[39m \u001b[33;03m        .. versionadded:: 0.18\u001b[39;00m\n\u001b[32m    274\u001b[39m \n\u001b[32m    275\u001b[39m \u001b[33;03m    normalize : {'true', 'pred', 'all'}, default=None\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m        Normalizes confusion matrix over the true (rows), predicted (columns)\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m        conditions or all the population. If None, confusion matrix will not be\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[33;03m        normalized.\u001b[39;00m\n\u001b[32m    279\u001b[39m \n\u001b[32m    280\u001b[39m \u001b[33;03m    Returns\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[33;03m    -------\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[33;03m    C : ndarray of shape (n_classes, n_classes)\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[33;03m        Confusion matrix whose i-th row and j-th\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[33;03m        column entry indicates the number of\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m        samples with true label being i-th class\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m        and predicted label being j-th class.\u001b[39;00m\n\u001b[32m    287\u001b[39m \n\u001b[32m    288\u001b[39m \u001b[33;03m    See Also\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m    --------\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[33;03m    ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\u001b[39;00m\n\u001b[32m    291\u001b[39m \u001b[33;03m        given an estimator, the data, and the label.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[33;03m        given the true and predicted labels.\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03m    ConfusionMatrixDisplay : Confusion Matrix visualization.\u001b[39;00m\n\u001b[32m    295\u001b[39m \n\u001b[32m    296\u001b[39m \u001b[33;03m    References\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[33;03m    ----------\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[33;03m    .. [1] `Wikipedia entry for the Confusion matrix\u001b[39;00m\n\u001b[32m    299\u001b[39m \u001b[33;03m           <https://en.wikipedia.org/wiki/Confusion_matrix>`_\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[33;03m           (Wikipedia and other references may use a different\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03m           convention for axes).\u001b[39;00m\n\u001b[32m    302\u001b[39m \n\u001b[32m    303\u001b[39m \u001b[33;03m    Examples\u001b[39;00m\n\u001b[32m    304\u001b[39m \u001b[33;03m    --------\u001b[39;00m\n\u001b[32m    305\u001b[39m \u001b[33;03m    >>> from sklearn.metrics import confusion_matrix\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[33;03m    >>> y_true = [2, 0, 2, 2, 0, 1]\u001b[39;00m\n\u001b[32m    307\u001b[39m \u001b[33;03m    >>> y_pred = [0, 0, 2, 2, 0, 2]\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[33;03m    >>> confusion_matrix(y_true, y_pred)\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    array([[2, 0, 0],\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[33;03m           [0, 0, 1],\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[33;03m           [1, 0, 2]])\u001b[39;00m\n\u001b[32m    312\u001b[39m \n\u001b[32m    313\u001b[39m \u001b[33;03m    >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\u001b[39;00m\n\u001b[32m    314\u001b[39m \u001b[33;03m    >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[33;03m    >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\u001b[39;00m\n\u001b[32m    316\u001b[39m \u001b[33;03m    array([[2, 0, 0],\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[33;03m           [0, 0, 1],\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[33;03m           [1, 0, 2]])\u001b[39;00m\n\u001b[32m    319\u001b[39m \n\u001b[32m    320\u001b[39m \u001b[33;03m    In the binary case, we can extract true positives, etc. as follows:\u001b[39;00m\n\u001b[32m    321\u001b[39m \n\u001b[32m    322\u001b[39m \u001b[33;03m    >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[33;03m    >>> (tn, fp, fn, tp)\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[33;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    326\u001b[39m     y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n\u001b[32m    327\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/numpy/core/numeric.py:2241\u001b[39m, in \u001b[36mallclose\u001b[39m\u001b[34m(a, b, rtol, atol, equal_nan)\u001b[39m\n\u001b[32m   2170\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_allclose_dispatcher)\n\u001b[32m   2171\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mallclose\u001b[39m(a, b, rtol=\u001b[32m1.e-5\u001b[39m, atol=\u001b[32m1.e-8\u001b[39m, equal_nan=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   2172\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2173\u001b[39m \u001b[33;03m    Returns True if two arrays are element-wise equal within a tolerance.\u001b[39;00m\n\u001b[32m   2174\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2239\u001b[39m \n\u001b[32m   2240\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2241\u001b[39m     res = \u001b[38;5;28mall\u001b[39m(\u001b[43misclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(res)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/numpy/core/numeric.py:2351\u001b[39m, in \u001b[36misclose\u001b[39m\u001b[34m(a, b, rtol, atol, equal_nan)\u001b[39m\n\u001b[32m   2349\u001b[39m yfin = isfinite(y)\n\u001b[32m   2350\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(xfin) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(yfin):\n\u001b[32m-> \u001b[39m\u001b[32m2351\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwithin_tol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2352\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2353\u001b[39m     finite = xfin & yfin\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/numpy/core/numeric.py:2332\u001b[39m, in \u001b[36misclose.<locals>.within_tol\u001b[39m\u001b[34m(x, y, atol, rtol)\u001b[39m\n\u001b[32m   2330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwithin_tol\u001b[39m(x, y, atol, rtol):\n\u001b[32m   2331\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m errstate(invalid=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m), _no_nep50_warning():\n\u001b[32m-> \u001b[39m\u001b[32m2332\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mless_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mabs\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m-\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mabs\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "9e272a56-5815-4409-8cac-bbd301264912",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "def softmax(S):\n",
        "    S = S - S.max(axis=1, keepdims=True)\n",
        "    P = np.exp(S); P /= P.sum(axis=1, keepdims=True)\n",
        "    return np.clip(P, 1e-15, 1-1e-15)\n",
        "\n",
        "vec_params = dict(analyzer='char_wb', ngram_range=(2,7), lowercase=False, sublinear_tf=True,\n",
        "                  min_df=2, max_df=0.98, max_features=500_000)\n",
        "alpha_grid = [0.5, 1.0, 2.0]\n",
        "\n",
        "best_sc = 1e9; best_oof = None; best_test = None; best_alpha = None\n",
        "for alpha in alpha_grid:\n",
        "    oof = np.zeros((len(train), 3)); scores = []; Ptest = np.zeros((len(test), 3))\n",
        "    for f, (tr, va) in enumerate(skf.split(train['text'], y), 1):\n",
        "        vec = TfidfVectorizer(**vec_params)\n",
        "        Xtr = vec.fit_transform(train['text'].iloc[tr])\n",
        "        Xva = vec.transform(train['text'].iloc[va])\n",
        "        Xte = vec.transform(test['text'])\n",
        "        clf = RidgeClassifier(alpha=alpha, random_state=42+f)\n",
        "        clf.fit(Xtr, y[tr])\n",
        "        pva = softmax(clf.decision_function(Xva)); oof[va] = pva\n",
        "        Ptest += softmax(clf.decision_function(Xte))\n",
        "        scores.append(log_loss(y[va], pva))\n",
        "    Ptest /= skf.n_splits\n",
        "    sc = float(np.mean(scores))\n",
        "    print(f'Ridge char_wb alpha={alpha} 10f OOF: {sc:.4f}')\n",
        "    if sc < best_sc:\n",
        "        best_sc = sc; best_oof = oof; best_test = Ptest; best_alpha = alpha\n",
        "\n",
        "print(f'Best Ridge char_wb OOF: {best_sc:.4f} at alpha={best_alpha}')\n",
        "pd.DataFrame(best_oof, columns=classes).to_csv('oof_10f_ridge_char_wb.csv', index=False)\n",
        "pd.DataFrame(best_test, columns=classes).to_csv('test_10f_ridge_char_wb.csv', index=False)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge char_wb alpha=0.5 10f OOF: 0.5758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge char_wb alpha=1.0 10f OOF: 0.5926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge char_wb alpha=2.0 10f OOF: 0.6223\nBest Ridge char_wb OOF: 0.5758 at alpha=0.5\n"
          ]
        }
      ]
    },
    {
      "id": "56a4bae2-fc62-4579-a754-ceb378a76318",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "def log_count_ratio(X, yb, alpha=0.1):\n",
        "    pos = np.asarray(X[yb==1].sum(0)).ravel() + alpha\n",
        "    neg = np.asarray(X[yb==0].sum(0)).ravel() + alpha\n",
        "    r = np.log(pos/neg); r[~np.isfinite(r)] = 0.0\n",
        "    return r\n",
        "\n",
        "def odds_norm(P, eps=1e-15):\n",
        "    P = np.clip(P, eps, 1-eps); odds = P/(1-P)\n",
        "    Q = odds / (odds.sum(axis=1, keepdims=True) + eps)\n",
        "    return Q / Q.sum(axis=1, keepdims=True)\n",
        "\n",
        "vec_params = dict(analyzer='word', ngram_range=(1,2), lowercase=True,\n",
        "                  min_df=1, max_df=0.95, binary=False)\n",
        "C = 2.0\n",
        "\n",
        "oof = np.zeros((len(train), 3)); scores = []; test_preds = []\n",
        "for f, (tr, va) in enumerate(skf.split(train['text'], y), 1):\n",
        "    vec = CountVectorizer(**vec_params)\n",
        "    Xtr_cnt = vec.fit_transform(train['text'].iloc[tr])\n",
        "    Xva_cnt = vec.transform(train['text'].iloc[va])\n",
        "    Xte_cnt = vec.transform(test['text'])\n",
        "    # Binary copies\n",
        "    Xtr_bin = Xtr_cnt.copy(); Xtr_bin.data[:] = 1\n",
        "    Xva_bin = Xva_cnt.copy(); Xva_bin.data[:] = 1\n",
        "    Xte_bin = Xte_cnt.copy(); Xte_bin.data[:] = 1\n",
        "    Pva = np.zeros((len(va), 3)); Pte = np.zeros((len(test), 3))\n",
        "    for c in range(3):\n",
        "        yb = (y[tr] == c).astype(int)\n",
        "        r = log_count_ratio(Xtr_cnt, yb, alpha=0.1)\n",
        "        clf = LogisticRegression(solver='liblinear', penalty='l2', C=C,\n",
        "                                 max_iter=3000, tol=1e-4, random_state=42+c)\n",
        "        clf.fit(Xtr_bin.multiply(csr_matrix(r)), yb)\n",
        "        Pva[:, c] = clf.predict_proba(Xva_bin.multiply(csr_matrix(r)))[:, 1]\n",
        "        Pte[:, c] = clf.predict_proba(Xte_bin.multiply(csr_matrix(r)))[:, 1]\n",
        "    Pva = odds_norm(Pva); oof[va] = Pva\n",
        "    Pte = odds_norm(Pte); test_preds.append(Pte)\n",
        "    scores.append(log_loss(y[va], Pva))\n",
        "    print(f'Word NB-SVM Fold {f}: {scores[-1]:.4f}')\n",
        "sc = float(np.mean(scores)); print(f'Word NB-SVM 10f OOF: {sc:.4f}')\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_word_nbsvm_correct.csv', index=False)\n",
        "pd.DataFrame(np.mean(test_preds, axis=0), columns=classes).to_csv('test_word_nbsvm_correct.csv', index=False)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 1: 0.5106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 2: 0.3948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 3: 0.4330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 4: 0.4460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 5: 0.4758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 6: 0.4432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 7: 0.4006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 8: 0.4490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 9: 0.4507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM Fold 10: 0.4749\nWord NB-SVM 10f OOF: 0.4479\n"
          ]
        }
      ]
    },
    {
      "id": "d31d8e0d-5abb-4c7e-93c6-04fc66086f16",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "vec_params = dict(analyzer='char_wb', ngram_range=(2,7), lowercase=False, sublinear_tf=True,\n",
        "                  min_df=2, max_df=0.98, max_features=500_000)\n",
        "\n",
        "oof = np.zeros((len(train), 3)); scores = []; Ptest = np.zeros((len(test), 3))\n",
        "for f, (tr, va) in enumerate(skf.split(train['text'], y), 1):\n",
        "    vec = TfidfVectorizer(**vec_params)\n",
        "    Xtr = vec.fit_transform(train['text'].iloc[tr])\n",
        "    Xva = vec.transform(train['text'].iloc[va])\n",
        "    Xte = vec.transform(test['text'])\n",
        "    clf = SGDClassifier(loss='log_loss', penalty='l2', alpha=5e-5, max_iter=5000, tol=1e-3,\n",
        "                        early_stopping=True, validation_fraction=0.1, n_iter_no_change=5,\n",
        "                        random_state=42+f)\n",
        "    clf.fit(Xtr, y[tr])\n",
        "    pva = clf.predict_proba(Xva); oof[va] = pva\n",
        "    Ptest += clf.predict_proba(Xte)\n",
        "    scores.append(log_loss(y[va], pva))\n",
        "    print(f'SGD char_wb Fold {f}: {scores[-1]:.4f}')\n",
        "Ptest /= skf.n_splits\n",
        "sc = float(np.mean(scores)); print(f'SGD char_wb 10f OOF: {sc:.4f}')\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_10f_sgd_char_wb.csv', index=False)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_10f_sgd_char_wb.csv', index=False)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD char_wb Fold 1: 0.5312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD char_wb Fold 2: 0.5245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD char_wb Fold 3: 0.5254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD char_wb Fold 4: 0.5291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD char_wb Fold 5: 0.5385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD char_wb Fold 6: 0.5370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD char_wb Fold 7: 0.5328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD char_wb Fold 8: 0.5211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD char_wb Fold 9: 0.5232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD char_wb Fold 10: 0.5202\nSGD char_wb 10f OOF: 0.5283\n"
          ]
        }
      ]
    },
    {
      "id": "722c6bfa-c6ee-4371-b674-a0076d2e2166",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "def log_count_ratio(X, yb, alpha):\n",
        "    pos = np.asarray(X[yb==1].sum(0)).ravel() + alpha\n",
        "    neg = np.asarray(X[yb==0].sum(0)).ravel() + alpha\n",
        "    r = np.log(pos/neg); r[~np.isfinite(r)] = 0.0\n",
        "    return r\n",
        "\n",
        "def odds_norm(P, eps=1e-15):\n",
        "    P = np.clip(P, eps, 1-eps); odds = P/(1-P)\n",
        "    Q = odds / (odds.sum(axis=1, keepdims=True) + eps)\n",
        "    return Q / Q.sum(axis=1, keepdims=True)\n",
        "\n",
        "param_grid = [\n",
        "    {'ngram': (1,2), 'min_df': 1, 'alpha': 0.05, 'C': 1.5},\n",
        "    {'ngram': (1,2), 'min_df': 1, 'alpha': 0.1, 'C': 2.0},\n",
        "    {'ngram': (1,2), 'min_df': 2, 'alpha': 0.15, 'C': 2.5},\n",
        "    {'ngram': (1,3), 'min_df': 1, 'alpha': 0.05, 'C': 2.0},\n",
        "    {'ngram': (1,3), 'min_df': 1, 'alpha': 0.1, 'C': 3.0},\n",
        "    {'ngram': (1,3), 'min_df': 2, 'alpha': 0.2, 'C': 3.0}\n",
        "]\n",
        "\n",
        "best_sc = 1e9; best_oof = None; best_test = None; best_params = None\n",
        "for p in param_grid:\n",
        "    oof = np.zeros((len(train), 3)); scores = []; test_preds = []\n",
        "    for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "        vec = CountVectorizer(analyzer='word', ngram_range=p['ngram'], lowercase=True,\n",
        "                              min_df=p['min_df'], max_df=0.95, binary=False)\n",
        "        Xtr_cnt = vec.fit_transform(train['text'].iloc[tr])\n",
        "        Xva_cnt = vec.transform(train['text'].iloc[va])\n",
        "        Xte_cnt = vec.transform(test['text'])\n",
        "        Xtr_bin = Xtr_cnt.copy(); Xtr_bin.data[:] = 1\n",
        "        Xva_bin = Xva_cnt.copy(); Xva_bin.data[:] = 1\n",
        "        Xte_bin = Xte_cnt.copy(); Xte_bin.data[:] = 1\n",
        "        Pva = np.zeros((len(va), 3)); Pte = np.zeros((len(test), 3))\n",
        "        for c in range(3):\n",
        "            yb = (y[tr]==c).astype(int)\n",
        "            r = log_count_ratio(Xtr_cnt, yb, alpha=p['alpha'])\n",
        "            clf = LogisticRegression(solver='liblinear', penalty='l2', C=p['C'],\n",
        "                                     max_iter=3000, tol=1e-4, random_state=42+c)\n",
        "            clf.fit(Xtr_bin.multiply(csr_matrix(r)), yb)\n",
        "            Pva[:,c] = clf.predict_proba(Xva_bin.multiply(csr_matrix(r)))[:,1]\n",
        "            Pte[:,c] = clf.predict_proba(Xte_bin.multiply(csr_matrix(r)))[:,1]\n",
        "        Pva = odds_norm(Pva); oof[va] = Pva\n",
        "        Pte = odds_norm(Pte); test_preds.append(Pte)\n",
        "        scores.append(log_loss(y[va], Pva))\n",
        "    sc = float(np.mean(scores)); print(f'NB-SVM {p} OOF: {sc:.4f}')\n",
        "    if sc < best_sc:\n",
        "        best_sc = sc; best_oof = oof; best_params = p\n",
        "        best_test = np.mean(test_preds, axis=0)\n",
        "\n",
        "print('Best Word NB-SVM OOF:', round(best_sc,4), 'params:', best_params)\n",
        "pd.DataFrame(best_oof, columns=classes).to_csv('oof_word_nbsvm_tuned.csv', index=False)\n",
        "pd.DataFrame(best_test, columns=classes).to_csv('test_word_nbsvm_tuned.csv', index=False)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM {'ngram': (1, 2), 'min_df': 1, 'alpha': 0.05, 'C': 1.5} OOF: 0.4409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM {'ngram': (1, 2), 'min_df': 1, 'alpha': 0.1, 'C': 2.0} OOF: 0.4479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM {'ngram': (1, 2), 'min_df': 2, 'alpha': 0.15, 'C': 2.5} OOF: 0.4987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM {'ngram': (1, 3), 'min_df': 1, 'alpha': 0.05, 'C': 2.0} OOF: 0.4592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM {'ngram': (1, 3), 'min_df': 1, 'alpha': 0.1, 'C': 3.0} OOF: 0.4667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NB-SVM {'ngram': (1, 3), 'min_df': 2, 'alpha': 0.2, 'C': 3.0} OOF: 0.4984\nBest Word NB-SVM OOF: 0.4409 params: {'ngram': (1, 2), 'min_df': 1, 'alpha': 0.05, 'C': 1.5}\n"
          ]
        }
      ]
    },
    {
      "id": "ed287b20-18c9-4c71-9a55-51b428bf68e5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "def odds_norm(P, eps=1e-15):\n",
        "    P = np.clip(P, eps, 1-eps); odds = P / (1 - P)\n",
        "    Q = odds / (odds.sum(axis=1, keepdims=True) + eps)\n",
        "    return Q / Q.sum(axis=1, keepdims=True)\n",
        "\n",
        "vec_params = dict(analyzer='char_wb', ngram_range=(2,6), lowercase=False, sublinear_tf=True,\n",
        "                  min_df=2, max_df=0.98, max_features=400_000)\n",
        "alpha_grid = [0.1, 0.5, 1.0]\n",
        "inner_cv_splits = 3\n",
        "\n",
        "best_sc = 1e9; best_oof = None; best_test = None; best_alpha = None\n",
        "for alpha in alpha_grid:\n",
        "    oof = np.zeros((len(train), 3)); scores = []; test_preds = []\n",
        "    for f, (tr, va) in enumerate(skf.split(train['text'], y), 1):\n",
        "        vec = TfidfVectorizer(**vec_params)\n",
        "        Xtr = vec.fit_transform(train['text'].iloc[tr])\n",
        "        Xva = vec.transform(train['text'].iloc[va])\n",
        "        Xte = vec.transform(test['text'])\n",
        "        Pva = np.zeros((len(va), 3)); Pte = np.zeros((len(test), 3))\n",
        "        for c in range(3):\n",
        "            yb_tr = (y[tr] == c).astype(int)\n",
        "            skf_inner = StratifiedKFold(n_splits=inner_cv_splits, shuffle=True, random_state=42 + c)\n",
        "            F_cal = []; z_cal = []\n",
        "            for i_tr, i_va in skf_inner.split(Xtr, yb_tr):\n",
        "                ridge = RidgeClassifier(alpha=alpha, random_state=42 + c)\n",
        "                ridge.fit(Xtr[i_tr], yb_tr[i_tr])\n",
        "                s = ridge.decision_function(Xtr[i_va])\n",
        "                if s.ndim > 1: s = s[:, 0]\n",
        "                F_cal.append(s.reshape(-1, 1)); z_cal.append(yb_tr[i_va])\n",
        "            F_cal = np.vstack(F_cal); z_cal = np.concatenate(z_cal)\n",
        "            platt = LogisticRegression(solver='lbfgs', C=1.0, max_iter=1000, random_state=42 + c)\n",
        "            platt.fit(F_cal, z_cal)\n",
        "            ridge_full = RidgeClassifier(alpha=alpha, random_state=42 + c)\n",
        "            ridge_full.fit(Xtr, yb_tr)\n",
        "            s_va = ridge_full.decision_function(Xva)\n",
        "            if s_va.ndim > 1: s_va = s_va[:, 0]\n",
        "            Pva[:, c] = platt.predict_proba(s_va.reshape(-1, 1))[:, 1]\n",
        "            s_te = ridge_full.decision_function(Xte)\n",
        "            if s_te.ndim > 1: s_te = s_te[:, 0]\n",
        "            Pte[:, c] = platt.predict_proba(s_te.reshape(-1, 1))[:, 1]\n",
        "        Pva = odds_norm(Pva); oof[va] = Pva\n",
        "        Pte = odds_norm(Pte); test_preds.append(Pte)\n",
        "        scores.append(log_loss(y[va], Pva))\n",
        "    sc = float(np.mean(scores)); print(f'Ridge Cal char_wb alpha={alpha} 10f OOF: {sc:.4f}')\n",
        "    if sc < best_sc:\n",
        "        best_sc = sc; best_oof = oof; best_test = np.mean(test_preds, axis=0); best_alpha = alpha\n",
        "\n",
        "print(f'Best Calibrated Ridge char_wb OOF: {best_sc:.4f} at alpha={best_alpha}')\n",
        "pd.DataFrame(best_oof, columns=classes).to_csv('oof_10f_cal_ridge_char_wb.csv', index=False)\n",
        "pd.DataFrame(best_test, columns=classes).to_csv('test_10f_cal_ridge_char_wb.csv', index=False)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Cal char_wb alpha=0.1 10f OOF: 0.4952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Cal char_wb alpha=0.5 10f OOF: 0.4188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Cal char_wb alpha=1.0 10f OOF: 0.4116\nBest Calibrated Ridge char_wb OOF: 0.4116 at alpha=1.0\n"
          ]
        }
      ]
    },
    {
      "id": "cc8c9fac-3084-4e0e-b509-916cb2d60073",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf10 = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Refined PL: thr=0.98 (top ~10%), w=0.15 on hstack_lr and char_wb_2_7\n",
        "sub = pd.read_csv('submission.csv')[classes].values\n",
        "maxp = sub.max(axis=1)\n",
        "target_frac = 0.10; q_thr = np.quantile(maxp, 1 - target_frac)\n",
        "thr = max(q_thr, 0.98)\n",
        "mask = maxp >= thr\n",
        "pseudo_text = test.loc[mask, 'text'].values\n",
        "pseudo_y = sub[mask].argmax(axis=1)\n",
        "pl_w = 0.15\n",
        "print(f'Refined PL rows: {mask.sum()} ({mask.mean():.1%}) | thr: {thr:.3f}')\n",
        "\n",
        "def retrain_hstack_pl(name, word_params, char_params, C):\n",
        "    oof = np.zeros((len(train), 3)); test_preds = []; scores=[]\n",
        "    for f,(tr,va) in enumerate(skf10.split(train['text'], y), 1):\n",
        "        Xtr_text = pd.concat([train['text'].iloc[tr], pd.Series(pseudo_text)], ignore_index=True)\n",
        "        ytr = np.concatenate([y[tr], pseudo_y])\n",
        "        sw = np.concatenate([np.ones(len(tr)), np.full(len(pseudo_y), pl_w)])\n",
        "        Xva_text = train['text'].iloc[va]\n",
        "\n",
        "        vw = TfidfVectorizer(**word_params); vc = TfidfVectorizer(**char_params)\n",
        "        Xtr = hstack([vw.fit_transform(Xtr_text), vc.fit_transform(Xtr_text)])\n",
        "        Xva = hstack([vw.transform(Xva_text), vc.transform(Xva_text)])\n",
        "        Xte = hstack([vw.transform(test['text']), vc.transform(test['text'])])\n",
        "        clf = LogisticRegression(solver='lbfgs', C=C, max_iter=3000, tol=1e-4, n_jobs=1, random_state=42+f)\n",
        "        clf.fit(Xtr, ytr, sample_weight=sw)\n",
        "        p = clf.predict_proba(Xva); oof[va] = p; test_preds.append(clf.predict_proba(Xte))\n",
        "        s = log_loss(y[va], p); scores.append(s); print(f'{name} Fold {f}: {s:.4f}')\n",
        "    sc = float(np.mean(scores)); print(f'{name} PL OOF: {sc:.4f}')\n",
        "    ptest = np.mean(test_preds, axis=0)\n",
        "    pd.DataFrame(oof, columns=classes).to_csv(f'oof_pl_refined_{name}.csv', index=False)\n",
        "    pd.DataFrame(ptest, columns=classes).to_csv(f'test_pl_refined_{name}.csv', index=False)\n",
        "\n",
        "def retrain_single_pl(name, vec_params, C):\n",
        "    oof = np.zeros((len(train), 3)); test_preds = []; scores=[]\n",
        "    for f,(tr,va) in enumerate(skf10.split(train['text'], y), 1):\n",
        "        Xtr_text = pd.concat([train['text'].iloc[tr], pd.Series(pseudo_text)], ignore_index=True)\n",
        "        ytr = np.concatenate([y[tr], pseudo_y])\n",
        "        sw = np.concatenate([np.ones(len(tr)), np.full(len(pseudo_y), pl_w)])\n",
        "        Xva_text = train['text'].iloc[va]\n",
        "\n",
        "        vec = TfidfVectorizer(**vec_params)\n",
        "        Xtr = vec.fit_transform(Xtr_text); Xva = vec.transform(Xva_text); Xte = vec.transform(test['text'])\n",
        "        clf = LogisticRegression(solver='lbfgs', C=C, max_iter=3000, tol=1e-4, n_jobs=1, random_state=42+f)\n",
        "        clf.fit(Xtr, ytr, sample_weight=sw)\n",
        "        p = clf.predict_proba(Xva); oof[va] = p; test_preds.append(clf.predict_proba(Xte))\n",
        "        s = log_loss(y[va], p); scores.append(s); print(f'{name} Fold {f}: {s:.4f}')\n",
        "    sc = float(np.mean(scores)); print(f'{name} PL OOF: {sc:.4f}')\n",
        "    ptest = np.mean(test_preds, axis=0)\n",
        "    pd.DataFrame(oof, columns=classes).to_csv(f'oof_pl_refined_{name}.csv', index=False)\n",
        "    pd.DataFrame(ptest, columns=classes).to_csv(f'test_pl_refined_{name}.csv', index=False)\n",
        "\n",
        "# Run refined PL on top 2\n",
        "retrain_hstack_pl(\n",
        "    name='hstack_lr',\n",
        "    word_params=dict(analyzer='word', ngram_range=(1,3), lowercase=True, sublinear_tf=True, min_df=2, max_df=0.95),\n",
        "    char_params=dict(analyzer='char_wb', ngram_range=(2,6), lowercase=False, sublinear_tf=True, min_df=2, max_df=0.98),\n",
        "    C=6.0\n",
        ")\n",
        "retrain_single_pl(\n",
        "    name='char_wb_2_7',\n",
        "    vec_params=dict(analyzer='char_wb', ngram_range=(2,7), lowercase=False, sublinear_tf=True, min_df=2, max_df=0.98),\n",
        "    C=8.0\n",
        ")\n",
        "\n",
        "print('Refined PL complete. Re-ensemble in Cell 56 to target <0.30 OOF.')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined PL rows: 196 (10.0%) | thr: 1.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 1: 0.3614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 2: 0.3544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 3: 0.3490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 4: 0.3659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 5: 0.3845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 6: 0.3730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 7: 0.3706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 8: 0.3523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 9: 0.3644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hstack_lr Fold 10: 0.3531\nhstack_lr PL OOF: 0.3629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 1: 0.3918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 2: 0.3858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 3: 0.3744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 4: 0.3942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 5: 0.4135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 6: 0.3965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 7: 0.4058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 8: 0.3769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 9: 0.3895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_wb_2_7 Fold 10: 0.3816\nchar_wb_2_7 PL OOF: 0.3910\nRefined PL complete. Re-ensemble in Cell 56 to target <0.30 OOF.\n"
          ]
        }
      ]
    },
    {
      "id": "8defdeb9-fbeb-48aa-ba5e-f16e5975c2da",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "vec_params = dict(analyzer='char_wb', ngram_range=(3,6), lowercase=False,\n",
        "                  min_df=3, max_df=0.95, binary=False, max_features=300_000)\n",
        "\n",
        "oof = np.zeros((len(train), 3)); scores = []; Ptest = np.zeros((len(test), 3))\n",
        "for f, (tr, va) in enumerate(skf.split(train['text'], y), 1):\n",
        "    vec = CountVectorizer(**vec_params)\n",
        "    Xtr = vec.fit_transform(train['text'].iloc[tr])\n",
        "    Xva = vec.transform(train['text'].iloc[va])\n",
        "    Xte = vec.transform(test['text'])\n",
        "    clf = ComplementNB(alpha=0.5, norm=True)\n",
        "    clf.fit(Xtr, y[tr])\n",
        "    pva = clf.predict_proba(Xva); oof[va] = pva\n",
        "    Ptest += clf.predict_proba(Xte)\n",
        "    scores.append(log_loss(y[va], pva))\n",
        "    print(f'Char ComplementNB Fold {f}: {scores[-1]:.4f}')\n",
        "Ptest /= skf.n_splits\n",
        "sc = float(np.mean(scores)); print(f'Char ComplementNB 10f OOF: {sc:.4f}')\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_char_cnb.csv', index=False)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_char_cnb.csv', index=False)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char ComplementNB Fold 1: 1.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char ComplementNB Fold 2: 1.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char ComplementNB Fold 3: 1.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char ComplementNB Fold 4: 1.0986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char ComplementNB Fold 5: 1.0986\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[153]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f, (tr, va) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(skf.split(train[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m], y), \u001b[32m1\u001b[39m):\n\u001b[32m     18\u001b[39m     vec = CountVectorizer(**vec_params)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     Xtr = \u001b[43mvec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     Xva = vec.transform(train[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].iloc[va])\n\u001b[32m     21\u001b[39m     Xte = vec.transform(test[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/feature_extraction/text.py:1377\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1369\u001b[39m             warnings.warn(\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1374\u001b[39m             )\n\u001b[32m   1375\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1380\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/feature_extraction/text.py:1276\u001b[39m, in \u001b[36m_count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1274\u001b[39m         feature_counter[feature_idx] = \u001b[32m1\u001b[39m\n\u001b[32m   1275\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m         feature_counter[feature_idx] += \u001b[32m1\u001b[39m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m   1278\u001b[39m     \u001b[38;5;66;03m# Ignore out-of-vocabulary items for fixed_vocab=True\u001b[39;00m\n\u001b[32m   1279\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "41d36489-592f-4c3b-9ba7-234f2220a185",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "vec_params = dict(analyzer='char_wb', ngram_range=(2,6), lowercase=False,\n",
        "                  min_df=2, max_df=0.95, binary=True, max_features=300_000)\n",
        "\n",
        "oof = np.zeros((len(train), 3)); scores = []; Ptest = np.zeros((len(test), 3))\n",
        "for f, (tr, va) in enumerate(skf.split(train['text'], y), 1):\n",
        "    vec = CountVectorizer(**vec_params)\n",
        "    Xtr = vec.fit_transform(train['text'].iloc[tr])\n",
        "    Xva = vec.transform(train['text'].iloc[va])\n",
        "    Xte = vec.transform(test['text'])\n",
        "    clf = BernoulliNB(alpha=0.5)\n",
        "    clf.fit(Xtr, y[tr])\n",
        "    pva = clf.predict_proba(Xva); oof[va] = pva\n",
        "    Ptest += clf.predict_proba(Xte)\n",
        "    scores.append(log_loss(y[va], pva))\n",
        "    print(f'Char BernoulliNB Fold {f}: {scores[-1]:.4f}')\n",
        "Ptest /= skf.n_splits\n",
        "sc = float(np.mean(scores)); print(f'Char BernoulliNB 10f OOF: {sc:.4f}')\n",
        "pd.DataFrame(oof, columns=classes).to_csv('oof_char_bnb.csv', index=False)\n",
        "pd.DataFrame(Ptest, columns=classes).to_csv('test_char_bnb.csv', index=False)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char BernoulliNB Fold 1: 3.6306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char BernoulliNB Fold 2: 3.3430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char BernoulliNB Fold 3: 3.2960\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[154]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m Xtr = vec.fit_transform(train[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].iloc[tr])\n\u001b[32m     20\u001b[39m Xva = vec.transform(train[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].iloc[va])\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m Xte = \u001b[43mvec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m clf = BernoulliNB(alpha=\u001b[32m0.5\u001b[39m)\n\u001b[32m     23\u001b[39m clf.fit(Xtr, y[tr])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/feature_extraction/text.py:1422\u001b[39m, in \u001b[36mtransform\u001b[39m\u001b[34m(self, raw_documents)\u001b[39m\n\u001b[32m   1406\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[32m   1407\u001b[39m \n\u001b[32m   1408\u001b[39m \u001b[33;03mExtract token counts out of raw text documents using the vocabulary\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1419\u001b[39m \u001b[33;03m    Document-term matrix.\u001b[39;00m\n\u001b[32m   1420\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1421\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1423\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIterable over raw text documents expected, string object received.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1424\u001b[39m     )\n\u001b[32m   1425\u001b[39m \u001b[38;5;28mself\u001b[39m._check_vocabulary()\n\u001b[32m   1427\u001b[39m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/feature_extraction/text.py:1264\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1263\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1265\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1266\u001b[39m             feature_idx = vocabulary[feature]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/feature_extraction/text.py:111\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m    109\u001b[39m             doc = ngrams(doc, stop_words)\n\u001b[32m    110\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m             doc = \u001b[43mngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/feature_extraction/text.py:312\u001b[39m, in \u001b[36m_char_wb_ngrams\u001b[39m\u001b[34m(self, text_document)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# bind method outside of loop to reduce overhead\u001b[39;00m\n\u001b[32m    310\u001b[39m ngrams_append = ngrams.append\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m text_document.split():\n\u001b[32m    313\u001b[39m     w = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + w + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    314\u001b[39m     w_len = \u001b[38;5;28mlen\u001b[39m(w)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "816a1587-a823-4bbf-b094-e0b25393df17",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "vec_params = dict(analyzer='char_wb', ngram_range=(2,7), lowercase=False,\n",
        "                  n_features=2_000_000, alternate_sign=False)\n",
        "c_grid = [3.0, 5.0, 8.0]\n",
        "\n",
        "best_sc = 1e9; best_oof = None; best_test = None; best_c = None\n",
        "for c in c_grid:\n",
        "    oof = np.zeros((len(train), 3)); scores = []; Ptest = np.zeros((len(test), 3))\n",
        "    for f, (tr, va) in enumerate(skf.split(train['text'], y), 1):\n",
        "        vec = HashingVectorizer(**vec_params)\n",
        "        Xtr = vec.fit_transform(train['text'].iloc[tr])\n",
        "        Xva = vec.transform(train['text'].iloc[va])\n",
        "        Xte = vec.transform(test['text'])\n",
        "        clf = LogisticRegression(solver='lbfgs', C=c, max_iter=3000, tol=1e-4, random_state=42+f)\n",
        "        clf.fit(Xtr, y[tr])\n",
        "        pva = clf.predict_proba(Xva); oof[va] = pva\n",
        "        Ptest += clf.predict_proba(Xte)\n",
        "        scores.append(log_loss(y[va], pva))\n",
        "    Ptest /= skf.n_splits\n",
        "    sc = float(np.mean(scores))\n",
        "    print(f'Hashing char_wb C={c} 10f OOF: {sc:.4f}')\n",
        "    if sc < best_sc:\n",
        "        best_sc = sc; best_oof = oof; best_test = Ptest; best_c = c\n",
        "\n",
        "print(f'Best Hashing char_wb OOF: {best_sc:.4f} at C={best_c}')\n",
        "pd.DataFrame(best_oof, columns=classes).to_csv('oof_hashing_char_wb.csv', index=False)\n",
        "pd.DataFrame(best_test, columns=classes).to_csv('test_hashing_char_wb.csv', index=False)"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "id": "2ca4f981-7f3c-45b1-ac65-43a95b5ec92d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Fix submission.csv format: load sample_submission, replace probs, ensure sum=1\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "probs = pd.read_csv('submission.csv')[['EAP', 'HPL', 'MWS']].values\n",
        "probs = np.clip(probs, 1e-15, 1-1e-15)\n",
        "probs /= probs.sum(axis=1, keepdims=True)\n",
        "sample_sub[['EAP', 'HPL', 'MWS']] = probs\n",
        "sample_sub.to_csv('submission.csv', index=False)\n",
        "print('Fixed submission.csv with id column and normalized probs')\n",
        "print('Probs shape:', probs.shape)\n",
        "print('Probs sum check:', (probs.sum(axis=1) - 1.0).max())\n",
        "print('Ready for submit_final_answer')"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed submission.csv with id column and normalized probs\nProbs shape: (1958, 3)\nProbs sum check: 2.220446049250313e-16\nReady for submit_final_answer\n"
          ]
        }
      ]
    },
    {
      "id": "94275b76-31b9-4d93-9b25-6d15feaf94d8",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "train = pd.read_csv('train.csv'); test = pd.read_csv('test.csv')\n",
        "train['text'] = train['text'].fillna(''); test['text'] = test['text'].fillna('')\n",
        "le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "def log_count_ratio(X, yb, alpha):\n",
        "    pos = np.asarray(X[yb==1].sum(0)).ravel() + alpha\n",
        "    neg = np.asarray(X[yb==0].sum(0)).ravel() + alpha\n",
        "    r = np.log(pos/neg); r[~np.isfinite(r)] = 0.0\n",
        "    return r\n",
        "\n",
        "def odds_norm(P, eps=1e-15):\n",
        "    P = np.clip(P, eps, 1-eps); odds = P/(1-P)\n",
        "    Q = odds / (odds.sum(axis=1, keepdims=True) + eps)\n",
        "    return Q / Q.sum(axis=1, keepdims=True)\n",
        "\n",
        "param_grid = [\n",
        "    {'ngram': (1,2), 'min_df': 1, 'alpha': 0.05, 'C': 1.5},\n",
        "    {'ngram': (1,2), 'min_df': 1, 'alpha': 0.1, 'C': 2.0},\n",
        "    {'ngram': (1,2), 'min_df': 2, 'alpha': 0.05, 'C': 1.5},\n",
        "    {'ngram': (1,2), 'min_df': 2, 'alpha': 0.1, 'C': 2.0},\n",
        "    {'ngram': (1,3), 'min_df': 1, 'alpha': 0.05, 'C': 1.5},\n",
        "    {'ngram': (1,3), 'min_df': 1, 'alpha': 0.1, 'C': 2.0},\n",
        "    {'ngram': (1,3), 'min_df': 2, 'alpha': 0.05, 'C': 1.5},\n",
        "    {'ngram': (1,3), 'min_df': 2, 'alpha': 0.1, 'C': 2.0},\n",
        "    {'ngram': (1,2), 'min_df': 1, 'alpha': 0.2, 'C': 2.5},\n",
        "    {'ngram': (1,2), 'min_df': 1, 'alpha': 0.2, 'C': 3.0},\n",
        "    {'ngram': (1,3), 'min_df': 1, 'alpha': 0.2, 'C': 2.5},\n",
        "    {'ngram': (1,3), 'min_df': 1, 'alpha': 0.2, 'C': 3.0}\n",
        "]\n",
        "\n",
        "best_sc = 1e9; best_oof = None; best_test = None; best_params = None\n",
        "for p in param_grid:\n",
        "    oof = np.zeros((len(train), 3)); scores = []; test_preds = []\n",
        "    for f,(tr,va) in enumerate(skf.split(train['text'], y),1):\n",
        "        vec = CountVectorizer(analyzer='word', ngram_range=p['ngram'], lowercase=True,\n",
        "                              min_df=p['min_df'], max_df=0.95, binary=False)\n",
        "        Xtr_cnt = vec.fit_transform(train['text'].iloc[tr])\n",
        "        Xva_cnt = vec.transform(train['text'].iloc[va])\n",
        "        Xte_cnt = vec.transform(test['text'])\n",
        "        Xtr_bin = Xtr_cnt.copy(); Xtr_bin.data[:] = 1\n",
        "        Xva_bin = Xva_cnt.copy(); Xva_bin.data[:] = 1\n",
        "        Xte_bin = Xte_cnt.copy(); Xte_bin.data[:] = 1\n",
        "        Pva = np.zeros((len(va), 3)); Pte = np.zeros((len(test), 3))\n",
        "        for c in range(3):\n",
        "            yb = (y[tr]==c).astype(int)\n",
        "            r = log_count_ratio(Xtr_cnt, yb, alpha=p['alpha'])\n",
        "            clf = LogisticRegression(solver='liblinear', penalty='l2', C=p['C'],\n",
        "                                     max_iter=3000, tol=1e-4, random_state=42+c)\n",
        "            clf.fit(Xtr_bin.multiply(csr_matrix(r)), yb)\n",
        "            Pva[:,c] = clf.predict_proba(Xva_bin.multiply(csr_matrix(r)))[:,1]\n",
        "            Pte[:,c] = clf.predict_proba(Xte_bin.multiply(csr_matrix(r)))[:,1]\n",
        "        Pva = odds_norm(Pva); oof[va] = Pva\n",
        "        Pte = odds_norm(Pte); test_preds.append(Pte)\n",
        "        scores.append(log_loss(y[va], Pva))\n",
        "    sc = float(np.mean(scores)); print(f'Word NB-SVM {p} OOF: {sc:.4f}')\n",
        "    if sc < best_sc:\n",
        "        best_sc = sc; best_oof = oof; best_params = p\n",
        "        best_test = np.mean(test_preds, axis=0)\n",
        "\n",
        "print('Best Word NB-SVM OOF:', round(best_sc,4), 'params:', best_params)\n",
        "pd.DataFrame(best_oof, columns=classes).to_csv('oof_word_nbsvm_tuned.csv', index=False)\n",
        "pd.DataFrame(best_test, columns=classes).to_csv('test_word_nbsvm_tuned.csv', index=False)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM {'ngram': (1, 2), 'min_df': 1, 'alpha': 0.05, 'C': 1.5} OOF: 0.4409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM {'ngram': (1, 2), 'min_df': 1, 'alpha': 0.1, 'C': 2.0} OOF: 0.4479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM {'ngram': (1, 2), 'min_df': 2, 'alpha': 0.05, 'C': 1.5} OOF: 0.4807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM {'ngram': (1, 2), 'min_df': 2, 'alpha': 0.1, 'C': 2.0} OOF: 0.4885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM {'ngram': (1, 3), 'min_df': 1, 'alpha': 0.05, 'C': 1.5} OOF: 0.4505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM {'ngram': (1, 3), 'min_df': 1, 'alpha': 0.1, 'C': 2.0} OOF: 0.4527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM {'ngram': (1, 3), 'min_df': 2, 'alpha': 0.05, 'C': 1.5} OOF: 0.4719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word NB-SVM {'ngram': (1, 3), 'min_df': 2, 'alpha': 0.1, 'C': 2.0} OOF: 0.4792\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[159]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m Xtr_cnt = vec.fit_transform(train[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].iloc[tr])\n\u001b[32m     47\u001b[39m Xva_cnt = vec.transform(train[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].iloc[va])\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m Xte_cnt = \u001b[43mvec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m Xtr_bin = Xtr_cnt.copy(); Xtr_bin.data[:] = \u001b[32m1\u001b[39m\n\u001b[32m     50\u001b[39m Xva_bin = Xva_cnt.copy(); Xva_bin.data[:] = \u001b[32m1\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/feature_extraction/text.py:1422\u001b[39m, in \u001b[36mtransform\u001b[39m\u001b[34m(self, raw_documents)\u001b[39m\n\u001b[32m   1406\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[32m   1407\u001b[39m \n\u001b[32m   1408\u001b[39m \u001b[33;03mExtract token counts out of raw text documents using the vocabulary\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1419\u001b[39m \u001b[33;03m    Document-term matrix.\u001b[39;00m\n\u001b[32m   1420\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1421\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1423\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIterable over raw text documents expected, string object received.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1424\u001b[39m     )\n\u001b[32m   1425\u001b[39m \u001b[38;5;28mself\u001b[39m._check_vocabulary()\n\u001b[32m   1427\u001b[39m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/feature_extraction/text.py:1264\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1263\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1265\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1266\u001b[39m             feature_idx = vocabulary[feature]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/sklearn/feature_extraction/text.py:106\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    105\u001b[39m     doc = decoder(doc)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    107\u001b[39m     doc = analyzer(doc)\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "id": "797bdb83-c069-42dc-aa07-85d7415dfe16",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from scipy.special import softmax\n",
        "from scipy.stats import entropy as ent\n",
        "\n",
        "train = pd.read_csv('train.csv'); le = LabelEncoder(); y = le.fit_transform(train['author']); classes = list(le.classes_)\n",
        "skf_meta = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Base pool files (OOF scores in comments)\n",
        "base_files = [\n",
        "    ('oof_10f_hstack_lr.csv', 'test_10f_hstack_lr.csv'),  # 0.3629\n",
        "    ('oof_pl_refined_hstack_lr.csv', 'test_pl_refined_hstack_lr.csv'),  # 0.3628\n",
        "    ('oof_10f_char_wb_2_7.csv', 'test_10f_char_wb_2_7.csv'),  # 0.3912\n",
        "    ('oof_10f_char_wb_3_7.csv', 'test_10f_char_wb_3_7.csv'),  # 0.3947\n",
        "    ('oof_10f_cal_ridge_char_wb.csv', 'test_10f_cal_ridge_char_wb.csv'),  # 0.4116\n",
        "    ('oof_calsvc_char.csv', 'test_calsvc_char.csv'),  # 0.4403\n",
        "    ('oof_stylo_word_lr.csv', 'test_stylo_word_lr.csv'),  # 0.4624\n",
        "    ('oof_word_nbsvm_improved2.csv', 'test_word_nbsvm_improved2.csv')  # 0.4358\n",
        "]\n",
        "\n",
        "# Load OOF and test preds\n",
        "oofs = []; tests = []; names = []\n",
        "for o_file, t_file in base_files:\n",
        "    o = pd.read_csv(o_file)[classes].values\n",
        "    t = pd.read_csv(t_file)[classes].values\n",
        "    oofs.append(o); tests.append(t)\n",
        "    names.append(o_file.replace('.csv', ''))\n",
        "print('Loaded', len(oofs), 'base models')\n",
        "\n",
        "# Compute meta features per base: max_prob, entropy, margin (top1 - top2)\n",
        "n_bases = len(oofs); n_train = len(train)\n",
        "meta_feats_train = np.zeros((n_train, 3 * n_bases))\n",
        "meta_feats_test = np.zeros((len(test), 3 * n_bases))\n",
        "for i, (oof, tst) in enumerate(zip(oofs, tests)):\n",
        "    start = i * 3\n",
        "    # max_prob\n",
        "    meta_feats_train[:, start] = oof.max(axis=1)\n",
        "    meta_feats_test[:, start] = tst.max(axis=1)\n",
        "    # entropy\n",
        "    meta_feats_train[:, start+1] = ent(oof, axis=1)\n",
        "    meta_feats_test[:, start+1] = ent(tst, axis=1)\n",
        "    # margin\n",
        "    top2 = np.partition(oof, -2, axis=1)[:, -2]\n",
        "    meta_feats_train[:, start+2] = oof.max(axis=1) - top2\n",
        "    top2_t = np.partition(tst, -2, axis=1)[:, -2]\n",
        "    meta_feats_test[:, start+2] = tst.max(axis=1) - top2_t\n",
        "\n",
        "# Stack base probs as logits for meta\n",
        "def to_logits(P): return np.log(np.clip(P, 1e-15, 1-1e-15) / (1 - np.clip(P, 1e-15, 1-1e-15)))\n",
        "logit_oofs = [to_logits(o) for o in oofs]\n",
        "X_logit_train = np.hstack(logit_oofs)\n",
        "X_logit_test = np.hstack([to_logits(t) for t in tests])\n",
        "\n",
        "# Full meta input: logits + meta_feats\n",
        "X_train = np.hstack([X_logit_train, meta_feats_train])\n",
        "X_test = np.hstack([X_logit_test, meta_feats_test])\n",
        "\n",
        "# 1. Greedy forward selection on base probs (simple mean, target 4-7)\n",
        "best_greedy = 1e9; sel_greedy = []\n",
        "while len(sel_greedy) < 7:\n",
        "    improved = False; cand = None\n",
        "    for i in range(len(oofs)):\n",
        "        if i in sel_greedy: continue\n",
        "        idx = sel_greedy + [i]\n",
        "        blend = np.mean([oofs[j] for j in idx], axis=0)\n",
        "        sc = log_loss(y, blend)\n",
        "        if sc < best_greedy - 1e-6:\n",
        "            best_greedy = sc; improved = True; cand = i\n",
        "    if not improved: break\n",
        "    sel_greedy.append(cand)\n",
        "print('Greedy selected:', [names[i] for i in sel_greedy], 'OOF:', round(best_greedy,4))\n",
        "\n",
        "# 2. Weighted average (Dirichlet on greedy selected)\n",
        "sel_oofs_g = [oofs[i] for i in sel_greedy]; sel_tests_g = [tests[i] for i in sel_greedy]\n",
        "rng = np.random.default_rng(42); best_w = None; best_w_sc = 1e9\n",
        "for _ in range(5000):\n",
        "    w = rng.dirichlet(np.ones(len(sel_oofs_g)))\n",
        "    sc = log_loss(y, sum(wi*oo for wi,oo in zip(w, sel_oofs_g)))\n",
        "    if sc < best_w_sc: best_w_sc = sc; best_w = w\n",
        "print('Weighted avg OOF:', round(best_w_sc,4))\n",
        "\n",
        "# 3. LR-on-logits 10f CV on full X_train (C grid)\n",
        "best_c = None; best_lr_sc = 1e9; best_oof_lr = None\n",
        "c_grid = [0.25, 0.5, 1.0, 1.5, 2.0, 3.0]\n",
        "for c in c_grid:\n",
        "    oof_lr = np.zeros((n_train, 3)); scs = []\n",
        "    for tr,va in skf_meta.split(X_train, y):\n",
        "        meta = LogisticRegression(solver='lbfgs', C=c, max_iter=2000, multi_class='multinomial', random_state=42)\n",
        "        meta.fit(X_train[tr], y[tr])\n",
        "        p = meta.predict_proba(X_train[va]); oof_lr[va] = p\n",
        "        scs.append(log_loss(y[va], p))\n",
        "    sc = float(np.mean(scs)); print(f'LR C={c} OOF: {sc:.4f}')\n",
        "    if sc < best_lr_sc: best_lr_sc = sc; best_c = c; best_oof_lr = oof_lr\n",
        "\n",
        "# 4. L1 meta (saga solver) on full X_train (C grid)\n",
        "best_c_l1 = None; best_l1_sc = 1e9; best_oof_l1 = None\n",
        "c_grid_l1 = [0.5, 1.0, 2.0]\n",
        "for c in c_grid_l1:\n",
        "    oof_l1 = np.zeros((n_train, 3)); scs = []\n",
        "    for tr,va in skf_meta.split(X_train, y):\n",
        "        meta = LogisticRegression(solver='saga', penalty='l1', C=c, max_iter=2000, multi_class='multinomial', random_state=42)\n",
        "        meta.fit(X_train[tr], y[tr])\n",
        "        p = meta.predict_proba(X_train[va]); oof_l1[va] = p\n",
        "        scs.append(log_loss(y[va], p))\n",
        "    sc = float(np.mean(scs)); print(f'L1 LR C={c} OOF: {sc:.4f}')\n",
        "    if sc < best_l1_sc: best_l1_sc = sc; best_c_l1 = c; best_oof_l1 = oof_l1\n",
        "\n",
        "# Pick best method and generate submission\n",
        "methods = {'greedy': best_greedy, 'weighted': best_w_sc, 'lr_l2': best_lr_sc, 'lr_l1': best_l1_sc}\n",
        "best_method = min(methods, key=methods.get); best_sc = methods[best_method]\n",
        "print(f'Best method: {best_method} OOF: {best_sc:.4f}')\n",
        "\n",
        "if best_method == 'greedy':\n",
        "    final_test = np.mean(sel_tests_g, axis=0)\n",
        "elif best_method == 'weighted':\n",
        "    final_test = sum(wi * tt for wi, tt in zip(best_w, sel_tests_g))\n",
        "elif best_method == 'lr_l2':\n",
        "    meta = LogisticRegression(solver='lbfgs', C=best_c, max_iter=2000, multi_class='multinomial', random_state=42)\n",
        "    meta.fit(X_train, y)\n",
        "    final_test = meta.predict_proba(X_test)\n",
        "else:  # l1\n",
        "    meta = LogisticRegression(solver='saga', penalty='l1', C=best_c_l1, max_iter=2000, multi_class='multinomial', random_state=42)\n",
        "    meta.fit(X_train, y)\n",
        "    final_test = meta.predict_proba(X_test)\n",
        "\n",
        "final_test = np.clip(final_test, 1e-15, 1-1e-15); final_test /= final_test.sum(axis=1, keepdims=True)\n",
        "pd.DataFrame(final_test, columns=classes).to_csv('submission.csv', index=False)\n",
        "print('New submission.csv saved with OOF:', round(best_sc,4))\n",
        "if best_sc <= 0.29381:\n",
        "    print('Bronze achieved! Proceed to submit_final_answer.')\n",
        "else:\n",
        "    print('Close to bronze; consider further improvements or submit as is for potential private LB medal.')"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 8 base models\nGreedy selected: ['oof_pl_refined_hstack_lr', 'oof_word_nbsvm_improved2', 'oof_10f_cal_ridge_char_wb'] OOF: 0.3287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted avg OOF: 0.3273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR C=0.25 OOF: 0.3149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR C=0.5 OOF: 0.3151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR C=1.0 OOF: 0.3153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR C=1.5 OOF: 0.3154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR C=2.0 OOF: 0.3154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR C=3.0 OOF: 0.3156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 LR C=0.5 OOF: 0.3145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 LR C=1.0 OOF: 0.3148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 LR C=2.0 OOF: 0.3150\nBest method: lr_l1 OOF: 0.3145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New submission.csv saved with OOF: 0.3145\nClose to bronze; consider further improvements or submit as is for potential private LB medal.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}