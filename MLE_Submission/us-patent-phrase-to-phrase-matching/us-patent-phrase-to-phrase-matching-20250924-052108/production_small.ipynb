{
  "cells": [
    {
      "id": "0b1ae046-a634-41c6-b9d4-ec2aa0e8f12e",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# IDF-weighted overlap features (1-2 grams), fold-safe. Outputs: oof_idf_overlap.csv, idf_overlap_test.csv\n",
        "import time, re, math, numpy as np, pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "def tokenize_words(s: str):\n",
        "    return re.findall(r\"\\w+\", str(s).lower())\n",
        "\n",
        "STOP = set(ENGLISH_STOP_WORDS)\n",
        "\n",
        "def filter_tokens(tokens):\n",
        "    # remove stopwords and single-character tokens (mostly noise)\n",
        "    return [t for t in tokens if (t not in STOP and len(t) > 1)]\n",
        "\n",
        "def gen_ngrams(tokens, n):\n",
        "    if n == 1:\n",
        "        return tokens\n",
        "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "\n",
        "def build_df(corpus_docs):\n",
        "    # corpus_docs: iterable of sets of terms\n",
        "    df = Counter()\n",
        "    for terms in corpus_docs:\n",
        "        if terms:\n",
        "            df.update(set(terms))\n",
        "    return df\n",
        "\n",
        "def idf_from_df(df_counter, N):\n",
        "    # log((N - df + 0.5)/(df + 0.5) + 1)\n",
        "    idf = {}\n",
        "    for t, df in df_counter.items():\n",
        "        idf[t] = math.log((N - df + 0.5)/(df + 0.5) + 1.0)\n",
        "    return idf\n",
        "\n",
        "def weighted_overlap_metrics(A_terms, B_terms, idf):\n",
        "    # IDF-weighted precision/recall/F1 and Jaccard\n",
        "    A_set, B_set = set(A_terms), set(B_terms)\n",
        "    if not A_set and not B_set:\n",
        "        return (0.0, 0.0, 0.0, 0.0, 0.0, 0, 0)\n",
        "    inter = A_set & B_set\n",
        "    union = A_set | B_set\n",
        "    w_inter = sum(idf.get(t, 0.0) for t in inter)\n",
        "    w_A = sum(idf.get(t, 0.0) for t in A_set) + 1e-12\n",
        "    w_B = sum(idf.get(t, 0.0) for t in B_set) + 1e-12\n",
        "    w_union = sum(idf.get(t, 0.0) for t in union) + 1e-12\n",
        "    prec = w_inter / w_A\n",
        "    rec = w_inter / w_B\n",
        "    f1 = 0.0 if (prec+rec) == 0 else (2*prec*rec)/(prec+rec)\n",
        "    jac = w_inter / w_union\n",
        "    return (prec, rec, f1, jac, w_inter, len(A_set), len(B_set))\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds_by_id.csv')\n",
        "train = train.merge(folds, on='id', how='left', validate='one_to_one')\n",
        "assert (train['fold']>=0).all(), 'Fold merge by id failed'\n",
        "\n",
        "# Pre-tokenize and build 1-2 gram bags per row (stopword-removed)\n",
        "A_tr_tok = [filter_tokens(tokenize_words(x)) for x in train['anchor'].astype(str).tolist()]\n",
        "B_tr_tok = [filter_tokens(tokenize_words(x)) for x in train['target'].astype(str).tolist()]\n",
        "A_te_tok = [filter_tokens(tokenize_words(x)) for x in test['anchor'].astype(str).tolist()]\n",
        "B_te_tok = [filter_tokens(tokenize_words(x)) for x in test['target'].astype(str).tolist()]\n",
        "\n",
        "def make_ngrams_pairs(tokens_list):\n",
        "    uni = [gen_ngrams(t,1) for t in tokens_list]\n",
        "    bi = [gen_ngrams(t,2) for t in tokens_list]\n",
        "    return uni, bi\n",
        "\n",
        "A_tr_uni, A_tr_bi = make_ngrams_pairs(A_tr_tok)\n",
        "B_tr_uni, B_tr_bi = make_ngrams_pairs(B_tr_tok)\n",
        "A_te_uni, A_te_bi = make_ngrams_pairs(A_te_tok)\n",
        "B_te_uni, B_te_bi = make_ngrams_pairs(B_te_tok)\n",
        "\n",
        "fold_arr = train['fold'].values.astype(int)\n",
        "n_tr = len(train); n_te = len(test)\n",
        "\n",
        "# Allocate OOF arrays\n",
        "cols = [\n",
        "    'idf1_prec','idf1_rec','idf1_f1','idf1_jac','idf1_wi',\n",
        "    'idf2_prec','idf2_rec','idf2_f1','idf2_jac','idf2_wi'\n",
        "]\n",
        "oof = np.zeros((n_tr, len(cols)), dtype=np.float32)\n",
        "te_fold_preds = []  # list of (n_te, len(cols)) arrays\n",
        "\n",
        "for f in sorted(np.unique(fold_arr)):\n",
        "    f0 = time.time()\n",
        "    tr_idx = np.where(fold_arr != f)[0]\n",
        "    va_idx = np.where(fold_arr == f)[0]\n",
        "    # Build corpus for IDF on train-only (union of anchor+target docs for each n-gram level)\n",
        "    # Unigrams\n",
        "    corpus_uni_docs = [set(A_tr_uni[i]) for i in tr_idx] + [set(B_tr_uni[i]) for i in tr_idx]\n",
        "    N_uni = len(corpus_uni_docs)\n",
        "    df_uni = build_df(corpus_uni_docs)\n",
        "    idf_uni = idf_from_df(df_uni, N_uni)\n",
        "    # Bigrams\n",
        "    corpus_bi_docs = [set(A_tr_bi[i]) for i in tr_idx] + [set(B_tr_bi[i]) for i in tr_idx]\n",
        "    N_bi = len(corpus_bi_docs)\n",
        "    df_bi = build_df(corpus_bi_docs)\n",
        "    idf_bi = idf_from_df(df_bi, N_bi)\n",
        "\n",
        "    # Compute OOF for this fold\n",
        "    for i in va_idx:\n",
        "        p1, r1, f1, j1, wi1, _, _ = weighted_overlap_metrics(A_tr_uni[i], B_tr_uni[i], idf_uni)\n",
        "        p2, r2, f2, j2, wi2, _, _ = weighted_overlap_metrics(A_tr_bi[i], B_tr_bi[i], idf_bi)\n",
        "        oof[i, :] = [p1, r1, f1, j1, wi1, p2, r2, f2, j2, wi2]\n",
        "\n",
        "    # Compute test features for this fold\n",
        "    te_mat = np.zeros((n_te, len(cols)), dtype=np.float32)\n",
        "    for j in range(n_te):\n",
        "        p1, r1, f1, j1, wi1, _, _ = weighted_overlap_metrics(A_te_uni[j], B_te_uni[j], idf_uni)\n",
        "        p2, r2, f2, j2, wi2, _, _ = weighted_overlap_metrics(A_te_bi[j], B_te_bi[j], idf_bi)\n",
        "        te_mat[j, :] = [p1, r1, f1, j1, wi1, p2, r2, f2, j2, wi2]\n",
        "    te_fold_preds.append(te_mat)\n",
        "    print(f'IDF-overlap fold {int(f)} done in {time.time()-f0:.1f}s', flush=True)\n",
        "\n",
        "# Aggregate test across folds (mean)\n",
        "te_mean = np.mean(np.stack(te_fold_preds, axis=0), axis=0).astype(np.float32)\n",
        "\n",
        "# Save artifacts\n",
        "oof_df = pd.DataFrame({'id': train['id']})\n",
        "for k, c in enumerate(cols):\n",
        "    oof_df[c] = oof[:, k]\n",
        "oof_df.to_csv('oof_idf_overlap.csv', index=False)\n",
        "\n",
        "te_df = pd.DataFrame({'id': test['id']})\n",
        "for k, c in enumerate(cols):\n",
        "    te_df[c] = te_mean[:, k]\n",
        "te_df.to_csv('idf_overlap_test.csv', index=False)\n",
        "\n",
        "print('Saved oof_idf_overlap.csv and idf_overlap_test.csv; elapsed', round((time.time()-t0)/60,2), 'min')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDF-overlap fold 0 done in 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDF-overlap fold 1 done in 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDF-overlap fold 2 done in 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDF-overlap fold 3 done in 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDF-overlap fold 4 done in 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_idf_overlap.csv and idf_overlap_test.csv; elapsed 0.02 min\n"
          ]
        }
      ]
    },
    {
      "id": "dfcd675e-bc87-4cf9-83be-045969ad7a93",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure rapidfuzz is installed (CPU-only, safe to add)\n",
        "import sys, subprocess\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'install', 'rapidfuzz==3.9.7'], check=True)\n",
        "print('rapidfuzz installed')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfuzz==3.9.7\n  Downloading rapidfuzz-3.9.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 87.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: rapidfuzz\nSuccessfully installed rapidfuzz-3.9.7\nrapidfuzz installed\n"
          ]
        }
      ]
    },
    {
      "id": "471522fc-f617-4639-94d5-cf1c7a23ab62",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# RapidFuzz token-set features (fold-safe; no fitting). Outputs: oof_fuzz.csv, fuzz_test.csv\n",
        "import time, numpy as np, pandas as pd\n",
        "from rapidfuzz import fuzz, utils\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds_by_id.csv')\n",
        "train = train.merge(folds, on='id', how='left', validate='one_to_one')\n",
        "assert (train['fold']>=0).all(), 'Fold merge by id failed'\n",
        "\n",
        "def prep_texts(df):\n",
        "    a = df['anchor'].astype(str).str.lower().tolist()\n",
        "    b = df['target'].astype(str).str.lower().tolist()\n",
        "    return a, b\n",
        "\n",
        "A_tr, B_tr = prep_texts(train)\n",
        "A_te, B_te = prep_texts(test)\n",
        "\n",
        "def compute_rf(a_list, b_list):\n",
        "    n = len(a_list)\n",
        "    s1 = np.zeros(n, dtype=np.float32)\n",
        "    s2 = np.zeros(n, dtype=np.float32)\n",
        "    proc = utils.default_process\n",
        "    for i, (a, b) in enumerate(zip(a_list, b_list)):\n",
        "        s1[i] = fuzz.token_set_ratio(a, b, processor=proc) / 100.0\n",
        "        s2[i] = fuzz.partial_token_set_ratio(a, b, processor=proc) / 100.0\n",
        "    return s1, s2\n",
        "\n",
        "tr_set, tr_partial = compute_rf(A_tr, B_tr)\n",
        "te_set, te_partial = compute_rf(A_te, B_te)\n",
        "\n",
        "pd.DataFrame({'id': train['id'], 'rf_token_set': tr_set, 'rf_partial_token_set': tr_partial}).to_csv('oof_fuzz.csv', index=False)\n",
        "pd.DataFrame({'id': test['id'], 'rf_token_set': te_set, 'rf_partial_token_set': te_partial}).to_csv('fuzz_test.csv', index=False)\n",
        "print('Saved oof_fuzz.csv and fuzz_test.csv; elapsed', round((time.time()-t0)/60,2), 'min')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_fuzz.csv and fuzz_test.csv; elapsed 0.0 min\n"
          ]
        }
      ]
    },
    {
      "id": "ceb6deef-8f47-4520-b064-d98ff9c5c37d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LCS (char/token) and char n-gram (3-5) similarities. Outputs: oof_lcs_char_ngrams.csv, lcs_char_ngrams_test.csv\n",
        "import time, numpy as np, pandas as pd, re\n",
        "from rapidfuzz.distance import LCSseq\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds_by_id.csv')\n",
        "train = train.merge(folds, on='id', how='left', validate='one_to_one')\n",
        "assert (train['fold']>=0).all(), 'Fold merge by id failed'\n",
        "\n",
        "def char_lcs_ratio(a: str, b: str) -> float:\n",
        "    # normalized_similarity returns [0,100]\n",
        "    return float(LCSseq.normalized_similarity(str(a), str(b))) / 100.0\n",
        "\n",
        "def tokenize_words(s: str):\n",
        "    return re.findall(r\"\\w+\", str(s).lower())\n",
        "\n",
        "def token_lcs_ratio(a: str, b: str) -> float:\n",
        "    ta = tokenize_words(a); tb = tokenize_words(b)\n",
        "    na, nb = len(ta), len(tb)\n",
        "    if na == 0 and nb == 0:\n",
        "        return 0.0\n",
        "    # DP LCS on tokens (phrases are short, so this is fine)\n",
        "    dp = [0] * (nb + 1)\n",
        "    for i in range(1, na + 1):\n",
        "        prev = 0\n",
        "        ai = ta[i-1]\n",
        "        for j in range(1, nb + 1):\n",
        "            tmp = dp[j]\n",
        "            if ai == tb[j-1]:\n",
        "                dp[j] = prev + 1\n",
        "            else:\n",
        "                if dp[j] < dp[j-1]:\n",
        "                    dp[j] = dp[j-1]\n",
        "            prev = tmp\n",
        "    lcs_len = dp[nb]\n",
        "    return float(lcs_len) / float(max(na, nb) if max(na, nb) > 0 else 1)\n",
        "\n",
        "def shingles(s: str, k: int):\n",
        "    s = str(s).lower()\n",
        "    if k <= 0:\n",
        "        return set()\n",
        "    if len(s) < k:\n",
        "        return {s} if s else set()\n",
        "    return {s[i:i+k] for i in range(len(s)-k+1)}\n",
        "\n",
        "def jaccard(a: set, b: set) -> float:\n",
        "    if not a and not b: return 0.0\n",
        "    return len(a & b) / (len(a | b) + 1e-12)\n",
        "\n",
        "def dice(a: set, b: set) -> float:\n",
        "    if not a and not b: return 0.0\n",
        "    return 2.0 * len(a & b) / (len(a) + len(b) + 1e-12)\n",
        "\n",
        "def compute_features(df: pd.DataFrame):\n",
        "    A = df['anchor'].astype(str).tolist()\n",
        "    B = df['target'].astype(str).tolist()\n",
        "    n = len(df)\n",
        "    lcs_char = np.zeros(n, dtype=np.float32)\n",
        "    lcs_tok  = np.zeros(n, dtype=np.float32)\n",
        "    jac3 = np.zeros(n, dtype=np.float32); jac4 = np.zeros(n, dtype=np.float32); jac5 = np.zeros(n, dtype=np.float32)\n",
        "    dice3 = np.zeros(n, dtype=np.float32); dice4 = np.zeros(n, dtype=np.float32); dice5 = np.zeros(n, dtype=np.float32)\n",
        "    for i, (a, b) in enumerate(zip(A, B)):\n",
        "        lcs_char[i] = char_lcs_ratio(a, b)\n",
        "        lcs_tok[i]  = token_lcs_ratio(a, b)\n",
        "        a3 = shingles(a, 3); b3 = shingles(b, 3)\n",
        "        a4 = shingles(a, 4); b4 = shingles(b, 4)\n",
        "        a5 = shingles(a, 5); b5 = shingles(b, 5)\n",
        "        jac3[i] = jaccard(a3, b3); jac4[i] = jaccard(a4, b4); jac5[i] = jaccard(a5, b5)\n",
        "        dice3[i] = dice(a3, b3); dice4[i] = dice(a4, b4); dice5[i] = dice(a5, b5)\n",
        "    return pd.DataFrame({\n",
        "        'lcs_char': lcs_char, 'lcs_tok': lcs_tok,\n",
        "        'char3_jac': jac3, 'char4_jac': jac4, 'char5_jac': jac5,\n",
        "        'char3_dice': dice3, 'char4_dice': dice4, 'char5_dice': dice5\n",
        "    })\n",
        "\n",
        "tr_feats = compute_features(train)\n",
        "te_feats = compute_features(test)\n",
        "\n",
        "tr_out = pd.concat([train[['id']].reset_index(drop=True), tr_feats.reset_index(drop=True)], axis=1)\n",
        "te_out = pd.concat([test[['id']].reset_index(drop=True), te_feats.reset_index(drop=True)], axis=1)\n",
        "tr_out.to_csv('oof_lcs_char_ngrams.csv', index=False)\n",
        "te_out.to_csv('lcs_char_ngrams_test.csv', index=False)\n",
        "print('Saved oof_lcs_char_ngrams.csv and lcs_char_ngrams_test.csv; elapsed', round((time.time()-t0)/60,2), 'min')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_lcs_char_ngrams.csv and lcs_char_ngrams_test.csv; elapsed 0.01 min\n"
          ]
        }
      ]
    },
    {
      "id": "85640263-e909-401d-87df-970921ba2b25",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick health check for latest LGBM stacker outputs\n",
        "import pandas as pd, numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "oof = pd.read_csv('oof_stack_lgbm.csv')  # expects columns: id, oof\n",
        "df = train[['id','score']].merge(oof, on='id', how='inner')\n",
        "p = pearsonr(df['oof'].astype(float).values, df['score'].astype(float).values)[0]\n",
        "print('OOF Pearson (oof_stack_lgbm.csv vs train score):', round(float(p), 6))\n",
        "\n",
        "sub = pd.read_csv('submission_stack_lgbm.csv')\n",
        "print('Submission stats: n=', len(sub), 'min=', float(sub['score'].min()), 'max=', float(sub['score'].max()), 'mean=', float(sub['score'].mean()))\n",
        "print(sub.head())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF Pearson (oof_stack_lgbm.csv vs train score): 0.752741\nSubmission stats: n= 3648 min= 0.0 max= 1.0 mean= 0.35697559763779885\n                 id     score\n0  2a988c7d98568627  0.128831\n1  75a3ae03b26e2f7e  0.301520\n2  0126c870aede9858  0.076760\n3  2cf662e1cc9b354e  0.314683\n4  8dfee5874de0b408  0.149585\n"
          ]
        }
      ]
    },
    {
      "id": "c2df8f94-3e21-4a87-a897-d034caa78cf3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Numeric and units normalization + overlap features (fold-safe, no fitting). Outputs: oof_numeric_units.csv, numeric_units_test.csv\n",
        "import re, unicodedata, time, numpy as np, pandas as pd\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# -------- Normalization utilities --------\n",
        "def nfkc(s: str) -> str:\n",
        "    return unicodedata.normalize('NFKC', s)\n",
        "\n",
        "REPLACEMENTS = {\n",
        "    '\\u00b5': 'u',  # micro sign\n",
        "    '\\u03bc': 'u',  # Greek mu\n",
        "    '\\u03a9': 'ohm',  # Omega\n",
        "    '\\u2126': 'ohm',  # Ohm symbol\n",
        "    '\\u00b0C': 'C',   # degree C -> C\n",
        "    '\\u00b0F': 'F',   # degree F -> F\n",
        "}\n",
        "\n",
        "def normalize_symbols(s: str) -> str:\n",
        "    s = nfkc(str(s))\n",
        "    for k, v in REPLACEMENTS.items():\n",
        "        s = s.replace(k, v)\n",
        "    # unify separators\n",
        "    s = s.replace('-', ' ').replace('_', ' ')\n",
        "    return s\n",
        "\n",
        "# Split attached number+unit like 10nm -> '10 nm'; 3.3kV -> '3.3 kV'\n",
        "NUM_UNIT_ATTACH = re.compile(r'(?i)(\\d+(?:[\\./]\\d+)?)([a-zA-Z%][a-zA-Z%/]*)')\n",
        "def split_attached_num_unit(s: str) -> str:\n",
        "    return NUM_UNIT_ATTACH.sub(r'\\1 \\2', s)\n",
        "\n",
        "def basic_clean(s: str) -> str:\n",
        "    s = normalize_symbols(s)\n",
        "    s = split_attached_num_unit(s)\n",
        "    return s\n",
        "\n",
        "# -------- Extraction --------\n",
        "NUM_RE = re.compile(r'(?i)\\b\\d+(?:[\\./]\\d+)?(?:e[+-]?\\d+)?\\b')\n",
        "UNIT_TOKEN_RE = re.compile(r'(?i)^[a-z][a-z0-9%/^-]{0,6}$')  # short-ish unit-like tokens\n",
        "\n",
        "def extract_numbers(text: str):\n",
        "    return [m.group(0) for m in NUM_RE.finditer(text)]\n",
        "\n",
        "def extract_tokens(text: str):\n",
        "    # Keep alnum and % / ^\n",
        "    return re.findall(r'[A-Za-z0-9%/^.]+', text.lower())\n",
        "\n",
        "def is_unit(tok: str) -> bool:\n",
        "    return bool(UNIT_TOKEN_RE.match(tok)) and not tok[0].isdigit()\n",
        "\n",
        "def parse_num(s: str) -> float | None:\n",
        "    try:\n",
        "        # replace '/' in decimals already handled; just use float safely\n",
        "        return float(s.replace('/', '.')) if '/' in s and s.count('/') == 1 else float(s)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def extract_num_unit_pairs(tokens):\n",
        "    pairs = []\n",
        "    for i, tok in enumerate(tokens):\n",
        "        if NUM_RE.fullmatch(tok):\n",
        "            # look ahead for a unit token\n",
        "            if i+1 < len(tokens) and is_unit(tokens[i+1]):\n",
        "                pairs.append((tok, tokens[i+1]))\n",
        "    return pairs\n",
        "\n",
        "def features_for_row(a: str, b: str):\n",
        "    a_txt = basic_clean(a)\n",
        "    b_txt = basic_clean(b)\n",
        "    nums_a = extract_numbers(a_txt)\n",
        "    nums_b = extract_numbers(b_txt)\n",
        "    toks_a = extract_tokens(a_txt)\n",
        "    toks_b = extract_tokens(b_txt)\n",
        "    # Units: collect tokens that look like units; prioritize ones following numbers\n",
        "    units_a = set([t for t in toks_a if is_unit(t)])\n",
        "    units_b = set([t for t in toks_b if is_unit(t)])\n",
        "    # Number counts and overlaps\n",
        "    cnt_num_a = len(nums_a); cnt_num_b = len(nums_b)\n",
        "    overlap_num_exact = len(set(nums_a) & set(nums_b))\n",
        "    # numeric deltas\n",
        "    vals_a = [parse_num(x) for x in nums_a]; vals_a = [v for v in vals_a if v is not None]\n",
        "    vals_b = [parse_num(x) for x in nums_b]; vals_b = [v for v in vals_b if v is not None]\n",
        "    if vals_a and vals_b:\n",
        "        # pairwise min absolute difference\n",
        "        mins = []\n",
        "        for va in vals_a:\n",
        "            md = min(abs(va - vb) for vb in vals_b)\n",
        "            mins.append(md)\n",
        "        min_abs_delta = float(min(mins))\n",
        "        mean_abs_delta = float(np.mean([abs(va - vb) for va in vals_a for vb in vals_b]))\n",
        "        any_equal_round0 = any(int(round(va)) == int(round(vb)) for va in vals_a for vb in vals_b)\n",
        "    else:\n",
        "        min_abs_delta = np.nan; mean_abs_delta = np.nan; any_equal_round0 = False\n",
        "    # unit overlaps\n",
        "    unit_overlap_cnt = len(units_a & units_b)\n",
        "    unit_union = len(units_a | units_b)\n",
        "    unit_jaccard = (unit_overlap_cnt / unit_union) if unit_union > 0 else 0.0\n",
        "    # number+unit bigram exact overlap\n",
        "    pairs_a = set(extract_num_unit_pairs(toks_a))\n",
        "    pairs_b = set(extract_num_unit_pairs(toks_b))\n",
        "    pair_overlap = len(pairs_a & pairs_b)\n",
        "    # simple ratios\n",
        "    num_count_diff = abs(cnt_num_a - cnt_num_b)\n",
        "    num_count_ratio = (min(cnt_num_a, cnt_num_b) / max(cnt_num_a, cnt_num_b)) if max(cnt_num_a, cnt_num_b) > 0 else 1.0\n",
        "    return {\n",
        "        'num_cnt_a': float(cnt_num_a),\n",
        "        'num_cnt_b': float(cnt_num_b),\n",
        "        'num_overlap_exact': float(overlap_num_exact),\n",
        "        'num_min_abs_delta': float(min_abs_delta) if min_abs_delta == min_abs_delta else np.nan,\n",
        "        'num_mean_abs_delta': float(mean_abs_delta) if mean_abs_delta == mean_abs_delta else np.nan,\n",
        "        'num_equal_round_int': float(any_equal_round0),\n",
        "        'unit_cnt_a': float(len(units_a)),\n",
        "        'unit_cnt_b': float(len(units_b)),\n",
        "        'unit_overlap_cnt': float(unit_overlap_cnt),\n",
        "        'unit_jaccard': float(unit_jaccard),\n",
        "        'numunit_pair_overlap': float(pair_overlap),\n",
        "        'num_count_diff': float(num_count_diff),\n",
        "        'num_count_ratio': float(num_count_ratio),\n",
        "    }\n",
        "\n",
        "def compute_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    A = df['anchor'].astype(str).tolist()\n",
        "    B = df['target'].astype(str).tolist()\n",
        "    rows = []\n",
        "    for i, (a, b) in enumerate(zip(A, B)):\n",
        "        rows.append(features_for_row(a, b))\n",
        "        if (i+1) % 5000 == 0:\n",
        "            print(f'.. {i+1} rows', flush=True)\n",
        "    out = pd.DataFrame(rows)\n",
        "    # cheap non-linearities\n",
        "    out['unit_overlap_cnt_sq'] = out['unit_overlap_cnt'] ** 2\n",
        "    out['num_overlap_exact_sq'] = out['num_overlap_exact'] ** 2\n",
        "    # replace NaNs in deltas with large sentinel (will be median-imputed in stacker anyway)\n",
        "    out['num_min_abs_delta'] = out['num_min_abs_delta'].fillna(1e6)\n",
        "    out['num_mean_abs_delta'] = out['num_mean_abs_delta'].fillna(1e6)\n",
        "    return out.astype('float32')\n",
        "\n",
        "tr_feats = compute_df(train)\n",
        "te_feats = compute_df(test)\n",
        "\n",
        "pd.concat([train[['id']].reset_index(drop=True), tr_feats.reset_index(drop=True)], axis=1).to_csv('oof_numeric_units.csv', index=False)\n",
        "pd.concat([test[['id']].reset_index(drop=True), te_feats.reset_index(drop=True)], axis=1).to_csv('numeric_units_test.csv', index=False)\n",
        "print('Saved oof_numeric_units.csv and numeric_units_test.csv; elapsed', round((time.time()-t0)/60,2), 'min')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 5000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 10000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 15000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 20000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 25000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 30000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_numeric_units.csv and numeric_units_test.csv; elapsed 0.01 min\n"
          ]
        }
      ]
    },
    {
      "id": "07be2865-81cb-4d56-b811-d81a176492b9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Abbreviation/acronym detection + expansion overlap features. Outputs: oof_acronym.csv, acronym_test.csv\n",
        "import re, time, numpy as np, pandas as pd\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Patterns: LONG (SHORT) | SHORT (LONG)\n",
        "# SHORT: 2-10 uppercase letters/digits with no spaces; allow hyphen optional inside\n",
        "# LONG: 3+ tokens letters/digits/-, at least as long as SHORT\n",
        "P_LONG_SHORT = re.compile(r\"([A-Za-z][A-Za-z0-9\\- ]{2,}?)\\s*\\(([A-Z0-9][A-Z0-9\\-]{1,9})\\)\")\n",
        "P_SHORT_LONG = re.compile(r\"\\b([A-Z0-9][A-Z0-9\\-]{1,9})\\b\\s*\\(([A-Za-z][A-Za-z0-9\\- ]{2,}?)\\)\")\n",
        "\n",
        "def extract_pairs(text: str):\n",
        "    pairs = {}  # short -> long\n",
        "    if not isinstance(text, str):\n",
        "        return pairs\n",
        "    for m in P_LONG_SHORT.finditer(text):\n",
        "        long = m.group(1).strip()\n",
        "        short = m.group(2).strip()\n",
        "        if len(short) >= 2 and len(long) >= len(short):\n",
        "            pairs[short] = long\n",
        "    for m in P_SHORT_LONG.finditer(text):\n",
        "        short = m.group(1).strip()\n",
        "        long = m.group(2).strip()\n",
        "        if len(short) >= 2 and len(long) >= len(short):\n",
        "            pairs[short] = long\n",
        "    return pairs\n",
        "\n",
        "def tokenize_simple(s: str):\n",
        "    return re.findall(r\"[A-Za-z0-9]+\", s.lower())\n",
        "\n",
        "def expand_text(s: str, pairs: dict):\n",
        "    # Replace whole-word SHORT with LONG (case-sensitive for SHORT); guard boundaries\n",
        "    if not pairs or not isinstance(s, str):\n",
        "        return s if isinstance(s, str) else ''\n",
        "    out = s\n",
        "    for short, long in pairs.items():\n",
        "        # whole word boundary replace; avoid catastrophic overlap by using regex\n",
        "        try:\n",
        "            out = re.sub(rf\"\\b{re.escape(short)}\\b\", long, out)\n",
        "        except re.error:\n",
        "            pass\n",
        "    return out\n",
        "\n",
        "def jaccard(a:set, b:set):\n",
        "    if not a and not b: return 0.0\n",
        "    return len(a & b) / (len(a | b) + 1e-12)\n",
        "\n",
        "def dice(a:set, b:set):\n",
        "    if not a and not b: return 0.0\n",
        "    return 2.0 * len(a & b) / (len(a) + len(b) + 1e-12)\n",
        "\n",
        "def features_for_row(a: str, b: str):\n",
        "    m_a = extract_pairs(a)\n",
        "    m_b = extract_pairs(b)\n",
        "    # union map prioritizing longer expansions when conflict\n",
        "    union = dict(m_a)\n",
        "    for k, v in m_b.items():\n",
        "        if k in union:\n",
        "            union[k] = v if len(v) > len(union[k]) else union[k]\n",
        "        else:\n",
        "            union[k] = v\n",
        "    a_exp = expand_text(a, union)\n",
        "    b_exp = expand_text(b, union)\n",
        "    ta = set(tokenize_simple(a))\n",
        "    tb = set(tokenize_simple(b))\n",
        "    tae = set(tokenize_simple(a_exp))\n",
        "    tbe = set(tokenize_simple(b_exp))\n",
        "    # base vs expanded overlaps\n",
        "    jac_base = jaccard(ta, tb)\n",
        "    dice_base = dice(ta, tb)\n",
        "    jac_exp = jaccard(tae, tbe)\n",
        "    dice_exp = dice(tae, tbe)\n",
        "    gain_jac = jac_exp - jac_base\n",
        "    gain_dice = dice_exp - dice_base\n",
        "    # acronym stats\n",
        "    n_acr_a = len(m_a); n_acr_b = len(m_b); n_acr_union = len(union)\n",
        "    acr_overlap = len(set(m_a.keys()) & set(m_b.keys()))\n",
        "    any_def = 1.0 if n_acr_union > 0 else 0.0\n",
        "    return {\n",
        "        'acr_any_def': float(any_def),\n",
        "        'acr_cnt_a': float(n_acr_a),\n",
        "        'acr_cnt_b': float(n_acr_b),\n",
        "        'acr_cnt_union': float(n_acr_union),\n",
        "        'acr_overlap_cnt': float(acr_overlap),\n",
        "        'acr_jaccard_base': float(jac_base),\n",
        "        'acr_dice_base': float(dice_base),\n",
        "        'acr_jaccard_exp': float(jac_exp),\n",
        "        'acr_dice_exp': float(dice_exp),\n",
        "        'acr_jaccard_gain': float(gain_jac),\n",
        "        'acr_dice_gain': float(gain_dice),\n",
        "    }\n",
        "\n",
        "def compute_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    A = df['anchor'].astype(str).tolist()\n",
        "    B = df['target'].astype(str).tolist()\n",
        "    rows = []\n",
        "    for i, (a, b) in enumerate(zip(A, B)):\n",
        "        rows.append(features_for_row(a, b))\n",
        "        if (i+1) % 5000 == 0:\n",
        "            print(f'.. {i+1} rows', flush=True)\n",
        "    out = pd.DataFrame(rows)\n",
        "    # simple non-linearities\n",
        "    out['acr_overlap_cnt_sq'] = (out['acr_overlap_cnt'] ** 2).astype(np.float32)\n",
        "    out['acr_cnt_union_log1p'] = np.log1p(out['acr_cnt_union']).astype(np.float32)\n",
        "    return out.astype('float32')\n",
        "\n",
        "tr_feats = compute_df(train)\n",
        "te_feats = compute_df(test)\n",
        "\n",
        "pd.concat([train[['id']].reset_index(drop=True), tr_feats.reset_index(drop=True)], axis=1).to_csv('oof_acronym.csv', index=False)\n",
        "pd.concat([test[['id']].reset_index(drop=True), te_feats.reset_index(drop=True)], axis=1).to_csv('acronym_test.csv', index=False)\n",
        "print('Saved oof_acronym.csv and acronym_test.csv; elapsed', round((time.time()-t0)/60,2), 'min')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 5000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 10000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 15000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 20000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 25000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 30000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_acronym.csv and acronym_test.csv; elapsed 0.01 min\n"
          ]
        }
      ]
    },
    {
      "id": "6ddee301-c3ee-40af-9140-d6da075ef642",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Soft token alignment (local alignment over tokens with JW/Stem matches). Outputs: oof_soft_align.csv, soft_align_test.csv\n",
        "import time, re, numpy as np, pandas as pd\n",
        "from rapidfuzz.distance import JaroWinkler\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Optional stemming\n",
        "try:\n",
        "    from nltk.stem import PorterStemmer\n",
        "    _stemmer = PorterStemmer()\n",
        "    def stem_token(tok: str) -> str:\n",
        "        return _stemmer.stem(tok)\n",
        "except Exception:\n",
        "    def stem_token(tok: str) -> str:\n",
        "        return tok\n",
        "\n",
        "_word_re = re.compile(r\"[a-zA-Z0-9]+(?:[-_./][a-zA-Z0-9]+)?\")\n",
        "def tokenize(text: str):\n",
        "    if not isinstance(text, str):\n",
        "        text = ''\n",
        "    text = text.lower()\n",
        "    toks = _word_re.findall(text)\n",
        "    return toks\n",
        "\n",
        "def jw_sim(a: str, b: str) -> float:\n",
        "    return float(JaroWinkler.normalized_similarity(a, b))  # 0..1\n",
        "\n",
        "def local_align_score(tokens_a, tokens_b, jw_thresh=0.90, match_exact=2.0, match_soft=1.2, mismatch=-0.5, gap=-0.7):\n",
        "    # Smith-Waterman style local alignment on token sequences with soft matches\n",
        "    na, nb = len(tokens_a), len(tokens_b)\n",
        "    if na == 0 or nb == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    stems_a = [stem_token(t) for t in tokens_a]\n",
        "    stems_b = [stem_token(t) for t in tokens_b]\n",
        "    H = np.zeros((na+1, nb+1), dtype=np.float32)\n",
        "    best = 0.0\n",
        "    matches = 0\n",
        "    for i in range(1, na+1):\n",
        "        ta = tokens_a[i-1]; sa = stems_a[i-1]\n",
        "        for j in range(1, nb+1):\n",
        "            tb = tokens_b[j-1]; sb = stems_b[j-1]\n",
        "            if ta == tb:\n",
        "                s = match_exact\n",
        "                is_match = True\n",
        "            elif sa == sb or jw_sim(ta, tb) >= jw_thresh:\n",
        "                s = match_soft\n",
        "                is_match = True\n",
        "            else:\n",
        "                s = mismatch\n",
        "                is_match = False\n",
        "            h_diag = H[i-1, j-1] + s\n",
        "            h_up = H[i-1, j] + gap\n",
        "            h_left = H[i, j-1] + gap\n",
        "            H[i, j] = max(0.0, h_diag, h_up, h_left)\n",
        "            if H[i, j] > best:\n",
        "                best = float(H[i, j])\n",
        "            if is_match and H[i, j] > 0:\n",
        "                matches += 1\n",
        "    # Normalize scores\n",
        "    norm_len = float(max(na, nb))\n",
        "    norm_best = best / (match_exact * norm_len + 1e-6)\n",
        "    match_ratio = matches / norm_len\n",
        "    return float(best), float(norm_best), float(match_ratio)\n",
        "\n",
        "def compute_soft_align(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    A = df['anchor'].astype(str).tolist()\n",
        "    B = df['target'].astype(str).tolist()\n",
        "    n = len(df)\n",
        "    raw = np.zeros(n, dtype=np.float32)\n",
        "    norm = np.zeros(n, dtype=np.float32)\n",
        "    mrat = np.zeros(n, dtype=np.float32)\n",
        "    for i, (a, b) in enumerate(zip(A, B)):\n",
        "        ta = tokenize(a); tb = tokenize(b)\n",
        "        r, z, mr = local_align_score(ta, tb)\n",
        "        raw[i] = r; norm[i] = z; mrat[i] = mr\n",
        "        if (i+1) % 5000 == 0:\n",
        "            print(f'.. {i+1} rows', flush=True)\n",
        "    out = pd.DataFrame({\n",
        "        'soft_align_raw': raw,\n",
        "        'soft_align_norm': norm,\n",
        "        'soft_align_match_ratio': mrat,\n",
        "    })\n",
        "    # simple non-linearities\n",
        "    out['soft_align_norm_sq'] = out['soft_align_norm'] ** 2\n",
        "    out['soft_align_raw_log1p'] = np.log1p(np.maximum(out['soft_align_raw'].values, 0.0)).astype(np.float32)\n",
        "    return out.astype('float32')\n",
        "\n",
        "tr_feats = compute_soft_align(train)\n",
        "te_feats = compute_soft_align(test)\n",
        "\n",
        "pd.concat([train[['id']].reset_index(drop=True), tr_feats.reset_index(drop=True)], axis=1).to_csv('oof_soft_align.csv', index=False)\n",
        "pd.concat([test[['id']].reset_index(drop=True), te_feats.reset_index(drop=True)], axis=1).to_csv('soft_align_test.csv', index=False)\n",
        "print('Saved oof_soft_align.csv and soft_align_test.csv; elapsed', round((time.time()-t0)/60,2), 'min')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 5000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 10000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 15000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 20000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 25000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 30000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_soft_align.csv and soft_align_test.csv; elapsed 0.02 min\n"
          ]
        }
      ]
    },
    {
      "id": "c4b2edd3-b4d5-492e-b124-a570573c73ed",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Unicode/confusables normalization + normalized lexical overlaps\n",
        "# Outputs: oof_norm_text.csv, norm_text_test.csv\n",
        "import re, unicodedata, time, numpy as np, pandas as pd\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# -------- Normalization utilities (NFKC + confusables + split number+unit + de-hyphen) --------\n",
        "REPLACEMENTS = {\n",
        "    '\\u00b5': 'u',   # micro sign\n",
        "    '\\u03bc': 'u',   # Greek mu\n",
        "    '\\u03a9': 'ohm', # Omega\n",
        "    '\\u2126': 'ohm', # Ohm symbol\n",
        "    '\\u00b0': 'deg', # degree symbol\n",
        "}\n",
        "\n",
        "NUM_UNIT_ATTACH = re.compile(r'(?i)(\\d+(?:[\\./]\\d+)?)([a-zA-Z%][a-zA-Z%/]*)')\n",
        "\n",
        "def nfkc(s: str) -> str:\n",
        "    return unicodedata.normalize('NFKC', s)\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = nfkc(str(s))\n",
        "    for k, v in REPLACEMENTS.items():\n",
        "        s = s.replace(k, v)\n",
        "    # unify hyphen/underscore to space, collapse whitespace\n",
        "    s = s.replace('-', ' ').replace('_', ' ')\n",
        "    # split attached number+unit 10nm -> '10 nm'\n",
        "    s = NUM_UNIT_ATTACH.sub(r'\\1 \\2', s)\n",
        "    # lowercase\n",
        "    s = s.lower()\n",
        "    # collapse multiple spaces\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "# Optional stemming\n",
        "try:\n",
        "    from nltk.stem import PorterStemmer\n",
        "    _stemmer = PorterStemmer()\n",
        "    def stem_token(tok: str) -> str:\n",
        "        return _stemmer.stem(tok)\n",
        "except Exception:\n",
        "    def stem_token(tok: str) -> str:\n",
        "        return tok\n",
        "\n",
        "_word_re = re.compile(r\"[a-z0-9]+(?:[./][a-z0-9]+)?\")\n",
        "def tokenize_stems(text: str):\n",
        "    if not isinstance(text, str):\n",
        "        text = ''\n",
        "    toks = _word_re.findall(text)\n",
        "    return [stem_token(t) for t in toks if t]\n",
        "\n",
        "def shingles(s: str, k: int):\n",
        "    if k <= 0:\n",
        "        return set()\n",
        "    if len(s) < k:\n",
        "        return {s} if s else set()\n",
        "    return {s[i:i+k] for i in range(len(s)-k+1)}\n",
        "\n",
        "def jaccard(a: set, b: set) -> float:\n",
        "    if not a and not b: return 0.0\n",
        "    return len(a & b) / (len(a | b) + 1e-12)\n",
        "\n",
        "def dice(a: set, b: set) -> float:\n",
        "    if not a and not b: return 0.0\n",
        "    return 2.0 * len(a & b) / (len(a) + len(b) + 1e-12)\n",
        "\n",
        "def compute_norm_feats(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    A = df['anchor'].astype(str).tolist()\n",
        "    B = df['target'].astype(str).tolist()\n",
        "    n = len(df)\n",
        "    tok_jac = np.zeros(n, dtype=np.float32)\n",
        "    tok_dice = np.zeros(n, dtype=np.float32)\n",
        "    c3_jac = np.zeros(n, dtype=np.float32)\n",
        "    c4_jac = np.zeros(n, dtype=np.float32)\n",
        "    c5_jac = np.zeros(n, dtype=np.float32)\n",
        "    for i, (a, b) in enumerate(zip(A, B)):\n",
        "        na = normalize_text(a); nb = normalize_text(b)\n",
        "        ta = set(tokenize_stems(na)); tb = set(tokenize_stems(nb))\n",
        "        tok_jac[i] = jaccard(ta, tb)\n",
        "        tok_dice[i] = dice(ta, tb)\n",
        "        a3 = shingles(na, 3); b3 = shingles(nb, 3)\n",
        "        a4 = shingles(na, 4); b4 = shingles(nb, 4)\n",
        "        a5 = shingles(na, 5); b5 = shingles(nb, 5)\n",
        "        c3_jac[i] = jaccard(a3, b3)\n",
        "        c4_jac[i] = jaccard(a4, b4)\n",
        "        c5_jac[i] = jaccard(a5, b5)\n",
        "        if (i+1) % 5000 == 0:\n",
        "            print(f'.. {i+1} rows', flush=True)\n",
        "    out = pd.DataFrame({\n",
        "        'norm_tok_jaccard': tok_jac,\n",
        "        'norm_tok_dice': tok_dice,\n",
        "        'norm_char3_jac': c3_jac,\n",
        "        'norm_char4_jac': c4_jac,\n",
        "        'norm_char5_jac': c5_jac,\n",
        "    })\n",
        "    # simple non-linearities\n",
        "    out['norm_tok_jaccard_sq'] = out['norm_tok_jaccard'] ** 2\n",
        "    out['norm_tok_dice_sq'] = out['norm_tok_dice'] ** 2\n",
        "    return out.astype('float32')\n",
        "\n",
        "tr_feats = compute_norm_feats(train)\n",
        "te_feats = compute_norm_feats(test)\n",
        "\n",
        "pd.concat([train[['id']].reset_index(drop=True), tr_feats.reset_index(drop=True)], axis=1).to_csv('oof_norm_text.csv', index=False)\n",
        "pd.concat([test[['id']].reset_index(drop=True), te_feats.reset_index(drop=True)], axis=1).to_csv('norm_text_test.csv', index=False)\n",
        "print('Saved oof_norm_text.csv and norm_text_test.csv; elapsed', round((time.time()-t0)/60,2), 'min')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 5000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 10000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 15000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 20000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 25000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 30000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_norm_text.csv and norm_text_test.csv; elapsed 0.01 min\n"
          ]
        }
      ]
    },
    {
      "id": "ff1007cc-2ef3-4cc4-8f3f-97ddd2fd3657",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Normalized IDF-overlap (1-3 grams), fold-safe. Outputs: oof_idf_overlap_norm.csv, idf_overlap_norm_test.csv\n",
        "import time, re, math, unicodedata, numpy as np, pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds_by_id.csv')\n",
        "train = train.merge(folds, on='id', how='left', validate='one_to_one')\n",
        "train['fold'] = train['fold'].astype(int)\n",
        "\n",
        "# -------- Normalization utilities (NFKC + confusables + sub/superscripts + number-unit split + de-hyphen) --------\n",
        "REPL = {\n",
        "    '\\u00b5': 'u',   # micro sign\n",
        "    '\\u03bc': 'u',   # Greek mu\n",
        "    '\\u03a9': 'ohm', # Omega\n",
        "    '\\u2126': 'ohm', # Ohm symbol\n",
        "    '\\u00b0C': 'deg C',  # degree C\n",
        "    '\\u00b0F': 'deg F',  # degree F\n",
        "    '\\u00b0': 'deg',     # bare degree\n",
        "    '\\u00d7': 'x',       # multiplication sign\n",
        "    '\\u2032': \"'\",      # prime\n",
        "    '\\u2033': '\"',      # double prime\n",
        "}\n",
        "_SUBS = str.maketrans('\u2080\u2081\u2082\u2083\u2084\u2085\u2086\u2087\u2088\u2089', '0123456789')\n",
        "_SUPS_MAP = { '\u00b2': '2', '\u00b3': '3', '\u207a': '+', '\u207b': '-' }\n",
        "NUM_UNIT_ATTACH = re.compile(r'(?i)(\\d+(?:[\\./]\\d+)?)([a-zA-Z%][a-zA-Z%/]*)')\n",
        "\n",
        "def nfkc(s: str) -> str:\n",
        "    return unicodedata.normalize('NFKC', s)\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = nfkc(str(s))\n",
        "    for k, v in REPL.items():\n",
        "        s = s.replace(k, v)\n",
        "    for k, v in _SUPS_MAP.items():\n",
        "        s = s.replace(k, v)\n",
        "    s = s.translate(_SUBS)\n",
        "    s = s.replace('-', ' ').replace('_', ' ')\n",
        "    s = NUM_UNIT_ATTACH.sub(r'\\1 \\2', s)\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def tokenize_words(s: str):\n",
        "    # after normalization, allow a-z0-9 and ./ inside tokens\n",
        "    return re.findall(r\"[a-z0-9]+(?:[./][a-z0-9]+)?\", normalize_text(s))\n",
        "\n",
        "def filter_tokens(tokens):\n",
        "    # no stopword removal here; keep technical tokens; drop empty/single char\n",
        "    return [t for t in tokens if len(t) > 1]\n",
        "\n",
        "def gen_ngrams(tokens, n):\n",
        "    if n == 1:\n",
        "        return tokens\n",
        "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "\n",
        "def build_df(corpus_docs):\n",
        "    df = Counter()\n",
        "    for terms in corpus_docs:\n",
        "        if terms:\n",
        "            df.update(set(terms))\n",
        "    return df\n",
        "\n",
        "def idf_from_df(df_counter, N):\n",
        "    idf = {}\n",
        "    for t, df in df_counter.items():\n",
        "        idf[t] = math.log((N - df + 0.5)/(df + 0.5) + 1.0)\n",
        "    return idf\n",
        "\n",
        "def weighted_overlap_metrics(A_terms, B_terms, idf):\n",
        "    A_set, B_set = set(A_terms), set(B_terms)\n",
        "    if not A_set and not B_set:\n",
        "        return (0.0, 0.0, 0.0, 0.0, 0.0, 0, 0)\n",
        "    inter = A_set & B_set\n",
        "    union = A_set | B_set\n",
        "    w_inter = sum(idf.get(t, 0.0) for t in inter)\n",
        "    w_A = sum(idf.get(t, 0.0) for t in A_set) + 1e-12\n",
        "    w_B = sum(idf.get(t, 0.0) for t in B_set) + 1e-12\n",
        "    w_union = sum(idf.get(t, 0.0) for t in union) + 1e-12\n",
        "    prec = w_inter / w_A\n",
        "    rec = w_inter / w_B\n",
        "    f1 = 0.0 if (prec+rec) == 0 else (2*prec*rec)/(prec+rec)\n",
        "    jac = w_inter / w_union\n",
        "    return (prec, rec, f1, jac, w_inter, len(A_set), len(B_set))\n",
        "\n",
        "# Pre-tokenize normalized 1-3 grams per row\n",
        "def make_ng123(tokens_list):\n",
        "    uni = [gen_ngrams(t,1) for t in tokens_list]\n",
        "    bi  = [gen_ngrams(t,2) for t in tokens_list]\n",
        "    tri = [gen_ngrams(t,3) for t in tokens_list]\n",
        "    return uni, bi, tri\n",
        "\n",
        "A_tr_tok = [filter_tokens(tokenize_words(x)) for x in train['anchor'].astype(str).tolist()]\n",
        "B_tr_tok = [filter_tokens(tokenize_words(x)) for x in train['target'].astype(str).tolist()]\n",
        "A_te_tok = [filter_tokens(tokenize_words(x)) for x in test['anchor'].astype(str).tolist()]\n",
        "B_te_tok = [filter_tokens(tokenize_words(x)) for x in test['target'].astype(str).tolist()]\n",
        "\n",
        "A_tr_u, A_tr_b, A_tr_t = make_ng123(A_tr_tok)\n",
        "B_tr_u, B_tr_b, B_tr_t = make_ng123(B_tr_tok)\n",
        "A_te_u, A_te_b, A_te_t = make_ng123(A_te_tok)\n",
        "B_te_u, B_te_b, B_te_t = make_ng123(B_te_tok)\n",
        "\n",
        "fold_arr = train['fold'].values.astype(int)\n",
        "n_tr = len(train); n_te = len(test)\n",
        "\n",
        "cols = [\n",
        "    'nidf1_prec','nidf1_rec','nidf1_f1','nidf1_jac','nidf1_wi',\n",
        "    'nidf2_prec','nidf2_rec','nidf2_f1','nidf2_jac','nidf2_wi',\n",
        "    'nidf3_prec','nidf3_rec','nidf3_f1','nidf3_jac','nidf3_wi'\n",
        "]\n",
        "oof = np.zeros((n_tr, len(cols)), dtype=np.float32)\n",
        "te_fold_preds = []\n",
        "\n",
        "for f in sorted(np.unique(fold_arr)):\n",
        "    f0 = time.time()\n",
        "    tr_idx = np.where(fold_arr != f)[0]\n",
        "    va_idx = np.where(fold_arr == f)[0]\n",
        "    # Build corpus IDF on train-only for each n-gram level\n",
        "    corpus_uni_docs = [set(A_tr_u[i]) for i in tr_idx] + [set(B_tr_u[i]) for i in tr_idx]\n",
        "    corpus_bi_docs  = [set(A_tr_b[i]) for i in tr_idx] + [set(B_tr_b[i]) for i in tr_idx]\n",
        "    corpus_tri_docs = [set(A_tr_t[i]) for i in tr_idx] + [set(B_tr_t[i]) for i in tr_idx]\n",
        "    idf_uni = idf_from_df(build_df(corpus_uni_docs), len(corpus_uni_docs))\n",
        "    idf_bi  = idf_from_df(build_df(corpus_bi_docs),  len(corpus_bi_docs))\n",
        "    idf_tri = idf_from_df(build_df(corpus_tri_docs), len(corpus_tri_docs))\n",
        "\n",
        "    # OOF for this fold\n",
        "    for i in va_idx:\n",
        "        p1, r1, f1, j1, wi1, _, _ = weighted_overlap_metrics(A_tr_u[i], B_tr_u[i], idf_uni)\n",
        "        p2, r2, f2, j2, wi2, _, _ = weighted_overlap_metrics(A_tr_b[i], B_tr_b[i], idf_bi)\n",
        "        p3, r3, f3, j3, wi3, _, _ = weighted_overlap_metrics(A_tr_t[i], B_tr_t[i], idf_tri)\n",
        "        oof[i, :] = [p1, r1, f1, j1, wi1, p2, r2, f2, j2, wi2, p3, r3, f3, j3, wi3]\n",
        "\n",
        "    # Test for this fold\n",
        "    te_mat = np.zeros((n_te, len(cols)), dtype=np.float32)\n",
        "    for j in range(n_te):\n",
        "        p1, r1, f1, j1, wi1, _, _ = weighted_overlap_metrics(A_te_u[j], B_te_u[j], idf_uni)\n",
        "        p2, r2, f2, j2, wi2, _, _ = weighted_overlap_metrics(A_te_b[j], B_te_b[j], idf_bi)\n",
        "        p3, r3, f3, j3, wi3, _, _ = weighted_overlap_metrics(A_te_t[j], B_te_t[j], idf_tri)\n",
        "        te_mat[j, :] = [p1, r1, f1, j1, wi1, p2, r2, f2, j2, wi2, p3, r3, f3, j3, wi3]\n",
        "    te_fold_preds.append(te_mat)\n",
        "    print(f'Normalized IDF-overlap fold {int(f)} done in {time.time()-f0:.1f}s', flush=True)\n",
        "\n",
        "# Aggregate test across folds (mean)\n",
        "te_mean = np.mean(np.stack(te_fold_preds, axis=0), axis=0).astype(np.float32)\n",
        "\n",
        "# Save\n",
        "oof_df = pd.DataFrame({'id': train['id']})\n",
        "for k, c in enumerate(cols):\n",
        "    oof_df[c] = oof[:, k]\n",
        "oof_df.to_csv('oof_idf_overlap_norm.csv', index=False)\n",
        "\n",
        "te_df = pd.DataFrame({'id': test['id']})\n",
        "for k, c in enumerate(cols):\n",
        "    te_df[c] = te_mean[:, k]\n",
        "te_df.to_csv('idf_overlap_norm_test.csv', index=False)\n",
        "\n",
        "print('Saved oof_idf_overlap_norm.csv and idf_overlap_norm_test.csv; elapsed', round((time.time()-t0)/60,2), 'min', flush=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized IDF-overlap fold 0 done in 0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized IDF-overlap fold 1 done in 0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized IDF-overlap fold 2 done in 0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized IDF-overlap fold 3 done in 0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized IDF-overlap fold 4 done in 0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_idf_overlap_norm.csv and idf_overlap_norm_test.csv; elapsed 0.04 min\n"
          ]
        }
      ]
    },
    {
      "id": "7c3a2834-5e5d-43b0-8658-e426e6941394",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fold-safe target encoding with shrinkage (m=10): TE_anchor, TE_anchor_ctx3, TE_target_ctx3 (+ counts)\n",
        "# Outputs: oof_te.csv, te_test.csv\n",
        "import time, numpy as np, pandas as pd\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds_by_id.csv')\n",
        "train = train.merge(folds, on='id', how='left', validate='one_to_one')\n",
        "train['fold'] = train['fold'].astype(int)\n",
        "NUM_FOLDS = int(train['fold'].max()) + 1\n",
        "\n",
        "# Keys\n",
        "anch = train['anchor'].astype(str).values\n",
        "targ = train['target'].astype(str).values\n",
        "ctx3_tr = train['context'].astype(str).str[:3].values\n",
        "ctx3_te = test['context'].astype(str).str[:3].values\n",
        "anch_te = test['anchor'].astype(str).values\n",
        "targ_te = test['target'].astype(str).values\n",
        "y = train['score'].astype(np.float32).values\n",
        "\n",
        "# Outputs\n",
        "oof_te_anchor = np.zeros(len(train), dtype=np.float32)\n",
        "oof_te_anchor_ctx3 = np.zeros(len(train), dtype=np.float32)\n",
        "oof_te_target_ctx3 = np.zeros(len(train), dtype=np.float32)\n",
        "oof_cnt_anchor = np.zeros(len(train), dtype=np.float32)\n",
        "oof_cnt_anchor_ctx3 = np.zeros(len(train), dtype=np.float32)\n",
        "oof_cnt_target_ctx3 = np.zeros(len(train), dtype=np.float32)\n",
        "\n",
        "te_te_anchor_acc = np.zeros(len(test), dtype=np.float64)\n",
        "te_te_anchor_ctx3_acc = np.zeros(len(test), dtype=np.float64)\n",
        "te_te_target_ctx3_acc = np.zeros(len(test), dtype=np.float64)\n",
        "te_cnt_anchor_acc = np.zeros(len(test), dtype=np.float64)\n",
        "te_cnt_anchor_ctx3_acc = np.zeros(len(test), dtype=np.float64)\n",
        "te_cnt_target_ctx3_acc = np.zeros(len(test), dtype=np.float64)\n",
        "\n",
        "m_anchor = 10.0\n",
        "m_ctx = 10.0\n",
        "\n",
        "for f in range(NUM_FOLDS):\n",
        "    f0 = time.time()\n",
        "    tr_idx = np.where(train['fold'].values != f)[0]\n",
        "    va_idx = np.where(train['fold'].values == f)[0]\n",
        "    # Global mean\n",
        "    gmean = float(y[tr_idx].mean()) if len(tr_idx) else float(y.mean())\n",
        "    # Group stats on train-only\n",
        "    df_tr = pd.DataFrame({\n",
        "        'anchor': anch[tr_idx],\n",
        "        'target': targ[tr_idx],\n",
        "        'ctx3': ctx3_tr[tr_idx],\n",
        "        'y': y[tr_idx],\n",
        "    })\n",
        "    # anchor only\n",
        "    grp_a = df_tr.groupby('anchor')['y']\n",
        "    mean_a = grp_a.mean().to_dict()\n",
        "    cnt_a = grp_a.size().to_dict()\n",
        "    # anchor+ctx3\n",
        "    df_tr['a_c'] = df_tr['anchor'] + '||' + df_tr['ctx3']\n",
        "    grp_ac = df_tr.groupby('a_c')['y']\n",
        "    mean_ac = grp_ac.mean().to_dict()\n",
        "    cnt_ac = grp_ac.size().to_dict()\n",
        "    # target+ctx3\n",
        "    df_tr['t_c'] = df_tr['target'] + '||' + df_tr['ctx3']\n",
        "    grp_tc = df_tr.groupby('t_c')['y']\n",
        "    mean_tc = grp_tc.mean().to_dict()\n",
        "    cnt_tc = grp_tc.size().to_dict()\n",
        "\n",
        "    # Assign to val with shrinkage enc = (sum + m*gmean)/(cnt + m)\n",
        "    for idx in va_idx:\n",
        "        a = anch[idx]\n",
        "        c3 = ctx3_tr[idx]\n",
        "        t = targ[idx]\n",
        "        key_ac = a + '||' + c3\n",
        "        key_tc = t + '||' + c3\n",
        "        ca = float(cnt_a.get(a, 0.0)); ma = float(mean_a.get(a, gmean))\n",
        "        cac = float(cnt_ac.get(key_ac, 0.0)); mac = float(mean_ac.get(key_ac, gmean))\n",
        "        ctc = float(cnt_tc.get(key_tc, 0.0)); mtc = float(mean_tc.get(key_tc, gmean))\n",
        "        oof_cnt_anchor[idx] = ca\n",
        "        oof_cnt_anchor_ctx3[idx] = cac\n",
        "        oof_cnt_target_ctx3[idx] = ctc\n",
        "        # back out sum = mean * cnt\n",
        "        enc_a = ((ma * ca) + m_anchor * gmean) / (ca + m_anchor) if (ca + m_anchor) > 0 else gmean\n",
        "        enc_ac = ((mac * cac) + m_ctx * gmean) / (cac + m_ctx) if (cac + m_ctx) > 0 else gmean\n",
        "        enc_tc = ((mtc * ctc) + m_ctx * gmean) / (ctc + m_ctx) if (ctc + m_ctx) > 0 else gmean\n",
        "        oof_te_anchor[idx] = enc_a\n",
        "        oof_te_anchor_ctx3[idx] = enc_ac\n",
        "        oof_te_target_ctx3[idx] = enc_tc\n",
        "\n",
        "    # Test encodings using same train-only stats; accumulate to average across folds\n",
        "    gmean_te = gmean\n",
        "    # anchor only\n",
        "    ca_te = np.array([float(cnt_a.get(a, 0.0)) for a in anch_te], dtype=np.float64)\n",
        "    ma_te = np.array([float(mean_a.get(a, gmean_te)) for a in anch_te], dtype=np.float64)\n",
        "    te_cnt_anchor_acc += ca_te\n",
        "    te_te_anchor_acc += (((ma_te * ca_te) + m_anchor * gmean_te) / (ca_te + m_anchor + 1e-12))\n",
        "    # anchor+ctx3\n",
        "    keys_ac_te = (pd.Series(anch_te) + '||' + pd.Series(ctx3_te)).tolist()\n",
        "    cac_te = np.array([float(cnt_ac.get(k, 0.0)) for k in keys_ac_te], dtype=np.float64)\n",
        "    mac_te = np.array([float(mean_ac.get(k, gmean_te)) for k in keys_ac_te], dtype=np.float64)\n",
        "    te_cnt_anchor_ctx3_acc += cac_te\n",
        "    te_te_anchor_ctx3_acc += (((mac_te * cac_te) + m_ctx * gmean_te) / (cac_te + m_ctx + 1e-12))\n",
        "    # target+ctx3\n",
        "    keys_tc_te = (pd.Series(targ_te) + '||' + pd.Series(ctx3_te)).tolist()\n",
        "    ctc_te = np.array([float(cnt_tc.get(k, 0.0)) for k in keys_tc_te], dtype=np.float64)\n",
        "    mtc_te = np.array([float(mean_tc.get(k, gmean_te)) for k in keys_tc_te], dtype=np.float64)\n",
        "    te_cnt_target_ctx3_acc += ctc_te\n",
        "    te_te_target_ctx3_acc += (((mtc_te * ctc_te) + m_ctx * gmean_te) / (ctc_te + m_ctx + 1e-12))\n",
        "\n",
        "    print(f'TE fold {f} done in {time.time()-f0:.1f}s', flush=True)\n",
        "\n",
        "# Average test across folds\n",
        "te_anchor = (te_te_anchor_acc / NUM_FOLDS).astype(np.float32)\n",
        "te_anchor_ctx3 = (te_te_anchor_ctx3_acc / NUM_FOLDS).astype(np.float32)\n",
        "te_target_ctx3 = (te_te_target_ctx3_acc / NUM_FOLDS).astype(np.float32)\n",
        "te_cnt_a = (te_cnt_anchor_acc / NUM_FOLDS).astype(np.float32)\n",
        "te_cnt_ac = (te_cnt_anchor_ctx3_acc / NUM_FOLDS).astype(np.float32)\n",
        "te_cnt_tc = (te_cnt_target_ctx3_acc / NUM_FOLDS).astype(np.float32)\n",
        "\n",
        "# Save\n",
        "oof_df = pd.DataFrame({\n",
        "    'id': train['id'],\n",
        "    'te_anchor': oof_te_anchor,\n",
        "    'te_anchor_ctx3': oof_te_anchor_ctx3,\n",
        "    'te_target_ctx3': oof_te_target_ctx3,\n",
        "    'te_cnt_anchor': oof_cnt_anchor,\n",
        "    'te_cnt_anchor_ctx3': oof_cnt_anchor_ctx3,\n",
        "    'te_cnt_target_ctx3': oof_cnt_target_ctx3,\n",
        "})\n",
        "oof_df.to_csv('oof_te.csv', index=False)\n",
        "\n",
        "te_df = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'te_anchor': te_anchor,\n",
        "    'te_anchor_ctx3': te_anchor_ctx3,\n",
        "    'te_target_ctx3': te_target_ctx3,\n",
        "    'te_cnt_anchor': te_cnt_a,\n",
        "    'te_cnt_anchor_ctx3': te_cnt_ac,\n",
        "    'te_cnt_target_ctx3': te_cnt_tc,\n",
        "})\n",
        "te_df.to_csv('te_test.csv', index=False)\n",
        "print('Saved oof_te.csv and te_test.csv; elapsed', round((time.time()-t0)/60,2), 'min', flush=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TE fold 0 done in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TE fold 1 done in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TE fold 2 done in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TE fold 3 done in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TE fold 4 done in 0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_te.csv and te_test.csv; elapsed 0.01 min\n"
          ]
        }
      ]
    },
    {
      "id": "51e370f6-dca6-4dc7-ae2c-3f5d75d67feb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Monge\u2013Elkan over normalized tokens with Jaro\u2013Winkler base\n",
        "# Outputs: oof_monge.csv, monge_test.csv\n",
        "import re, unicodedata, time, numpy as np, pandas as pd\n",
        "from rapidfuzz.distance import JaroWinkler\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Normalization (same spirit as prior sprints)\n",
        "REPL = {\n",
        "    '\\u00b5': 'u',\n",
        "    '\\u03bc': 'u',\n",
        "    '\\u03a9': 'ohm',\n",
        "    '\\u2126': 'ohm',\n",
        "    '\\u00b0C': 'deg C',\n",
        "    '\\u00b0F': 'deg F',\n",
        "    '\\u00b0': 'deg',\n",
        "    '\\u00d7': 'x',\n",
        "    '\\u2032': \"'\",\n",
        "    '\\u2033': '\"',\n",
        "}\n",
        "_SUBS = str.maketrans('\u2080\u2081\u2082\u2083\u2084\u2085\u2086\u2087\u2088\u2089', '0123456789')\n",
        "_SUPS_MAP = { '\u00b2': '2', '\u00b3': '3', '\u207a': '+', '\u207b': '-' }\n",
        "NUM_UNIT_ATTACH = re.compile(r'(?i)(\\d+(?:[\\./]\\d+)?)([a-zA-Z%][a-zA-Z%/]*)')\n",
        "\n",
        "def nfkc(s: str) -> str:\n",
        "    return unicodedata.normalize('NFKC', s)\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = nfkc(str(s))\n",
        "    for k, v in REPL.items():\n",
        "        s = s.replace(k, v)\n",
        "    for k, v in _SUPS_MAP.items():\n",
        "        s = s.replace(k, v)\n",
        "    s = s.translate(_SUBS)\n",
        "    s = s.replace('-', ' ').replace('_', ' ')\n",
        "    s = NUM_UNIT_ATTACH.sub(r'\\1 \\2', s)\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "_word_re = re.compile(r\"[a-z0-9]+(?:[./][a-z0-9]+)?\")\n",
        "def tokenize(text: str):\n",
        "    if not isinstance(text, str):\n",
        "        text = ''\n",
        "    text = normalize_text(text)\n",
        "    return _word_re.findall(text)\n",
        "\n",
        "def jw_sim(a: str, b: str) -> float:\n",
        "    return float(JaroWinkler.normalized_similarity(a, b))  # 0..1\n",
        "\n",
        "def monge_elkan(A: list[str], B: list[str], jw_thresh: float = 0.0):\n",
        "    # Asymmetric Monge\u2013Elkan: for each token in A, take max JW to any token in B, then mean\n",
        "    if not A:\n",
        "        return 0.0\n",
        "    if not B:\n",
        "        return 0.0\n",
        "    m = 0.0\n",
        "    for ta in A:\n",
        "        best = 0.0\n",
        "        for tb in B:\n",
        "            s = jw_sim(ta, tb)\n",
        "            if s > best:\n",
        "                best = s\n",
        "        if best >= jw_thresh:\n",
        "            m += best\n",
        "    return float(m / max(len(A), 1))\n",
        "\n",
        "def compute_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    A = df['anchor'].astype(str).tolist()\n",
        "    B = df['target'].astype(str).tolist()\n",
        "    n = len(df)\n",
        "    me_ab = np.zeros(n, dtype=np.float32)\n",
        "    me_ba = np.zeros(n, dtype=np.float32)\n",
        "    me_sym_mean = np.zeros(n, dtype=np.float32)\n",
        "    me_sym_max = np.zeros(n, dtype=np.float32)\n",
        "    me_sym_min = np.zeros(n, dtype=np.float32)\n",
        "    for i, (a, b) in enumerate(zip(A, B)):\n",
        "        ta = tokenize(a); tb = tokenize(b)\n",
        "        s_ab = monge_elkan(ta, tb, jw_thresh=0.0)\n",
        "        s_ba = monge_elkan(tb, ta, jw_thresh=0.0)\n",
        "        me_ab[i] = s_ab\n",
        "        me_ba[i] = s_ba\n",
        "        me_sym_mean[i] = 0.5 * (s_ab + s_ba)\n",
        "        me_sym_max[i] = max(s_ab, s_ba)\n",
        "        me_sym_min[i] = min(s_ab, s_ba)\n",
        "        if (i+1) % 5000 == 0:\n",
        "            print(f'.. {i+1} rows', flush=True)\n",
        "    out = pd.DataFrame({\n",
        "        'me_jw_ab': me_ab,\n",
        "        'me_jw_ba': me_ba,\n",
        "        'me_jw_mean': me_sym_mean,\n",
        "        'me_jw_max': me_sym_max,\n",
        "        'me_jw_min': me_sym_min,\n",
        "    })\n",
        "    # simple non-linearities\n",
        "    out['me_jw_mean_sq'] = out['me_jw_mean'] ** 2\n",
        "    return out.astype('float32')\n",
        "\n",
        "tr_feats = compute_df(train)\n",
        "te_feats = compute_df(test)\n",
        "\n",
        "pd.concat([train[['id']].reset_index(drop=True), tr_feats.reset_index(drop=True)], axis=1).to_csv('oof_monge.csv', index=False)\n",
        "pd.concat([test[['id']].reset_index(drop=True), te_feats.reset_index(drop=True)], axis=1).to_csv('monge_test.csv', index=False)\n",
        "print('Saved oof_monge.csv and monge_test.csv; elapsed', round((time.time()-t0)/60,2), 'min', flush=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 5000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 10000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 15000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 20000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 25000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. 30000 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_monge.csv and monge_test.csv; elapsed 0.01 min\n"
          ]
        }
      ]
    },
    {
      "id": "52f7a0c6-a293-4d92-9777-246712d2be09",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Char 3-gram TF-IDF cosine similarity (fold-safe). Outputs: oof_char3_tfidf_cos.csv, char3_tfidf_cos_test.csv\n",
        "import time, re, unicodedata, numpy as np, pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds_by_id.csv')\n",
        "train = train.merge(folds, on='id', how='left', validate='one_to_one')\n",
        "train['fold'] = train['fold'].astype(int)\n",
        "NUM_FOLDS = int(train['fold'].max()) + 1\n",
        "\n",
        "def nfkc(s: str) -> str:\n",
        "    return unicodedata.normalize('NFKC', s)\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = nfkc(str(s)).lower()\n",
        "    s = s.replace('-', ' ').replace('_', ' ')\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "A_tr = train['anchor'].astype(str).apply(normalize_text).tolist()\n",
        "B_tr = train['target'].astype(str).apply(normalize_text).tolist()\n",
        "A_te = test['anchor'].astype(str).apply(normalize_text).tolist()\n",
        "B_te = test['target'].astype(str).apply(normalize_text).tolist()\n",
        "\n",
        "n_tr = len(train); n_te = len(test)\n",
        "oof = np.zeros(n_tr, dtype=np.float32)\n",
        "te_acc = np.zeros(n_te, dtype=np.float64)\n",
        "\n",
        "def rowwise_cosine(X, Y):\n",
        "    # X, Y are sparse matrices with same shape and aligned rows\n",
        "    # cosine = (x\u00b7y) / (||x|| ||y||)\n",
        "    num = (X.multiply(Y)).sum(axis=1).A1.astype(np.float64)\n",
        "    x2 = X.multiply(X).sum(axis=1).A1.astype(np.float64)\n",
        "    y2 = Y.multiply(Y).sum(axis=1).A1.astype(np.float64)\n",
        "    den = np.sqrt(np.maximum(x2, 1e-12)) * np.sqrt(np.maximum(y2, 1e-12))\n",
        "    c = num / np.maximum(den, 1e-12)\n",
        "    # Clamp to [0,1] numeric safety\n",
        "    c = np.clip(c, 0.0, 1.0)\n",
        "    return c.astype(np.float32)\n",
        "\n",
        "for f in range(NUM_FOLDS):\n",
        "    f0 = time.time()\n",
        "    tr_idx = np.where(train['fold'].values != f)[0]\n",
        "    va_idx = np.where(train['fold'].values == f)[0]\n",
        "    vec = TfidfVectorizer(analyzer='char', ngram_range=(3,3), min_df=3)\n",
        "    corpus = [A_tr[i] for i in tr_idx] + [B_tr[i] for i in tr_idx]\n",
        "    V = vec.fit_transform(corpus)\n",
        "    Va = vec.transform([A_tr[i] for i in va_idx])\n",
        "    Vb = vec.transform([B_tr[i] for i in va_idx])\n",
        "    oof[va_idx] = rowwise_cosine(Va, Vb)\n",
        "    # Test for this fold\n",
        "    Ta = vec.transform(A_te)\n",
        "    Tb = vec.transform(B_te)\n",
        "    te_acc += rowwise_cosine(Ta, Tb).astype(np.float64)\n",
        "    print(f'char3 tfidf fold {f} done in {time.time()-f0:.1f}s', flush=True)\n",
        "\n",
        "te_mean = (te_acc / NUM_FOLDS).astype(np.float32)\n",
        "\n",
        "pd.DataFrame({'id': train['id'], 'char3_tfidf_cos': oof}).to_csv('oof_char3_tfidf_cos.csv', index=False)\n",
        "pd.DataFrame({'id': test['id'], 'char3_tfidf_cos': te_mean}).to_csv('char3_tfidf_cos_test.csv', index=False)\n",
        "print('Saved oof_char3_tfidf_cos.csv and char3_tfidf_cos_test.csv; elapsed', round((time.time()-t0)/60,2), 'min', flush=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char3 tfidf fold 0 done in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char3 tfidf fold 1 done in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char3 tfidf fold 2 done in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char3 tfidf fold 3 done in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char3 tfidf fold 4 done in 0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_char3_tfidf_cos.csv and char3_tfidf_cos_test.csv; elapsed 0.03 min\n"
          ]
        }
      ]
    },
    {
      "id": "8ca7387c-99a0-4209-bdee-ab4c79e5d6ee",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fold-safe transforms (iso, z, rank) for embedding single-column OOFs: mpnet_st, e5_asym, bge\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from pathlib import Path\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds_by_id.csv')\n",
        "train = train.merge(folds, on='id', how='left', validate='one_to_one')\n",
        "fold_arr = train['fold'].values.astype(int)\n",
        "NUM_FOLDS = int(train['fold'].max()) + 1\n",
        "y = train['score'].astype(np.float32).values\n",
        "\n",
        "def load_single(oof_path, sub_path):\n",
        "    if (not Path(oof_path).exists()) or (not Path(sub_path).exists()):\n",
        "        return None, None\n",
        "    oof = pd.read_csv(oof_path); sub = pd.read_csv(sub_path)\n",
        "    oof_cols = [c for c in oof.columns if c != 'id']\n",
        "    sub_cols = [c for c in sub.columns if c != 'id']\n",
        "    if not oof_cols or not sub_cols:\n",
        "        return None, None\n",
        "    oc = oof_cols[-1]; sc = sub_cols[-1]\n",
        "    o = train[['id']].merge(oof[['id', oc]], on='id', how='left')[oc].astype(np.float32).values\n",
        "    t = test[['id']].merge(sub[['id', sc]], on='id', how='left')[sc].astype(np.float32).values\n",
        "    return o, t\n",
        "\n",
        "cands = [\n",
        "    ('mpnet', 'oof_mpnet_st.csv', 'submission_mpnet_st.csv'),\n",
        "    ('e5',    'oof_e5_asym.csv',   'submission_e5_asym.csv'),\n",
        "    ('bge',   'oof_bge.csv',       'submission_bge.csv'),\n",
        "]\n",
        "\n",
        "out_oof = {'id': train['id'].values}\n",
        "out_te  = {'id': test['id'].values}\n",
        "\n",
        "for tag, oofp, subp in cands:\n",
        "    o, t = load_single(oofp, subp)\n",
        "    if o is None:\n",
        "        print(f'Skip {tag}: missing {oofp} or {subp}', flush=True)\n",
        "        continue\n",
        "    print(f'Embedding transforms for {tag}: source {oofp}, {subp}', flush=True)\n",
        "    o_iso  = np.zeros(len(train), dtype=np.float32)\n",
        "    o_z    = np.zeros(len(train), dtype=np.float32)\n",
        "    o_rank = np.zeros(len(train), dtype=np.float32)\n",
        "    t_iso_acc  = np.zeros(len(test), dtype=np.float64)\n",
        "    t_z_acc    = np.zeros(len(test), dtype=np.float64)\n",
        "    t_rank_acc = np.zeros(len(test), dtype=np.float64)\n",
        "    for f in range(NUM_FOLDS):\n",
        "        tr = fold_arr != f\n",
        "        va = fold_arr == f\n",
        "        # isotonic\n",
        "        iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "        iso.fit(o[tr], y[tr])\n",
        "        o_iso[va] = iso.transform(o[va]).astype(np.float32)\n",
        "        t_iso_acc += iso.transform(t).astype(np.float64)\n",
        "        # z-score\n",
        "        mu = float(o[tr].mean()); sd = float(o[tr].std()) or 1.0\n",
        "        o_z[va] = (o[va] - mu) / sd\n",
        "        t_z_acc += (t - mu) / sd\n",
        "        # rank\n",
        "        ref = np.sort(o[tr].astype(np.float32))\n",
        "        if ref.size > 0:\n",
        "            j_va = np.searchsorted(ref, o[va], side='right')\n",
        "            o_rank[va] = j_va / max(ref.size - 1, 1)\n",
        "            j_te = np.searchsorted(ref, t, side='right')\n",
        "            t_rank_acc += (j_te / max(ref.size - 1, 1))\n",
        "    out_oof[f'{tag}_z'] = o_z.astype(np.float32)\n",
        "    out_oof[f'{tag}_rank'] = o_rank.astype(np.float32)\n",
        "    out_oof[f'{tag}_iso'] = o_iso.astype(np.float32)\n",
        "    out_te[f'{tag}_z'] = (t_z_acc / NUM_FOLDS).astype(np.float32)\n",
        "    out_te[f'{tag}_rank'] = (t_rank_acc / NUM_FOLDS).astype(np.float32)\n",
        "    out_te[f'{tag}_iso'] = (t_iso_acc / NUM_FOLDS).astype(np.float32)\n",
        "\n",
        "pd.DataFrame(out_oof).to_csv('oof_embed_transforms.csv', index=False)\n",
        "pd.DataFrame(out_te).to_csv('embed_transforms_test.csv', index=False)\n",
        "print('Saved oof_embed_transforms.csv and embed_transforms_test.csv')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding transforms for mpnet: source oof_mpnet_st.csv, submission_mpnet_st.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding transforms for e5: source oof_e5_asym.csv, submission_e5_asym.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding transforms for bge: source oof_bge.csv, submission_bge.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_embed_transforms.csv and embed_transforms_test.csv\n"
          ]
        }
      ]
    },
    {
      "id": "3a44cf9d-f59c-4ef6-b4b9-e72473f2c0d6",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PatentSBERTa cosine + fold-safe transforms (raw, iso, z, rank)\n",
        "import numpy as np, pandas as pd, time\n",
        "from pathlib import Path\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds_by_id.csv')\n",
        "train = train.merge(folds, on='id', how='left', validate='one_to_one')\n",
        "train['fold'] = train['fold'].astype(int)\n",
        "F = int(train['fold'].max()) + 1\n",
        "y = train['score'].astype(np.float32).values\n",
        "\n",
        "MODEL = 'AI-Growth-Lab/PatentSBERTa'\n",
        "print('Loading SentenceTransformer:', MODEL, 'on CPU...', flush=True)\n",
        "st = SentenceTransformer(MODEL, device='cpu')\n",
        "\n",
        "def enc(texts, bs=128):\n",
        "    return st.encode(texts, batch_size=bs, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False).astype(np.float32)\n",
        "\n",
        "# Precompute embeddings for all rows once\n",
        "a_tr = enc(train['anchor'].astype(str).tolist())\n",
        "b_tr = enc(train['target'].astype(str).tolist())\n",
        "a_te = enc(test['anchor'].astype(str).tolist())\n",
        "b_te = enc(test['target'].astype(str).tolist())\n",
        "\n",
        "# Cosine since normalized -> dot product\n",
        "raw_tr = (a_tr * b_tr).sum(axis=1).astype(np.float32)\n",
        "raw_te = (a_te * b_te).sum(axis=1).astype(np.float32)\n",
        "\n",
        "# Fold-safe iso/z/rank\n",
        "oof_raw  = np.zeros(len(train), dtype=np.float32)\n",
        "oof_iso  = np.zeros(len(train), dtype=np.float32)\n",
        "oof_z    = np.zeros(len(train), dtype=np.float32)\n",
        "oof_rank = np.zeros(len(train), dtype=np.float32)\n",
        "te_iso_acc  = np.zeros(len(test), dtype=np.float64)\n",
        "te_z_acc    = np.zeros(len(test), dtype=np.float64)\n",
        "te_rank_acc = np.zeros(len(test), dtype=np.float64)\n",
        "\n",
        "fold_arr = train['fold'].values.astype(int)\n",
        "for f in range(F):\n",
        "    tr = fold_arr != f; va = fold_arr == f\n",
        "    # assign raw directly (no fit needed)\n",
        "    oof_raw[va] = raw_tr[va]\n",
        "    # isotonic\n",
        "    iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "    iso.fit(raw_tr[tr], y[tr])\n",
        "    oof_iso[va] = iso.transform(raw_tr[va]).astype(np.float32)\n",
        "    te_iso_acc += iso.transform(raw_te).astype(np.float64)\n",
        "    # z\n",
        "    mu = float(raw_tr[tr].mean()); sd = float(raw_tr[tr].std()) or 1.0\n",
        "    oof_z[va] = (raw_tr[va] - mu) / sd\n",
        "    te_z_acc += (raw_te - mu) / sd\n",
        "    # rank\n",
        "    ref = np.sort(raw_tr[tr].astype(np.float32))\n",
        "    if ref.size > 0:\n",
        "        j_va = np.searchsorted(ref, raw_tr[va], side='right')\n",
        "        oof_rank[va] = j_va / max(ref.size - 1, 1)\n",
        "        j_te = np.searchsorted(ref, raw_te, side='right')\n",
        "        te_rank_acc += (j_te / max(ref.size - 1, 1))\n",
        "\n",
        "te_iso  = (te_iso_acc / F).astype(np.float32)\n",
        "te_z    = (te_z_acc / F).astype(np.float32)\n",
        "te_rank = (te_rank_acc / F).astype(np.float32)\n",
        "\n",
        "# Save\n",
        "pd.DataFrame({\n",
        "    'id': train['id'],\n",
        "    'patentsberta_raw': oof_raw,\n",
        "    'patentsberta_iso': oof_iso,\n",
        "    'patentsberta_z': oof_z,\n",
        "    'patentsberta_rank': oof_rank,\n",
        "}).to_csv('oof_patentsberta.csv', index=False)\n",
        "pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'patentsberta_raw': raw_te.astype(np.float32),\n",
        "    'patentsberta_iso': te_iso,\n",
        "    'patentsberta_z': te_z,\n",
        "    'patentsberta_rank': te_rank,\n",
        "}).to_csv('patentsberta_test.csv', index=False)\n",
        "print('Saved oof_patentsberta.csv and patentsberta_test.csv; elapsed', round((time.time()-t0)/60,2), 'min', flush=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SentenceTransformer: AI-Growth-Lab/PatentSBERTa on CPU...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_patentsberta.csv and patentsberta_test.csv; elapsed 1.59 min\n"
          ]
        }
      ]
    },
    {
      "id": "7daef096-be71-4283-a8bc-ece8c7b5b693",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Char 4/5-gram TF-IDF cosine similarities (fold-safe). Outputs: oof_char45_tfidf_cos.csv, char45_tfidf_cos_test.csv\n",
        "import time, re, unicodedata, numpy as np, pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds_by_id.csv')\n",
        "train = train.merge(folds, on='id', how='left', validate='one_to_one')\n",
        "train['fold'] = train['fold'].astype(int)\n",
        "NUM_FOLDS = int(train['fold'].max()) + 1\n",
        "\n",
        "def nfkc(s: str) -> str:\n",
        "    return unicodedata.normalize('NFKC', s)\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = nfkc(str(s)).lower()\n",
        "    s = s.replace('-', ' ').replace('_', ' ')\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "A_tr = train['anchor'].astype(str).apply(normalize_text).tolist()\n",
        "B_tr = train['target'].astype(str).apply(normalize_text).tolist()\n",
        "A_te = test['anchor'].astype(str).apply(normalize_text).tolist()\n",
        "B_te = test['target'].astype(str).apply(normalize_text).tolist()\n",
        "\n",
        "n_tr = len(train); n_te = len(test)\n",
        "oof4 = np.zeros(n_tr, dtype=np.float32); oof5 = np.zeros(n_tr, dtype=np.float32)\n",
        "te4_acc = np.zeros(n_te, dtype=np.float64); te5_acc = np.zeros(n_te, dtype=np.float64)\n",
        "\n",
        "def rowwise_cosine(X, Y):\n",
        "    num = (X.multiply(Y)).sum(axis=1).A1.astype(np.float64)\n",
        "    x2 = X.multiply(X).sum(axis=1).A1.astype(np.float64)\n",
        "    y2 = Y.multiply(Y).sum(axis=1).A1.astype(np.float64)\n",
        "    den = np.sqrt(np.maximum(x2, 1e-12)) * np.sqrt(np.maximum(y2, 1e-12))\n",
        "    c = num / np.maximum(den, 1e-12)\n",
        "    return np.clip(c, 0.0, 1.0).astype(np.float32)\n",
        "\n",
        "for f in range(NUM_FOLDS):\n",
        "    f0 = time.time()\n",
        "    tr_idx = np.where(train['fold'].values != f)[0]\n",
        "    va_idx = np.where(train['fold'].values == f)[0]\n",
        "    # 4-gram\n",
        "    vec4 = TfidfVectorizer(analyzer='char', ngram_range=(4,4), min_df=3)\n",
        "    corp4 = [A_tr[i] for i in tr_idx] + [B_tr[i] for i in tr_idx]\n",
        "    V4 = vec4.fit_transform(corp4)\n",
        "    Va4 = vec4.transform([A_tr[i] for i in va_idx]); Vb4 = vec4.transform([B_tr[i] for i in va_idx])\n",
        "    oof4[va_idx] = rowwise_cosine(Va4, Vb4)\n",
        "    Ta4 = vec4.transform(A_te); Tb4 = vec4.transform(B_te)\n",
        "    te4_acc += rowwise_cosine(Ta4, Tb4).astype(np.float64)\n",
        "    # 5-gram\n",
        "    vec5 = TfidfVectorizer(analyzer='char', ngram_range=(5,5), min_df=3)\n",
        "    corp5 = corp4  # reuse\n",
        "    V5 = vec5.fit_transform(corp5)\n",
        "    Va5 = vec5.transform([A_tr[i] for i in va_idx]); Vb5 = vec5.transform([B_tr[i] for i in va_idx])\n",
        "    oof5[va_idx] = rowwise_cosine(Va5, Vb5)\n",
        "    Ta5 = vec5.transform(A_te); Tb5 = vec5.transform(B_te)\n",
        "    te5_acc += rowwise_cosine(Ta5, Tb5).astype(np.float64)\n",
        "    print(f'char45 tfidf fold {f} done in {time.time()-f0:.1f}s', flush=True)\n",
        "\n",
        "te4 = (te4_acc / NUM_FOLDS).astype(np.float32)\n",
        "te5 = (te5_acc / NUM_FOLDS).astype(np.float32)\n",
        "\n",
        "pd.DataFrame({'id': train['id'], 'char4_tfidf_cos': oof4, 'char5_tfidf_cos': oof5}).to_csv('oof_char45_tfidf_cos.csv', index=False)\n",
        "pd.DataFrame({'id': test['id'], 'char4_tfidf_cos': te4, 'char5_tfidf_cos': te5}).to_csv('char45_tfidf_cos_test.csv', index=False)\n",
        "print('Saved oof_char45_tfidf_cos.csv and char45_tfidf_cos_test.csv; elapsed', round((time.time()-t0)/60,2), 'min', flush=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char45 tfidf fold 0 done in 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char45 tfidf fold 1 done in 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char45 tfidf fold 2 done in 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char45 tfidf fold 3 done in 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char45 tfidf fold 4 done in 0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_char45_tfidf_cos.csv and char45_tfidf_cos_test.csv; elapsed 0.06 min\n"
          ]
        }
      ]
    },
    {
      "id": "aefba815-5855-4764-bfa3-ab286c869276",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# anferico/bert-for-patents cosine + fold-safe transforms (raw, iso, z, rank)\n",
        "import numpy as np, pandas as pd, time\n",
        "from pathlib import Path\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds_by_id.csv')\n",
        "train = train.merge(folds, on='id', how='left', validate='one_to_one')\n",
        "train['fold'] = train['fold'].astype(int)\n",
        "F = int(train['fold'].max()) + 1\n",
        "y = train['score'].astype(np.float32).values\n",
        "\n",
        "MODEL = 'anferico/bert-for-patents'\n",
        "print('Loading SentenceTransformer:', MODEL, 'on CPU...', flush=True)\n",
        "st = SentenceTransformer(MODEL, device='cpu')\n",
        "\n",
        "def enc(texts, bs=128):\n",
        "    return st.encode(texts, batch_size=bs, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False).astype(np.float32)\n",
        "\n",
        "# Precompute embeddings for all rows once\n",
        "a_tr = enc(train['anchor'].astype(str).tolist())\n",
        "b_tr = enc(train['target'].astype(str).tolist())\n",
        "a_te = enc(test['anchor'].astype(str).tolist())\n",
        "b_te = enc(test['target'].astype(str).tolist())\n",
        "\n",
        "# Cosine since normalized -> dot product\n",
        "raw_tr = (a_tr * b_tr).sum(axis=1).astype(np.float32)\n",
        "raw_te = (a_te * b_te).sum(axis=1).astype(np.float32)\n",
        "\n",
        "# Fold-safe iso/z/rank\n",
        "oof_raw  = np.zeros(len(train), dtype=np.float32)\n",
        "oof_iso  = np.zeros(len(train), dtype=np.float32)\n",
        "oof_z    = np.zeros(len(train), dtype=np.float32)\n",
        "oof_rank = np.zeros(len(train), dtype=np.float32)\n",
        "te_iso_acc  = np.zeros(len(test), dtype=np.float64)\n",
        "te_z_acc    = np.zeros(len(test), dtype=np.float64)\n",
        "te_rank_acc = np.zeros(len(test), dtype=np.float64)\n",
        "\n",
        "fold_arr = train['fold'].values.astype(int)\n",
        "for f in range(F):\n",
        "    tr = fold_arr != f; va = fold_arr == f\n",
        "    oof_raw[va] = raw_tr[va]\n",
        "    iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "    iso.fit(raw_tr[tr], y[tr])\n",
        "    oof_iso[va] = iso.transform(raw_tr[va]).astype(np.float32)\n",
        "    te_iso_acc += iso.transform(raw_te).astype(np.float64)\n",
        "    mu = float(raw_tr[tr].mean()); sd = float(raw_tr[tr].std()) or 1.0\n",
        "    oof_z[va] = (raw_tr[va] - mu) / sd\n",
        "    te_z_acc += (raw_te - mu) / sd\n",
        "    ref = np.sort(raw_tr[tr].astype(np.float32))\n",
        "    if ref.size > 0:\n",
        "        j_va = np.searchsorted(ref, raw_tr[va], side='right')\n",
        "        oof_rank[va] = j_va / max(ref.size - 1, 1)\n",
        "        j_te = np.searchsorted(ref, raw_te, side='right')\n",
        "        te_rank_acc += (j_te / max(ref.size - 1, 1))\n",
        "\n",
        "te_iso  = (te_iso_acc / F).astype(np.float32)\n",
        "te_z    = (te_z_acc / F).astype(np.float32)\n",
        "te_rank = (te_rank_acc / F).astype(np.float32)\n",
        "\n",
        "# Save\n",
        "pd.DataFrame({\n",
        "    'id': train['id'],\n",
        "    'bertpat_raw': oof_raw,\n",
        "    'bertpat_iso': oof_iso,\n",
        "    'bertpat_z': oof_z,\n",
        "    'bertpat_rank': oof_rank,\n",
        "}).to_csv('oof_bertpat.csv', index=False)\n",
        "pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'bertpat_raw': raw_te.astype(np.float32),\n",
        "    'bertpat_iso': te_iso,\n",
        "    'bertpat_z': te_z,\n",
        "    'bertpat_rank': te_rank,\n",
        "}).to_csv('bertpat_test.csv', index=False)\n",
        "print('Saved oof_bertpat.csv and bertpat_test.csv; elapsed', round((time.time()-t0)/60,2), 'min', flush=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SentenceTransformer: anferico/bert-for-patents on CPU...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No sentence-transformers model found with name anferico/bert-for-patents. Creating a new one with MEAN pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_bertpat.csv and bertpat_test.csv; elapsed 4.29 min\n"
          ]
        }
      ]
    },
    {
      "id": "6a3ddfb3-28b0-40e5-8d51-fd1c2864c1c7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Length and stopword-stripped overlap features (no fitting). Outputs: oof_len_stop.csv, len_stop_test.csv\n",
        "import re, time, numpy as np, pandas as pd\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "STOP = set(ENGLISH_STOP_WORDS)\n",
        "word_re = re.compile(r\"\\w+\")\n",
        "\n",
        "def toks_no_stop(s: str):\n",
        "    toks = [t.lower() for t in word_re.findall(str(s))]\n",
        "    return [t for t in toks if (t not in STOP and len(t) > 1)]\n",
        "\n",
        "def jaccard(a:set, b:set) -> float:\n",
        "    if not a and not b: return 0.0\n",
        "    return len(a & b) / (len(a | b) + 1e-12)\n",
        "\n",
        "def dice(a:set, b:set) -> float:\n",
        "    if not a and not b: return 0.0\n",
        "    return 2.0 * len(a & b) / (len(a) + len(b) + 1e-12)\n",
        "\n",
        "def compute(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    A = df['anchor'].astype(str).tolist()\n",
        "    B = df['target'].astype(str).tolist()\n",
        "    n = len(df)\n",
        "    len_ca = np.zeros(n, dtype=np.float32); len_cb = np.zeros(n, dtype=np.float32)\n",
        "    len_wa = np.zeros(n, dtype=np.float32); len_wb = np.zeros(n, dtype=np.float32)\n",
        "    len_cdiff = np.zeros(n, dtype=np.float32); len_cminmax = np.zeros(n, dtype=np.float32)\n",
        "    len_wdiff = np.zeros(n, dtype=np.float32); len_wminmax = np.zeros(n, dtype=np.float32)\n",
        "    jac = np.zeros(n, dtype=np.float32); di = np.zeros(n, dtype=np.float32)\n",
        "    ov_cnt = np.zeros(n, dtype=np.float32); uni_cnt = np.zeros(n, dtype=np.float32)\n",
        "    for i, (a, b) in enumerate(zip(A, B)):\n",
        "        ta = toks_no_stop(a); tb = toks_no_stop(b)\n",
        "        sa, sb = set(ta), set(tb)\n",
        "        ca = len(a); cb = len(b)\n",
        "        wa = len(ta); wb = len(tb)\n",
        "        len_ca[i] = ca; len_cb[i] = cb\n",
        "        len_wa[i] = wa; len_wb[i] = wb\n",
        "        len_cdiff[i] = abs(ca - cb)\n",
        "        mx = max(ca, cb) or 1.0; len_cminmax[i] = min(ca, cb) / mx\n",
        "        len_wdiff[i] = abs(wa - wb)\n",
        "        mxw = max(wa, wb) or 1.0; len_wminmax[i] = min(wa, wb) / mxw\n",
        "        jac[i] = jaccard(sa, sb)\n",
        "        di[i] = dice(sa, sb)\n",
        "        inter = sa & sb; union = sa | sb\n",
        "        ov_cnt[i] = float(len(inter)); uni_cnt[i] = float(len(union))\n",
        "    out = pd.DataFrame({\n",
        "        'len_char_a': len_ca, 'len_char_b': len_cb,\n",
        "        'len_char_diff': len_cdiff, 'len_char_minmax': len_cminmax,\n",
        "        'len_word_a': len_wa, 'len_word_b': len_wb,\n",
        "        'len_word_diff': len_wdiff, 'len_word_minmax': len_wminmax,\n",
        "        'nostop_jaccard': jac, 'nostop_dice': di,\n",
        "        'nostop_overlap_cnt': ov_cnt, 'nostop_union_cnt': uni_cnt,\n",
        "    })\n",
        "    # simple non-linearities\n",
        "    out['len_char_minmax_sq'] = (out['len_char_minmax'] ** 2).astype(np.float32)\n",
        "    out['len_word_minmax_sq'] = (out['len_word_minmax'] ** 2).astype(np.float32)\n",
        "    out['nostop_jaccard_sq'] = (out['nostop_jaccard'] ** 2).astype(np.float32)\n",
        "    return out.astype('float32')\n",
        "\n",
        "tr_feats = compute(train)\n",
        "te_feats = compute(test)\n",
        "\n",
        "pd.concat([train[['id']].reset_index(drop=True), tr_feats.reset_index(drop=True)], axis=1).to_csv('oof_len_stop.csv', index=False)\n",
        "pd.concat([test[['id']].reset_index(drop=True), te_feats.reset_index(drop=True)], axis=1).to_csv('len_stop_test.csv', index=False)\n",
        "print('Saved oof_len_stop.csv and len_stop_test.csv; elapsed', round((time.time()-t0)/60,2), 'min', flush=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_len_stop.csv and len_stop_test.csv; elapsed 0.01 min\n"
          ]
        }
      ]
    },
    {
      "id": "81aff9e4-c8ff-40a3-a0f0-be3b7f4f9bd0",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fold-safe KNN regression meta-features from patent embeddings (PatentSBERTa, bert-for-patents)\n",
        "# Outputs: oof_knn_meta.csv, knn_meta_test.csv\n",
        "import numpy as np, pandas as pd, time, re\n",
        "from pathlib import Path\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test  = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds_by_id.csv')\n",
        "train = train.merge(folds, on='id', how='left', validate='one_to_one')\n",
        "train['fold'] = train['fold'].astype(int)\n",
        "NUM_FOLDS = int(train['fold'].max()) + 1\n",
        "y = train['score'].astype(np.float32).values\n",
        "\n",
        "def l2norm(mat: np.ndarray) -> np.ndarray:\n",
        "    n = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-12\n",
        "    return (mat / n).astype(np.float32)\n",
        "\n",
        "def pair_rep(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
        "    # Construct pair vector [a, b, |a-b|, a*b] then L2-normalize row-wise\n",
        "    return l2norm(np.concatenate([a, b, np.abs(a-b), a*b], axis=1))\n",
        "\n",
        "def knn_wmean_from_index(nn: NearestNeighbors, Xq: np.ndarray, y_ref: np.ndarray, k: int, p: int = 1) -> np.ndarray:\n",
        "    # cosine metric returns distance d = 1 - cos_sim => sim = 1 - d\n",
        "    dists, inds = nn.kneighbors(Xq, n_neighbors=k, return_distance=True)\n",
        "    sims = np.maximum(0.0, 1.0 - dists)\n",
        "    w = (sims ** p)\n",
        "    wy = (w * y_ref[inds])\n",
        "    denom = np.sum(w, axis=1, keepdims=True) + 1e-12\n",
        "    return (np.sum(wy, axis=1, keepdims=True) / denom).astype(np.float32).ravel()\n",
        "\n",
        "def build_group_indices(keys: np.ndarray):\n",
        "    mp = {}\n",
        "    for i, k in enumerate(keys):\n",
        "        mp.setdefault(k, []).append(i)\n",
        "    return mp\n",
        "\n",
        "def compute_knn_meta_for_encoder(model_name: str, tag: str, bs: int = 128):\n",
        "    print(f'Encoding with {model_name} ...', flush=True)\n",
        "    st = SentenceTransformer(model_name, device='cpu')\n",
        "    def enc(texts):\n",
        "        return st.encode(texts, batch_size=bs, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False).astype(np.float32)\n",
        "    a_tr = enc(train['anchor'].astype(str).tolist())\n",
        "    b_tr = enc(train['target'].astype(str).tolist())\n",
        "    a_te = enc(test['anchor'].astype(str).tolist())\n",
        "    b_te = enc(test['target'].astype(str).tolist())\n",
        "    V_tr = pair_rep(a_tr, b_tr)\n",
        "    V_te = pair_rep(a_te, b_te)\n",
        "\n",
        "    # Outputs per encoder\n",
        "    oof_g10 = np.zeros(len(train), dtype=np.float32)\n",
        "    oof_g25 = np.zeros(len(train), dtype=np.float32)\n",
        "    oof_anch = np.zeros(len(train), dtype=np.float32)\n",
        "    oof_cpc3 = np.zeros(len(train), dtype=np.float32)\n",
        "    te_g10_acc = np.zeros(len(test), dtype=np.float64)\n",
        "    te_g25_acc = np.zeros(len(test), dtype=np.float64)\n",
        "    te_anch_acc = np.zeros(len(test), dtype=np.float64)\n",
        "    te_cpc3_acc = np.zeros(len(test), dtype=np.float64)\n",
        "\n",
        "    fold_arr = train['fold'].values.astype(int)\n",
        "    anchors_tr = train['anchor'].astype(str).values\n",
        "    cpc3_tr = train['context'].astype(str).str[:3].values\n",
        "    anchors_te = test['anchor'].astype(str).values\n",
        "    cpc3_te = test['context'].astype(str).str[:3].values\n",
        "\n",
        "    for f in range(NUM_FOLDS):\n",
        "        f0 = time.time()\n",
        "        tr_idx = np.where(fold_arr != f)[0]\n",
        "        va_idx = np.where(fold_arr == f)[0]\n",
        "        V_ref = V_tr[tr_idx]\n",
        "        y_ref = y[tr_idx]\n",
        "        # Global KNN on train-only\n",
        "        nn_global = NearestNeighbors(metric='cosine', algorithm='brute', n_jobs=-1).fit(V_ref)\n",
        "        V_va = V_tr[va_idx]\n",
        "        oof_g10[va_idx] = knn_wmean_from_index(nn_global, V_va, y_ref, k=10, p=1)\n",
        "        oof_g25[va_idx] = knn_wmean_from_index(nn_global, V_va, y_ref, k=25, p=1)\n",
        "\n",
        "        # Grouped indices on train-only for within-anchor and within-cpc3\n",
        "        grp_anchor = build_group_indices(anchors_tr[tr_idx])\n",
        "        grp_cpc3 = build_group_indices(cpc3_tr[tr_idx])\n",
        "        # Prebuild per-group NN for groups with >=3 instances\n",
        "        nn_anchor = {}\n",
        "        for g, inds in grp_anchor.items():\n",
        "            if len(inds) >= 3:\n",
        "                nn_anchor[g] = NearestNeighbors(metric='cosine', algorithm='brute', n_jobs=-1).fit(V_ref[np.array(inds)])\n",
        "        nn_cpc3 = {}\n",
        "        for g, inds in grp_cpc3.items():\n",
        "            if len(inds) >= 3:\n",
        "                nn_cpc3[g] = NearestNeighbors(metric='cosine', algorithm='brute', n_jobs=-1).fit(V_ref[np.array(inds)])\n",
        "\n",
        "        # Within-anchor\n",
        "        for loc, idx in enumerate(va_idx):\n",
        "            a = anchors_tr[idx]\n",
        "            if a in nn_anchor:\n",
        "                oof_anch[idx] = knn_wmean_from_index(nn_anchor[a], V_tr[idx:idx+1], y_ref, k=min(25, nn_anchor[a].n_samples_fit_), p=1)[0]\n",
        "            else:\n",
        "                oof_anch[idx] = oof_g10[idx]\n",
        "        # Within-cpc3\n",
        "        for loc, idx in enumerate(va_idx):\n",
        "            c3 = cpc3_tr[idx]\n",
        "            if c3 in nn_cpc3:\n",
        "                oof_cpc3[idx] = knn_wmean_from_index(nn_cpc3[c3], V_tr[idx:idx+1], y_ref, k=min(25, nn_cpc3[c3].n_samples_fit_), p=1)[0]\n",
        "            else:\n",
        "                oof_cpc3[idx] = oof_g10[idx]\n",
        "\n",
        "        # Test predictions using train-only index; average across folds\n",
        "        te_g10_acc += knn_wmean_from_index(nn_global, V_te, y_ref, k=10, p=1).astype(np.float64)\n",
        "        te_g25_acc += knn_wmean_from_index(nn_global, V_te, y_ref, k=25, p=1).astype(np.float64)\n",
        "        # For grouped test, use available group NN else fallback to global k10\n",
        "        te_anchor_tmp = np.zeros(len(test), dtype=np.float64)\n",
        "        for j in range(len(test)):\n",
        "            a = anchors_te[j]\n",
        "            if a in nn_anchor:\n",
        "                te_anchor_tmp[j] = float(knn_wmean_from_index(nn_anchor[a], V_te[j:j+1], y_ref, k=min(25, nn_anchor[a].n_samples_fit_), p=1)[0])\n",
        "            else:\n",
        "                te_anchor_tmp[j] = float(knn_wmean_from_index(nn_global, V_te[j:j+1], y_ref, k=10, p=1)[0])\n",
        "        te_anch_acc += te_anchor_tmp\n",
        "        te_cpc3_tmp = np.zeros(len(test), dtype=np.float64)\n",
        "        for j in range(len(test)):\n",
        "            c3 = cpc3_te[j]\n",
        "            if c3 in nn_cpc3:\n",
        "                te_cpc3_tmp[j] = float(knn_wmean_from_index(nn_cpc3[c3], V_te[j:j+1], y_ref, k=min(25, nn_cpc3[c3].n_samples_fit_), p=1)[0])\n",
        "            else:\n",
        "                te_cpc3_tmp[j] = float(knn_wmean_from_index(nn_global, V_te[j:j+1], y_ref, k=10, p=1)[0])\n",
        "        te_cpc3_acc += te_cpc3_tmp\n",
        "        print(f'[{tag}] fold {f} done in {time.time()-f0:.1f}s', flush=True)\n",
        "\n",
        "    te_g10 = (te_g10_acc / NUM_FOLDS).astype(np.float32)\n",
        "    te_g25 = (te_g25_acc / NUM_FOLDS).astype(np.float32)\n",
        "    te_anch = (te_anch_acc / NUM_FOLDS).astype(np.float32)\n",
        "    te_cpc3 = (te_cpc3_acc / NUM_FOLDS).astype(np.float32)\n",
        "\n",
        "    cols = {\n",
        "        f'knn_{tag}_wmean10': oof_g10,\n",
        "        f'knn_{tag}_wmean25': oof_g25,\n",
        "        f'knn_{tag}_anchor': oof_anch,\n",
        "        f'knn_{tag}_cpc3': oof_cpc3,\n",
        "    }\n",
        "    cols_te = {\n",
        "        f'knn_{tag}_wmean10': te_g10,\n",
        "        f'knn_{tag}_wmean25': te_g25,\n",
        "        f'knn_{tag}_anchor': te_anch,\n",
        "        f'knn_{tag}_cpc3': te_cpc3,\n",
        "    }\n",
        "    return cols, cols_te\n",
        "\n",
        "# Compute for both encoders\n",
        "cols_patberta, cols_te_patberta = compute_knn_meta_for_encoder('AI-Growth-Lab/PatentSBERTa', tag='patberta')\n",
        "cols_bertpat, cols_te_bertpat = compute_knn_meta_for_encoder('anferico/bert-for-patents', tag='bertpat')\n",
        "\n",
        "# Save\n",
        "oof_df = pd.DataFrame({'id': train['id']})\n",
        "te_df = pd.DataFrame({'id': test['id']})\n",
        "for k, v in {**cols_patberta, **cols_bertpat}.items():\n",
        "    oof_df[k] = v.astype(np.float32)\n",
        "for k, v in {**cols_te_patberta, **cols_te_bertpat}.items():\n",
        "    te_df[k] = v.astype(np.float32)\n",
        "oof_df.to_csv('oof_knn_meta.csv', index=False)\n",
        "te_df.to_csv('knn_meta_test.csv', index=False)\n",
        "print('Saved oof_knn_meta.csv and knn_meta_test.csv; elapsed', round((time.time()-t0)/60,2), 'min', flush=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding with AI-Growth-Lab/PatentSBERTa ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[patberta] fold 0 done in 477.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[patberta] fold 1 done in 494.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[patberta] fold 2 done in 503.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[patberta] fold 3 done in 505.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[patberta] fold 4 done in 497.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding with anferico/bert-for-patents ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No sentence-transformers model found with name anferico/bert-for-patents. Creating a new one with MEAN pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[bertpat] fold 0 done in 496.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[bertpat] fold 1 done in 507.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[bertpat] fold 2 done in 515.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[bertpat] fold 3 done in 527.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[bertpat] fold 4 done in 517.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_knn_meta.csv and knn_meta_test.csv; elapsed 89.81 min\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}