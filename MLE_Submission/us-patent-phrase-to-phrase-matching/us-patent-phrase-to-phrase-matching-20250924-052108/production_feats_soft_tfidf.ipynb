{
  "cells": [
    {
      "id": "3cb78b3d-9fb6-4e71-b213-fe3a7f0b572d",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Soft TF-IDF (1-2g stems) with JW=0.90, fold-safe, on normalized text. Outputs: oof_soft_tfidf_norm.csv and soft_tfidf_norm_test.csv\n",
        "import time, math, re, gc, sys, unicodedata\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from rapidfuzz.distance import JaroWinkler\n",
        "\n",
        "t0 = time.time()\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds_by_id.csv')  # columns: id, fold\n",
        "train = train.merge(folds, on='id', how='left')\n",
        "\n",
        "# ---------- Normalization utilities (NFKC + confusables + number-unit split + de-hyphen) ----------\n",
        "REPL = {\n",
        "    '\\u00b5': 'u',   # micro sign\n",
        "    '\\u03bc': 'u',   # Greek mu\n",
        "    '\\u03a9': 'ohm', # Omega\n",
        "    '\\u2126': 'ohm', # Ohm symbol\n",
        "    '\\u00b0C': 'deg C',  # degree C\n",
        "    '\\u00b0F': 'deg F',  # degree F\n",
        "    '\\u00b0': 'deg',     # bare degree\n",
        "    '\\u00d7': 'x',       # multiplication sign\n",
        "    '\\u2032': \"'\",      # prime\n",
        "    '\\u2033': '\"',      # double prime\n",
        "}\n",
        "\n",
        "# Sub/superscripts\n",
        "_SUBS = str.maketrans('\u2080\u2081\u2082\u2083\u2084\u2085\u2086\u2087\u2088\u2089', '0123456789')\n",
        "_SUPS_MAP = { '\u00b2': '2', '\u00b3': '3', '\u207a': '+', '\u207b': '-' }\n",
        "\n",
        "NUM_UNIT_ATTACH = re.compile(r'(?i)(\\d+(?:[\\./]\\d+)?)([a-zA-Z%][a-zA-Z%/]*)')\n",
        "\n",
        "def nfkc(s: str) -> str:\n",
        "    return unicodedata.normalize('NFKC', s)\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = nfkc(str(s))\n",
        "    for k, v in REPL.items():\n",
        "        s = s.replace(k, v)\n",
        "    for k, v in _SUPS_MAP.items():\n",
        "        s = s.replace(k, v)\n",
        "    s = s.translate(_SUBS)\n",
        "    # unify hyphen/underscore to space\n",
        "    s = s.replace('-', ' ').replace('_', ' ')\n",
        "    # split attached number+unit 10nm -> '10 nm'\n",
        "    s = NUM_UNIT_ATTACH.sub(r'\\1 \\2', s)\n",
        "    # lowercase and collapse spaces\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "# ---------- Tokenizer with Porter stemming (no external corpora needed) ----------\n",
        "try:\n",
        "    from nltk.stem import PorterStemmer\n",
        "    _stemmer = PorterStemmer()\n",
        "    def stem_token(tok: str) -> str:\n",
        "        return _stemmer.stem(tok)\n",
        "except Exception:\n",
        "    def stem_token(tok: str) -> str:\n",
        "        return tok\n",
        "\n",
        "_word_re = re.compile(r\"[a-z0-9]+(?:[./][a-z0-9]+)?\")  # normalized regex (post-normalize_text)\n",
        "def tokenize_stems(text: str):\n",
        "    if not isinstance(text, str):\n",
        "        text = ''\n",
        "    text = normalize_text(text)\n",
        "    toks = _word_re.findall(text)\n",
        "    return [stem_token(t) for t in toks if t]\n",
        "\n",
        "# sklearn will build ngrams over tokens returned by tokenizer\n",
        "def analyzer(text: str):\n",
        "    return tokenize_stems(text)\n",
        "\n",
        "# ---------- Soft TF-IDF computation ----------\n",
        "JW_THRESH = 0.90\n",
        "\n",
        "def jw_sim(a: str, b: str) -> float:\n",
        "    return float(JaroWinkler.normalized_similarity(a, b))\n",
        "\n",
        "def sparse_to_weight_dict(vec, feature_names):\n",
        "    ind = vec.indices\n",
        "    data = vec.data\n",
        "    return {feature_names[i]: float(w) for i, w in zip(ind, data)}\n",
        "\n",
        "def vec_norm_squared(vec):\n",
        "    return float((vec.power(2)).sum())\n",
        "\n",
        "def soft_tfidf_score(wA: dict, wB: dict, cache: dict) -> float:\n",
        "    if not wA or not wB:\n",
        "        return 0.0\n",
        "    num = 0.0\n",
        "    for ta, wa in wA.items():\n",
        "        best = 0.0; best_tb = None\n",
        "        for tb, wb in wB.items():\n",
        "            key = (ta, tb) if ta <= tb else (tb, ta)\n",
        "            s = cache.get(key)\n",
        "            if s is None:\n",
        "                s = jw_sim(ta, tb)\n",
        "                cache[key] = s\n",
        "            if s >= JW_THRESH and s > best:\n",
        "                best = s; best_tb = tb\n",
        "        if best_tb is not None:\n",
        "            num += wa * best * wB[best_tb]\n",
        "    return num\n",
        "\n",
        "def compute_soft_pair(vecA, vecB, feature_names, jw_cache):\n",
        "    wA = sparse_to_weight_dict(vecA, feature_names)\n",
        "    wB = sparse_to_weight_dict(vecB, feature_names)\n",
        "    num = soft_tfidf_score(wA, wB, jw_cache)\n",
        "    na = math.sqrt(vec_norm_squared(vecA))\n",
        "    nb = math.sqrt(vec_norm_squared(vecB))\n",
        "    if na == 0.0 or nb == 0.0:\n",
        "        return 0.0\n",
        "    return float(num / (na * nb))\n",
        "\n",
        "# ---------- Fold-safe computation ----------\n",
        "NUM_FOLDS = int(train['fold'].max()) + 1\n",
        "print(f'Found {NUM_FOLDS} folds', flush=True)\n",
        "\n",
        "oof_vals = np.zeros(len(train), dtype=np.float32)\n",
        "len_diff_arr = np.zeros(len(train), dtype=np.float32)\n",
        "\n",
        "# For per-anchor train-only ranks/stats per fold\n",
        "oof_rank = np.full(len(train), np.nan, dtype=np.float32)\n",
        "oof_pct = np.full(len(train), np.nan, dtype=np.float32)\n",
        "oof_gap_top = np.full(len(train), np.nan, dtype=np.float32)\n",
        "\n",
        "# For test, we'll average per-fold test predictions\n",
        "test_soft_matrix = []\n",
        "test_len_diff_list = []\n",
        "\n",
        "def doc_len_tokens(text):\n",
        "    return len(tokenize_stems(text))\n",
        "\n",
        "for fold in range(NUM_FOLDS):\n",
        "    t_fold0 = time.time()\n",
        "    tr_idx = np.where(train['fold'].values != fold)[0]\n",
        "    va_idx = np.where(train['fold'].values == fold)[0]\n",
        "    tr_df = train.iloc[tr_idx].reset_index(drop=True)\n",
        "    va_df = train.iloc[va_idx].reset_index(drop=True)\n",
        "    print(f'Fold {fold}: train {len(tr_df)} val {len(va_df)}', flush=True)\n",
        "\n",
        "    # Build corpus on train-only (A \u222a B), on normalized text\n",
        "    corpus = pd.concat([tr_df['anchor'], tr_df['target']], axis=0).astype(str).tolist()\n",
        "    vectorizer = TfidfVectorizer(analyzer='word', tokenizer=analyzer, preprocessor=None, lowercase=False, ngram_range=(1,2), min_df=3)\n",
        "    V = vectorizer.fit_transform(corpus)\n",
        "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "    print(f'  Vocab size: {len(feature_names)}', flush=True)\n",
        "\n",
        "    # Transform val and test (anchor/target separately) with same vectorizer\n",
        "    va_anchor = vectorizer.transform(va_df['anchor'].astype(str).tolist())\n",
        "    va_target = vectorizer.transform(va_df['target'].astype(str).tolist())\n",
        "\n",
        "    # Compute per-row soft tfidf for val\n",
        "    jw_cache = {}\n",
        "    vals = np.zeros(len(va_df), dtype=np.float32)\n",
        "    v_len_diff = np.zeros(len(va_df), dtype=np.float32)\n",
        "    for i in range(len(va_df)):\n",
        "        if (i+1) % 1000 == 0:\n",
        "            print(f'    val row {i+1}/{len(va_df)}', flush=True)\n",
        "        vals[i] = compute_soft_pair(va_anchor[i], va_target[i], feature_names, jw_cache)\n",
        "        v_len_diff[i] = abs(doc_len_tokens(va_df.at[i,'anchor']) - doc_len_tokens(va_df.at[i,'target']))\n",
        "\n",
        "    oof_vals[va_idx] = vals\n",
        "    len_diff_arr[va_idx] = v_len_diff\n",
        "\n",
        "    # Per-anchor ranks/pct/gap computed on train-only within this fold\n",
        "    tr_anchor = vectorizer.transform(tr_df['anchor'].astype(str).tolist())\n",
        "    tr_target = vectorizer.transform(tr_df['target'].astype(str).tolist())\n",
        "    jw_cache_tr = {}\n",
        "    tr_soft = np.zeros(len(tr_df), dtype=np.float32)\n",
        "    for i in range(len(tr_df)):\n",
        "        if (i+1) % 5000 == 0:\n",
        "            print(f'    train ref row {i+1}/{len(tr_df)}', flush=True)\n",
        "        tr_soft[i] = compute_soft_pair(tr_anchor[i], tr_target[i], feature_names, jw_cache_tr)\n",
        "    tr_anchor_col = tr_df['anchor'].values\n",
        "\n",
        "    # Build per-anchor sorted lists from train-only\n",
        "    ref_scores = defaultdict(list)\n",
        "    for a, s in zip(tr_anchor_col, tr_soft):\n",
        "        ref_scores[a].append(float(s))\n",
        "    for a in ref_scores:\n",
        "        ref_scores[a].sort()\n",
        "\n",
        "    # Assign ranks/pct/gap for OOF val rows using train-only reference\n",
        "    for loc, idx in enumerate(va_idx):\n",
        "        a = train.at[idx, 'anchor']\n",
        "        s = oof_vals[idx]\n",
        "        arr = ref_scores.get(a)\n",
        "        if not arr:\n",
        "            continue\n",
        "        j = np.searchsorted(arr, s, side='right')\n",
        "        pct = j / len(arr)\n",
        "        rank = j\n",
        "        gap_top = (arr[-1] - s)\n",
        "        oof_pct[idx] = pct\n",
        "        oof_rank[idx] = rank\n",
        "        oof_gap_top[idx] = gap_top\n",
        "\n",
        "    # Test predictions for this fold (will be averaged across folds)\n",
        "    te_anchor = vectorizer.transform(test['anchor'].astype(str).tolist())\n",
        "    te_target = vectorizer.transform(test['target'].astype(str).tolist())\n",
        "    jw_cache_te = {}\n",
        "    te_vals = np.zeros(len(test), dtype=np.float32)\n",
        "    te_len_diff = np.zeros(len(test), dtype=np.float32)\n",
        "    for i in range(len(test)):\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print(f'    test row {i+1}/{len(test)}', flush=True)\n",
        "        te_vals[i] = compute_soft_pair(te_anchor[i], te_target[i], feature_names, jw_cache_te)\n",
        "        te_len_diff[i] = abs(doc_len_tokens(test.at[i,'anchor']) - doc_len_tokens(test.at[i,'target']))\n",
        "    test_soft_matrix.append(te_vals)\n",
        "    test_len_diff_list.append(te_len_diff)\n",
        "\n",
        "    dt = time.time() - t_fold0\n",
        "    print(f'Fold {fold} done in {dt/60:.2f} min', flush=True)\n",
        "    del vectorizer, V, va_anchor, va_target, tr_anchor, tr_target, jw_cache, jw_cache_tr, jw_cache_te\n",
        "    gc.collect()\n",
        "\n",
        "# ---------- Build OOF dataframe with transforms ----------\n",
        "oof = pd.DataFrame({'id': train['id'], 'anchor': train['anchor'], 'soft_tfidf': oof_vals, 'len_diff': len_diff_arr, 'rank': oof_rank, 'pct': oof_pct, 'gap_to_top': oof_gap_top})\n",
        "oof['soft_tfidf_sq'] = oof['soft_tfidf'] ** 2\n",
        "oof['one_minus_soft'] = 1.0 - oof['soft_tfidf']\n",
        "oof['fisher_z'] = np.arctanh(np.clip(oof['soft_tfidf'] * 2 - 1, -0.999999, 0.999999))  # map [0,1]->[-1,1] then Fisher-z\n",
        "oof['soft_x_len_diff'] = oof['soft_tfidf'] * oof['len_diff']\n",
        "oof = oof.drop(columns=['anchor'])\n",
        "\n",
        "# Save OOF (normalized-text variant)\n",
        "oof.to_csv('oof_soft_tfidf_norm.csv', index=False)\n",
        "print('Saved oof_soft_tfidf_norm.csv', oof.shape, flush=True)\n",
        "\n",
        "# ---------- Aggregate test across folds and compute ranks vs OOF distribution ----------\n",
        "test_soft = np.mean(np.vstack(test_soft_matrix), axis=0).astype(np.float32)\n",
        "test_len_diff = np.mean(np.vstack(test_len_diff_list), axis=0).astype(np.float32)\n",
        "te = pd.DataFrame({'id': test['id'], 'anchor': test['anchor'], 'soft_tfidf': test_soft, 'len_diff': test_len_diff})\n",
        "te['soft_tfidf_sq'] = te['soft_tfidf'] ** 2\n",
        "te['one_minus_soft'] = 1.0 - te['soft_tfidf']\n",
        "te['fisher_z'] = np.arctanh(np.clip(te['soft_tfidf'] * 2 - 1, -0.999999, 0.999999))\n",
        "te['soft_x_len_diff'] = te['soft_tfidf'] * te['len_diff']\n",
        "\n",
        "# Build per-anchor reference from OOF for percentile and gap_to_top\n",
        "ref = oof[['id','soft_tfidf']].join(train[['id','anchor']].set_index('id'), on='id')\n",
        "ref_groups = defaultdict(list)\n",
        "for a, s in zip(ref['anchor'].values, ref['soft_tfidf'].values):\n",
        "    ref_groups[a].append(float(s))\n",
        "for a in ref_groups:\n",
        "    ref_groups[a].sort()\n",
        "\n",
        "rank_list = np.full(len(te), np.nan, dtype=np.float32)\n",
        "pct_list = np.full(len(te), np.nan, dtype=np.float32)\n",
        "gap_list = np.full(len(te), np.nan, dtype=np.float32)\n",
        "for i in range(len(te)):\n",
        "    a = te.at[i, 'anchor']\n",
        "    s = te.at[i, 'soft_tfidf']\n",
        "    arr = ref_groups.get(a)\n",
        "    if arr:\n",
        "        j = np.searchsorted(arr, s, side='right')\n",
        "        pct_list[i] = j / len(arr)\n",
        "        rank_list[i] = j\n",
        "        gap_list[i] = (arr[-1] - s)\n",
        "\n",
        "te['rank'] = rank_list\n",
        "te['pct'] = pct_list\n",
        "te['gap_to_top'] = gap_list\n",
        "te = te.drop(columns=['anchor'])\n",
        "te.to_csv('soft_tfidf_norm_test.csv', index=False)\n",
        "print('Saved soft_tfidf_norm_test.csv', te.shape, flush=True)\n",
        "\n",
        "print('All done in', round((time.time()-t0)/60,2), 'min', flush=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 folds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: train 26739 val 6086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Vocab size: 5224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 1000/6086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 2000/6086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 3000/6086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 4000/6086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 5000/6086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 6000/6086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 5000/26739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 10000/26739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 15000/26739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 20000/26739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 25000/26739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    test row 2000/3648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 done in 0.12 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: train 26310 val 6515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Vocab size: 5177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 1000/6515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 2000/6515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 3000/6515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 4000/6515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 5000/6515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 6000/6515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 5000/26310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 10000/26310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 15000/26310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 20000/26310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 25000/26310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    test row 2000/3648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 done in 0.12 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: train 26195 val 6630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Vocab size: 5146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 1000/6630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 2000/6630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 3000/6630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 4000/6630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 5000/6630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 6000/6630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 5000/26195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 10000/26195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 15000/26195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 20000/26195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 25000/26195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    test row 2000/3648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 done in 0.12 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: train 25884 val 6941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Vocab size: 5052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 1000/6941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 2000/6941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 3000/6941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 4000/6941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 5000/6941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 6000/6941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 5000/25884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 10000/25884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 15000/25884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 20000/25884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 25000/25884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    test row 2000/3648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 done in 0.12 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: train 26172 val 6653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Vocab size: 5159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 1000/6653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 2000/6653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 3000/6653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 4000/6653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 5000/6653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    val row 6000/6653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 5000/26172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 10000/26172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 15000/26172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 20000/26172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    train ref row 25000/26172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    test row 2000/3648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 done in 0.12 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved oof_soft_tfidf_norm.csv (32825, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved soft_tfidf_norm_test.csv (3648, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All done in 0.61 min\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}