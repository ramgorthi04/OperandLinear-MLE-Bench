{
  "cells": [
    {
      "id": "c89940ff-877e-4b3e-bcec-ffa507d3b3c3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Force linker to use pip-installed CUDA/NCCL libs first, then restart kernel\n",
        "import os, sys, subprocess, pathlib\n",
        "\n",
        "SITE = \"/app/.pip-target\"\n",
        "\n",
        "# Ensure pip-target is first on sys.path\n",
        "if SITE not in sys.path:\n",
        "    sys.path.insert(0, SITE)\n",
        "\n",
        "# Build LD_LIBRARY_PATH with pip libs FIRST (order matters)\n",
        "libs = [\n",
        "    f\"{SITE}/torch/lib\",\n",
        "    f\"{SITE}/nvidia/nccl/lib\",\n",
        "    f\"{SITE}/nvidia/cublas/lib\",\n",
        "    f\"{SITE}/nvidia/cuda_runtime/lib\",\n",
        "    f\"{SITE}/nvidia/cuda_nvrtc/lib\",\n",
        "    f\"{SITE}/nvidia/cuda_cupti/lib\",\n",
        "    f\"{SITE}/nvidia/cudnn/lib\",\n",
        "    f\"{SITE}/nvidia/cufft/lib\",\n",
        "    f\"{SITE}/nvidia/cusparse/lib\",\n",
        "    f\"{SITE}/nvidia/cusolver/lib\",\n",
        "    f\"{SITE}/nvidia/curand/lib\",\n",
        "    f\"{SITE}/nvidia/nvjitlink/lib\",\n",
        "]\n",
        "ld = \":\".join([p for p in libs if os.path.isdir(p)])\n",
        "os.environ[\"LD_LIBRARY_PATH\"] = ld + ((\":\" + os.environ[\"LD_LIBRARY_PATH\"]) if \"LD_LIBRARY_PATH\" in os.environ else \"\")\n",
        "\n",
        "# Preload pip NCCL and NVJitLink to avoid /usr/local/nvidia shadowing\n",
        "nccl = f\"{SITE}/nvidia/nccl/lib/libnccl.so.2\"\n",
        "nvjl = f\"{SITE}/nvidia/nvjitlink/lib/libnvJitLink.so.12\"\n",
        "pre = \":\".join([p for p in (nccl, nvjl) if os.path.exists(p)])\n",
        "if pre:\n",
        "    os.environ[\"LD_PRELOAD\"] = pre + ((\":\" + os.environ[\"LD_PRELOAD\"]) if \"LD_PRELOAD\" in os.environ else \"\")\n",
        "\n",
        "# Optional: show which libnccl libtorch_cuda will use\n",
        "libtorch_cuda = pathlib.Path(SITE).joinpath(\"torch\", \"lib\", \"libtorch_cuda.so\")\n",
        "if libtorch_cuda.exists():\n",
        "    print(\"ldd libtorch_cuda.so:\")\n",
        "    subprocess.run([\"ldd\", str(libtorch_cuda)], check=False)\n",
        "\n",
        "# Restart kernel so dynamic linker picks up new paths\n",
        "import sys as _sys, os as _os\n",
        "print(\"Restarting kernel to apply LD_LIBRARY_PATH/LD_PRELOAD...\")\n",
        "_os.execv(_sys.executable, [_sys.executable] + _sys.argv)"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "id": "93597e45-5eb3-4e9f-bbb2-9733bc6196fb",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Monotonic LightGBM stacker with fold-safe CE isotonic calibration\n",
        "# Loads OOF/train features and test counterparts, builds constraints, trains 5-fold CV, outputs OOF and submission.\n",
        "import time, gc, math, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from scipy.stats import pearsonr\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "t0 = time.time()\n",
        "pd.set_option('display.max_columns', 200)\n",
        "\n",
        "# ---------- Load base data and folds ----------\n",
        "train = pd.read_csv('train.csv')  # id, anchor, target, context, score\n",
        "test = pd.read_csv('test.csv')\n",
        "folds = pd.read_csv('folds_by_id.csv')  # id, fold\n",
        "train = train.merge(folds, on='id', how='left', validate='one_to_one')\n",
        "assert (train['fold']>=0).all(), 'Fold merge by id failed'\n",
        "NUM_FOLDS = int(train['fold'].max()) + 1\n",
        "print('Folds:', NUM_FOLDS, flush=True)\n",
        "\n",
        "# ---------- Utilities to load features ----------\n",
        "def load_oof(oof_path, id_col='id'):\n",
        "    df = pd.read_csv(oof_path)\n",
        "    assert id_col in df.columns, f'Missing id in {oof_path}'\n",
        "    return df\n",
        "\n",
        "def load_test(test_path, id_col='id'):\n",
        "    df = pd.read_csv(test_path)\n",
        "    assert id_col in df.columns, f'Missing id in {test_path}'\n",
        "    return df\n",
        "\n",
        "def add_feats(oof_path, test_path, train_df, test_df, prefix=None):\n",
        "    oof_df = load_oof(oof_path)\n",
        "    te_df  = load_test(test_path)\n",
        "    oof_cols = [c for c in oof_df.columns if c != 'id']\n",
        "    te_cols  = [c for c in te_df.columns if c != 'id']\n",
        "    common = [c for c in oof_cols if c in te_cols]\n",
        "    if len(common) == 0:\n",
        "        print(f'WARN: no common feature columns between {oof_path} and {test_path}; skipping', flush=True)\n",
        "        return train_df, test_df, []\n",
        "    oof_df = oof_df[['id'] + common].copy()\n",
        "    te_df  = te_df[['id'] + common].copy()\n",
        "    if prefix:\n",
        "        rename_map = {c: f'{prefix}_{c}' for c in common}\n",
        "        oof_df = oof_df.rename(columns=rename_map)\n",
        "        te_df  = te_df.rename(columns=rename_map)\n",
        "        feat_cols = [f'{prefix}_{c}' for c in common]\n",
        "    else:\n",
        "        feat_cols = common\n",
        "    train_df = train_df.merge(oof_df, on='id', how='left')\n",
        "    test_df  = test_df.merge(te_df, on='id', how='left')\n",
        "    return train_df, test_df, feat_cols\n",
        "\n",
        "def add_single_column_from_submission(oof_path, submission_path, train_df, test_df, new_name):\n",
        "    oof_df = pd.read_csv(oof_path)\n",
        "    oof_cols = [c for c in oof_df.columns if c != 'id']\n",
        "    assert len(oof_cols) >= 1, f'No feature cols in {oof_path}'\n",
        "    col = oof_cols[-1]\n",
        "    oof_df = oof_df[['id', col]].rename(columns={col: new_name})\n",
        "    sub_df = pd.read_csv(submission_path)\n",
        "    sub_cols = [c for c in sub_df.columns if c != 'id']\n",
        "    assert len(sub_cols) >= 1, f'No feature cols in {submission_path}'\n",
        "    scol = sub_cols[-1]\n",
        "    sub_df = sub_df[['id', scol]].rename(columns={scol: new_name})\n",
        "    train_df = train_df.merge(oof_df, on='id', how='left')\n",
        "    test_df  = test_df.merge(sub_df, on='id', how='left')\n",
        "    return train_df, test_df, [new_name]"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folds: 5\n"
          ]
        }
      ]
    },
    {
      "id": "d44c2b9e-4c84-4200-abf0-476d158dd420",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time, gc, re\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy.stats import pearsonr\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Start from base frames\n",
        "trF = train[['id','fold','score']].copy()\n",
        "teF = test[['id']].copy()\n",
        "feature_cols = []\n",
        "\n",
        "def safe_add(oof_p, te_p, prefix=None):\n",
        "    global trF, teF, feature_cols\n",
        "    if Path(oof_p).exists() and Path(te_p).exists():\n",
        "        trF, teF, fc = add_feats(oof_p, te_p, trF, teF, prefix)\n",
        "        feature_cols.extend(fc)\n",
        "\n",
        "def safe_add_sub(oof_p, sub_p, new_name):\n",
        "    global trF, teF, feature_cols\n",
        "    if Path(oof_p).exists() and Path(sub_p).exists():\n",
        "        trF, teF, fc = add_single_column_from_submission(oof_p, sub_p, trF, teF, new_name)\n",
        "        feature_cols.extend(fc)\n",
        "\n",
        "# Embedding singles (if you made OOF+submission)\n",
        "safe_add_sub('oof_mpnet_st.csv','submission_mpnet_st.csv','mpnet_st_raw')\n",
        "safe_add_sub('oof_e5_asym.csv','submission_e5_asym.csv','e5_asym_raw')\n",
        "safe_add_sub('oof_bge.csv','submission_bge.csv','bge_raw')\n",
        "\n",
        "# Legacy lexical\n",
        "safe_add('oof_soft_tfidf.csv','soft_tfidf_test.csv','soft_tfidf')\n",
        "safe_add('oof_bm25_var.csv','bm25_var_test.csv','bm25')\n",
        "safe_add('oof_idf_overlap.csv','idf_overlap_test.csv','idf')\n",
        "safe_add('oof_char_edit.csv','char_edit_test.csv','char')\n",
        "safe_add('oof_lcs_char_ngrams.csv','lcs_char_ngrams_test.csv','lcs')\n",
        "safe_add('oof_fuzz.csv','fuzz_test.csv','fuzz')\n",
        "safe_add('oof_soft_align.csv','soft_align_test.csv','softalign')\n",
        "safe_add('oof_numeric_units.csv','numeric_units_test.csv','numunit')\n",
        "safe_add('oof_acronym.csv','acronym_test.csv','acronym')\n",
        "# New: char3 TF-IDF cosine\n",
        "safe_add('oof_char3_tfidf_cos.csv','char3_tfidf_cos_test.csv','char3')\n",
        "# New: char4/5 TF-IDF cosine\n",
        "safe_add('oof_char45_tfidf_cos.csv','char45_tfidf_cos_test.csv','char45')\n",
        "\n",
        "# Normalized lexical (keep alongside legacy; prune later)\n",
        "safe_add('oof_soft_tfidf_norm.csv','soft_tfidf_norm_test.csv','nsoft_tfidf')\n",
        "safe_add('oof_bm25_var_norm.csv','bm25_var_norm_test.csv','nbm25')\n",
        "safe_add('oof_idf_overlap_norm.csv','idf_overlap_norm_test.csv','nidf')\n",
        "safe_add('oof_norm_text.csv','norm_text_test.csv','nlex')\n",
        "\n",
        "# Monge\u2013Elkan (NEW)\n",
        "safe_add('oof_monge.csv','monge_test.csv','monge')\n",
        "\n",
        "# Cross-encoders (ingest OOF + submissions as single columns if present)\n",
        "safe_add_sub('oof_ce_minilm.csv','submission_ce_minilm.csv','ce_minilm_raw')\n",
        "safe_add_sub('oof_ce_large.csv','submission_ce_large.csv','ce_large_raw')\n",
        "safe_add_sub('oof_ce_stsb.csv','submission_ce_stsb.csv','ce_stsb_raw')\n",
        "safe_add_sub('oof_ce_bge_rerank.csv','submission_ce_bge_rerank.csv','ce_bge_rerank_raw')\n",
        "\n",
        "# CE transformed feature block (raw, iso, z, rank) from MiniLM CE\n",
        "safe_add('oof_ce_plain_feats.csv','ce_plain_feats_test.csv','ceplain')\n",
        "\n",
        "# Embedding transforms (fold-safe iso/z/rank for mpnet/e5/bge)\n",
        "safe_add('oof_embed_transforms.csv','embed_transforms_test.csv','emb')\n",
        "\n",
        "# PatentSBERTa transforms (raw/iso/z/rank)\n",
        "safe_add('oof_patentsberta.csv','patentsberta_test.csv','patberta')\n",
        "\n",
        "# anferico/bert-for-patents transforms (raw/iso/z/rank)\n",
        "safe_add('oof_bertpat.csv','bertpat_test.csv','bertpat')\n",
        "\n",
        "# Length + stopword-stripped overlaps (NEW)\n",
        "safe_add('oof_len_stop.csv','len_stop_test.csv','lenstop')\n",
        "\n",
        "# KNN meta from embeddings (NEW)\n",
        "safe_add('oof_knn_meta.csv','knn_meta_test.csv','knn')\n",
        "\n",
        "# Target encoding (NEW)\n",
        "if Path('oof_te.csv').exists() and Path('te_test.csv').exists():\n",
        "    trF, teF, fc = add_feats('oof_te.csv','te_test.csv', trF, teF, prefix='te')\n",
        "    feature_cols.extend(fc)\n",
        "\n",
        "print(f'Loaded features: {len(feature_cols)}', flush=True)\n",
        "\n",
        "# Build matrices\n",
        "y = trF['score'].values.astype(np.float32)\n",
        "X = trF[feature_cols].astype(np.float32)\n",
        "X_te = teF[feature_cols].astype(np.float32)\n",
        "\n",
        "# Fill and simple FE\n",
        "X = X.fillna(0.0)\n",
        "X_te = X_te.fillna(0.0)\n",
        "for c in ['soft_tfidf_sim','bm25_okapi_ab','bm25_okapi_ba','ce_minilm_raw','ce_large_raw']:\n",
        "    if c in X.columns:\n",
        "        X[f'{c}_sq'] = (X[c]**2).astype(np.float32)\n",
        "        X_te[f'{c}_sq'] = (X_te[c]**2).astype(np.float32)\n",
        "# Add squares for CE transformed features\n",
        "for c in list(X.columns):\n",
        "    if c.startswith('ceplain_'):\n",
        "        sq = f'{c}_sq'\n",
        "        if sq not in X.columns:\n",
        "            X[sq] = (X[c]**2).astype(np.float32)\n",
        "            X_te[sq] = (X_te[c]**2).astype(np.float32)\n",
        "# Add squares for char3/char45 tfidf cosine to increase capacity on these sims\n",
        "for c in list(X.columns):\n",
        "    if c.startswith('char3_') or c.startswith('char45_'):\n",
        "        sq = f'{c}_sq'\n",
        "        if sq not in X.columns:\n",
        "            X[sq] = (X[c]**2).astype(np.float32)\n",
        "            X_te[sq] = (X_te[c]**2).astype(np.float32)\n",
        "\n",
        "# Cheap CE*lexical interactions (limited) to squeeze extra signal\n",
        "ce_core = [c for c in X.columns if c in ('ceplain_ce_plain_iso','ceplain_ce_plain_raw')]\n",
        "sim_targets = []\n",
        "for c in X.columns:\n",
        "    name = c.lower()\n",
        "    if ('char3_tfidf_cos' in name) or ('char4_tfidf_cos' in name) or ('char5_tfidf_cos' in name):\n",
        "        sim_targets.append(c)\n",
        "    if ('bm25' in name) and (name.endswith('_okapi_ab') or name.endswith('_okapi_ba')):\n",
        "        sim_targets.append(c)\n",
        "# Optional small expansion: keep monge/soft-align OFF\n",
        "sim_targets = list(dict.fromkeys(sim_targets))\n",
        "for ce_c in ce_core:\n",
        "    for s_c in sim_targets:\n",
        "        ix = f'{ce_c}__x__{s_c}'\n",
        "        if ix not in X.columns:\n",
        "            X[ix] = (X[ce_c] * X[s_c]).astype(np.float32)\n",
        "            X_te[ix] = (X_te[ce_c] * X_te[s_c]).astype(np.float32)\n",
        "\n",
        "# Interactions: patent encoders (raw/iso) \u00d7 lexical sims (char3/char45, bm25 okapi ab/ba)\n",
        "enc_core = []\n",
        "for c in X.columns:\n",
        "    if (c.startswith('patberta_') or c.startswith('bertpat_')) and (c.endswith('_raw') or c.endswith('_iso')):\n",
        "        enc_core.append(c)\n",
        "enc_core = list(dict.fromkeys(enc_core))\n",
        "for e_c in enc_core:\n",
        "    for s_c in sim_targets:\n",
        "        ix = f'{e_c}__x__{s_c}'\n",
        "        if ix not in X.columns:\n",
        "            X[ix] = (X[e_c] * X[s_c]).astype(np.float32)\n",
        "            X_te[ix] = (X_te[e_c] * X_te[s_c]).astype(np.float32)\n",
        "\n",
        "# Drop near-constant\n",
        "std = X.std()\n",
        "keep = std[std > 1e-4].index.tolist()\n",
        "X = X[keep]; X_te = X_te[keep]\n",
        "\n",
        "# Redundancy pruning (|corr|>0.97), prefer CE highest, then TE, then normalized/embeds\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "pref = {c:0 for c in X.columns}\n",
        "for c in X.columns:\n",
        "    # Highest priority: any CE family (explicit ceplain_* or generic ce_ prefix)\n",
        "    if c.startswith(('ceplain', 'ce_')):\n",
        "        pref[c] = 4\n",
        "    # Next: target encodings\n",
        "    elif c.startswith('te_'):\n",
        "        pref[c] = 3\n",
        "    # Then normalized lexical, embedding transforms, and KNN metas\n",
        "    elif c.startswith(('nsoft','nbm25','nidf','nlex','emb_','patberta_','bertpat_','knn_')) or ('norm' in c):\n",
        "        pref[c] = 2\n",
        "    else:\n",
        "        pref[c] = pref.get(c, 0)\n",
        "\n",
        "def protect_key(col: str):\n",
        "    if col.startswith('ceplain'): return 'ceplain'\n",
        "    if col.startswith('ce_'): return 'ce'\n",
        "    if col.startswith('patberta_'): return 'patberta'\n",
        "    if col.startswith('bertpat_'): return 'bertpat'\n",
        "    if col.startswith('char3_'): return 'char3'\n",
        "    return None\n",
        "\n",
        "corr = X.corr().abs()\n",
        "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "drop = set()\n",
        "thr = 0.97\n",
        "for c in X.columns:\n",
        "    if c in drop: continue\n",
        "    partners = upper.index[upper[c] > thr].tolist()\n",
        "    for p in partners:\n",
        "        if p in drop: continue\n",
        "        pk_c = protect_key(c); pk_p = protect_key(p)\n",
        "        # Group-wise keep: don't drop within the same protected family\n",
        "        if pk_c is not None and pk_c == pk_p:\n",
        "            continue\n",
        "        keep_c = c if pref[c] >= pref[p] else p\n",
        "        drop_c = p if keep_c == c else c\n",
        "        drop.add(drop_c)\n",
        "cols = [c for c in X.columns if c not in drop]\n",
        "trX_f = X[cols].copy(); teX_f = X_te[cols].copy()\n",
        "n_ce_cols = sum([1 for c in trX_f.columns if c.startswith(('ceplain','ce_'))])\n",
        "print(f'Final features: {trX_f.shape[1]} (CE cols kept: {n_ce_cols})', flush=True)\n",
        "\n",
        "# Monotone constraints OFF for this run (more flexible LGBM)\n",
        "USE_MONO = False\n",
        "mono = []\n",
        "inc_pat = re.compile(r'(te_|bm25|tfidf|overlap|fuzz|align|sim|ce_|monge|ceplain|emb_|patberta|bertpat|knn_)', re.I)\n",
        "for c in trX_f.columns:\n",
        "    mono.append(1 if inc_pat.search(c) else 0)\n",
        "\n",
        "# Train LGBM 5-fold with seed bagging (stabilize oof/test)\n",
        "SEEDS = [42, 2025, 7]\n",
        "folds_arr = train['fold'].values.astype(int)\n",
        "oof_pred_seeds = []; test_pred_seeds = []\n",
        "for sd in SEEDS:\n",
        "    params = {\n",
        "        'objective':'regression','metric':'rmse','learning_rate':0.035,\n",
        "        'num_leaves':127,'min_data_in_leaf':32,'feature_fraction':0.8,\n",
        "        'bagging_fraction':0.8,'bagging_freq':1,'lambda_l1':0.05,'lambda_l2':1.0,\n",
        "        'seed':sd,'verbose':-1,'n_jobs':-1,\n",
        "    }\n",
        "    oof_pred = np.zeros(len(train), dtype=np.float32)\n",
        "    test_pred = np.zeros(len(test), dtype=np.float32)\n",
        "    for f in range(int(folds_arr.max())+1):\n",
        "        tr_idx = np.where(folds_arr!=f)[0]; va_idx = np.where(folds_arr==f)[0]\n",
        "        dtr = lgb.Dataset(trX_f.iloc[tr_idx].values, label=y[tr_idx], free_raw_data=False)\n",
        "        dva = lgb.Dataset(trX_f.iloc[va_idx].values, label=y[va_idx], free_raw_data=False)\n",
        "        p = params.copy()\n",
        "        if USE_MONO:\n",
        "            p['monotone_constraints'] = mono\n",
        "        booster = lgb.train(p, dtr, num_boost_round=20000, valid_sets=[dva],\n",
        "                            callbacks=[lgb.early_stopping(300, verbose=False)])\n",
        "        oof_pred[va_idx] = booster.predict(trX_f.iloc[va_idx].values, num_iteration=booster.best_iteration).astype(np.float32)\n",
        "        test_pred += booster.predict(teX_f.values, num_iteration=booster.best_iteration).astype(np.float32)\n",
        "        print(f'[LGBM sd={sd} Fold {f}] r={pearsonr(oof_pred[va_idx], y[va_idx])[0]:.6f}', flush=True)\n",
        "    test_pred /= (int(folds_arr.max())+1)\n",
        "    print(f'[LGBM seed {sd}] OOF r={pearsonr(oof_pred, y)[0]:.6f}', flush=True)\n",
        "    oof_pred_seeds.append(oof_pred); test_pred_seeds.append(test_pred)\n",
        "\n",
        "# Average across seeds\n",
        "oof_pred = np.mean(oof_pred_seeds, axis=0).astype(np.float32)\n",
        "test_pred = np.mean(test_pred_seeds, axis=0).astype(np.float32)\n",
        "print('LGBM (seed-bagged) OOF r=', pearsonr(oof_pred, y)[0])\n",
        "\n",
        "# Save + keep vars for downstream cells\n",
        "pd.DataFrame({'id': train['id'], 'oof': oof_pred}).to_csv('oof_stack_lgbm.csv', index=False)\n",
        "pd.DataFrame({'id': test['id'], 'score': np.clip(test_pred, 0.0, 1.0)}).to_csv('submission_stack_lgbm.csv', index=False)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded features: 216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final features: 122 (CE cols kept: 32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM sd=42 Fold 0] r=0.782490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM sd=42 Fold 1] r=0.775062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM sd=42 Fold 2] r=0.767728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM sd=42 Fold 3] r=0.777447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM sd=42 Fold 4] r=0.760847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM seed 42] OOF r=0.772405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM sd=2025 Fold 0] r=0.783774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM sd=2025 Fold 1] r=0.774771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM sd=2025 Fold 2] r=0.768888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM sd=2025 Fold 3] r=0.776623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM sd=2025 Fold 4] r=0.761805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM seed 2025] OOF r=0.772797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM sd=7 Fold 0] r=0.783449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM sd=7 Fold 1] r=0.773661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM sd=7 Fold 2] r=0.768969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM sd=7 Fold 3] r=0.777169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM sd=7 Fold 4] r=0.762570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LGBM seed 7] OOF r=0.772827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGBM (seed-bagged) OOF r= 0.7743376726078489\n"
          ]
        }
      ]
    },
    {
      "id": "d15d7482-4e5c-49fb-9c88-9dd93ce0ed88",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CatBoost and XGBoost secondary models + blends (with seed bagging)\n",
        "import sys, subprocess, numpy as np, pandas as pd, time, gc\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Ensure deps\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'catboost==1.2.7', 'xgboost==2.1.1'], check=True)\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "import xgboost as xgb\n",
        "\n",
        "assert 'trX_f' in globals() and 'teX_f' in globals() and 'y' in globals(), 'Design matrices not found; run cell 0 first'\n",
        "\n",
        "X = trX_f.values.astype(np.float32)\n",
        "X_te = teX_f.values.astype(np.float32)\n",
        "fold_arr = train['fold'].values.astype(int)\n",
        "n_te = len(test)\n",
        "\n",
        "# ---------- CatBoost with seed bagging ----------\n",
        "seeds = [42, 2025, 7, 101, 303]\n",
        "cat_oof_seeds, cat_te_seeds = [], []\n",
        "for sd in seeds:\n",
        "    cat_oof = np.zeros(len(train), dtype=np.float32)\n",
        "    cat_te_accum = np.zeros(n_te, dtype=np.float32)\n",
        "    for f in range(NUM_FOLDS):\n",
        "        f0 = time.time()\n",
        "        tr_idx = np.where(fold_arr != f)[0]\n",
        "        va_idx = np.where(fold_arr == f)[0]\n",
        "        tr_pool = Pool(X[tr_idx], label=y[tr_idx])\n",
        "        va_pool = Pool(X[va_idx], label=y[va_idx])\n",
        "        te_pool = Pool(X_te)\n",
        "        params = dict(\n",
        "            loss_function='RMSE',\n",
        "            depth=7,\n",
        "            learning_rate=0.05,\n",
        "            l2_leaf_reg=12.0,\n",
        "            subsample=0.8,\n",
        "            rsm=0.8,\n",
        "            random_seed=sd,\n",
        "            iterations=20000,\n",
        "            od_type='Iter',\n",
        "            od_wait=300,\n",
        "            verbose=False,\n",
        "            allow_writing_files=False,\n",
        "            thread_count=-1,\n",
        "        )\n",
        "        model = CatBoostRegressor(**params)\n",
        "        model.fit(tr_pool, eval_set=va_pool, use_best_model=True, verbose=False)\n",
        "        cat_oof[va_idx] = model.predict(va_pool).astype(np.float32)\n",
        "        cat_te_accum += model.predict(te_pool).astype(np.float32)\n",
        "        r = pearsonr(cat_oof[va_idx], y[va_idx])[0]\n",
        "        print(f'[CatBoost sd={sd} Fold {f}] r={r:.6f}; elapsed {time.time()-f0:.1f}s', flush=True)\n",
        "        del model, tr_pool, va_pool; gc.collect()\n",
        "    cat_te = cat_te_accum / NUM_FOLDS\n",
        "    print(f'[CatBoost seed {sd}] r={pearsonr(cat_oof, y)[0]:.6f}', flush=True)\n",
        "    cat_oof_seeds.append(cat_oof); cat_te_seeds.append(cat_te)\n",
        "\n",
        "cat_oof = np.mean(cat_oof_seeds, axis=0).astype(np.float32)\n",
        "cat_te  = np.mean(cat_te_seeds, axis=0).astype(np.float32)\n",
        "pd.DataFrame({'id': train['id'], 'oof_cat': cat_oof}).to_csv('oof_stack_cat.csv', index=False)\n",
        "pd.DataFrame({'id': test['id'], 'score': np.clip(cat_te, 0.0, 1.0)}).to_csv('submission_stack_cat.csv', index=False)\n",
        "print('CatBoost OOF r=', round(float(pearsonr(cat_oof, y)[0]), 6), flush=True)\n",
        "\n",
        "# ---------- XGBoost with seed bagging ----------\n",
        "xgb_oof_seeds, xgb_te_seeds = [], []\n",
        "for sd in seeds:\n",
        "    xgb_oof = np.zeros(len(train), dtype=np.float32)\n",
        "    xgb_te_accum = np.zeros(n_te, dtype=np.float32)\n",
        "    for f in range(NUM_FOLDS):\n",
        "        f0 = time.time()\n",
        "        tr_idx = np.where(fold_arr != f)[0]\n",
        "        va_idx = np.where(fold_arr == f)[0]\n",
        "        dtr = xgb.DMatrix(X[tr_idx], label=y[tr_idx])\n",
        "        dva = xgb.DMatrix(X[va_idx], label=y[va_idx])\n",
        "        dte = xgb.DMatrix(X_te)\n",
        "        params = {\n",
        "            'objective': 'reg:squarederror',\n",
        "            'eval_metric': 'rmse',\n",
        "            'eta': 0.05,\n",
        "            'max_depth': 7,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'lambda': 2.5,\n",
        "            'tree_method': 'hist',\n",
        "            'seed': sd,\n",
        "            'nthread': -1,\n",
        "        }\n",
        "        evallist = [(dva, 'val')]\n",
        "        booster = xgb.train(params, dtr, num_boost_round=6000, evals=evallist, early_stopping_rounds=300, verbose_eval=100)\n",
        "        xgb_oof[va_idx] = booster.predict(dva, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        xgb_te_accum += booster.predict(dte, iteration_range=(0, booster.best_iteration+1)).astype(np.float32)\n",
        "        r = pearsonr(xgb_oof[va_idx], y[va_idx])[0]\n",
        "        print(f'[XGB sd={sd} Fold {f}] r={r:.6f}; elapsed {time.time()-f0:.1f}s', flush=True)\n",
        "        del booster, dtr, dva; gc.collect()\n",
        "    xgb_te = xgb_te_accum / NUM_FOLDS\n",
        "    print(f'[XGB seed {sd}] r={pearsonr(xgb_oof, y)[0]:.6f}', flush=True)\n",
        "    xgb_oof_seeds.append(xgb_oof); xgb_te_seeds.append(xgb_te)\n",
        "\n",
        "xgb_oof = np.mean(xgb_oof_seeds, axis=0).astype(np.float32)\n",
        "xgb_te  = np.mean(xgb_te_seeds, axis=0).astype(np.float32)\n",
        "pd.DataFrame({'id': train['id'], 'oof_xgb': xgb_oof}).to_csv('oof_stack_xgb.csv', index=False)\n",
        "pd.DataFrame({'id': test['id'], 'score': np.clip(xgb_te, 0.0, 1.0)}).to_csv('submission_stack_xgb.csv', index=False)\n",
        "print('XGBoost OOF r=', round(float(pearsonr(xgb_oof, y)[0]), 6), flush=True)\n",
        "\n",
        "# ---------- Simple blends ----------\n",
        "artifacts = {}\n",
        "if 'oof_pred' in globals():\n",
        "    artifacts['lgbm'] = (oof_pred, test_pred)\n",
        "artifacts['cat'] = (cat_oof, cat_te)\n",
        "artifacts['xgb'] = (xgb_oof, xgb_te)\n",
        "if 'ridge_oof' in globals() and 'ridge_te' in globals():\n",
        "    artifacts['ridge'] = (ridge_oof, ridge_te)\n",
        "\n",
        "def try_blend(keys, weights_grid):\n",
        "    best = (-1.0, None, None)\n",
        "    for ws in weights_grid:\n",
        "        assert abs(sum(ws)-1.0) < 1e-6\n",
        "        oof_b = np.zeros_like(y, dtype=np.float32)\n",
        "        te_b = np.zeros(n_te, dtype=np.float32)\n",
        "        for k, w in zip(keys, ws):\n",
        "            oof_b += w * artifacts[k][0]\n",
        "            te_b  += w * artifacts[k][1]\n",
        "        r = pearsonr(oof_b, y)[0]\n",
        "        if r > best[0]:\n",
        "            best = (r, ws, te_b.copy())\n",
        "    return best\n",
        "\n",
        "# 2-way blends\n",
        "pairs = [('lgbm','cat'), ('lgbm','xgb'), ('cat','xgb')] if 'lgbm' in artifacts else [('cat','xgb')]\n",
        "w2 = [(0.5,0.5), (0.6,0.4), (0.7,0.3), (0.4,0.6)]\n",
        "for a,b in pairs:\n",
        "    r, ws, te_b = try_blend([a,b], w2)\n",
        "    print(f'Best 2-way {a}+{b}: r={r:.6f} weights={ws}', flush=True)\n",
        "\n",
        "# 3-way blend lgbm+cat+xgb\n",
        "if all(k in artifacts for k in ('lgbm','cat','xgb')):\n",
        "    w3 = [\n",
        "        (0.4,0.3,0.3), (0.5,0.3,0.2), (0.5,0.25,0.25),\n",
        "        (0.33,0.33,0.34), (0.6,0.2,0.2)\n",
        "    ]\n",
        "    r3, ws3, te3 = try_blend(['lgbm','cat','xgb'], w3)\n",
        "    print(f'Best 3-way lgbm+cat+xgb: r={r3:.6f} weights={ws3}', flush=True)\n",
        "\n",
        "print('Done secondary models.', flush=True)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorch 2.4.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.4.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.8.90 which is incompatible.\ntorch 2.4.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.8.93 which is incompatible.\ntorch 2.4.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.8.90 which is incompatible.\ntorch 2.4.1+cu121 requires nvidia-cudnn-cu12==9.1.0.70; platform_system "
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Design matrices not found; run cell 0 first",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CatBoostRegressor, Pool\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgb\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mtrX_f\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mteX_f\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m(), \u001b[33m'\u001b[39m\u001b[33mDesign matrices not found; run cell 0 first\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     12\u001b[39m X = trX_f.values.astype(np.float32)\n\u001b[32m     13\u001b[39m X_te = teX_f.values.astype(np.float32)\n",
            "\u001b[31mAssertionError\u001b[39m: Design matrices not found; run cell 0 first"
          ]
        }
      ]
    },
    {
      "id": "fce0e975-5675-4f61-8827-06d95956e102",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ElasticNet on dense feature matrix as an extra calibrated base\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "\n",
        "assert 'trX_f' in globals() and 'teX_f' in globals() and 'y' in globals() and 'NUM_FOLDS' in globals(), 'Run cells 0-1 first'\n",
        "\n",
        "folds_arr = train['fold'].values.astype(int)\n",
        "X_all = trX_f.values.astype(np.float32); X_te_all = teX_f.values.astype(np.float32)\n",
        "alphas = [0.003, 0.01]; l1s = [0.3, 0.5]\n",
        "best = (-1.0, None, None)\n",
        "for a in alphas:\n",
        "    for l1 in l1s:\n",
        "        oof = np.zeros(len(train), dtype=np.float32); te_acc = np.zeros(len(test), dtype=np.float32)\n",
        "        for f in range(NUM_FOLDS):\n",
        "            tr = folds_arr!=f; va = folds_arr==f\n",
        "            ss = StandardScaler(with_mean=True, with_std=True)\n",
        "            Xtr = ss.fit_transform(X_all[tr]); Xva = ss.transform(X_all[va]); Xte = ss.transform(X_te_all)\n",
        "            en = ElasticNet(alpha=a, l1_ratio=l1, random_state=42, max_iter=4000)\n",
        "            en.fit(Xtr, y[tr])\n",
        "            oof[va] = en.predict(Xva).astype(np.float32)\n",
        "            te_acc += en.predict(Xte).astype(np.float32)\n",
        "        r = pearsonr(oof, y)[0]\n",
        "        print(f'ElasticNet dense a={a} l1={l1} OOF r={r:.6f}', flush=True)\n",
        "        if r > best[0]:\n",
        "            best = (r, oof.copy(), (te_acc/NUM_FOLDS).astype(np.float32))\n",
        "print(f'ElasticNet dense BEST OOF r={best[0]:.6f}', flush=True)\n",
        "elastic_oof, elastic_te = best[1].astype(np.float32), best[2].astype(np.float32)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet dense a=0.003 l1=0.3 OOF r=0.738455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet dense a=0.003 l1=0.5 OOF r=0.736260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet dense a=0.01 l1=0.3 OOF r=0.731633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet dense a=0.01 l1=0.5 OOF r=0.727119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet dense BEST OOF r=0.738455\n"
          ]
        }
      ]
    },
    {
      "id": "7c6fe06a-c0fc-4092-b38e-a272cd462587",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# NNLS meta-blend over model OOFs + rank-average baseline + fold-safe isotonic calibration\n",
        "import numpy as np, pandas as pd, time\n",
        "from scipy.optimize import nnls\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from pathlib import Path\n",
        "\n",
        "assert 'y' in globals(), 'Run cell 0 first to define y'\n",
        "\n",
        "# Load train/test ids early for file-based fallbacks\n",
        "train_df_ids = pd.read_csv('train.csv')[['id']]\n",
        "test_df_ids  = pd.read_csv('test.csv')[['id']]\n",
        "\n",
        "# Helpers to load bases from disk if globals are missing\n",
        "def try_load_base_from_files(oof_path, sub_path, oof_col, sub_col='score'):\n",
        "    if Path(oof_path).exists() and Path(sub_path).exists():\n",
        "        try:\n",
        "            oof_df = pd.read_csv(oof_path); sub_df = pd.read_csv(sub_path)\n",
        "            oof_arr = train_df_ids.merge(oof_df, on='id', how='left')[oof_col].astype(np.float32).values\n",
        "            te_arr  = test_df_ids.merge(sub_df, on='id', how='left')[sub_col].astype(np.float32).values\n",
        "            return oof_arr, te_arr\n",
        "        except Exception as e:\n",
        "            print(f'WARN: failed loading {oof_path}/{sub_path}:', e, flush=True)\n",
        "    return None, None\n",
        "\n",
        "# Attempt to populate missing globals from saved artifacts\n",
        "if not ('oof_pred' in globals() and 'test_pred' in globals()):\n",
        "    lgbm_oof_f, lgbm_te_f = try_load_base_from_files('oof_stack_lgbm.csv','submission_stack_lgbm.csv','oof','score')\n",
        "    if lgbm_oof_f is not None:\n",
        "        oof_pred = lgbm_oof_f; test_pred = lgbm_te_f\n",
        "if not ('cat_oof' in globals() and 'cat_te' in globals()):\n",
        "    cat_oof_f, cat_te_f = try_load_base_from_files('oof_stack_cat.csv','submission_stack_cat.csv','oof_cat','score')\n",
        "    if cat_oof_f is not None:\n",
        "        cat_oof = cat_oof_f; cat_te = cat_te_f\n",
        "if not ('xgb_oof' in globals() and 'xgb_te' in globals()):\n",
        "    xgb_oof_f, xgb_te_f = try_load_base_from_files('oof_stack_xgb.csv','submission_stack_xgb.csv','oof_xgb','score')\n",
        "    if xgb_oof_f is not None:\n",
        "        xgb_oof = xgb_oof_f; xgb_te = xgb_te_f\n",
        "\n",
        "# Collect available base models\n",
        "bases = []\n",
        "if 'oof_pred' in globals() and 'test_pred' in globals():\n",
        "    bases.append(('lgbm', oof_pred.astype(np.float32), test_pred.astype(np.float32)))\n",
        "if 'cat_oof' in globals() and 'cat_te' in globals():\n",
        "    bases.append(('cat', cat_oof.astype(np.float32), cat_te.astype(np.float32)))\n",
        "if 'xgb_oof' in globals() and 'xgb_te' in globals():\n",
        "    bases.append(('xgb', xgb_oof.astype(np.float32), xgb_te.astype(np.float32)))\n",
        "if 'ridge_oof' in globals() and 'ridge_te' in globals():\n",
        "    bases.append(('ridge', ridge_oof.astype(np.float32), ridge_te.astype(np.float32)))\n",
        "if 'ce_meta_oof' in globals() and 'ce_meta_te' in globals():\n",
        "    bases.append(('ce_meta', ce_meta_oof.astype(np.float32), ce_meta_te.astype(np.float32)))\n",
        "# Optional dense ElasticNet base (if computed in prior cell)\n",
        "if 'elastic_oof' in globals() and 'elastic_te' in globals():\n",
        "    bases.append(('elastic', elastic_oof.astype(np.float32), elastic_te.astype(np.float32)))\n",
        "\n",
        "# Priority 1: Add TF-IDF Ridge as an external calibrated NNLS base if files exist\n",
        "def load_single_col(path):\n",
        "    df = pd.read_csv(path)\n",
        "    cols = [c for c in df.columns if c != 'id']\n",
        "    assert len(cols) >= 1, f'No non-id column in {path}'\n",
        "    return df[cols[-1]].values.astype(np.float32)\n",
        "\n",
        "tfidf_oof = tfidf_te = None\n",
        "if Path('oof_tfidf_ridge.csv').exists() and Path('submission_tfidf.csv').exists():\n",
        "    try:\n",
        "        tfidf_oof = load_single_col('oof_tfidf_ridge.csv')\n",
        "        tfidf_te  = load_single_col('submission_tfidf.csv')\n",
        "        bases.append(('tfidf_ridge', tfidf_oof, tfidf_te))\n",
        "        print('Added TF-IDF Ridge base to NNLS.')\n",
        "    except Exception as e:\n",
        "        print('Failed to load TF-IDF Ridge base:', e)\n",
        "\n",
        "# Keep only strongest bases for NNLS (include tfidf_ridge first, then exclude per expert tweak)\n",
        "keep = {'lgbm','cat','xgb','tfidf_ridge'}\n",
        "bases = [b for b in bases if b[0] in keep]\n",
        "print('Bases kept for NNLS (pre-exclude):', [b[0] for b in bases], flush=True)\n",
        "\n",
        "# Exclude tfidf_ridge for lean 3-base + ranks\n",
        "bases = [b for b in bases if b[0] != 'tfidf_ridge']\n",
        "print('Bases kept for NNLS (final):', [b[0] for b in bases], flush=True)\n",
        "\n",
        "# Local, fold-safe fold array independent of notebook state\n",
        "folds_df = pd.read_csv('folds_by_id.csv')  # id, fold\n",
        "merged_folds = train_df_ids.merge(folds_df, on='id', how='left', validate='one_to_one')\n",
        "fold_arr = merged_folds['fold'].values.astype(int)\n",
        "NUM_FOLDS = int(merged_folds['fold'].max()) + 1\n",
        "\n",
        "def fold_iso_with_te(oof, te, y, folds):\n",
        "    o2 = np.zeros_like(oof, np.float32); te_list = []\n",
        "    F = int(folds.max()) + 1\n",
        "    for f in range(F):\n",
        "        tr = folds != f; va = folds == f\n",
        "        iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "        iso.fit(oof[tr], y[tr])\n",
        "        o2[va] = iso.transform(oof[va]).astype(np.float32)\n",
        "        te_list.append(iso.transform(te).astype(np.float32))\n",
        "    te_avg = (np.mean(np.stack(te_list, axis=0), axis=0)).astype(np.float32)\n",
        "    return o2, te_avg, te_list  # calibrated OOF, calibrated test (avg), per-fold calibrated test list\n",
        "\n",
        "def fold_rank_from_calibrated(cal_oof, te_folds, folds):\n",
        "    F = int(folds.max()) + 1\n",
        "    r_oof = np.zeros_like(cal_oof, np.float32)\n",
        "    te_acc = np.zeros_like(te_folds[0], np.float64)\n",
        "    for f in range(F):\n",
        "        tr = folds != f; va = folds == f\n",
        "        ref = np.sort(cal_oof[tr].astype(np.float32))\n",
        "        if ref.size == 0:\n",
        "            continue\n",
        "        j_va = np.searchsorted(ref, cal_oof[va], side='right')\n",
        "        r_oof[va] = (j_va / max(ref.size - 1, 1)).astype(np.float32)\n",
        "        j_te = np.searchsorted(ref, te_folds[f], side='right')\n",
        "        te_acc += (j_te / max(ref.size - 1, 1)).astype(np.float64)\n",
        "    r_te = (te_acc / F).astype(np.float32)\n",
        "    return r_oof, r_te\n",
        "\n",
        "# New: fold-safe z-score copies from calibrated\n",
        "def fold_z_from_calibrated(cal_oof, te_folds, folds):\n",
        "    F = int(folds.max()) + 1\n",
        "    z_oof = np.zeros_like(cal_oof, np.float32)\n",
        "    te_acc = np.zeros_like(te_folds[0], np.float64)\n",
        "    for f in range(F):\n",
        "        tr = folds != f; va = folds == f\n",
        "        m = float(cal_oof[tr].mean()); s = float(cal_oof[tr].std() + 1e-8)\n",
        "        z_oof[va] = ((cal_oof[va] - m) / s).astype(np.float32)\n",
        "        te_acc += ((te_folds[f] - m) / s).astype(np.float64)\n",
        "    z_te = (te_acc / F).astype(np.float32)\n",
        "    return z_oof, z_te\n",
        "\n",
        "# Enable rank copies as advised\n",
        "USE_RANK_COPIES = True\n",
        "\n",
        "# Build design matrix with raw, calibrated, rank, and z-score copies\n",
        "names_all = []\n",
        "blocks_tr = []\n",
        "blocks_te = []\n",
        "\n",
        "for name, tr, te in bases:\n",
        "    # raw copies\n",
        "    names_all.append(name + '_raw'); blocks_tr.append(tr.reshape(-1,1)); blocks_te.append(te.reshape(-1,1))\n",
        "    # calibrated copies\n",
        "    cal_tr, cal_te, cal_te_folds = fold_iso_with_te(tr, te, y.astype(np.float32), fold_arr)\n",
        "    names_all.append(name); blocks_tr.append(cal_tr.reshape(-1,1)); blocks_te.append(cal_te.reshape(-1,1))\n",
        "    # rank copies from calibrated\n",
        "    if USE_RANK_COPIES:\n",
        "        r_tr, r_te = fold_rank_from_calibrated(cal_tr, cal_te_folds, fold_arr)\n",
        "        names_all.append(name + '_rank'); blocks_tr.append(r_tr.reshape(-1,1)); blocks_te.append(r_te.reshape(-1,1))\n",
        "    # z-score copies from calibrated\n",
        "    z_tr, z_te = fold_z_from_calibrated(cal_tr, cal_te_folds, fold_arr)\n",
        "    names_all.append(name + '_z'); blocks_tr.append(z_tr.reshape(-1,1)); blocks_te.append(z_te.reshape(-1,1))\n",
        "\n",
        "P_tr = np.hstack(blocks_tr).astype(np.float64) if blocks_tr else None\n",
        "P_te = np.hstack(blocks_te).astype(np.float64) if blocks_te else None\n",
        "y_vec = y.astype(np.float64)\n",
        "\n",
        "print('NNLS over bases:', names_all, flush=True)\n",
        "\n",
        "# NNLS weights with tiny L2 (non-negative ridge via augmentation)\n",
        "def fit_nnls_l2(P_tr, y_vec, alpha: float):\n",
        "    K = P_tr.shape[1]\n",
        "    if alpha <= 0:\n",
        "        w, _ = nnls(P_tr, y_vec)\n",
        "    else:\n",
        "        A = np.vstack([P_tr, np.sqrt(alpha) * np.eye(K, dtype=np.float64)])\n",
        "        b = np.concatenate([y_vec, np.zeros(K, dtype=np.float64)])\n",
        "        w, _ = nnls(A, b)\n",
        "    s = w.sum() if w.sum() > 0 else 1.0\n",
        "    return w / s\n",
        "\n",
        "alphas = [0.0, 1e-5, 5e-5, 1e-4, 5e-4]\n",
        "best = (-1.0, None)\n",
        "for a in alphas:\n",
        "    w_try = fit_nnls_l2(P_tr, y_vec, a)\n",
        "    r_try = pearsonr((P_tr @ w_try).astype(np.float32), y)[0]\n",
        "    print(f'NNLS L2 alpha={a:g} OOF r={r_try:.6f}')\n",
        "    if r_try > best[0]:\n",
        "        best = (r_try, w_try)\n",
        "best_r_nnls, w_norm = best\n",
        "print('Chosen NNLS (possibly L2) OOF r=', round(float(best_r_nnls),6))\n",
        "blend_oof = (P_tr @ w_norm).astype(np.float32)\n",
        "blend_te  = (P_te @ w_norm).astype(np.float32)\n",
        "\n",
        "# Optional tiny manual 3-way sweep on calibrated lgbm/cat/xgb (no ranks), per expert advice\n",
        "sweep_best_r = -1.0; sweep_best_te = None; sweep_best_oof = None\n",
        "name_to_idx = {n:i for i,n in enumerate(names_all)}\n",
        "have_three = all(k in name_to_idx for k in ('lgbm','cat','xgb'))\n",
        "if have_three:\n",
        "    i_l, i_c, i_x = name_to_idx['lgbm'], name_to_idx['cat'], name_to_idx['xgb']\n",
        "    col_l = P_tr[:, i_l]; col_c = P_tr[:, i_c]; col_x = P_tr[:, i_x]\n",
        "    te_l = P_te[:, i_l]; te_c = P_te[:, i_c]; te_x = P_te[:, i_x]\n",
        "    grid_l = [0.18, 0.19, 0.20, 0.21, 0.22]\n",
        "    grid_c = [0.57, 0.58, 0.59, 0.60, 0.61]\n",
        "    for wl in grid_l:\n",
        "        for wc in grid_c:\n",
        "            wx = 1.0 - wl - wc\n",
        "            if wx < 0.17 or wx > 0.23:\n",
        "                continue\n",
        "            oof_try = (wl*col_l + wc*col_c + wx*col_x).astype(np.float32)\n",
        "            r_try = pearsonr(oof_try, y)[0]\n",
        "            if r_try > sweep_best_r:\n",
        "                sweep_best_r = r_try\n",
        "                sweep_best_oof = oof_try\n",
        "                sweep_best_te = (wl*te_l + wc*te_c + wx*te_x).astype(np.float32)\n",
        "    if sweep_best_r > -1:\n",
        "        print('Manual sweep best OOF r=', round(float(sweep_best_r),6), flush=True)\n",
        "\n",
        "# Rank-average baseline (robustness check)\n",
        "def rank01(a):\n",
        "    order = a.argsort(kind='mergesort')\n",
        "    ranks = np.empty_like(order, dtype=np.float64)\n",
        "    ranks[order] = np.arange(len(a), dtype=np.float64)\n",
        "    return ranks / max(len(a)-1, 1)\n",
        "\n",
        "K = P_tr.shape[1]\n",
        "ranks_tr = [rank01(P_tr[:,i]) for i in range(K)]\n",
        "ranks_te = [rank01(P_te[:,i]) for i in range(K)]\n",
        "rank_avg_oof = np.mean(np.vstack(ranks_tr), axis=0).astype(np.float32)\n",
        "rank_avg_te = np.mean(np.vstack(ranks_te), axis=0).astype(np.float32)\n",
        "r_rank = pearsonr(rank_avg_oof, y)[0]\n",
        "print('Rank-average OOF r=', round(float(r_rank), 6), flush=True)\n",
        "\n",
        "# Choose best between NNLS and (if available) manual sweep\n",
        "best_oof = blend_oof\n",
        "best_te = blend_te\n",
        "best_r = best_r_nnls\n",
        "if sweep_best_oof is not None and float(sweep_best_r) > float(best_r):\n",
        "    best_oof = sweep_best_oof\n",
        "    best_te = sweep_best_te\n",
        "    best_r = sweep_best_r\n",
        "    print('Using manual 3-way sweep (beats NNLS). Final OOF r=', round(float(best_r),6), flush=True)\n",
        "else:\n",
        "    print('Using NNLS. Final OOF r=', round(float(best_r),6), flush=True)\n",
        "\n",
        "# Save artifacts\n",
        "pd.DataFrame({'id': train_df_ids['id'], 'oof': best_oof}).to_csv('oof_stack_nnls_raw.csv', index=False)\n",
        "pd.DataFrame({'id': test_df_ids['id'], 'score': np.clip(best_te, 0.0, 1.0)}).to_csv('submission_stack_nnls_raw.csv', index=False)\n",
        "print('Saved calibrated NNLS blend (or manual sweep if better).', flush=True)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added TF-IDF Ridge base to NNLS.\nBases kept for NNLS (pre-exclude): ['lgbm', 'cat', 'xgb', 'tfidf_ridge']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bases kept for NNLS (final): ['lgbm', 'cat', 'xgb']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNLS over bases: ['lgbm_raw', 'lgbm', 'lgbm_rank', 'lgbm_z', 'cat_raw', 'cat', 'cat_rank', 'cat_z', 'xgb_raw', 'xgb', 'xgb_rank', 'xgb_z']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNLS L2 alpha=0 OOF r=0.776951\nNNLS L2 alpha=1e-05 OOF r=0.776951\nNNLS L2 alpha=5e-05 OOF r=0.776951\nNNLS L2 alpha=0.0001 OOF r=0.776951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNLS L2 alpha=0.0005 OOF r=0.776951\nChosen NNLS (possibly L2) OOF r= 0.776951\nManual sweep best OOF r= 0.77675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank-average OOF r= 0.742897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using NNLS. Final OOF r= 0.776951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved calibrated NNLS blend (or manual sweep if better).\n"
          ]
        }
      ]
    },
    {
      "id": "02880869-fbd6-43c3-a028-887d8a09c6aa",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fold-safe post-calibration of final NNLS blend: global + per-anchor + per-CPC3 isotonic\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Load train/test and raw NNLS outputs\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df  = pd.read_csv('test.csv')\n",
        "folds    = pd.read_csv('folds_by_id.csv')\n",
        "oof_raw  = pd.read_csv('oof_stack_nnls_raw.csv')  # id,oof\n",
        "sub_raw  = pd.read_csv('submission_stack_nnls_raw.csv')  # id,score\n",
        "\n",
        "train_df = train_df.merge(folds, on='id', how='left', validate='one_to_one')\n",
        "train_df['fold'] = train_df['fold'].astype(int)\n",
        "y = train_df['score'].astype(np.float32).values\n",
        "pred_tr = train_df[['id']].merge(oof_raw, on='id', how='left')['oof'].astype(np.float32).values\n",
        "pred_te = test_df[['id']].merge(sub_raw, on='id', how='left')['score'].astype(np.float32).values\n",
        "anchors_tr = train_df['anchor'].astype(str).values\n",
        "cpc3_tr    = train_df['context'].astype(str).str[:3].values\n",
        "anchors_te = test_df['anchor'].astype(str).values\n",
        "cpc3_te    = test_df['context'].astype(str).str[:3].values\n",
        "fold_arr   = train_df['fold'].values.astype(int)\n",
        "F = int(fold_arr.max()) + 1\n",
        "\n",
        "oof_global = np.zeros_like(pred_tr, dtype=np.float32)\n",
        "oof_anchor = np.zeros_like(pred_tr, dtype=np.float32)\n",
        "oof_cpc3   = np.zeros_like(pred_tr, dtype=np.float32)\n",
        "te_global_acc = np.zeros_like(pred_te, dtype=np.float64)\n",
        "te_anchor_acc = np.zeros_like(pred_te, dtype=np.float64)\n",
        "te_cpc3_acc   = np.zeros_like(pred_te, dtype=np.float64)\n",
        "\n",
        "MIN_GRP = 20  # minimum train instances to fit a group iso; else fallback to global\n",
        "\n",
        "for f in range(F):\n",
        "    tr = fold_arr != f; va = fold_arr == f\n",
        "    # Global isotonic on train-only\n",
        "    iso_g = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "    iso_g.fit(pred_tr[tr], y[tr])\n",
        "    oof_global[va] = iso_g.transform(pred_tr[va]).astype(np.float32)\n",
        "    te_global_acc += iso_g.transform(pred_te).astype(np.float64)\n",
        "\n",
        "    # Build group maps from train-only\n",
        "    # anchors\n",
        "    anchor_to_idx = {}\n",
        "    for i in np.where(tr)[0]:\n",
        "        a = anchors_tr[i]\n",
        "        if a not in anchor_to_idx: anchor_to_idx[a] = []\n",
        "        anchor_to_idx[a].append(i)\n",
        "    # cpc3\n",
        "    cpc3_to_idx = {}\n",
        "    for i in np.where(tr)[0]:\n",
        "        c = cpc3_tr[i]\n",
        "        if c not in cpc3_to_idx: cpc3_to_idx[c] = []\n",
        "        cpc3_to_idx[c].append(i)\n",
        "\n",
        "    # Fit per-anchor iso where enough samples, else use global\n",
        "    anchor_iso = {}\n",
        "    for a, idxs in anchor_to_idx.items():\n",
        "        if len(idxs) >= MIN_GRP:\n",
        "            iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "            iso.fit(pred_tr[idxs], y[idxs])\n",
        "            anchor_iso[a] = iso\n",
        "    # Apply to validation fold\n",
        "    for i in np.where(va)[0]:\n",
        "        a = anchors_tr[i]\n",
        "        if a in anchor_iso:\n",
        "            oof_anchor[i] = anchor_iso[a].transform([pred_tr[i]]).astype(np.float32)[0]\n",
        "        else:\n",
        "            oof_anchor[i] = oof_global[i]\n",
        "    # Apply to test and accumulate\n",
        "    tmp = np.zeros(len(test_df), dtype=np.float64)\n",
        "    for j in range(len(test_df)):\n",
        "        a = anchors_te[j]\n",
        "        if a in anchor_iso:\n",
        "            tmp[j] = float(anchor_iso[a].transform([pred_te[j]])[0])\n",
        "        else:\n",
        "            tmp[j] = float(iso_g.transform([pred_te[j]])[0])\n",
        "    te_anchor_acc += tmp\n",
        "\n",
        "    # Fit per-CPC3 iso where enough samples\n",
        "    cpc3_iso = {}\n",
        "    for c, idxs in cpc3_to_idx.items():\n",
        "        if len(idxs) >= MIN_GRP:\n",
        "            iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "            iso.fit(pred_tr[idxs], y[idxs])\n",
        "            cpc3_iso[c] = iso\n",
        "    # Apply to validation fold\n",
        "    for i in np.where(va)[0]:\n",
        "        c = cpc3_tr[i]\n",
        "        if c in cpc3_iso:\n",
        "            oof_cpc3[i] = cpc3_iso[c].transform([pred_tr[i]]).astype(np.float32)[0]\n",
        "        else:\n",
        "            oof_cpc3[i] = oof_global[i]\n",
        "    # Apply to test and accumulate\n",
        "    tmp2 = np.zeros(len(test_df), dtype=np.float64)\n",
        "    for j in range(len(test_df)):\n",
        "        c = cpc3_te[j]\n",
        "        if c in cpc3_iso:\n",
        "            tmp2[j] = float(cpc3_iso[c].transform([pred_te[j]])[0])\n",
        "        else:\n",
        "            tmp2[j] = float(iso_g.transform([pred_te[j]])[0])\n",
        "    te_cpc3_acc += tmp2\n",
        "\n",
        "# Average test across folds\n",
        "te_global = (te_global_acc / F).astype(np.float32)\n",
        "te_anchor = (te_anchor_acc / F).astype(np.float32)\n",
        "te_cpc3   = (te_cpc3_acc / F).astype(np.float32)\n",
        "\n",
        "# Simple hierarchy at OOF: prefer anchor if differs from global (i.e., had model), else cpc3, else global\n",
        "use_anchor = (np.abs(oof_anchor - oof_global) > 1e-12)\n",
        "use_cpc3   = (~use_anchor) & (np.abs(oof_cpc3 - oof_global) > 1e-12)\n",
        "oof_cal = oof_global.copy()\n",
        "oof_cal[use_cpc3] = oof_cpc3[use_cpc3]\n",
        "oof_cal[use_anchor] = oof_anchor[use_anchor]\n",
        "\n",
        "# Same hierarchy for test based on availability proportions (approximate via train groups):\n",
        "# If an anchor had a model in at least one fold (captured by te_anchor != te_global), prefer it; else if cpc3 had a model, use it; else global.\n",
        "use_anchor_te = (np.abs(te_anchor - te_global) > 1e-8)\n",
        "use_cpc3_te   = (~use_anchor_te) & (np.abs(te_cpc3 - te_global) > 1e-8)\n",
        "te_cal = te_global.copy()\n",
        "te_cal[use_cpc3_te] = te_cpc3[use_cpc3_te]\n",
        "te_cal[use_anchor_te] = te_anchor[use_anchor_te]\n",
        "\n",
        "# Evaluate and save\n",
        "r_raw = pearsonr(pred_tr, y)[0]\n",
        "r_cal = pearsonr(oof_cal, y)[0]\n",
        "print('Post-calibration OOF r: raw=', round(float(r_raw), 6), 'calibrated=', round(float(r_cal), 6), 'delta=', round(float(r_cal - r_raw), 6), flush=True)\n",
        "\n",
        "pd.DataFrame({'id': train_df['id'], 'oof': oof_cal.astype(np.float32)}).to_csv('oof_stack_nnls_calibrated.csv', index=False)\n",
        "pd.DataFrame({'id': test_df['id'], 'score': np.clip(te_cal.astype(np.float32), 0.0, 1.0)}).to_csv('submission_stack_nnls_calibrated.csv', index=False)\n",
        "print('Saved submission_stack_nnls_calibrated.csv', flush=True)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "id": "a095bd5d-04df-48f4-bf05-0e6efb4cc689",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# CE diagnostics + CE-only ElasticNet meta (fold-safe)\n",
        "import numpy as np, pandas as pd, time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "assert 'train' in globals() and 'test' in globals() and 'trX_f' in globals() and 'teX_f' in globals() and 'y' in globals(), 'Run cell 0 first'\n",
        "\n",
        "# Identify CE families present\n",
        "ce_tags = []\n",
        "for tag in ['ce_large','ce_bge_rerank','ce_l12','ce_stsb']:\n",
        "    if f'{tag}_raw' in trX_f.columns and f'{tag}_iso' in trX_f.columns:\n",
        "        ce_tags.append(tag)\n",
        "print('CE tags found:', ce_tags, flush=True)\n",
        "\n",
        "# Diagnostics: OOF Pearson for ce_*_raw and ce_*_iso\n",
        "fold_arr = train['fold'].values.astype(int)\n",
        "good_ce = []\n",
        "for tag in ce_tags:\n",
        "    r_raw = pearsonr(trX_f[f'{tag}_raw'].astype(float).values, y)[0]\n",
        "    r_iso = pearsonr(trX_f[f'{tag}_iso'].astype(float).values, y)[0]\n",
        "    print(f'{tag}: raw r={r_raw:.6f}, iso r={r_iso:.6f}', flush=True)\n",
        "    if (r_raw is not None and r_raw >= 0.65) and (r_iso is not None and r_iso >= 0.60):\n",
        "        good_ce.append(tag)\n",
        "print('Good CE tags kept:', good_ce, flush=True)\n",
        "if not good_ce:\n",
        "    good_ce = ce_tags  # fallback keep all\n",
        "\n",
        "# Build CE-only feature matrices\n",
        "keep_cols = []\n",
        "for tag in good_ce:\n",
        "    for c in (f'{tag}_raw', f'{tag}_iso', f'{tag}_raw_sq', f'{tag}_raw_x_iso'):\n",
        "        if c in trX_f.columns:\n",
        "            keep_cols.append(c)\n",
        "# Add small lexical anchors\n",
        "for c in ['soft_tfidf','bm25_okapi_ab','bm25_okapi_ba','bm25_okapi_ab_sq','bm25_okapi_ba_sq']:\n",
        "    if c in trX_f.columns:\n",
        "        keep_cols.append(c)\n",
        "keep_cols = list(dict.fromkeys(keep_cols))  # de-dup\n",
        "print('CE-meta features:', len(keep_cols))\n",
        "X_all = trX_f[keep_cols].values.astype(np.float32)\n",
        "X_te_all = teX_f[keep_cols].values.astype(np.float32)\n",
        "\n",
        "# Per-fold ElasticNet with standardization; grid search\n",
        "alphas = [1e-3, 3e-3, 1e-2, 3e-2, 1e-1]\n",
        "l1_ratios = [0.1, 0.3, 0.5]\n",
        "ce_meta_oof = np.zeros(len(train), dtype=np.float32)\n",
        "ce_meta_te_acc = { (a,l): np.zeros(len(test), dtype=np.float32) for a in alphas for l in l1_ratios }\n",
        "oof_by_cfg = { (a,l): np.zeros(len(train), dtype=np.float32) for a in alphas for l in l1_ratios }\n",
        "\n",
        "for f in range(NUM_FOLDS):\n",
        "    tr_idx = np.where(fold_arr != f)[0]\n",
        "    va_idx = np.where(fold_arr == f)[0]\n",
        "    X_tr = X_all[tr_idx]; X_va = X_all[va_idx]\n",
        "    y_tr = y[tr_idx]\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    X_tr_s = scaler.fit_transform(X_tr)\n",
        "    X_va_s = scaler.transform(X_va)\n",
        "    X_te_s = scaler.transform(X_te_all)\n",
        "    for a in alphas:\n",
        "        for l in l1_ratios:\n",
        "            mdl = ElasticNet(alpha=a, l1_ratio=l, random_state=42, max_iter=2000)\n",
        "            mdl.fit(X_tr_s, y_tr)\n",
        "            preds_va = mdl.predict(X_va_s).astype(np.float32)\n",
        "            oof_by_cfg[(a,l)][va_idx] = preds_va\n",
        "            ce_meta_te_acc[(a,l)] += mdl.predict(X_te_s).astype(np.float32)\n",
        "\n",
        "# Pick best cfg by OOF Pearson\n",
        "best_cfg = None; best_r = -1.0\n",
        "for a in alphas:\n",
        "    for l in l1_ratios:\n",
        "        r = pearsonr(oof_by_cfg[(a,l)], y)[0]\n",
        "        print(f'ElasticNet a={a} l1={l}: OOF r={r:.6f}', flush=True)\n",
        "        if r > best_r:\n",
        "            best_r = r; best_cfg = (a,l)\n",
        "a,l = best_cfg\n",
        "print('Chosen CE-ElasticNet:', best_cfg, 'OOF r=', round(float(best_r),6), flush=True)\n",
        "ce_meta_oof = oof_by_cfg[best_cfg].astype(np.float32)\n",
        "ce_meta_te = (ce_meta_te_acc[best_cfg] / NUM_FOLDS).astype(np.float32)\n",
        "pd.DataFrame({'id': train['id'], 'oof_ce_meta': ce_meta_oof}).to_csv('oof_ce_meta.csv', index=False)\n",
        "pd.DataFrame({'id': test['id'], 'score': np.clip(ce_meta_te, 0.0, 1.0)}).to_csv('submission_ce_meta.csv', index=False)\n",
        "print('Saved CE meta artifacts.', flush=True)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE tags found: ['ce_large', 'ce_bge_rerank', 'ce_l12', 'ce_stsb']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ce_large: raw r=0.551378, iso r=0.550467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ce_bge_rerank: raw r=0.445772, iso r=0.443981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ce_l12: raw r=0.445772, iso r=0.443981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ce_stsb: raw r=0.426387, iso r=0.445756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good CE tags kept: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE-meta features: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.521e+00, tolerance: 1.779e-01\n  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.192e-01, tolerance: 1.779e-01\n  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.586e+00, tolerance: 1.763e-01\n  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.658e-01, tolerance: 1.763e-01\n  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.405e+00, tolerance: 1.736e-01\n  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.054e-01, tolerance: 1.736e-01\n  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.677e+00, tolerance: 1.752e-01\n  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/app/.pip-target/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.444e-01, tolerance: 1.752e-01\n  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet a=0.001 l1=0.1: OOF r=0.619543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet a=0.001 l1=0.3: OOF r=0.619184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet a=0.001 l1=0.5: OOF r=0.618623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet a=0.003 l1=0.1: OOF r=0.618726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet a=0.003 l1=0.3: OOF r=0.617381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet a=0.003 l1=0.5: OOF r=0.616541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet a=0.01 l1=0.1: OOF r=0.616727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet a=0.01 l1=0.3: OOF r=0.612233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet a=0.01 l1=0.5: OOF r=0.611205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet a=0.03 l1=0.1: OOF r=0.611173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet a=0.03 l1=0.3: OOF r=0.610777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet a=0.03 l1=0.5: OOF r=0.610408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet a=0.1 l1=0.1: OOF r=0.609684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet a=0.1 l1=0.3: OOF r=0.606673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet a=0.1 l1=0.5: OOF r=0.605102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen CE-ElasticNet: (0.001, 0.1) OOF r= 0.619543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved CE meta artifacts.\n"
          ]
        }
      ]
    },
    {
      "id": "fc2267d8-10ce-4f4a-afaa-2f12594029db",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fixed NNLS (no standardization), try with/without bias; include CE-ElasticNet if available; no calibration\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.optimize import nnls\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "assert 'y' in globals(), 'Run cell 0 first to define y'\n",
        "\n",
        "# Collect bases\n",
        "bases = []\n",
        "names = []\n",
        "if 'oof_pred' in globals() and 'test_pred' in globals():\n",
        "    names.append('lgbm'); bases.append((oof_pred.astype(np.float64), test_pred.astype(np.float64)))\n",
        "if 'cat_oof' in globals() and 'cat_te' in globals():\n",
        "    names.append('cat'); bases.append((cat_oof.astype(np.float64), cat_te.astype(np.float64)))\n",
        "if 'xgb_oof' in globals() and 'xgb_te' in globals():\n",
        "    names.append('xgb'); bases.append((xgb_oof.astype(np.float64), xgb_te.astype(np.float64)))\n",
        "if 'ridge_oof' in globals() and 'ridge_te' in globals():\n",
        "    names.append('ridge'); bases.append((ridge_oof.astype(np.float64), ridge_te.astype(np.float64)))\n",
        "if 'ce_meta_oof' in globals() and 'ce_meta_te' in globals():\n",
        "    names.append('ce_meta'); bases.append((ce_meta_oof.astype(np.float64), ce_meta_te.astype(np.float64)))\n",
        "\n",
        "P_tr = np.column_stack([b[0] for b in bases]) if bases else None\n",
        "P_te = np.column_stack([b[1] for b in bases]) if bases else None\n",
        "y_vec = y.astype(np.float64)\n",
        "print('Fixed NNLS over bases:', names, flush=True)\n",
        "\n",
        "def fit_nnls(P_tr, P_te, add_bias: bool):\n",
        "    if add_bias:\n",
        "        ones_tr = np.ones((P_tr.shape[0], 1), dtype=np.float64)\n",
        "        ones_te = np.ones((P_te.shape[0], 1), dtype=np.float64)\n",
        "        A_tr = np.hstack([P_tr, ones_tr])\n",
        "        A_te = np.hstack([P_te, ones_te])\n",
        "        w, _ = nnls(A_tr, y_vec)\n",
        "        pred_tr = A_tr @ w\n",
        "        pred_te = A_te @ w\n",
        "        return w, pred_tr.astype(np.float32), pred_te.astype(np.float32), True\n",
        "    else:\n",
        "        w, _ = nnls(P_tr, y_vec)\n",
        "        pred_tr = P_tr @ w\n",
        "        pred_te = P_te @ w\n",
        "        return w, pred_tr.astype(np.float32), pred_te.astype(np.float32), False\n",
        "\n",
        "best = (-1.0, None, None, None, None)\n",
        "for add_bias in (False, True):\n",
        "    w, trp, tep, ab = fit_nnls(P_tr, P_te, add_bias)\n",
        "    r = pearsonr(trp, y)[0]\n",
        "    print(f'NNLS (bias={ab}) OOF r={r:.6f}; weights_dim={len(w)}', flush=True)\n",
        "    if r > best[0]:\n",
        "        best = (r, w, trp, tep, ab)\n",
        "\n",
        "best_r, best_w, best_trp, best_tep, best_bias = best\n",
        "print('Chosen NNLS variant: bias=', best_bias, 'OOF r=', round(float(best_r), 6), flush=True)\n",
        "print('Weights:', best_w.round(6), flush=True)\n",
        "\n",
        "pd.DataFrame({'id': train['id'], 'oof': best_trp}).to_csv('oof_stack_nnls_fixed.csv', index=False)\n",
        "pd.DataFrame({'id': test['id'], 'score': np.clip(best_tep, 0.0, 1.0)}).to_csv('submission_stack_nnls_fixed.csv', index=False)\n",
        "print('Saved submission_stack_nnls_fixed.csv', flush=True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed NNLS over bases: ['lgbm', 'cat', 'xgb', 'ridge', 'ce_meta']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNLS (bias=False) OOF r=0.745432; weights_dim=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNLS (bias=True) OOF r=0.745432; weights_dim=6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen NNLS variant: bias= True OOF r= 0.745432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: [0.       0.624995 0.308901 0.066712 0.       0.00142 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_stack_nnls_fixed.csv\n"
          ]
        }
      ]
    },
    {
      "id": "0e414466-a1c0-4bb9-9a00-e4237419d728",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Export best (raw vs calibrated) NNLS blend to submission.csv based on OOF Pearson\n",
        "import pandas as pd, numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "train = pd.read_csv('train.csv')[['id','score']]\n",
        "\n",
        "def oof_r(path: str) -> float | None:\n",
        "    try:\n",
        "        o = pd.read_csv(path)\n",
        "        df = train.merge(o, on='id', how='inner')\n",
        "        r = pearsonr(df['oof'].astype(float).values, df['score'].astype(float).values)[0]\n",
        "        return float(r)\n",
        "    except Exception as e:\n",
        "        print('Failed to eval', path, e)\n",
        "        return None\n",
        "\n",
        "raw_oof_path = 'oof_stack_nnls_raw.csv'\n",
        "cal_oof_path = 'oof_stack_nnls_calibrated.csv'\n",
        "raw_sub_path = 'submission_stack_nnls_raw.csv'\n",
        "cal_sub_path = 'submission_stack_nnls_calibrated.csv'\n",
        "\n",
        "r_raw = oof_r(raw_oof_path)\n",
        "r_cal = oof_r(cal_oof_path) if (pd.Series([cal_oof_path]).map(lambda p: pd.io.common.file_exists(p)).iloc[0]) else None\n",
        "print('OOF raw r=', r_raw, 'cal r=', r_cal)\n",
        "\n",
        "best_path = raw_sub_path\n",
        "if r_cal is not None and r_cal > (r_raw if r_raw is not None else -1):\n",
        "    best_path = cal_sub_path\n",
        "    print('Choosing calibrated submission')\n",
        "else:\n",
        "    print('Choosing raw submission')\n",
        "\n",
        "sub = pd.read_csv(best_path)\n",
        "sub.rename(columns={'score':'score'}, inplace=True)\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "print('Wrote submission.csv from', best_path, 'shape=', sub.shape, 'min=', float(sub.score.min()), 'max=', float(sub.score.max()), 'mean=', float(sub.score.mean()))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOF raw r= 0.7770992116688046 cal r= 0.7677005185973185\nChoosing raw submission\nWrote submission.csv from submission_stack_nnls_raw.csv shape= (3648, 2) min= 0.006142874 max= 0.99729794 mean= 0.38283369440383763\n"
          ]
        }
      ]
    },
    {
      "id": "870f9d35-a7cc-49e2-879e-43a1e3068f52",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Environment fix: install exact cu121 torch stack and verify GPU access\n",
        "import os, sys, subprocess, shutil, time\n",
        "from pathlib import Path\n",
        "\n",
        "def pip(*args):\n",
        "    print(\">\", *args, flush=True)\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", *args], check=True)\n",
        "\n",
        "print(\"[GPU Setup] Uninstalling any existing torch stack...\", flush=True)\n",
        "for pkg in (\"torch\",\"torchvision\",\"torchaudio\",\"nvidia-nccl-cu12\",\"nvidia-nvjitlink-cu12\"):\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", pkg], check=False)\n",
        "\n",
        "# Clean stray site dirs that can shadow correct wheels (idempotent)\n",
        "for d in (\n",
        "    \"/app/.pip-target/torch\",\n",
        "    \"/app/.pip-target/torch-2.8.0.dist-info\",\n",
        "    \"/app/.pip-target/torch-2.4.1.dist-info\",\n",
        "    \"/app/.pip-target/torchvision\",\n",
        "    \"/app/.pip-target/torchvision-0.23.0.dist-info\",\n",
        "    \"/app/.pip-target/torchvision-0.19.1.dist-info\",\n",
        "    \"/app/.pip-target/torchaudio\",\n",
        "    \"/app/.pip-target/torchaudio-2.8.0.dist-info\",\n",
        "    \"/app/.pip-target/torchaudio-2.4.1.dist-info\",\n",
        "    \"/app/.pip-target/torchgen\",\n",
        "    \"/app/.pip-target/functorch\",\n",
        "):\n",
        "    if os.path.exists(d):\n",
        "        print(\"Removing\", d, flush=True)\n",
        "        shutil.rmtree(d, ignore_errors=True)\n",
        "\n",
        "print(\"[GPU Setup] Installing exact cu121 torch stack...\", flush=True)\n",
        "pip(\"install\",\n",
        "    \"--index-url\", \"https://download.pytorch.org/whl/cu121\",\n",
        "    \"--extra-index-url\", \"https://pypi.org/simple\",\n",
        "    \"torch==2.4.1\", \"torchvision==0.19.1\", \"torchaudio==2.4.1\")\n",
        "\n",
        "Path(\"constraints.txt\").write_text(\"torch==2.4.1\\ntorchvision==0.19.1\\ntorchaudio==2.4.1\\n\")\n",
        "\n",
        "print(\"[GPU Setup] Reinstalling key deps under constraints without touching torch...\", flush=True)\n",
        "pip(\"install\", \"-c\", \"constraints.txt\",\n",
        "    \"transformers==4.44.2\", \"accelerate==0.34.2\",\n",
        "    \"sentencepiece\", \"scikit-learn\",\n",
        "    \"--upgrade-strategy\", \"only-if-needed\")\n",
        "\n",
        "import torch\n",
        "print(\"torch:\", torch.__version__, \"built CUDA:\", getattr(torch.version, \"cuda\", None), flush=True)\n",
        "print(\"CUDA available:\", torch.cuda.is_available(), flush=True)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0), flush=True)\n",
        "else:\n",
        "    print(\"[WARN] CUDA still not available. Check container GPU visibility and any site-package shadowing.\", flush=True)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GPU Setup] Uninstalling any existing torch stack...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.8.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling torch-2.8.0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Successfully uninstalled torch-2.8.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchvision 0.19.1+cu121\nUninstalling torchvision-0.19.1+cu121:\n  Successfully uninstalled torchvision-0.19.1+cu121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchaudio 2.4.1+cu121\nUninstalling torchaudio-2.4.1+cu121:\n  Successfully uninstalled torchaudio-2.4.1+cu121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: nvidia-nccl-cu12 2.28.3\nUninstalling nvidia-nccl-cu12-2.28.3:\n  Successfully uninstalled nvidia-nccl-cu12-2.28.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: nvidia-nvjitlink-cu12 12.9.86\nUninstalling nvidia-nvjitlink-cu12-12.9.86:\n  Successfully uninstalled nvidia-nvjitlink-cu12-12.9.86\nRemoving /app/.pip-target/torch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing /app/.pip-target/torch-2.4.1.dist-info\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing /app/.pip-target/torchgen\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GPU Setup] Installing exact cu121 torch stack...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> install --index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://pypi.org/simple torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121, https://pypi.org/simple\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (799.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 MB 439.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.1/7.1 MB 508.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.4/3.4 MB 406.7 MB/s eta 0:00:00\nCollecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 34.7 MB/s eta 0:00:00\nCollecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 261.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 315.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 518.9 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 347.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 370.4 MB/s eta 0:00:00\nCollecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 473.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 402.3 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 251.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 312.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 420.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 306.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 116.4 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 262.6 MB/s eta 0:00:00\nCollecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 565.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 228.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 499.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pillow!=8.3.*,>=5.3.0\n  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.6/6.6 MB 322.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 443.2 MB/s eta 0:00:00\nCollecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 247.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MarkupSafe>=2.0\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 527.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 2.21.0 requires fsspec[http]<=2024.6.1,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\nWARNING: Target directory /app/.pip-target/torch-2.4.1+cu121.dist-info already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0 sympy-1.14.0 torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121 triton-3.0.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.4.5.107.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.1.0.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec-2025.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/MarkupSafe-3.0.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.1.3.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.0.2.54.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.2.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.20.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow-11.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/pillow.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PIL already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GPU Setup] Reinstalling key deps under constraints without touching torch...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> install -c constraints.txt transformers==4.44.2 accelerate==0.34.2 sentencepiece scikit-learn --upgrade-strategy only-if-needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.44.2\n  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.5/9.5 MB 127.2 MB/s eta 0:00:00\nCollecting accelerate==0.34.2\n  Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 324.4/324.4 KB 115.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 275.6 MB/s eta 0:00:00\nCollecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.7/9.7 MB 145.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting requests\n  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.7/64.7 KB 404.5 MB/s eta 0:00:00\nCollecting filelock\n  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\nCollecting huggingface-hub<1.0,>=0.23.2\n  Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 563.3/563.3 KB 524.0 MB/s eta 0:00:00\nCollecting pyyaml>=5.1\n  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 763.0/763.0 KB 548.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy>=1.17\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 191.3 MB/s eta 0:00:00\nCollecting packaging>=20.0\n  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.5/66.5 KB 420.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting regex!=2019.12.17\n  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 799.0/799.0 KB 526.1 MB/s eta 0:00:00\nCollecting safetensors>=0.4.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 485.8/485.8 KB 422.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers<0.20,>=0.19\n  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.6/3.6 MB 81.8 MB/s eta 0:00:00\nCollecting tqdm>=4.27\n  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 KB 462.2 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch>=1.10.0\n  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.1/797.1 MB 160.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psutil\n  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 291.2/291.2 KB 479.2 MB/s eta 0:00:00\nCollecting threadpoolctl>=3.1.0\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy>=1.8.0\n  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 35.9/35.9 MB 479.9 MB/s eta 0:00:00\nCollecting joblib>=1.2.0\n  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 308.4/308.4 KB 60.5 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hf-xet<2.0.0,>=1.1.3\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 492.7 MB/s eta 0:00:00\nCollecting typing-extensions>=3.7.4.3\n  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.6/44.6 KB 370.6 MB/s eta 0:00:00\nCollecting fsspec>=2023.5.0\n  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 199.3/199.3 KB 480.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting networkx\n  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.0/2.0 MB 421.5 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.6/121.6 MB 198.5 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu12==2.20.5\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 176.2/176.2 MB 109.6 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.1/99.1 KB 471.3 MB/s eta 0:00:00\nCollecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 823.6/823.6 KB 420.8 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==9.1.0.70\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 195.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==3.0.0\n  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 110.9 MB/s eta 0:00:00\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 23.7/23.7 MB 159.7 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.0/196.0 MB 159.2 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.5/56.5 MB 138.3 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.2/124.2 MB 286.1 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 410.6/410.6 MB 266.1 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jinja2\n  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.9/134.9 KB 500.2 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.1/14.1 MB 196.1 MB/s eta 0:00:00\nCollecting sympy\n  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.3/6.3 MB 353.8 MB/s eta 0:00:00\nCollecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.7/39.7 MB 246.9 MB/s eta 0:00:00\nCollecting certifi>=2017.4.17\n  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 161.2/161.2 KB 258.3 MB/s eta 0:00:00\nCollecting idna<4,>=2.5\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 KB 452.6 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting charset_normalizer<4,>=2\n  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 150.3/150.3 KB 511.2 MB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 129.8/129.8 KB 459.0 MB/s eta 0:00:00\nCollecting MarkupSafe>=2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nCollecting mpmath<1.4,>=1.1.0\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 536.2/536.2 KB 177.8 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: mpmath, urllib3, typing-extensions, tqdm, threadpoolctl, sympy, sentencepiece, safetensors, regex, pyyaml, psutil, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, joblib, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, triton, scipy, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, scikit-learn, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, accelerate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 2.21.0 requires fsspec[http]<=2024.6.1,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\nWARNING: Target directory /app/.pip-target/accelerate-0.34.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/accelerate already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/transformers-4.44.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/transformers already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tokenizers already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tokenizers-0.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/functorch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torch already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/torchgen already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/huggingface_hub-0.35.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/huggingface_hub already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusolver_cu12-11.4.5.107.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_learn-1.7.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sklearn already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scikit_learn.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2-3.1.6.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/jinja2 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cudnn_cu12-9.1.0.70.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cusparse_cu12-12.1.0.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/requests-2.32.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/requests already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy-1.16.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/scipy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton-3.0.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/triton already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/certifi-2025.8.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/certifi already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/charset_normalizer-3.4.3.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/charset_normalizer already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock-3.19.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/filelock already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec-2025.9.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/fsspec already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/hf_xet-1.1.10.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/hf_xet already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/idna-3.10.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/idna already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib-1.5.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/joblib already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/markupsafe already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/MarkupSafe-3.0.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx-3.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/networkx already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy.libs already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/numpy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cublas_cu12-12.1.3.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_cupti_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_nvrtc_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cuda_runtime_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_cufft_cu12-11.0.2.54.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_curand_cu12-10.3.2.106.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nccl_cu12-2.20.5.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvjitlink_cu12-12.9.86.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia_nvtx_cu12-12.1.105.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/nvidia already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging-25.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/packaging already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/psutil-7.1.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/psutil already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/_yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/yaml already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/PyYAML-6.0.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/regex-2025.9.18.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/regex already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/safetensors already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/safetensors-0.6.2.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sentencepiece-0.2.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sentencepiece already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/sympy already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/isympy.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl-3.6.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/threadpoolctl.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tqdm-4.67.1.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/tqdm already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/__pycache__ already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions-4.15.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/typing_extensions.py already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/urllib3-2.5.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/urllib3 already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/mpmath already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/share already exists. Specify --upgrade to force replacement.\nWARNING: Target directory /app/.pip-target/bin already exists. Specify --upgrade to force replacement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed MarkupSafe-3.0.2 accelerate-0.34.2 certifi-2025.8.3 charset_normalizer-3.4.3 filelock-3.19.1 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.35.1 idna-3.10 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 networkx-3.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 packaging-25.0 psutil-7.1.0 pyyaml-6.0.2 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sentencepiece-0.2.1 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.19.1 torch-2.4.1 tqdm-4.67.1 transformers-4.44.2 triton-3.0.0 typing-extensions-4.15.0 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "/app/.pip-target/torch/lib/libtorch_cuda.so: undefined symbol: ncclCommRegister",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[121]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[GPU Setup] Reinstalling key deps under constraints without touching torch...\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     40\u001b[39m pip(\u001b[33m\"\u001b[39m\u001b[33minstall\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m-c\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mconstraints.txt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtransformers==4.44.2\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33maccelerate==0.34.2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msentencepiece\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mscikit-learn\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m--upgrade-strategy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33monly-if-needed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtorch:\u001b[39m\u001b[33m\"\u001b[39m, torch.__version__, \u001b[33m\"\u001b[39m\u001b[33mbuilt CUDA:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(torch.version, \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCUDA available:\u001b[39m\u001b[33m\"\u001b[39m, torch.cuda.is_available(), flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pip-target/torch/__init__.py:290\u001b[39m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[32m    289\u001b[39m         _load_global_deps()\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "\u001b[31mImportError\u001b[39m: /app/.pip-target/torch/lib/libtorch_cuda.so: undefined symbol: ncclCommRegister"
          ]
        }
      ]
    },
    {
      "id": "60dc4817-3216-41e5-bec7-d17eef445ec3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick 2-base calibrated NNLS (lgbm + cat) trial from saved artifacts; overwrite raw artifacts if better\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from scipy.optimize import nnls\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Load targets and per-model OOF/test from disk to avoid kernel state dependency\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df  = pd.read_csv('test.csv')\n",
        "y = train_df['score'].astype(np.float32).values\n",
        "\n",
        "lgbm_oof = pd.read_csv('oof_stack_lgbm.csv').merge(train_df[['id']], on='id', how='right')['oof'].astype(np.float32).values\n",
        "lgbm_te  = pd.read_csv('submission_stack_lgbm.csv').merge(test_df[['id']], on='id', how='right')['score'].astype(np.float32).values\n",
        "cat_oof  = pd.read_csv('oof_stack_cat.csv').merge(train_df[['id']], on='id', how='right')['oof_cat'].astype(np.float32).values\n",
        "cat_te   = pd.read_csv('submission_stack_cat.csv').merge(test_df[['id']], on='id', how='right')['score'].astype(np.float32).values\n",
        "\n",
        "# Fold array by id (fold-safe)\n",
        "folds_df = pd.read_csv('folds_by_id.csv')\n",
        "fold_arr = train_df[['id']].merge(folds_df, on='id', how='left', validate='one_to_one')['fold'].astype(int).values\n",
        "\n",
        "def fold_iso(oof, te, y, folds):\n",
        "    F = int(folds.max()) + 1\n",
        "    o2 = np.zeros_like(oof, np.float32); te_list = []\n",
        "    for f in range(F):\n",
        "        tr = folds != f; va = folds == f\n",
        "        iso = IsotonicRegression(increasing=True, out_of_bounds='clip')\n",
        "        iso.fit(oof[tr], y[tr])\n",
        "        o2[va] = iso.transform(oof[va]).astype(np.float32)\n",
        "        te_list.append(iso.transform(te).astype(np.float32))\n",
        "    te_avg = (np.mean(np.stack(te_list,0),0)).astype(np.float32)\n",
        "    return o2, te_avg\n",
        "\n",
        "# Calibrate each base\n",
        "l_tr, l_te = fold_iso(lgbm_oof, lgbm_te, y.astype(np.float32), fold_arr)\n",
        "c_tr, c_te = fold_iso(cat_oof,  cat_te,  y.astype(np.float32), fold_arr)\n",
        "\n",
        "P_tr = np.column_stack([l_tr, c_tr]).astype(np.float64)\n",
        "P_te = np.column_stack([l_te, c_te]).astype(np.float64)\n",
        "w, _ = nnls(P_tr, y.astype(np.float64))\n",
        "w = w / (w.sum() if w.sum() > 0 else 1.0)\n",
        "two_oof = (P_tr @ w).astype(np.float32)\n",
        "two_te  = (P_te @ w).astype(np.float32)\n",
        "r_two = pearsonr(two_oof, y)[0]\n",
        "print('2-base (lgbm+cat) NNLS OOF r=', round(float(r_two),6), 'weights=', w.round(6), flush=True)\n",
        "\n",
        "# Compare to current best (from oof_stack_nnls_raw.csv), overwrite if better\n",
        "try:\n",
        "    cur = pd.read_csv('oof_stack_nnls_raw.csv')\n",
        "    r_cur = pearsonr(train_df[['id']].merge(cur, on='id', how='left')['oof'].astype(float).values, y)[0]\n",
        "except Exception:\n",
        "    r_cur = -1.0\n",
        "print('Current raw NNLS OOF r=', round(float(r_cur),6))\n",
        "if r_two > r_cur:\n",
        "    pd.DataFrame({'id': train_df['id'], 'oof': two_oof}).to_csv('oof_stack_nnls_raw.csv', index=False)\n",
        "    pd.DataFrame({'id': test_df['id'], 'score': np.clip(two_te, 0.0, 1.0)}).to_csv('submission_stack_nnls_raw.csv', index=False)\n",
        "    print('Overwrote raw NNLS artifacts with 2-base blend.', flush=True)\n",
        "else:\n",
        "    print('Kept existing raw NNLS artifacts.', flush=True)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2-base (lgbm+cat) NNLS OOF r= 0.776865 weights= [0.390442 0.609558]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current raw NNLS OOF r= 0.776908\nKept existing raw NNLS artifacts.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}